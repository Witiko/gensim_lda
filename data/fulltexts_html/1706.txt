 

We gratefully acknowledge support from
the Simons Foundation
and member institutions 
arXiv.org > cs > arXiv:1706.04304
 
 
 
(Help | Advanced search)

Full-text links: 
Download:
    • PDF 
    • Other formats 
(license)
Current browse context:
cs.LG
< prev | next > 
new | recent | 1706
Change to browse by:
cs 
References & Citations
    • NASA ADS 
DBLP - CS Bibliography
listing | bibtex 
Bangrui Chen
Peter I. Frazier
Google Scholar
Bookmark
(what is this?) 
       
Computer Science > Machine Learning
Title: Dueling Bandits With Weak Regret
Authors: Bangrui Chen, Peter I. Frazier
(Submitted on 14 Jun 2017)
Abstract: We consider online content recommendation with implicit feedback through pairwise comparisons, formalized as the so-called dueling bandit problem. We study the dueling bandit problem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms pulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm pulled is the Condorcet winner. We propose a new algorithm for this problem, Winner Stays (WS), with variations for each kind of regret: WS for weak regret (WS-W) has expected cumulative weak regret that is $O(N^2)$, and $O(N\log(N))$ if arms have a total order; WS for strong regret (WS-S) has expected cumulative strong regret of $O(N^2 + N \log(T))$, and $O(N\log(N)+N\log(T))$ if arms have a total order. WS-W is the first dueling bandit algorithm with weak regret that is constant in time. WS is simple to compute, even for problems with many arms, and we demonstrate through numerical experiments on simulated and real data that WS has significantly smaller regret than existing algorithms in both the weak- and strong-regret settings. 
Subjects: 
Machine Learning (cs.LG)
Cite as: 
arXiv:1706.04304 [cs.LG]
 
(or arXiv:1706.04304v1 [cs.LG] for this version)
Bibliographic data
[Enable Bibex(What is Bibex?)]
Submission history
From: Bangrui Chen [view email] 
[v1] Wed, 14 Jun 2017 03:44:32 GMT (1612kb,D)
Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) 
Link back to: arXiv, form interface, contact.

