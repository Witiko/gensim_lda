Skip navigation 
    • Home 
    • Help 
        ◦ How to publish in SciDok: 
        ◦ Information on publishing 
        ◦ Embargo 
        ◦ How to cite 
        ◦ FAQ 
        ◦ Information on OpenAccess 
        ◦ Information on Creativ Commens 
            1. Information on Creativ Commens
(creativecommons.org) 
            2. Information on Creativ Commens
(german) 
        ◦ Contact 
    • language 
        ◦ Deutsch 
        ◦ English 
    • Sign on to: 
        ◦ My publications / Publish 
        ◦ Receive email
updates 
        ◦ Edit Profile 
 
 
 
    1. SciDok - Der Wissenschaftsserver der UdS 
    2. MI - Fakultät für Mathematik und Informatik 
Ihr Browser führt kein JavaScript aus, darum stehen Ihnen nicht alle Funktionen zur Verfügung. 
Zur korrekten Nutzung wird JavaScript benötigt. Bitte aktivieren sie JavaScript in den Einstellungen Ihres Browsers.
Please activate JavaScript in your browser settings.
Please use this identifier to cite or link to this item: doi:10.22028/D291-26942 
Files in This Item:
File
Description
Size
Format
 
Kai_Hui_PhD_thesis.pdf

1,91 MB
Adobe PDF
View/Open
Title: 
Automatic methods for low-cost evaluation and position-aware models for neural information retrieval
Authors: 
Hui, Kai
Language: 
English
Issue Date: 
2017
SWD key words: 
Information Retrieval
Free key words: 
IR-Modell
IR-Evaluation
DDC groups: 
004 Computer science, internet
Publikation type: 
Doctoral Thesis
Abstract: 
An information retrieval (IR) system assists people in consuming huge amount of data, where the evaluation and the construction of such systems are important. However, there exist two difficulties: the overwhelmingly large number of query-document pairs to judge, making IR evaluation a manually laborious task; and the complicated patterns to model due to the non-symmetric, heterogeneous relationships between a query-document pair, where different interaction patterns such as term dependency and proximity have been demonstrated to be useful, yet are non-trivial for a single IR model to encode. In this thesis we attempt to address both difficulties from the perspectives of IR evaluation and of the retrieval model respectively, by reducing the manual cost with automatic methods, by investigating the usage of crowdsourcing in collecting preference judgments, and by proposing novel neural retrieval models. In particular, to address the large number of query-document pairs in IR evaluation, a low-cost selective labeling method is proposed to pick out a small subset of representative documents for manual judgments in favor of the follow-up prediction for the remaining query-document pairs; furthermore, a language-model based cascade measure framework is developed to evaluate the novelty and diversity, utilizing the content of the labeled documents to mitigate incomplete labels. In addition, we also attempt to make the preference judgments practically usable by empirically investigating different properties of the judgments when collected via crowdsourcing; and by proposing a novel judgment mechanism, making a compromise between the judgment quality and the number of judgments. Finally, to model different complicated patterns in a single retrieval model, inspired by the recent advances in deep learning, we develop novel neural IR models to incorporate different patterns like term dependency, query proximity, density of relevance, and query coverage in a single model. We demonstrate their superior performances through evaluations on different datasets.

Ein Information-Retrieval (IR) System hilft Menschen bei der Arbeit mit großen Datenmengen, daher ist die Entwicklung und Evaluation solcher Systeme wichtig. Allerdings gibt es zwei Herausforderungen: die große Anzahl von Anfrage-Dokument-Paaren, die manuelle IREvaluation schwierig macht; sowie die komplizierten zu modellierenden Muster, aufgrund der nicht-symmetrischen, heterogenen Beziehung zwischen einem Anfragen und Dokumenten, wo erwiesen ist dass verschiedene Interaktionsmuster wie Termabhängigkeiten und Termnähe wichtig sind, aber nicht einfach durch ein einzelnes IR-Modell zu erfassen sind. In dieser Dissertation versuchen wir, beide Herausforderungen aus der Perspektive der IR-Evaluation bzw. der IR-Modellierung anzugehen, indem wir die manuellen Kosten mit automatischen Methoden reduzieren, indem wir die Verwendung von Crowdsourcing bei der Erfassung von Präferenzbewertungen untersuchen und indem wir neue neuronale IR-Modelle vorschlagen. Um die große Anzahl von Anfrage-Dokument-Paaren in der IR-Evaluation in Angriff zu nehmen, schlagen wir eine kostengünstige selektive Bewertungsmethode vor, die nur eine kleine Untermenge von repräsentativen Dokumenten für manuelle Beurteilungen auswählt, deren Ergebnisse dann extrapoliert werden; darüber hinaus wird ein unüberwachtes sprachmodellbasiertes Gütemaß für Neuheit und Diversität vorgeschlagen, wobei der Inhalt der bewerteten Dokumente genutzt wird, um unvollständige Bewertungen zu kompensieren. Außerdem versuchen wir Präferenzbewertungen praktisch nutzbar zu machen, indem wir empirisch verschiedene Eigenschaften der Bewertungen beim Sammeln über Crowdsourcing untersuchen, und indem wir einen neuartigen Bewertungsmechanismus entwickeln, der einen Kompromiss zwischen der Bewertungsqualität und der Anzahl der Bewertungen macht. Abschließend, um verschiedene komplizierte Muster in einem einzigen IR-Modell zu erfassen, inspiriert von den jüngsten Fortschritten bei Deep-Learning-Verfahren, entwickeln wir neuartige neuronale IR-Modelle, die verschiedene Muster wie Termabhängigkeit, Termnähe, Relevanzdichte sowie Anfrageabdeckung in einem einzelnen IR-Modell integrieren. Experimente auf verschiedenen Datensätzen zeigen die überlegene Performance des vorgeschlagenen IR-Modells.
URI: 
urn:nbn:de:bsz:291-scidok-ds-269423
hdl:20.500.11880/26894
http://dx.doi.org/10.22028/D291-26942
Advisor: 
Berberich, Klaus
Date of oral examination: 
4-Dec-2017
Date issued: 
6-Dec-2017
Faculty: 
MI - Fakultät für Mathematik und Informatik
Institute: 
MI - Informatik
Appears in Collections:
MI - Fakultät für Mathematik und Informatik


Items in SciDok are protected by copyright, with all rights reserved, unless otherwise indicated.
SULB  - Feedback  -  legal notice   
