
 
Skip to main content 
This service is more advanced with JavaScript available, learn more at http://activatejavascript.org 
Advertisement
Hide 

SpringerLink
Search SpringerLink 
  
Search 
    • Home 
    • Contact us 
    • Log in 
Menu
    • Home 
    • Contact us 
    • Log in 
Cooperative Design, Visualization, and Engineering  
International Conference on Cooperative Design, Visualization and Engineering 
CDVE 2018: Cooperative Design, Visualization, and Engineering pp 299-303 | Cite as
Improving Word Representation Quality Trained by word2vec via a More Efficient Hierarchical Clustering Method
    • Authors
    • Authors and affiliations
    • Zhaolin Yuan
    • Xiaojuan Ban
    • Jinlong Hu
    • Zhaolin Yuan
        ◦ 1
    • Xiaojuan Ban
        ◦ 1
      Email author
    • Jinlong Hu
        ◦ 1
    1. 1.School of Computer and Communication EngineeringUniversity of Science and Technology BeijingBeijingChina
Conference paper
First Online: 12 September 2018
    • 47 Downloads 
Part of the Lecture Notes in Computer Science book series (LNCS, volume 11151)
Abstract
In traditional word2vec methods, hierarchical softmax algorithm uses the whole vocabulary to construct a Huffman tree and it trains each pair of words just in logarithmic time consumption. But due to the lack of consideration about cooperation of each word in the corpus, it will reduce the performance of language model and the trained word vectors. In this paper, we substitute a purely data-driven method for the original Huffman-tree method to rebuild the binary tree. The new construction method utilizes the semantical and syntactical cooperation of words to cluster the words hierarchically. The cooperation of words is reflected in the word vectors which collected from the initial Huffman-tree training procedure. Our methods substantially improve the performances of word vectors in semantical and syntactical tasks.
Keywords
Language models Hierarchical neural network word2vec 
The National Key Research and Development Program of China (No. 2016YFB0700500), and the National Science Foundation of China (No. 61572075, No. 61702036), and Fundamental Research Funds for the Central Universities (No. FRF-TP-17-012A1), and China Postdoctoral Science Foundation (No. 2017M620619).

This is a preview of subscription content, log in to check access.
Cite paper 
    • How to cite? 
    • .RIS Papers Reference Manager RefWorks Zotero 
    • .ENW EndNote 
    • .BIB BibTeX JabRef Mendeley 
References
    1. 1.
       Bengio, Y., Ducharme, R., Vincent, P., Janvin, C.: A neural probabilistic language model. J. Mach. Learn. Res. 3, 1137–1155 (2003)zbMATHGoogle Scholar
    2. 2.
       Chen, D., Manning, C.: A fast and accurate dependency parser using neural networks. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (i), pp. 740–750 (2014)Google Scholar
    3. 3.
       Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P.: Natural language processing (almost) from scratch. J. Mach. Learn. Res. 12, 2493–2537 (2011)zbMATHGoogle Scholar
    4. 4.
       Sienčnik, S.K.: Adapting word2vec to named entity recognition (2015)Google Scholar
    5. 5.
       McMahon, J.G., Smith, F.J.: Improving statistical language model performance with automatically generated word hierarchies. Comput. Linguist. 22(2), 217–247 (1996)Google Scholar
    6. 6.
       Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word representations in vector space, pp. 1–12 (2013)Google Scholar
    7. 7.
       Morin, F., Bengio, Y.: Hierarchical probabilistic neural network language model. In: Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics (AISTATS 2005), pp. 246–252, March 2005Google Scholar
    8. 8.
       Pereira, F., Tishby, N., Lee, L.: Distributional clustering of English words (1994)Google Scholar
    9. 9.
       Řehůřek, R., Sojka, P.: Software framework for topic modelling with large corpora. In: Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pp. 45–50. ELRA, Valletta, May 2010Google Scholar
    10. 10.
       Wikipedia Contributors: Language model – Wikipedia, the free encyclopedia (2018). (Online: Accessed 9 May 2018)Google Scholar
Copyright information
© Springer Nature Switzerland AG 2018
About this paper
CrossMark 
Cite this paper as: 
Yuan Z., Ban X., Hu J. (2018) Improving Word Representation Quality Trained by word2vec via a More Efficient Hierarchical Clustering Method. In: Luo Y. (eds) Cooperative Design, Visualization, and Engineering. CDVE 2018. Lecture Notes in Computer Science, vol 11151. Springer, Cham 
    • First Online 12 September 2018 
    • DOI https://doi.org/10.1007/978-3-030-00560-3_43 
    • Publisher Name Springer, Cham 
    • Print ISBN 978-3-030-00559-7 
    • Online ISBN 978-3-030-00560-3 
    • eBook Packages Computer Science 
    • Buy this book on publisher's site 
    • Reprints and Permissions 
Personalised recommendations

Improving Word Representation Quality Trained by word2vec via a More Efficient Hierarchical Clustering Method 
Cite paper 
    • How to cite? 
    • .RIS Papers Reference Manager RefWorks Zotero 
    • .ENW EndNote 
    • .BIB BibTeX JabRef Mendeley 
Buy options 
Actions
Log in to check access 
Buy eBook 
EUR 55.92 
Buy paper (PDF) 
EUR 30.19 
    • Instant download 
    • Readable on all devices 
    • Own it forever 
    • Local sales tax included if applicable 
Learn about institutional subscriptions 
Cite paper 
    • How to cite? 
    • .RIS Papers Reference Manager RefWorks Zotero 
    • .ENW EndNote 
    • .BIB BibTeX JabRef Mendeley 
Advertisement
Hide 

Over 10 million scientific documents at your fingertips
Switch Edition 
    • Academic Edition 
    • Corporate Edition 
    • Home 
    • Impressum 
    • Legal information 
    • Privacy statement 
    • How we use cookies 
    • Accessibility 
    • Contact us 
Springer Nature 
© 2017 Springer Nature Switzerland AG. Part of Springer Nature.
Not logged in Not affiliated 62.245.124.181 

