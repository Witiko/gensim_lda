 
Skip to main content 
This service is more advanced with JavaScript available, learn more at http://activatejavascript.org 
Advertisement
Hide 

SpringerLink
Search SpringerLink 
  
Search 
    • Home 
    • Contact us 
    • Log in 
Statistical Language and Speech Processing  
International Conference on Statistical Language and Speech Processing 
SLSP 2015: Statistical Language and Speech Processing pp 267-274 | Cite as
On Continuous Space Word Representations as Input of LSTM Language Model
    • Authors
    • Authors and affiliations
    • Daniel Soutner
    • Luděk Müller
Conference paper
First Online: 17 November 2015
    • 3 Readers 
    • 534 Downloads 
Part of the Lecture Notes in Computer Science book series (LNCS, volume 9449)
Abstract
Artificial neural networks have become the state-of-the-art in the task of language modelling whereas Long-Short Term Memory (LSTM) networks seem to be an efficient architecture. The continuous skip-gram and the continuous bag of words (CBOW) are algorithms for learning quality distributed vector representations that are able to capture a large number of syntactic and semantic word relationships. In this paper, we carried out experiments with a combination of these powerful models: the continuous representations of words trained with skip-gram/CBOW/GloVe method, word cache expressed as a vector using latent Dirichlet allocation (LDA). These all are used on the input of LSTM network instead of 1-of-N coding traditionally used in language models. The proposed models are tested on Penn Treebank and MALACH corpus.
Keywords
Language modelling Neural networks LSTM Skip-gram CBOW GloVe word2vec LDA 

This is a preview of subscription content, log in to check access.

Notes
Acknowledgements
Access to computing and storage facilities owned by parties and projects contributing to the National Grid Infrastructure MetaCentrum, provided under the programme “Projects of Large Infrastructure for Research, Development, and Innovations” (LM2010005), is greatly appreciated.
This research was supported by the Ministry of Culture Czech Republic, project No. DF12P01OVV022.
References
    1. 1.
       Bengio, Y., Ducharme, R., Vincent, P., Janvin, C.: A neural probabilistic language model. J. Mach. Learn. Res. 3, 1137–1155 (2003)zbMATHGoogle Scholar
    2. 2.
       Blei, D.M., Ng, Y.A., Jordan, M.I., Lafferty, J.: Latent dirichlet allocation. J. Mach. Learn. Res. 3, 993–1022 (2003)zbMATHGoogle Scholar
    3. 3.
       Charniak, E., et al.: BLLIP 1987–89 WSJ Corpus Release 1. Linguistic Data Consortium, Philadelphia (2000)Google Scholar
    4. 4.
       Hochreiter, S., Schmidhuber, J.: Long Short-term memory. Neural Comput. 9(8), 1735–1780 (1997)CrossRefGoogle Scholar
    5. 5.
       Mikolov, T., Kombrink, S., Deoras, A., Burget, L., Černocký, J.: RNNLM - Recurrent Neural Network Language Modeling Toolkit (2011)Google Scholar
    6. 6.
       Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word representations in vector space. In: Proceedings of Workshop at ICLR (2013)Google Scholar
    7. 7.
       Mikolov, T., Sutskever, I., Chen, K., Corrado G., Dean, J.: Distributed representations of words and phrases and their compositionality. In: Proceedings of NIPS (2013)Google Scholar
    8. 8.
       Mikolov, T., Yih, W., Zweig, G.: Linguistic regularities in continuous space word representations. In: Proceedings of NAACL HLT (2013)Google Scholar
    9. 9.
       Mnih, A., Kavukcuoglu, K.: Learning word embeddings efficiently with noise-contrastive estimation. Adv. Neural Inf. Process. Syst. 26, 2265–2273 (2013)Google Scholar
    10. 10.
       Pennington, J., Socher, J., Manning., C.D.: GloVe: global vectors for word representation. In: Empricial Methods in Natural Language Processing (EMNLP) (2014)Google Scholar
    11. 11.
       Ramabhadran, B., et al.: USC-SFI MALACH Interviews and transcripts english LDC2012S05. Web Download. Philadelphia: Linguistic Data Consortium (2012)Google Scholar
    12. 12.
       Řehůřek, R., Sojka, P.: Software framework for topic modelling with large corpora. In: Proceedings of LREC 2010 Workshop New Challenges for NLP Frameworks. University of Malta, Valletta, Malta, pp. 4650–4655 (2010). ISBN 2-9517408-6-7Google Scholar
    13. 13.
       Soutner, D., Müller, L.: Application of LSTM neural networks in language modelling. In: Habernal, I. (ed.) TSD 2013. LNCS, vol. 8082, pp. 105–112. Springer, Heidelberg (2013)Google Scholar
    14. 14.
       Sundermeyer, M., Schlüter, R., Ney, H.: LSTM neural networks for language modeling. In: Interspeech (2012)Google Scholar
Copyright information
© Springer International Publishing Switzerland 2015
Authors and Affiliations
    • Daniel Soutner
        ◦ 1
      Email author
    • Luděk Müller
        ◦ 1
    1. 1.NTIS - New Technologies for the Information Society, Faculty of Applied ScienceUniversity of West BohemiaPilsenCzech Republic
About this paper
Cite this paper as: 
Soutner D., Müller L. (2015) On Continuous Space Word Representations as Input of LSTM Language Model. In: Dediu AH., Martín-Vide C., Vicsi K. (eds) Statistical Language and Speech Processing. SLSP 2015. Lecture Notes in Computer Science, vol 9449. Springer, Cham 
    • First Online 17 November 2015 
    • DOI https://doi.org/10.1007/978-3-319-25789-1_25 
    • Publisher Name Springer, Cham 
    • Print ISBN 978-3-319-25788-4 
    • Online ISBN 978-3-319-25789-1 
    • eBook Packages Computer Science 
    • Buy this book on publisher's site 
    • Reprints and Permissions 
Personalised recommendations

Cite paper 
    • How to cite? 
    • .RIS Papers Reference Manager RefWorks Zotero 
    • .ENW EndNote 
    • .BIB BibTeX JabRef Mendeley 
Buy options 
Actions
Log in to check access 
Buy eBook 
EUR 47.59 
Buy paper (PDF) 
EUR 30.19 
    • Instant download 
    • Readable on all devices 
    • Own it forever 
    • Local sales tax included if applicable 
Learn about institutional subscriptions 
Cite paper 
    • How to cite? 
    • .RIS Papers Reference Manager RefWorks Zotero 
    • .ENW EndNote 
    • .BIB BibTeX JabRef Mendeley 
Advertisement
Hide 

Over 10 million scientific documents at your fingertips
Switch Edition
    • Academic Edition 
    • Corporate Edition 
    • Home 
    • Impressum 
    • Legal information 
    • Privacy statement 
    • How we use cookies 
    • Accessibility 
    • Contact us 
Springer Nature 
© 2017 Springer Nature Switzerland AG. Part of Springer Nature.
Not logged in Not affiliated 62.245.124.181 









    • Your Privacy
    • Strictly Necessary Cookies
    • Performance Cookies
    • Functional Cookies
    • Targeting Cookies
    • More Information
Privacy Preference Centre
Active
Always Active



Save Settings
Allow All
We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners in accordance with our Privacy Statement. You can manage your preferences in Manage Cookies.
Close
OK
Manage Cookies

