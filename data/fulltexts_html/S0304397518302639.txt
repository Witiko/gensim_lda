 Skip to main content 

Journals & BooksRegisterSign in
    • 
    • Journals & Books
    • Register
    • Sign InHelp
 
Outline

Get AccessGet Access


Export
Advanced
Outline
    1. Abstract
    2. Keywords
    3. 1. Introduction
    4. 2. Related work
    5. 3. A quantum-inspired multimodal representation model for representation learning
    6. 4. A Quantum Interference-inspired Multimodal decision Fusion (QIMF) strategy
    7. 5. Our Quantum-inspired Multimodal Sentiment Analysis (QMSA) framework
    8. 6. Experiments
    9. 7. Conclusions
    10. Acknowledgements
    11. References
Show full outline
Figures (11)
    1. 
    2. 
    3. 
    4. 
    5. 
    6. 
Show all figures
Tables (6)
    1. Table 1
    2. Table 2
    3. Table 3
    4. Table 4
    5. Table 5
    6. Table 6
JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page.

Theoretical Computer Science
Available online 23 April 2018
In Press, Corrected ProofWhat are Corrected Proof articles?

A quantum-inspired multimodal sentiment analysis framework

Author links open overlay panelYazhouZhangaDaweiSongbcPengZhangaPanpanWangaJingfeiLiaXiangLiaBenyouWanga
Show more
https://doi.org/10.1016/j.tcs.2018.04.029Get rights and content

Abstract
Multimodal sentiment analysis aims to capture diversified sentiment information implied in data that are of different modalities (e.g., an image that is associated with a textual description or a set of textual labels). The key challenge is rooted on the “semantic gap” between different low-level content features and high-level semantic information. Existing approaches generally utilize a combination of multimodal features in a somehow heuristic way. However, how to employ and combine multiple information from different sources effectively is still an important yet largely unsolved problem. To address the problem, in this paper, we propose a Quantum-inspired Multimodal Sentiment Analysis (QMSA) framework. The framework consists of a Quantum-inspired Multimodal Representation (QMR) model (which aims to fill the “semantic gap” and model the correlations between different modalities via density matrix), and a Multimodal decision Fusion strategy inspired by Quantum Interference (QIMF) in the double-slit experiment (in which the sentiment label is analogous to a photon, and the data modalities are analogous to slits). Extensive experiments are conducted on two large scale datasets, which are collected from the Getty Images and Flickr photo sharing platform. The experimental results show that our approach significantly outperforms a wide range of baselines and state-of-the-art methods.
Keywords
Multimodal sentiment analysis
Quantum theory
Decision fusion
Information fusion
Recommended articlesCiting articles (0)
© 2018 Published by Elsevier B.V.
Recommended articles
            ▪ A survey of multimodal sentiment analysis
      Image and Vision Computing, Volume 65, 2017, pp. 3-14
      Purchase PDF
      View details
      
            ▪ Fusing audio, visual and textual clues for sentiment analysis from multimodal content
      Neurocomputing, Volume 174, Part A, 2016, pp. 50-59
      Purchase PDF
      View details
      
            ▪ Ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis
      Neurocomputing, Volume 261, 2017, pp. 217-230
      Purchase PDF
      View details
      
12Next
Citing articles (0)
Article Metrics
View article metrics
About ScienceDirectRemote accessShopping cartContact and supportTerms and conditionsPrivacy policy
We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the use of cookies.
Copyright © 2018 Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.
 

