 
Skip to main content Skip to sections 
This service is more advanced with JavaScript available, learn more at http://activatejavascript.org 
Advertisement
Hide 

SpringerLink
Search SpringerLink 
  
Search 
    • Home 
    • Contact us 
    • Log in 
Integrating Materials and Manufacturing Innovation  
Download PDF 
Integrating Materials and Manufacturing Innovation
June 2017, Volume 6, Issue 2, pp 187–196 | Cite as
Classification of Journal Articles in a Search for New Experimental Thermophysical Property Data: a Case Study
    • Authors
    • Authors and affiliations
    • Adele Peskin
    • Alden Dima
Open Access
Technical Article
First Online: 07 June 2017
    • 1.2k Downloads 
Abstract
We present a case study in which we use natural language processing and machine learning techniques to automatically select candidate scientific articles that may contain new experimental thermophysical property data from thousands of articles available in five different relevant journals. The National Institute of Standards and Technology (NIST) Thermodynamic Research Center (TRC) maintains a large database of available thermophysical property data extracted from articles that are manually selected for content. Over time, the number of articles requiring manual inspection has grown and assistance from machine-based methods is needed. Previous work used topic modeling along with classification techniques to classify these journal articles into those with data for the TRC database and those without. These techniques have produced classifications with accuracy between 85 and 90%. However, the TRC does not want to lose data from the misclassified articles that contain relevant information. In this study, we start with these topic modeling and classification techniques, and then enhance the model using information relevant to the TRC’s selection process. Our goal is to minimize the number of articles that require manual selection without missing articles of importance. Through a series of selection methods, we eliminate those articles for which we can determine a rejection criterion. We can reduce the number of articles that are not of interest by 70.8% while retaining 98.7% of the articles of interest. We have also found that topic model classification improves when the corpus of words is derived from specific sections of the articles rather than the entire articles, and we improve on our classification by using a combination of topic models from different sections of the article. Our best classification used only the Experimental and Literature Cited sections.
Keywords
Document classification Topic models Association rules Natural language processing 

Background
With increasing numbers of journal articles published every year, sorting through journals for data or other information is an increasingly difficult task. Yet to make new experimental data or theoretical findings available to a large community of users, sorting through individual articles for pertinent information is necessary, so researchers can build on prior knowledge to advance their fields. The Thermodynamic Research Center (TRC) at the National Institute of Standards and Technology (NIST) provides thermophysical and thermochemical property data for scientific research and industrial process design, using an XML-based standard, ThermoML, for data storage and exchange [1]. It has partnered with five major journals to capture and critically evaluate new thermodynamic data as it appears for publication [2, 3]. The automation of the data capture process has been an ongoing and successful project. With an increasing number of journal articles each year, a current goal remains to automate the selection of journal articles for data capture. This part of the process is still done manually, but with increasing demand must also be automated soon.
Prior work attempting to automate the selection of journal articles containing thermophysical property data has moved this effort forward. Using articles from the Journal of Chemical Engineering Data (JCED) that have gone through the manual selection process, automated article classification using several different classification algorithms have been compared with the manual classification. Classification accuracy reached a plateau of about 86% using a combination of a Latent Dirichlet Allocation-based document topic model and an AdaBoost M1/J48 decision tree classifier [4]. To achieve higher accuracy and to not eliminate the remaining articles of interest, additional information about the journal articles and the thermophysical property data is necessary to enhance the selection process.
We build on our past efforts using topic modeling to extract features with 85 to 90% classification accuracy. The next step is to use the information specific to this set of articles and about the TRC selection process to enhance the selection. We begin with the set of rules supplied by the TRC that help to define whether or not an article is of interest (http://trc.nist.gov/GDC.html#scope). Articles are rejected if they do not comply with any of the following criteria: (1) articles must contain new experimental results for thermodynamic or transport properties; (2) chemical systems must describe mixtures with three components or less; (3) materials must have unique chemical formulas (no polymers, oils, gums, foods, clathrates, etc.); (4) acid dissociation constants and specific ionic properties such as pH are not of interest; and (5) reaction properties including equilibrium constants are not of interest.
These five rules give us a starting point to look for specific information in these journal articles. For example, articles must contain experimental data; therefore, they must contain an experimental section that describes the process of collecting thermophysical property data. The mention of certain chemicals should indicate the rejection of articles, described by criteria 2 and 3. There are a set of experimental measurements that should also indicate article rejection, described by criteria 4 and 5. We have collected data on many journal articles to answer these questions.
We begin with a description of the data we have available for this study. We are limited in part by the data available to us, since we are dealing with copyrighted material. We then describe our selection process and give results both for the JCED articles with manual reference data identifying both selected and rejected articles, and for four other journals for which we have access to only the selected articles. Our goal in this study is twofold: we hope to help the TRC alleviate their dependence on manual selection, and at the same time we hope to gain insight into the journal selection process in general, using the valuable large journal article collection available to us.
Data
The TRC has provided us with 3448 articles from JCED between the years 2008 and 2012 that have been manually sorted. And 1914 of these contain data that is stored in their database; 1534 do not. For later years, and for the other four journals of interest, we have available only those articles that contain data for the TRC database: 694 articles from JCED 2013–2015, 29 articles from the 2008–2015 International Journal of Thermophysics (IJT), 538 articles from the 2008–2015 Thermochimica Acta (TCA), 391 articles from the 2008–2015 Fluid Phase Equilibria (FPE), and 1041 articles from the 2008–2015 Journal of Chemical Thermodynamics (JCT). We will use the 3448 JCED articles to test our screening process, because we have manual selection data available to us identifying both the accepted and rejected articles. For all other articles, we will examine how well the process developed with the JCED articles works on those articles, and see how closely related the format and content of the other journals are to the JCED format and content. We will use the terms “TRC articles” to refer to those articles that have been manually selected as relevant to the TRC and “non-TRC articles” to refer to those articles that have been manually selected as not relevant.
Methods
We describe our overall approach to this screening process, and the methods we use to carry out the process, including the methods we used to increase our topic modeling classification accuracy by splitting each journal article into five sections and classifying based on combinations of these sections.
Overall Approach
Our goal is to automatically identify articles not relevant to the TRC curation process without losing any potential data due to false negatives. Therefore, we will only mark as non-relevant those articles for which we have a specific reason for elimination.
Our overall approach, as outlined in Fig. 1, consisted of (1) topic model-based classification; (2) eliminating articles that did not satisfy rule #1, those with no thermophysical property information or no experimental methods sections; (3) eliminating articles that do not describe materials with unique chemical formulas, the polymer articles, the clathrate hydrate articles, and others from a list of materials that we have collected while examining many of the 3448 articles; (4) screening articles that classify as non-TRC articles for specific reasons for rejection; and (5) manually sorting through remaining non-TRC articles for which we do not know the reason for rejection to gather new information to repeat the steps 4 and 5. Unless otherwise mentioned, these steps are all accomplished using our own Python code.
Open image in new window 
Fig. 1 
Overall selection scheme
At the fifth step, we had the option to use automated machine learning techniques, but we instead chose to examine the nuances of this data. The manual step at this point allowed us to develop an inclusive list of explanations for the complexity of article classification for the TRC curation process. We learned a great deal about the content of these articles by looking at many articles that did not classify as expected in the earlier screening. The TRC devotes time to manually screening articles with a very high accuracy. We wish to make that process more efficient without sacrificing accuracy. With the small sample of articles available for use, it is not possible to automatically create a model that describes the TRC manual process that will include highly non-frequent but still important topics, without some manual input of those ideas into our model. An underlying assumption in our work is that expert knowledge is necessary to supplement what can be learned from our small data set until there is enough data to fully automate the process.
Topic Modeling and Classification
Topic modeling refers to a set of techniques for finding abstract topics in a natural language corpus [5]. A document can be represented as a vector of word frequencies. Topic modeling techniques find sets of words over an entire corpus that define topics within that corpus. Since document vectors can be very sparse, different techniques are applied to extract information from the sparse matrices associated with the vectors of all documents from a corpus. There are many available topic modeling techniques, each of which use a different method to extract information from sparse matrices. Latent semantic indexing (LSI) uses reduced singular value decomposition (SVD) of the document-term matrix to produce a lower rank approximation of the matrix which exploits the co-occurrence of terms as it projects documents into lower-dimensional space [6]. Random projections (RP) reduces the dimensionality of a sparse matrix by multiplying the original vectors by a random matrix, where the product has a reduced dimension [7]. Non-negative matrix factorization (NMF) factors the sparse matrix into two non-negative matrices. The non-negative constraints reduce the dimensionality of the information [8]. Latent Dirichlet allocation (LDA) is a more recent, probabilistic generative technique where documents are modeled as having a per-document topic distribution and a per-topic word distribution [9]. In this article, we chose LSI because it is a very fast technique (more than 25 times faster than LDA for our corpus of 3448 articles) and the number of tests that we ran was large. In addition, although the results of other topic modeling techniques gave somewhat higher classification results, LSI topic modeling gave results that differed from the best models by less than 5% [4]. After testing our classification using 20, 50, 100, 150, or 200 topics in the LSI model, the 100-topic model was selected for all other tests.
We took the collection of 3448 articles that contained both TRC and non-TRC articles, and performed LSI topic modeling followed by random forest classification using the document topic weights as predictors. Topic modeling was performed using gensim [10], and the classification was done using Waikato Environment for Knowledge Analysis (WEKA) random forest classifier [11]. Topic models were formed using article abstracts, introductions, experimental sections, results sections, and literature sections, individually and in combinations. Each journal article was originally split into these five sections as described below to check for the presence of experimental work. For each trial, the appropriate sections of all 3448 articles were tokenized. Topic weights from the LSI model were then used for random forest classification in the following process. The 3448 articles were split into test and training groups. Unless otherwise specified, the articles were split into 2 groups of 1724 articles and each group was used as training data with the other group as test data. For the tenfold cross validation, the 3448 were split into 10 groups. The classifier was trained and evaluated repeatedly using each group as the test data and the other 90% of the data as the training data. Tenfold cross-validation was performed for the data presented in Table 1 only to verify the results of classification based on different parts of the articles. It is not a part of the overall scheme shown in Fig. 1. Applying a topic model to the words collected from individual sections as well as groups of sections both gave us insight into the information in the articles and increased our classification accuracy over just using the entire articles.
Breaking Articles into Sections
To break the articles into sections, we used the text files from a pdftotext conversion, and looked for individual lines containing any of slightly different names for section headings. The output of the pdftotext conversion was filtered using the Linux strings command. For all of the work with these text files, the Natural Language Toolkit (NLTK) was used for stop word removal and stemming [12]. Article titles were collected using a Linux pdftohtml converter, looking only at the first 50 lines of each HTML file, searching for lines containing more than 2 words that start with a “<b>” or lines that do not start with an HTML tag, eliminating those lines with key words for each journal, such as “Thermochimica” and “Fluid.” The versions of the pdftohtml and pdftotext converters used are referenced below [13].
Association Rule Mining
Association rule mining was developed to find co-occurrences in large collections of data and has been widely used to reveal hidden associations between data items [14]. Sentences from selected article sections were parsed and tagged using the Python pattern.en software (http://www.clips.ua.ac.be/pages/pattern-en), and then fed to an association rule algorithm (http://magpiehall.com/learning-association-rules-with-python/). Bigrams and trigrams (word pairs and triplets) were manually selected based on frequency from the association rule algorithm’s output. Association rule mining was used throughout our study to obtain high correlation bigrams and trigrams that describe specific information of interest. For example, we gathered information about typical equipment used and secondary names for thermophysical properties of interest. The articles of interest to the TRC share the use of common equipment, and we wanted to see if we could select non-TRC articles by an absence of reference to our equipment list. However, we found that many TRC articles reference other articles for their experimental methods instead of describing them, so that we could not use this as a selection criterion.
Article Rejection Based on Rules 1 and 3
From the 3448 JCED articles between 2008 and 2012, we collected single words and word pairs that describe all of the thermophysical properties of interest for the TRC database. We began with an initial list of properties supplied by the TRC, and augmented that list based on both association rule mining results and our own observations, to find variations of the property names, for example “vapor” vs. “vapour.” Articles were then rejected if they did not contain any reference to any of thermophysical properties on our lists. All the TRC articles in this set (1914/3448) included references to one or more of these properties. Similarly, we created a list of single words and word pairs that describe the most common polymeric materials in these articles, and other materials without unique chemical formulas, to eliminate articles whose term frequencies of any of these materials was high. Polymer articles with more than 15 references to non-unique materials were eliminated and articles with other materials with more than 12 references to non-unique materials were eliminated. These thresholds were chosen empirically by looking for a value that captured significant numbers of non-TRC articles without rejecting too many TRC articles, and may be changed to suit the needs the of the TRC later. Each journal article was divided into five sections (see above): abstract, introduction, experimental section, results and discussion, and literature cited. We used this division in several ways, but initially we used it to reject articles with no experimental section.
Further Screening
The goal of our screening process was to find as many of the non-TRC articles as possible. We began by finding all the articles that were classified as non-TRC articles using two topic model-based classifications: topic model-based classification based on the text from entire articles, and topic model-based classification based on the text from the results and literature sections of the articles. We included the model generated from the results and literature section texts because that model gave the best classification results in our study. Both TRC and non-TRC articles were included in the topic model classification of non-TRC results, although the number of non-TRC articles was higher. The success of our screening was judged by the number of non-TRC articles we eliminated vs. how few TRC articles were chosen for elimination.
Collection of Table Legend Information
All the data in the JCED articles are contained in tables; it is a requirement of this journal (http://trc.nist.gov/JCED-Support.html). We collected table legend information and used it in two different ways. Table legends were collected from the pdftotext output by searching for lines beginning with the word “table” followed by a number, for example “Table 3,” followed by lines of numerical information, including more than three consecutive numbers. Since table legends are sometimes lengthy, particularly in the text version where individual table column headings are sometimes each put on a different line by the pdftotext converter, we allowed for up to 20 lines of text before the numerical data starts. Errors in locating the table legend data were caused by a pdftotext errors, in which the legend and the table values were falsely separated, or where words were broken up incorrectly by the pdftotext converter.
Once the table legends were collected, we ensured that the legends contained at least one of the thermophysical properties in our lists. Then the table legend was checked against other lists of words for relevance. We verified that the legends contain other words that are either highly associated with articles of interest (such as “experimental”) or words associated with articles that are not of interest (such as “literature values,” “calculated,” or “predicted”). Experimental articles often provide tables of values for comparison with the new data, and we do not want to use such information. Association rule mining was used on the table legend data to find single, double, and triple words associated both with the TRC data and the non-TRC data, to distinguish between data associated with properties that the TRC collects (such as “enthalpy,” where the data describes enthalpy of vaporization) from data that they do not collect (such as “enthalpy of protonation”). We verified that property data was associated only with ionic species or only as a function of pH, if that information is available from the table legends. We discovered that a very small subset of the articles had experimental data only in the supplementary materials and caused these TRC articles to be mistakenly rejected in our screening. Future work will address both the text conversion errors and the errors caused by not having the supplementary materials available that are associated with the articles.
Screening Using Automated Term Extraction
We used a list of automatically generated multi-word sequences to identify non-TRC articles that we could not identify through other screening. The word sequences were very specific to the 1534 non-TRC articles in our collection and were collected through a series of steps: (1) LSI topic modeling was used to generate a list of the most important individual lemmatized words from the whole corpus of 3448 articles, using a single topic, which produced a list of words with only positive-valued weights. (2) The highest-ranking 500 words output from the LSI model were then grouped with other words using association rule mining. The associated phrases were then extracted from the lemmatized text. (3) For each of the 3448 articles, we collected a list of occurrences of these associated words. (4) Lists of phrases from each of the 3448 articles were screened to find 647 frequent terms that only appeared in one or more of the 1534 non-TRC lists and did not appear in any of the 1914 TRC article lists. (5) To shorten this list, word frequencies of each of these 647 terms were found for each of the 3448 articles without regard for word order within each term. The original phrases were ordered sets of words. The list of 647 terms was shortened to a list of 400 terms.
For the remaining articles that were classified as non-TRC articles in the topic model-based classifications and contained property information in table legends that also passed our screening, we performed a final screening by examining the article titles. These were screened for indications that they were non-TRC articles by searching for high-frequency terms from rejected articles, such as “protonation” and “kinetics.” The titles indicated that some articles that should have been rejected because they concerned polymers or materials with non-unique chemical formulas, but the term frequencies of these material names were not high enough for rejection earlier in the process.
Classification Expectations
Of our set of 3448 articles, 1534 do not contain data relevant to the TRC database. Ideally, we would like to find these in our search. However, the constraints of the search make it necessary to settle for a more realistic goal. Approximately 10% of the articles (about 150) classify as TRC articles in topic model-based classification, and these are not subject to our final screening. In addition, there are two classes of articles that we believe we will not be able to identify as accurately as the rest of the articles: articles with more than three component mixtures, and articles with materials with non-unique chemical formulas with low-term frequencies for the materials of interest. We can estimate the occurrences of these classes of articles in our data and then set a goal based on the remaining articles.
For example, let us assume that there are two articles that describe the collection of density data using the identical technique under the same temperature and pressure ranges. Let us also assume that the first article describes a ternary mixture and the second a mixture with the exact same three components plus an additional fourth component. The first article would be manually classified as a TRC article while the second would be classified as a non-TRC article because it describes a mixture with more than three components. Their topic modeling results and table legend data would look very much the same. We estimated that 150 articles in our corpus of 3448 were manually rejected only because they have 4 components. To do this, we found all articles in the group of 1534 non-TRC articles that contain the term “quaternary,” and then use association rule mining to find words pairs that indicate that the quaternary adjective refers to the main component of the article and not to other information. In an association mining test, the term “quaternary” is highly associated with the word “system.” Of the 1534 non-TRC articles, 108 contain this word pair, “quaternary system.” We have also discovered a number of quaternary system articles that do not use the term “quaternary” in the article; most simply use “+” signs between the components, for example, “a mixture containing A + B + C + D.” These are difficult to distinguish from ternary systems, but we estimate that there are at least 50 of these in our set of 3448 articles, meaning that at least 150 out of our 1534 non-TRC articles were rejected due to the number of chemical components in the material of interest.
Similarly, there are articles manually selected as non-TRC articles because they contain polymer data, as well as articles that contain mostly polymer data with a very small amount of non-polymer data. To estimate their numbers, we find the term frequencies of common polymer names, as both single words and word pairs, and then count the number of the 1914 TRC articles that include at least five references to any of these names. We examined term frequencies of other materials that lead to article rejections, such as clathrate hydrates. We found 70 TRC articles with at least 5 polymer references and 29 articles with more than 5 references to other excluded materials, allowing us to estimate that we will miss approximately 20 TRC articles in our screening. There are also several articles concerning materials that do not occur often enough to be in the lists of materials that we check. We estimate another 30 or so non-TRC articles were manually rejected because of the material described in the article. Altogether, it is likely that we will miss 150 (classification) + 150 (more than 3 components) + 30 (non-unique materials) non-TRC articles and 20 (non-unique materials) TRC articles, leaving about 1200 non-TRC articles to find in our screening (78.2% of the non-TRC articles), and an expectation of at least 20 TRC articles that are incorrectly screened (1.0% of the TRC articles).
Results and Discussion
Overall
Our approach does not fully automate the article selection process, but it does require significantly less time than the current manual selection process by reducing the number of articles that need to be manually screened. If a small number of false negative results were allowed, then a more fully automated model would have been possible. The requirements imposed by the TRC’s stringent original five rules and their requirement not to miss data even though it might come from an article that is unrelated to our data set make some manual intervention necessary at this point in time. With time and increasing the size of our data set, the TRC requirements will become easier to fulfill. A rule such as “no fluids that are polymers” is a difficult rule to universally enforce because, for example, articles may discuss polymer compounds but not collect data on those compounds, or articles may discuss non-polymeric compounds but include some polymer data in their tables.
Topic Model Classification Results
Table 1 shows the results of the LSI topic model, followed by random forest classification using different sections of the journal articles. We evaluated classification on all sections of the articles, and on subgroups of the sections using tenfold cross validation. The 3448 articles were randomly divided into 10 groups using the Python random.shuffle command on the list of file names. For each section or group of sections tested, we created a topic model and classifier using nine of the ten groups, and then found topic weights and random forest classifications for the tenth group using that model and classifier. The values in the table are a weighted average over the ten tests for each table entry. F-measures are reported to show the overall results. The number of true positives and true negatives are also reported to show which combinations of sections of the articles were most useful in classifying either the TRC or non-TRC articles. The highest F-measures were achieved using a combination of the Literature Cited section with either the Experimental section (F-measure is 0.896) or the Results section (F-measure 0.895) or both (F-measure 0.896). Both were higher than the F-measure associated with entire articles (F-measure 0.889). The Literature section by itself did almost as well as it did in combination with the Experimental and Results section (F-measure 0.890), the only single section to score close to the highest in the list. The combinations that resulted in the highest true positive score were the Experimental/Literature pair and the Experimental/Results/Literature set (1784/1914 articles found). The combination that resulted in the highest true negative score was the Results/Literature pair (1259/1534 articles found). We use two sets of topic modeling classification in our selection process: the model using the entire article and the model using the Results/Literature pair, because we are looking for the non-TRC articles. By combining the outputs of these classifications, we increase the number of negative articles to screen for other information. Our initial topic model-based classifications for the corpus of 3448 articles selected 1510 articles for further screening, of which 204 were TRC articles and 1306 were non-TRC articles: our task was to separate these 2 groups and label the non-TRC articles with reasons for elimination. The tenfold cross validation was repeated a second time using a different random ordering of the 3448 articles. Differences in the final F-measures ranged from 0.00050 to 0.00366. The shuffling of the order had very little effect on the final comparison of classification by sections (see Table 1).
Table 1 
Topic model/random forest classification results using different sections of the journal articles: 3448 articles, 1914 positives, 1534 negatives. Each table entry is the average of a tenfold cross-validation test
Sections used
True positives
False negatives
False positives
True negatives
F-measure
Error ± 1 std. (from two measurements)
Abstracts
1810
104
721
813
.814
0.000447
Introductions
1830
84
944
590
.781
0.000055
Experimental
1610
304
279
1255
.847
0.000804
Results
1736
178
292
1242
.881
0.000623
Literature Cited
1772
142
296
1238
.890
0.003658
All sections
1765
149
292
1242
.889
0.001716
Results/Literature
1773
141
275
1259
.895
0.000706
Exper./Literature
1784
130
283
1251
.896
0.001900
Exper./Results
1779
135
289
1245
.894
0.001857
Exp./Res./Lit.
1784
130
284
1250
.896
0.000241
Screening Process Results
The screening process is designed to find the articles that do not contain data for the TRC database. Our goal is to make the number of these articles as close to the estimated 1200 non-TRC articles as we can. When we screened the initial 1510 articles that classified as non-TRC by the topic model classification, we eliminated 162 articles because they described polymers, 268 were eliminated because they contained other non-unique chemical materials, 50 were eliminated because they did not reference any of the thermophysical properties of interest, and 114 were eliminated because they had no experimental section. The remaining 908 articles underwent further screening. Among the eliminated articles were eight TRC articles: two polymer articles with a small amount of non-polymer data, four other material articles with a small amount of data on materials of interest, and two articles with no experimental section (these articles were corrections to other articles with some new data).
The next part of the process is iterative: the remaining articles are put through a screening and then all remaining articles are checked manually for information to make the process better. The screening process is updated with new information and the process repeated. The screening process has three parts: (1) Table legend information is screened to make sure that there is thermophysical property data of interest in the tables of the article as described above. (2) Articles are screened for 400 highest frequency non-TRC terms. (3) Article titles are screened. The title screening was not in our original screening process, but added in the second round after examining our results.
In the first round of screening, we were left with 226 non-TRC articles that should have been rejected by our analysis but were not. We looked through the text files of each of these to survey their contents, and this information is collected in Table 2. Briefly, 40 of these were found to have more than 3 components, 7 were polymer articles, 10 were hydrate articles, 35 were articles with other non-unique chemical formulas, and 7 were articles on crystalline solid materials. Nine of the articles were theoretical articles. Seven of the articles described solid-liquid equilibria data, which the TRC does not collect. Another seven discussed ternary diffusion coefficients, and only binary diffusion coefficient data is currently collected. The remaining articles contained information related to rules #4 and 5 above: specific ionic properties and non-equilibrium (reaction) properties. Ionic properties have proved hard to distinguish from articles describing the same thermophysical properties for non-ionic materials. We found 27 articles on ionic compounds with activity coefficient data in their tables, for example, that resemble many of the TRC articles that include activity coefficient measurements. The articles on ionic liquids, whose data was captured, are very hard to distinguish from the articles on other ionic compounds, whose data was not captured. More work in this area remains to recognize these articles. After the initial screening, we learned to check for ternary diffusion coefficient data (in all sections of the articles except for the introduction and the literature sections) and now eliminate those articles whose only thermophysical property is ternary diffusion coefficient data. In addition, we collected titles of the 226 articles and screen for the most commonly used terms in the non-TRC titles.
Table 2 
Results of our first round of screening: articles that classified as non-TRC with our topic models, but had good table legend information
Reason for manual rejection
Number of articles
More than 3 components
40
Polymer articles
7
Hydrate articles
10
Other non-unique formula articles
35
Crystalline solid articles
7
Non-experimental articles
9
Solid-liquid equilibrium articles
7
Ternary diffusion coefficients
7
Non-equilibrium properties
21
Ionic properties
90
Total
226
Our final screening numbers showed that we found 1086 of the expected 1200 non-TRC articles, 90.5% of the expected articles, 71.8% of all the 1534 non-TRC articles, and falsely rejected 25 TRC articles, leaving 98.7% of the TRC articles. In addition to the 2 polymer articles, 4 other material articles, and 2 articles without experimental sections, an additional 17 other TRC articles were falsely eliminated in our screening. These included three articles with conductivity measurements in supplementary materials that we did not have, five articles with pdftotext errors that corrupted the table legend data, one article that described density measurements estimated by a method that used the experimental data, an article on activity coefficient data, and seven articles whose table legend data did not include mention of relevant thermophysical properties. Of these seven, two contain table legend property information only in column headings, and five used different wording for property names in the legends. More work remains to be able to obtain this information in the future.
Comparisons with other Methods
A comparison of our results with the LSI-based classification with classification based on other features is shown in Table 4. We performed several comparisons of our automated results with the TRC manual results during the process of improving our method. Our method found 89 articles that had been mislabeled as being not relevant in our data set, so we presented them to the TRC. Of these 89 articles, 62 were correctly identified by our software. Forty eight of those 89 articles had been scheduled to be put in their relevant directory but had not yet been processed, and 14 had unresolved data problems (i.e., the data were found but problems existed with the manuscript). The remaining 27 were not relevant and we used those articles to help improve our algorithm. The manual screening by the TRC, although very time consuming, proved to be 100% accurate according to our findings.
In addition to the LSI-based features, we also evaluated classification using two other sets of features extracted from the articles: Bag of Words (word frequencies) and Tfidf [15]. We compared the final number of false positive articles and the number of correctly identified non-TRC articles. The number of false positives remained approximately the same: 25 using the LSI model, 24 using the Tfidf model, and 23 using the Bag of Words model. The number of correctly labelled non-TRC articles was 1086 for the LSI model, 1051 for the Tfidf model, and 1054 for the Bag of Words model, or 70.8, 68.5, and 68.7%, respectively. While the choice of features did not make a significant difference, the classification based on the LSI model identified more non-relevant articles than classifiers based on Bags of Words or Tfidf.
Results on Remaining Data with Known TRC Data
The rest of the articles available for our study, from five different journals, have all had data captured for the TRC database. We can test our model on these articles to get information about how many of the TRC articles were falsely eliminated, but we cannot get information on how many of the non-TRC articles would be found by our screening. We ran the same screening on these articles and the results are shown in Table 3. For four of the journals, all except TCA, the topic models created with JCED articles worked approximately as well on the articles from the other journals. For the TCA articles, the number that classified as TRC articles using the JCED model was quite low (65%), so we augmented the topic models with TCA articles, and those results are also shown in Table 2. Each half of the TCA articles were classified using the 3448 JCED articles and the other half of the TCA articles. The results show that the screening worked well for the other journals. PDF conversion varied in its accuracy across the journals, and pdftotext errors were the cause of several misclassifications. A better converter is necessary to increase the accuracy of this process.
Table 3 
Screening results for articles manually selected as TRC papers from five journals. Papers were screened based on the JCED 2008–2012 set of papers, whose results are given for comparison
Journal
JCED 2008–12
JCED 2013–15
IJT 2008–15
JCT 2008–15
FPE 2008–15
TCA 2008–15
# of papers
1914
694
29
1041
391
538
Classified as TRC in topic model
1710
589
26
861
325
496
Articles remaining to screen
204
105
3
180
66
42
# rejected: polymer term frequency
2
10
0
9
2
2
# rejected: other materials term frequency
4
3
0
2
0
0
# rejected: no experimental section
2
0
0
0
0
0
# rejected: overall pdftotext errors
0
1
0
5
28
6
# rejected: table legend pdftotext error
5
3
1
18
5
1
# rejected: table legend informationa 
10
2
0
23
3
6
# rejected: title or term frequency information
2
5
0
5
0
0
Total rejected
25
24
1
62
38
15
Screened as TRC
98.7%
96.5%
96.6%
94.0%
90.3%
96.2%
aTable legend property information errors include (a) data in supplementary tables not available to us; (b) property names given in different forms than in the original set of JCED papers, for example, abbreviations; and (c) property information only given in column headings of tables
The differences we found while evaluating the four other journals’ articles by using models from the JCED articles were smaller than expected. There were small differences in the HTML formats to extract the article titles. Except for the Thermochimica Act (TCA) articles, there were minor differences in classification using the JCED topic model-based classification. The TCA property names had some slight variations and spelling differences compared with the other journals. TCA contains articles with a different range of thermophysical property frequencies, so many of the articles have different content than the JCED articles, and the topic model-based classification using only JCED articles did not work as well as for the other journals. Each journal format created slightly different pdftotext conversion issues, which we can hopefully fix with a better converter.
In the process of journal article selection, we have shown new results in topic model-based classification by using sections of each journal article rather than the whole article, as opposed to using different feature selection methods (see Table 4). Interestingly, the Literature section of each article has been shown to be the most beneficial to accurate classification. We believe that this is because many of the experimental techniques described in these articles are classical techniques referenced by highly cited papers and authors. Mining all of the literature data remains for future work, but an interesting challenge is to classify references to key articles as an indicator of certain types of experimental work. The fact that the introductions had the lowest F-measure from the classifications indicates that background material is not as useful for discriminating between articles as any of the other article sections. Classification based on the abstracts had the next lowest accuracy, implying that although merely looking at the abstracts of articles might be more convenient or efficient than looking at the articles, it is not sufficient for ensuring optimal classification accuracy.
Table 4 
Comparison with other methods
Method
Total TRC-relevant articles rejected (false positives)
Total TRC non-relevant articles identified (true negatives)
Manual screening
0
1534
LSI
25
1086
Bag of words
23
1054
Tfidf
24
1051
Conclusions
Our topic model classifications based on text from specific article sections instead of the entire articles revealed two interesting results. The first is that we obtained better classification results using portions of the articles instead of the entire article. In particular, the Literature Cited section was very important for the classification. The classification accuracy improved by including either the experimental section or the results section. Second, by combining the use of our topic models, we obtained better classification than using a single model.
Our classification scheme also taught us the advantage gained by adding additional information into the model that is specific to the application. The best classification accuracy from our previous work, classification using topic modeling followed by random forest classification, was improved in our current model because we tried to use as much information about the journal articles as we could incorporate. We started with a set of rules that were used for manual classification and incorporated those into our model, along with other thermodynamic criteria.
We tested the classification model that we built using manually selected JCED articles on articles from four other journals known to include thermophysical property data. We found that our JCED classification model worked very well for three of these journals, International Journal of Thermophysics, Fluid Phase Equilibria, and Journal of Chemical Thermodynamics. For the fourth journal, Thermochimica Acta, our LSI topic model needed to be updated with additional articles from Thermochimica Acta. Once the topic model was updated, our classification model worked well on the articles from this fourth journal as well.
Our work clearly shows the value in understanding the content of the data in the journal process selection, which is known to be important. Each individual need for journal classification will have specific requirements associated with it. The results of all our studies can now be applied to new incoming JCED articles to lower the number of articles that need to be manually screened.
The method presented here includes manual intervention and is not fully automated. Full automation will be addressed in the future. We will build larger data sets that include more of the complete domain at hand. We expect that with additional articles added to our reference set, more accurate classification is possible and less manual effort will be required. Future work will also focus on better distinguishing the articles with ionic properties, improving the PDF text extraction process, and mining the figure and table legend information.
References
    1. 1.
       Frenkel M, Chirico R, Diky V et al (2011) Extension of ThermoML: the IUPAC standard for thermodynamic data communications. Pure Appl Chem 83(10):1937–1969CrossRefGoogle Scholar
    2. 2.
       Chirico RD, Frenkel M, Magee JW, Diky V, Muzny CD, et.al. (2013) Improvement of quality in publication of experimental thermophysical property data: Challenges, assessment tools, global implementation, and online support. J. Chem. Eng. Data. 58:2699–2716.Google Scholar
    3. 3.
       Diky V, Chirico R, Wilhoit r, Dong Q, Frenkel M (2003) Windows-based guided data capture software for mass-scale thermophysical and thermochemical property data collection. J Chem Inf Comput Sci 43(13):15–24CrossRefGoogle Scholar
    4. 4.
       Dima A, Youssef S, Kroenlein K (in progress) Classification of Scientific Journal Articles for the NIST Thermodynamics Research Center, private communication.Google Scholar
    5. 5.
       Xie P, and Xing E (2103) Integrating Document Clustering and Topic Modeling. arXiv Prepr. ArXiv. 1309.6874.Google Scholar
    6. 6.
       Deerwester S, Dumais S, Landauer T (1990) Indexing by latent semantic analysis. J Am Soc Inf Sci 41(6):391–407Google Scholar
    7. 7.
       Kaski (1998) Dimensionality reduction by random mapping: Fast similarity computation for clustering, 1998 I.E. International Joint Conference on Neural Networks, volume 1, pp. 413–418Google Scholar
    8. 8.
       Berry MW, Gillis N, and Glineur F (2009) Document classification using nonnegative matrix factorization and underapproximation. IEEE International Symposium on Circuits and Systems, ISCAS. 2009, 2782–2785.Google Scholar
    9. 9.
       Blei D, Ng A, Jordan M (2003) Latent dirichlet allocation. J Mach Learn Res 3:993–1022Google Scholar
    10. 10.
       Řehůřek R, and Sojka P (2010) Software framework for topic modelling with large corpora. LREC 2010 workshop New Challenges for NLP Frameworks. 46–50.Google Scholar
    11. 11.
       Hall M, Frank E, Holmes G, Pfahringer B, Reutemann P, and Witten I (2009) The WEKA data mining software: an update. ACM SIGKDD Explor Newslett. 11,1:10–18.Google Scholar
    12. 12.
       Bird S, Klein E, and Loper E (2009) Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. Sebastopol: O’Reilly Media.Google Scholar
    13. 13.
       The Poppler Developers, http://poppler.freedesktop.org, Copyright 2005–2009
    14. 14.
       Agrawal R, and Srikant R, Fast algorithms for mining association rules (1994) Proc. 20th int. conf. very large data bases, VLDB. 487–499.Google Scholar
    15. 15.
       Yang Y, and Pedersen JO (1997) A comparative study on feature selection in text categorization. ICML. 97:412–420.Google Scholar
Copyright information
© The Minerals, Metals & Materials Society (outside the USA) 2017
Authors and Affiliations
    • Adele Peskin
        ◦ 1
      Email authorView author's OrcID profile
    • Alden Dima
        ◦ 2
    1. 1.National Institute of Standards and TechnologyBoulderUSA
    2. 2.National Institute of Standards and TechnologyGaithersburgUSA
About this article
CrossMark 
Cite this article as: 
Peskin, A. & Dima, A. Integr Mater Manuf Innov (2017) 6: 187. https://doi.org/10.1007/s40192-017-0096-1 
    • Received 23 January 2017 
    • Accepted 09 May 2017 
    • First Online 07 June 2017 
    • DOI https://doi.org/10.1007/s40192-017-0096-1 
    • Publisher Name Springer International Publishing 
    • Print ISSN 2193-9764 
    • Online ISSN 2193-9772 
    • About this journal 
This article is published under an open access license. Please check the 'Copyright Information' section for details of this license and what re-use is permitted. If your intended use exceeds what is permitted by the license or if you are unable to locate the licence and re-use information, please contact the Rights and Permissions team. 
Personalised recommendations

Cite article 
    • How to cite? 
    • .RIS Papers Reference Manager RefWorks Zotero 
    • .ENW EndNote 
    • .BIB BibTeX JabRef Mendeley 
Share article 
Download PDF 
Actions
Download PDF 
Cite article 
    • How to cite? 
    • .RIS Papers Reference Manager RefWorks Zotero 
    • .ENW EndNote 
    • .BIB BibTeX JabRef Mendeley 
Share article 
Table of contents
    • Article 
    • Abstract 
    • Background 
    • Data 
    • Methods 
    • Overall Approach 
    • Topic Modeling and Classification 
    • Breaking Articles into Sections 
    • Association Rule Mining 
    • Article Rejection Based on Rules 1 and 3 
    • Further Screening 
    • Collection of Table Legend Information 
    • Screening Using Automated Term Extraction 
    • Classification Expectations 
    • Results and Discussion 
    • Conclusions 
    • References 
    • Copyright information 
    • Authors and Affiliations 
    • About this article 
Advertisement
Hide 

Over 10 million scientific documents at your fingertips
Switch Edition
    • Academic Edition 
    • Corporate Edition 
    • Home 
    • Impressum 
    • Legal information 
    • Privacy statement 
    • How we use cookies 
    • Accessibility 
    • Contact us 
Springer Nature 
© 2017 Springer Nature Switzerland AG. Part of Springer Nature.
Not logged in Not affiliated 62.245.124.181 
 









    • Your Privacy
    • Strictly Necessary Cookies
    • Performance Cookies
    • Functional Cookies
    • Targeting Cookies
    • More Information
Privacy Preference Centre
Active
Always Active



Save Settings
Allow All
We use cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners in accordance with our Privacy Statement. You can manage your preferences in Manage Cookies.
Close
OK
Manage Cookies

