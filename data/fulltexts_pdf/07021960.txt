1370
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 3, JUNE 2015
Why so many people? Explaining Nonhabitual
Transport Overcrowding With Internet Data
Francisco C. Pereira, Member, IEEE, Filipe Rodrigues, Evgheni Polisciuc, and Moshe Ben-Akiva
Abstract—Public transport
smartcard data can be used for
detection of
large crowds.
By comparing statistics on habitual
behavior (e.g., average by time of day), one can specifically iden-
tify nonhabitual
crowds,
which are often very problematic for
transport systems. While habitual overcrowding (e.g., peak hour)
is well
understood both by traffic managers and travelers,
non-
habitual overcrowding hotspots can become even more disruptive
and unpleasant because they are generally unexpected. By quickly
understanding such cases,
a transport
manager can react
and
mitigate transport system disruptions. We propose a probabilistic
data analysis model
that breaks each nonhabitual
overcrowding
hotspot into a set of explanatory components.
The potential
ex-
planatory components are initially retrieved from social networks
and special
events
websites
and then processed through text-
analysis techniques.
Finally,
for each such component,
the prob-
abilistic model estimates a specific share in the total overcrowding
counts.
We first
validate with synthetic data and then test
our
model with real data from the public transport system (EZLink)
of Singapore, focused on three case study areas. We demonstrate
that it is able to generate explanations that are intuitively plausible
and consistent both locally (correlation coefficient, i.e., CC, from
85% to 99% for the three areas) and globally (CC from 41.2%
to 83.9%). This model is directly applicable to any other domain
sensitive to crowd formation due to large social events (e.g., com-
munications, water, energy, waste).
Index Terms—Information extraction, machine learning, smart-
cards, special events, travel demand modeling, web mining.
I.
I
NTRODUCTION
D
UE TO THE amount and quality of pervasive technolo-
gies such as radiofrequency identification,
smartcards,
and mobile phone communications,
it
is possible to detect
crowds in almost real time with very low risk for privacy. Crowd
detection can be valuable for
safety reasons,
as well
as for
real-time supply/demand management of transportation,
com-
munications,
food stock,
logistics,
water,
or any other system
sensitive to aggregated human behavior.
Although such tech-
Manuscript
received January 5,
2014;
revised June 4,
2014;
accepted
October 2, 2014. Date of publication January 26, 2015; date of current version
May 29, 2015. This work was supported by the National Research Foundation
of
Singapore (SMART program)
and by the Fundação para a Ciência e
Tecnologia under Project PTDC/ECM-TRA/1898/2012. The Associate Editor
for this paper was W.-H. Lin.
F. C. Pereira is with the Singapore–MIT Alliance for Research and Technol-
ogy, Singapore 138602 (e-mail: camara@smart.mit.edu).
F.
Rodrigues and E.
Polisciuc are with University of Coimbra,
3000-214
Coimbra, Portugal.
M.
Ben-Akiva is with Massachusetts Institute of Technology,
Cambridge,
MA 02139 USA.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TITS.2014.2368119
nologies help detect
and quantify crowds,
they have limited
power in explaining why they happen.
For recurring crowds,
such as peak-hour
commuting,
this explanatory challenge is
trivial,
but
the same cannot
be said of
nonhabitual
cases.
Without local context knowledge,
it is not possible to discern
an explanation.
Fortunately,
another pervasive technology exists:
the Inter-
net,
which is rich in local
context.
Information about
public
special
events
(e.g.,
sports
games,
concerts,
parades,
sales,
demonstrations,
festivals) is abundant,
and so are social
net-
works (e.g.,
Twitter,
Facebook) and other platforms that have
dynamic context content (e.g., news feeds).
Particularly for public transport operations and management,
the treatment of overcrowding depends on understanding why
people are there, and where/when they will go next. Only then
can the manager react accordingly (e.g., add extra buses, trains,
send taxis).
For
example,
by knowing that
an overcrowding
hotspot is due to a concert,
one can also estimate its duration
(until
about
after
the concert
begins)
and a possible next
hotspot
(after
the concert
ends).
If
instead it
were due to a
series of small scattered events, the treatment may be different
(e.g., no single ending hotspot). Maybe even more importantly,
by understanding such impacts on a
posthoc
analysis, one can
also better prepare for the next time that similar events happen.
This paper proposes to solve the following problem:
given
a nonhabitual large crowd (an overcrowding hotspot), what are
its potential causes and how do they individually contribute to
the overall impact? We will particularly focus on the problem
of public transport overcrowding in special events areas as the
main practical motivation and case study.
Given the importance of
these social
phenomena,
many
traffic management
centers have teams of people that
are re-
sponsible for periodically scanning the Internet and newspapers
in search for special events. In fact, for very large events, this
problem is generally solved,
albeit
manually.
The challenge
comes when multiple smaller events co-occur in the same area
to form a relevant hotspot. It is not only harder to find them but
also extremely difficult to intuitively estimate their aggregated
impact.
We identify and measure the overcrowding hotspots by ana-
lyzing 4 months of public transport data from the city-state of
Singapore. Considering that the transport system is designed to
support much more than the expected value (mean) of habitual
demand,
we define a hotspot
as a continuous period where
observed demand (e.g., number of arrivals) repeatedly exceeds
a high percentile (e.g., 90%). The overcrowding hotspot impact
is measured as the total sum of demand above the median line.
1524-9050 © 2015
IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.
PEREIRA et al.: EXPLAINING NONHABITUAL TRANSPORT OVERCROWDING WITH INTERNET DATA
1371
During the whole period of the data set, we collected special
events data from five websites,
1
as well as their Facebook likes
and Google hits. While the latter two are numerical in nature,
the former include unstructured text
descriptions.
Hence,
we
apply an information extraction technique, called topic model-
ing [1], which transforms such data into a set of features under-
standable from a machine learning algorithm’s perspective.
Since we only have observations
of
aggregated impacts
rather than the individual
events,
we propose a Bayesian hi-
erarchical additive model, where each hotspot is formalized as
a sum of the potential explanatory components.
We explicitly
model
uncertainty on the parameters by using the Infer.NET
platform [2].
Since we do not
have ground truth data on the individual
event
impacts,
we validate the model
in two ways:
1) using
synthesized impact data, based on real event descriptions; and
2) comparing the sum of estimations from our model with the
observed real-world sums.
This way,
we demonstrate that our
model approximates the (simulated) ground truth and that the
results on a real-world case are globally feasible.
This methodology is applicable beyond the specific case of
public transport overcrowding as long as the key research ques-
tion and ingredients remain. For example, during special events,
cell phone,
WiFi network,
energy,
waste,
or catering/logistics
systems may equally suffer from disruptions.
If there is both
pervasive and explanatory data to quantify and correlate the
impacts,
the general procedure remains the same.
We provide
the source code together with the synthesized data set for the
interested reader.
2
The main contributions of this paper are a fully implemented
model
for
inferring latent
demand contributions
in special
events scenarios (with code available to the reader,
runnable
in the Infer.NET platform); the application of a state-of-the-art
topic modeling technique [latent Dirichlet allocation (LDA)] in
the context of intelligent transportation systems; and a valida-
tion technique with synthetic data (also available to the reader)
that follows a realistic methodology and that can be used as a
benchmark for future works on the same problem.
The next section will focus on discussing related work and
present
some literature related to the building blocks of our
solution such as topic models and hierarchical models.
Then,
in Section III,
we will
explain how we determine the over-
crowding hotspots,
and Section IV will show how we collect
potential explanatory data from the Web. Our Bayesian model is
explained in Section V and is followed by experimentation and
validation (Section VI). We specifically analyze a few hotspots
in Section VII and then end this paper with a discussion and the
conclusion (Sections VIII and IX, respectively).
II.
L
ITERATURE
R
EVIEW
A.
Detecting Mobility Patterns With Pervasive Data
In June 2014, a search with the keywords “cell phone” and
“human mobility” in Google returns about
14.4 k entries.
In
Google Scholar,
we can find over 1000 entries that
mention
1
These
websites
were
www.eventful.com,
upcoming.org,
last.fm,
timeoutsingapore.com and singaporeexpo.com.sg.
2
https://dl.dropboxusercontent.com/u/1344277/PereiraEtAl2014.zip
these words explicitly,
with about
98 since January,
2014.
If
we add other types of data such as GPS or smartcard data, these
numbers will increase even more dramatically. Therefore, run-
ning the risk of sheer incompleteness, we mention a few papers
we consider seminal to the area or relate more to our work.
Using a large cell-phone data set, Marta González et al. [3]
showed that
individual
mobility travel
patterns generally fol-
low a single spatial
probability distribution,
indicating that,
despite their
inherent
heterogeneity,
humans
follow simple
reproducible patterns.
In fact,
this asserts the remarkable yet
not
so surprising fact
that
human mobility is habitual
for the
vast majority of the time. This principle has been behind several
other works, for example, to estimate disease spreading [4] or
vehicular network routing protocols [5].
Despite other studies that stretch the boundaries of that prin-
ciple and verify that it is widely persistent (e.g.,
[6] and [7]),
mobility behavior heterogeneity is recognized to create predic-
tability challenges.
This is particularly important
when it
in-
volves large crowds.
As pointed out by Potier et al.
[8],
even
for well-known big events (e.g.,
Olympic games),
demand is
inevitably more difficult
to forecast
than habitual
mobility,
particularly in the case of
open-gate events.
In facing these
constraints, authorities tend to rely on trial and error experience
(for
recurring events),
checklists
(e.g.,
[9])
and sometimes
invest in a reactive approach rather than planning, as happens in
Germany, with the (real-time traffic and traveller information)
and its active traffic management [10] and in The Netherlands
[11].
However,
such tools have limited applicability,
partic-
ularly for
smaller
and medium events,
which are harder
to
capture and to evaluate.
Calabrese et
al.
[12] use a massive cell-phone data set
to
study public home distributions
for
different
types
of
spe-
cial
events (e.g.,
sports,
concerts,
theatre).
They identified a
strong correlation between public neighborhood distributions
and event types. This is a key finding since it implies that such
heterogeneous cases are still
predictable as long as we have
sufficient
event
information.
They did not,
however,
consider
multiple event interactions or deeper explanatory content (e.g.,
event description text).
B.
Role of the Internet
The Internet
is now the main channel
for public event
an-
nouncements.
Except
for
very small
niches,
organizers that
seek a reasonable audience announce their events on one or
more popular
websites.
This turns the Internet
into the best
source for extracting special
events information.
In addition,
we can explore online popularity features,
such as Facebook
likes or Google trends.
In an earlier work [13],
we compared
an origin/destination (OD) prediction model with and without
simple information obtained from the Internet,
such as event
type or
whether
the performer/event
had a Wikipedia page.
We verified that such information could reduce the root-mean-
squared error by more than 50% in each OD.
This study was
done on a single spatially isolated venue that
had one event
at a time.
When we applied it to more complex locations,
we
verified that a deeper analysis was needed to cope with multiple
concurrent events.
1372
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 3, JUNE 2015
The Internet has served as a valuable source for other aspects
of mobility research.
For example,
Twitter has been used for
opinion mining on public bus [14] and inference of home lo-
cations [15]; points of interest from Foursquare, Yahoo! Local,
and others have supported studies on urban region functions
[16] and job-related trips [17]; and Flickr has been used to study
the geographical distribution of activities (e.g.,
beach,
hiking,
sunset) [18] or to recommend touristic routes [19].
C.
Topic Models
Even if we have all the web pages that announce our events,
a considerable amount
of the relevant
information will
be in
textual form.
To obtain an automated system,
we still need to
convert such data into a proper representation that a machine
can understand.
Explicitly including the text,
word by word,
in a machine learning model
would increase its dimension-
ality much beyond the reasonable.
On the other hand,
hand
coding rules that
find certain “relevant” words (e.g.,
“rock,”
“pop,” “football,” “festival”) would incur in plenty of subjective
judgment
and lack of flexibility.
Natural
language is rich in
synonymy and polysemy,
different
announcers and locations
may use different words,
in addition,
it is not always obvious
which words are more “relevant” from the perspective of
a
machine learning model.
The approach of topic modeling research to these questions
is to re-represent
a text
document
as a finite set
of
topics.
These topics correspond to sets of words that tend to co-occur
together rather than a single word associated to a specific topic.
For example,
a rock festival
textual
description could have a
weight
w
1
assigned to topic 1 (e.g., words related to concerts in
general),
w
2
of topic 2 (e.g., words related to festivals),
w
3
of
topic 3 (e.g., words related to the venue descriptions), and so on.
In particular, we use a specific technique that is called LDA. For
the readers that are familiar with principal components analysis
(PCA),
there is a simple analogy: PCA re-represents a signal
as a linear combination of its eigenvectors,
whereas LDA re-
represents a text as a linear combination of topics.
This way,
we reduce the dimensionality from the total number of different
words of a text to the number of topics, typically very low.
In LDA,
each document
is
represented as
a distribution
over topics,
and each topic is a distribution over words.
For-
mally, given each document
d
defined as a vector
wd
of
n
words,
wd = {w
d,1
, . . . w
d,n
}
and parameter
K
, representing the num-
ber of different topics, LDA assumes the following generative
process.
1)
Draw a
topic
β
k
from
β
k
∼
Dirichlet
(η)
for
k =
1
, . . . , K
.
2)
For each document
d
:
a)
Draw
topics
proportions
θ
d
such
that
θ
d
∼
Dirichlet
(α)
.
b)
For each word
w
d,n
:
i) Draw topic assignment
z
d,n
∼
Multinomial
(θ
d
)
.
ii) Draw word
w
d,n
∼
Multinomial
(β
zd,n
)
.
The parameters
α
and
η
are hyperparameters that indicate,
respectively, the priors on per-document topic distribution and
per-topic word distribution,
respectively.
Thus,
w
d,n
are the
only observable variables, all the others are latent in this mode.
For a set
of
D
documents,
given the parameters
α
and
η
,
the
joint distribution of a topic mixture
θ
, word-topic mixtures
β
,
topics
z
, and a set of
N
words is given by
p(θ, β, z, w|α, η) =
K

k=1
p(β
k
|η)
D

d=1
p(θ
d
|α)
×
N

n=1
(p(z
d,n
|θ
d
)p(w
d,n
|β
k
, k = z
d,n
)) .
Broadly speaking, the training task is to find the posterior dis-
tribution of the latent variables (the per-document topic propor-
tions
θ
d
, the per-word topic assignments
z
d,n
and the topics
β
k
)
that maximize this probability. As with most generative models,
the exact
inference of such values is intractable to compute,
therefore approximate inference techniques are used,
namely
Markov Chain Monte Carlo methods (e.g., Gibbs sampling), or
variational inference, or expectation-maximization. For further
details on this procedure please refer to the original article of
Blei et al. [1] and to practical implementation documents (e.g.,
GenSim [22]).
With a trained LDA topic model, one can apply the same gen-
eral procedure to assign topics to every new document through
Euclidian projection on the topics [1], which is generally a very
fast procedure.
A final remark relates to the document representation that is
typically adopted for LDA and similar techniques, known as the
bag-of-words representation.
Having a dictionary with W dif-
ferent words, this representation translates each document into
a vector with dimensionality W,
where each element contains
the frequency of a dictionary word observed in the document.
This technique obviously disregards the original order of words
in the text, being based purely on word counts.
D.
Hierarchical Models
A hierarchical
model
aims
to capture effects
at
two or
more levels [23].
The top level
represents the most
general
parameters (e.g.,
global
mean and intercept),
and the lower
levels introduce effects specific to subpopulations. In our case,
we first
decompose a hotspot
impact
into a nonexplainable
and an explainable component. The nonexplainable component
represents all excessive demand for which we cannot find ex-
planations online. Its existence is more obvious in days without
any events.
3
At the second level, the explainable component is
expanded into a summation of individual event contributions.
Since this model is a summation of several individual sub-
models, it is an additive model. We apply the Bayesian frame-
work to estimate its parameters, using the Infer.NET platform
[2], hence the title Bayesian hierarchical additive model.
III.
I
DENTIFYING
O
VERCROWDING
H
OTSPOTS
There is
no golden rule threshold above which we can
identify overcrowding.
The intuition is that
it
should happen
3
This does not correspond to the residual on a regression model since we do
not assume it to be normally distributed with 0 mean.
PEREIRA et al.: EXPLAINING NONHABITUAL TRANSPORT OVERCROWDING WITH INTERNET DATA
1373
Fig. 1.
Overcrowding hotspots detection and measurement.
whenever the supply (e.g.,
buses) is insufficient to satisfy the
demand (e.g.,
travelers),
which leads to very heavily loaded
vehicles
or,
ultimately,
to denied boardings.
The latter
are
nonobservable from our data set, so are estimates of bus or train
loading; therefore, we resort to indirect measurements such as
total number of arrivals.
In order to cope with demand fluctuations, transport systems
are generally designed with reasonable spare capacity; thus, we
need to define the point above which we consider it under stress.
For any given study area and point in time, we define such point
to correspond to the 90% percentile, i.e., whenever the number
of arrivals exceeds such threshold, we consider that overcrowd-
ing is occurring. This threshold choice is based on our intuition
and experience together with discussions with local experts, not
being attached to a strong theory or experimental study. How-
ever, our main contribution is methodological and all principles
should remain the same, either by choosing another threshold or
detecting hotspots differently (e.g.,
sensing denied boardings,
monitoring bus load).
We quantify the impact by summing up the excess amount
of arrivals above the median line in a continuous time frame,
discretized by 30-min intervals.
Fig.
1 visualizes this calcu-
lation.
On 24-12-2012,
there were three hotspots in this area
(Singapore Expo). In fact, there were two simultaneous events
during several
hours (Megatex,
related to IT and electronics;
and Kawin–Kawin Makan–Makan 2012, an event about Malay
food and lifestyle products).
Whenever hotspots are both short
in time and with small
relative impact
(e.g.,
below 5% of
the mean,
only 30 min),
we remove them as they should not represent a problem from
transportation management perspective.
Our data set consists of 4 months of smartcard public trans-
port
data from Singapore’s EZLink system.
This is a tap-in/
tapout system both for buses and subway (MRT), which means
we can infer both departure and arrival locations for any trip.
For the purposes of this specific study,
we selected trips that
start/end in three areas that
are sensitive to multiple special
events:
Stadium;
Expo;
and Esplanade.
The Stadium area is
TABLE I
G
ENERAL
S
TATISTICS
: A
VERAGES
(+ − σ)
AND
T
OTALS
dominated by two venues,
the Singapore Indoor Stadium and
the Kallang theatre. The Expo consists of a single large venue,
but
it
is common to host
multiple simultaneously (and unre-
lated)
events.
The Esplanade has 47 venues and is a lively
touristic area near the business district. It has several shopping
malls nearby and sits in front
of
the iconic marina bay of
Singapore.
Table I shows some descriptive statistics from these areas.
IV.
R
ETRIEVING
P
OTENTIAL
E
XPLANATIONS
F
ROM THE
W
EB
For
each overcrowding hotspot,
we want
to find a set
of
candidate explanations from the web.
Two general techniques
exist
to capture such data automatically,
namely,
application
programming interfaces (APIs) and screen scraping. The choice
entirely depends on the website.
Some websites provide an
exhaustive API that we can use to retrieve the data, otherwise
we need to resort
to individual
calls,
page by page (screen
scraping).
Either way,
access may be restricted or prohibited
by terms of
service;
therefore,
the we implement
individual
event
data retrievers for each website whenever it
is so per-
mitted. We use five different websites: eventful.com, upcoming.
org, last.fm, timeoutsingapore.com, and Singapore Expo’s own
website (singaporeexpo.com.sg).
For potential duplicates that share the same venue/area and
day,
we use the Jaro–Winkler
string distance [24]
with a
conservative threshold (e.g.,
>
85% similarity) to identify and
merge them. Whenever we find different textual description, we
concatenate them.
Each event
record contains title,
venue,
web source,
date,
starttime,
endtime,
latitude,
longitude,
address,
URL,
descrip-
tion,
categories,
and when available the event
price.
Unfor-
tunately,
this information also contains plenty of
noise.
For
example,
the majority of
start
and end times
is
absent
or
“default” (e.g., from 00:00 to 23:59), and the same sometimes
happens with latitude/longitude (e.g.,
center of the map).
The
latter can be corrected by using the venue name,
but
for the
former,
we could not
determine any particular
times.
As a
consequence,
each such event is potentially associated to any
impact hotspot of the corresponding day and area.
The description text
is run through a LDA process as ex-
plained in Section II-C. One key parameter for this process is
the number of topics. After trying with a range of values, from
15 to 40, the value that yielded the best model results was 25.
We will assume this value for the remainder of this paper. The
other
parameters,
i.e.,
α
and
η
priors,
were kept
as default
(1.0/(number of topics)). To understand whether this was a safe
choice, we ran several iterations with different initial
α
and
η
priors and they generally converged to similar outcomes.
1374
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 3, JUNE 2015
TABLE II
G
ENERAL
S
TATISTICS ON
D
ATA
F
ROM THE
I
NTERNET
For each event,
we also capture two online popularity indi-
cators,
namely the number of Facebook likes and the number
of
hits in Google of
the event
title query.
We retrieve the
Facebook page with a semi-automatic procedure: we follow the
event URL (which is sometimes a Facebook page) in search of
candidate pages.
Whenever there is more than one candidate,
we manually select the correct one. For Google hits, we search
with the event
title within and without
quotes (yielding two
separate features).
In Table II, we summarize a few statistics of this data set.
We can see that
the most
comprehensive ones are eventful
and timeout, whereas the one with more detailed descriptions is
upcoming. Expo homepage and last.fm have much less, yet very
directed information, the former contains all events that happen
in Expo (thus a relevant filter in itself), whereas the latter is only
focused on music events.
V.
B
AYESIAN
H
IERARCHICAL
A
DDITIVE
M
ODEL
The Individual
Event
contributions are not
observed (i.e.,
they are latent),
but
we do know that
they contribute to the
global
observed impact.
We will
also assume that
individual
impacts are mutually exclusive (e.g., no one attends two events)
and independently distributed and that there will be a parcel that
is unexplainable,
i.e.,
some trips will neither be related to the
extracted events nor to the usual commuting patterns. Thus, we
say that a hotspot impact,
h
,
is given by
4
h = a + b
,
where
a
is the nonexplainable component and
b
is the explainable one.
b
is itself a summation of the
k
events,
e
k
. Formally, we define
a
and
b
as
a ∼ N (α
T
x
a
, σ
a
)
(1)
b =
K

k=1
e
k
,
with
e
k
∼ N (β
T
x
e
k
, σ
k
)
(2)
where
x
a
,
α
, and
σ
a
are the attributes, parameter vectors, and
variance for the nonexplainable component
a
, respectively.
The explainable part,
i.e.,
b,
is determined itself by a sum
of event
contributions,
i.e.,
e
k
,
the second level
of our linear
model.
Each
x
ek
corresponds to the individual
attributes of
event
k
(e.g.,
topic-assignments,
categories,
Facebook likes,
etc.), and
β
and
σ
k
correspond to the event attributes’ param-
eters and the variance associated with that event, respectively.
Notice that, at both levels, we assumed a Gaussian distribution
for the nonexplainable and individual event contributions.
4
For clarity of notation, we will simplify the full notation,
h
r
,
j = a
r,j
+
b
r,j
as
h = a + b
, throughout the article, where r would be the area index, and
j
the hotspot index.
Fig. 2.
Plate notation for our model. Legend: tod
=
time of day; dow
=
day
of week; dist
=
distance of bus/train stop to venue; cats
=
categories; topics
=
lda topics;
ghits
=
Google hits;
likes
=
Facebook likes;
a =
nonexplainable
part;
e
k
=
explaining components.
w
i
and
α
j
=
are model parameters.
The functional form of the components
a
and
e
k
follows a
linear model;
however,
these could be replaced by any other
nonlinear form.
This linear formulation will
be kept
for this
paper, whereas we will leave the extension to nonlinear ones for
future work. Note that the general diagram (of Fig. 2) will still
hold,
whereas only functional form of individual components
need to be changed.
For
the remainder
of
this section,
we apply the Bayesian
framework [25], which relies on three concepts: the likelihood
function, or the probability that a model with a specific set of pa-
rameters predicts the observed data; the prior, which represents
assumptions with respect to model components (e.g., variables,
parameters); and the posterior, which provides the probability
distribution of
the model
parameters
or
other
variables
of
interest
after
observing the data.
A major
advantage of
this
framework is that, for each variable of our model, it will provide
a distribution of values as opposed to a single estimate.
For
example,
the estimation of a classical linear regression model
will
result
on a set
of individual
parameter values and each
prediction made will consist of a single new value, whereas its
Bayesian counter-part will result in a probability distribution of
values for the parameters and for the predictions themselves.
Of course,
one can resort only to the most probable values
of each distribution and reduce it
to a non-Bayesian model,
however in some cases the information lost would be precious.
In our particular example,
simply choosing the most probable
values for the parameters we can straightforwardly obtain pre-
dictions for the totals, as in a classic linear regression. However,
we can go further by taking advantage of the distributions of
the parameters to obtain also a distribution of values for totals
as opposed to a single point estimate.
Since we actually have
these observed totals, we can know how well our model is tuned
(a good model should provide high probability to the observed
value). More importantly, we can use this information to revisit
the parameter distributions again to select values that are more
consistent
with the totals.
In practice,
for
each hotspot,
the
observed totals work together with the parameter distributions
to adapt
the model
to the most
likely values.
This feedback
mechanism is possible with the Bayesian framework and is
embedded in the Infer.NET platform [2].
PEREIRA et al.: EXPLAINING NONHABITUAL TRANSPORT OVERCROWDING WITH INTERNET DATA
1375
The advantages and challenges of the Bayesian framework in
comparison with other machine learning approaches have been
discussed extensively elsewhere and are beyond the scope of
this paper. To know more, we strongly recommend the book of
Christropher Bishop [25].
In Fig. 2, we graphically present our model. Arrows indicate
conditional dependence (e.g.,
a depends on
x
a
),
nodes corre-
spond to variables. Some are observed (e.g., the sum,
h
), others
are nonobserved (e.g.,
event contributions
e
k
).
Rectangles,
or
plates,
are used to group variables that
repeat
together.
This
representation is known as plate notation [26].
We recall that
our main goal is to obtain the values for
a
and
e
k
and that they
sum up to
h
. This relationship can be represented through their
joint probability distribution
5
p(h, a, e|α, β, X) = p(h|a, e)p(a|α, x
a
)
K

k=1
p(e
k
|β, x
e
k
)
(3)
where we defined
e={e
1
, . . . , e
K
}
and
X={x
a
, x
e1
, . . . , x
eK
}
for compactness.
It
may be helpful
to notice the relationship
between Fig. 2 and the expansion on the right-hand side of the
equation,
where we can see the conditional dependences.
The
likelihood function for the observed sum
h
is
p(h|α, β, X) =
 
p(h, a, e|α, β, X) da de
=
 
p(h|a,e)p(a|α, x
a
)
K

k=1
p(e
k
|β, x
e
k
) da de.
By making use of the Bayes rule,
we can define the joint
posterior
p(α, β|h, X) =
p(h|α, β, X)p(α)p(β)
 
p(h|α, β, X)p(α)p(β) dα dβ
.
(4)
The integral in the denominator is the normalization factor
and
p(α)
and
p(β)
are the priors, which will follow a standard
Gaussian distribution
(N (
0, 1
))
.
We can finally estimate the posteriors for
a
and
e
as
p(a|h, X) =

p(a|α, X)p(α|h, X) dα
(5)
p(e|h, X) =

p(e|β, X)p(β|h, X) dβ
(6)
where we can use (1) and (2) for
p(a|α, X)
and
p(e|β, X)
, re-
spectively, and
p(α|h, X) =

p(α, β|h, X) dβ
and
p(β|h, X) =

p(α, β|h, X) dα
.
We implemented this model
in the Infer.NET framework
[2],
which has the necessary approximate Bayesian inference
and Gaussian distribution treatment
tools that
help make it
computationally efficient. We made our code freely available.
1
5
Since
e
deterministically dictates
b
, we replaced
b
by
e
from the beginning.
TABLE III
S
YNTHETIC
D
ATA
R
ESULTS
VI.
M
ODEL
V
ALIDATION
A.
Synthesized Data Experiments
As happens in many other cases (e.g., aggregated cellphone
statistics), we have access to total values but not to the individ-
ual contributions.
So,
how to validate our model,
then? First,
we need to test
the model
as if
we had observed individual
contributions.
We do this by generating simulated data that
complies with our assumptions. Afterward, in the next section,
we test
how well
our
model
fits
with respect
to the total
(observed) values.
If
we cluster
the events data set
(from Section IV)
using
the events characteristics,
we end up with sets of events that
are somehow related in feature space.
Let
us
assume that
each cluster centroid is assigned its own impact,
manually or
randomly.
This value represents the impact
of a hypothetical
event,
which does not necessarily exist in the database.
Now,
let
us assign impacts to the real
events using the distance to
their cluster centroid,
i.e.,
c
.
For each event
e,
its impact
is
determined by
dist(e, c)
−1
.
With this procedure, we are not forcing our model structure
into the data (i.e., we are not assigning specific parameter values
to
α
and
β
, we do not even know if there are such parameters,
which are able to fit
the simulated values),
instead we use
similarity between events to introduce consistency,
regardless
of area or day.
The individual
impacts of simultaneously occurring events
are added up and the resulting sum is perturbed according to
some percentage of noise
(N (
0, 0.1
∗ b))
.
The final
result
is
provided to our
model
as the observed hotspot
impact.
The
obtained individual impacts are then compared with the ground
truth (simulated) values according to three error statistics: the
mean absolute error (MAE) provides the absolute magnitude of
the error for each impact; the root relative squared error (RRSE)
shows the quality of the model
relative to a naive predictor
based on the average of all
observations for that
venue;
the
correlation coefficient (CC) gives an insight on how our model
results are correlated with the ideal results.
Table III shows the results for the areas of Stadium,
Expo,
and Esplanade.
Our model has different performance throughout the differ-
ent areas. In Stadium, it is able to replicate particularly well the
contributions,
which is not
surprising since this area is more
homogeneous than the others (often with only one event in a
day).
Despite being much more heterogeneous,
in both Expo
and Esplanade, the model can still achieve a significant CC and
considerably outperform the average based predictor.
B.
Real Data Experiments
The observations that
we have consist
of total
hotspot
im-
pacts according to Section III. We now want to test our model’s
capability of
recovering such aggregated impacts
without
1376
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 3, JUNE 2015
TABLE IV
R
EAL
D
ATA
R
ESULTS
knowing the individual
impacts,
it
will
only count
with the
known features such as location,
day of
week,
event
type,
topics, etc. (vectors
x
a
and
x
ek
, as described in Fig. 2). We do
this by first estimating the parameters (
α
and
β
) with a subset
of the observations (training set) then generating the aggregated
hotspot impacts with the remaining subset (test set). We apply
the tenfold cross-validation methodology (see,
e.g.,
[25]).
We
use the same error metrics as in the previous section. Table IV
shows the summary of the results.
We remind that a hospot can span through many consecutive
hours, which may lead to very large arrival totals, particularly
in the Expo and Esplanade areas. Thus, the relevance of MAE is
difficult to assess. On the other hand, for these areas, the values
for RRSE and CC indicate that
the model
is able to provide
good performance,
whereas for the Esplanade the results are
less conclusive.
Notice that
this task is not
what
the model
was designed
for.
The total
observed sum is not
a deterministic constraint
anymore, now it becomes an extra unknown. However, this ex-
ercise serves the validation of our model by allowing us to com-
pare the sum totals (now estimated) with the observed ground
truth. Notwithstanding this more complicated task, it is able to
approximate the totals well in two of the cases (Stadium and
Expo).
If our model assumptions were wrong,
the predictions
should be considerably off, because the magnitude of the totals
varies according to the time duration of the hotspot and because
the individual event proportions could be wrong.
The specific
Esplanade case will be analyzed in the following section.
VII.
E
XPLAINING
H
OTSPOTS
The ultimate goal
of our algorithm is to break down each
overcrowding hotspot
into a set
of explanatory components.
Here, we present the results for our entire data set. Before, we
have validated individual component predictions through a syn-
thetic data set and the aggregated totals with the observations.
This time, however, we do not have observations on individual
events.
And even if we had access to individual participation
data (e.g., through ticket sale statistics), it would not necessarily
reveal
the correct
numbers of public transport
users for that
specific event. Thus, our evaluation will now be qualitative.
Figs.
3–5 illustrate some of the results.
6
For each hotspot,
we show the global
impact
(inner circle) and the breakdown
(outer circle). The area size of the inner circle is relative to the
maximum hotspot impact observed in that location in our data
set. The outer circle will contain as many segments as potential
explanatory events plus the nonexplainable component (in red).
For example,
on 201211-10,
Expo had a high impact hotspot
(top left diagram in Fig. 3) comprised of eight different events,
with roughly the same size. The nonexplainable component was
6
Full set in https://dl.dropboxusercontent.com/u/1344277/PereiraEtAl2014.
zip.
Fig. 3.
Twelve events from Expo area.
Fig. 4.
Twelve events from Stadium area.
PEREIRA et al.: EXPLAINING NONHABITUAL TRANSPORT OVERCROWDING WITH INTERNET DATA
1377
Fig. 5.
Twelve events from Esplanade area.
in fact
small
(red segment).
Differently,
on 2012-11-19,
the
same area had 2 events,
one of which explains almost half of
a relatively small hotspot, if comparing with the previous case.
For Stadium and Expo,
we can see that the nonexplainable
component
is generally smaller than the explainable one and
that the breakdown is not evenly distributed. This happens be-
cause the model maximizes consistency across different events.
For
example,
two similar
events in two occasions will
tend
to have similar impacts although the overall totals and sets of
concurrent events may be different.
Cases with multiple hotspots in the same day are interesting
to analyze. For example, in Fig. 3, Expo had 3 hotspots on 2012-
11-11,
with minor fluctuations on the impacts and individual
breakdowns.
There were 10 different
medium-sized events
(3 sale events, 2 movie and anime industry events, 1 parenthood
and 1 pet
ownership event,
2 home furniture and decoration
related exhibits) that spanned throughout the day. Differently, in
Stadium (see Fig. 4), the hotspots for 2013-02-22 have totally
opposite behaviors.
This was a fanmeet
event
with a Korean
music and TV celebrity,
which started at 20:00 (we note that
the largest impact is between 17:30 and 21:00). While the algo-
rithm is confident in the first hotspot, it does not assign the same
explanation to the second one and leaves it mostly unexplained.
The case of Esplanade (see Fig.
5) shows unclear patterns
as our algorithm was generally unable to go beyond an even
breakdown. In fact, a careful look at the data shows that there
are sometimes multiple small events being announced for that
area,
from game watching nights at
bars to theatre sessions.
Outliers do exist (e.g., opera concerts) but the algorithm would
probably need more such cases to extract them.
Nevertheless,
it
shows capability of ruling out
some as insignificant
events
(it assigns 0 impact to them).
Fig. 6.
Impact breakdown for Expo 2012-12-24 (same as Fig. 1).
Fig. 7.
Impact breakdown for Stadium 2012-11-25.
Let us now analyze a few cases in detail, In Fig. 6, we show
the hotspot breakdown of Fig. 1 according to our model. We no-
tice that it was Christmas eve and there were two events: Mega-
tex, an IT and electronics fair; Kawin–Kawin Makan–Makan, a
Malay products event. Our model proposes that the majority of
the impacts relate to the electronics event, which is intuitively
plausible, particularly on the day before Christmas and knowing
that Singapore has a well-known tech-savvy culture.
In Fig. 7, we show the breakdown of a single hotspot, from
12:30 to 14:30 (the other 2 were filtered out due to small impact
and duration).
This was a tennis event,
“Clash of Continents
2012,” and people arrived mostly for the last final matches. The
“Dance drama opera warriors” was held at 20:00 at the Kallang
1378
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 16, NO. 3, JUNE 2015
Fig. 8.
Impact breakdown for Esplanade 2012-11-23.
theatre.
Intuitively,
there is no surprise that
an international
sports event
attracts more people than a classical
music one.
This is an example where the text description can play a role. If
it were a pop concert (also music) and a local basketball game
(also sports), the results could be drastically different.
Finally,
Fig.
8 represents again the most
challenging case
for our model, the Esplanade. Kalaa Utsavam is an Indian arts
festival that has several events that,
aggregated together,
gen-
erate the largest impact.
Intuitively,
this is plausible given the
presence of Indian origin population and culture in Singapore.
However,
the results are very clear.
For example,
“Ten years
shooting home” is a photography contest
event
that
may not
have brought nearly as many people as the “International Con-
ference on business management
and information systems.”
Regardless of this subjective analysis, given our analysis of the
model and data (e.g., see Fig. 5 and Table IV), a longer timeline
and an improved data cleaning should increase the quality of
this model.
VIII.
D
ISCUSSION
Our model
was designed to explain hotspots that
were al-
ready observed,
but
it
can be used as
a demand predictor
as
could be seen in real
data experiments
(Section VI-B).
However,
to properly fit that purpose,
it needs to be extended
with a few key features:
a time series component,
to cope
with seasonality effects;
a habitual
behavior
component,
to
also account
for
the baseline demand;
a spatial
correlations
component,
to deal
with demand variations
in other
areas
(e.g., competing simultaneous events). Each of these extensions
deserve particular attention and are more data greedy (e.g., need
for
larger
time window;
information about
school
holidays,
weather forecast) and changes to the model itself.
The current model is linear at both levels due to our design
choice.
It
is simple to estimate yet
it
contains all
necessary
components to prove our
concept.
However,
the problem at
hand lends itself to nonlinearities.
For example,
online popu-
larity will hardly have a linear relationship with real demand
(e.g., an artist with millions of likes/Google hits may not attract
proportionally more people than another one with thousands).
One of our next steps is to extend the model with a Gaussian
Processes component at the second level (individual impacts).
The definition and quantification of
hotspots
is
also an
arguable yet
flexible component
of
our
methodology.
With
negligible changes other than data availability, we can apply it
to breakdown influence of events in trips by origin/destination,
bus lines,
different
mode (e.g.,
taxi),
or even go beyond the
transport
domain (e.g.,
cell-phone usage,
food consumption,
credit card usage, water, energy). Generally, the model applies
to any analysis of large crowds,
aggregated both in time and
space, under the assumption that these are partially caused by
events announced on the web.
IX.
C
ONCLUSION AND
F
UTURE
W
ORK
We have presented a machine learning model
that
breaks
aggregated crowd observations into their constituent explana-
tory components.
We extract candidate explanations from the
Internet under the assumption that, except for habitual behavior
(e.g.,
commuting),
such crowds are often motivated by public
events announced in the Web. Since we do not have individual
event
observations,
we treat
them as nonobserved,
or latent,
variables and rely on the total sum to constrain their estimation.
This model has a two-layer structure,
each one is a sum of
components. At the top level, we consider explainable and non-
explainable components,
and at the lower level,
we disaggre-
gate the explainable component into its candidate explanations
retrieved from the Internet.
We tested this hierarchical additive model on a public trans-
port
data set
from the city-state of
Singapore.
We identi-
fied overcrowding hotspots by comparing the observed people
counts (bus or subway arrivals) with a conservative threshold
(90% quantile) at 30-min intervals. We quantified the hotspots
by summing up consecutive “excessive” counts. For each such
hotspot,
we retrieved the potential
explanations from several
event announcement websites and extracted relevant available
information such as event title, category, venue, and description
among others.
We applied LDA [1] to extract topics from the
text descriptions.
All these features were organized together in our Bayesian
hierarchical
additive model,
which was implemented on the
Infer.NET framework [2]. Results with synthetic data show that
the model
is able to retrieve the correct
results with a CC of
at
least
85% and a RRSE below 56%.
Results with real
data
show that the same model is able to recover the observed total
impacts with a CC from 41.2% to 83.9% and RRSE from 55%
to 85%, even though this is a harder task than what the designed
was built for. A qualitative analysis on a case study in Singapore
PEREIRA et al.: EXPLAINING NONHABITUAL TRANSPORT OVERCROWDING WITH INTERNET DATA
1379
shows that the results of the hotspot impacts breakdowns into
different possible explanation are intuitively plausible.
A
CKNOWLEDGMENT
The authors would like to thank the Land Transport Authority
of Singapore for providing the data collected from the EZLink
system.
The authors also thank S.
Robinson,
A.
Ghorpade,
and Y. Kim for their helpful reviews and comments.
R
EFERENCES
[1]
D.
M.
Blei,
A.
Y.
Ng,
and M.
I.
Jordan,
“Latent
Dirichlet
allocation,”
J. Mach. Learn. Res., vol. 3, pp. 993–1022, Mar. 2003. [Online]. http://dl.
acm.org/citation.cfm?id=944919.944937
[2]
T. Minka, J. Winn, J. Guiver, and D. Knowles, Infer.NET 2.5, Microsoft
Research Cambridge, 2012. [Online]. Available: http://research.microsoft.
com/infernet
[3]
M. C. Gonzalez, C. A. Hidalgo, and A.-L. Barabasi, “Understanding indi-
vidual human mobility patterns,” Nature, vol. 453, no. 7196, pp. 779–782,
Jun. 2008. [Online]. Available: http://dx.doi.org/10.1038/nature06958
[4]
B. Adams and D. D. Kapan, “Man bites mosquito: Understanding the con-
tribution of human movement to vector-borne disease dynamics,” PLoS
ONE, vol. 4, no. 8, p. e6763, Aug. 2009.
[5]
G. Xue, Z. Li, H. Zhu, and Y. Liu, “Traffic-known urban vehicular route
prediction based on partial
mobility patterns,” in Proc.
15th ICPADS,
2009, pp. 369–375.
[6]
C.
Song,
Z.
Qu,
N.
Blumm,
and A.-L.
Barabsi,
“Limits of predictabil-
ity in human mobility,” Science,
vol.
327,
no.
5968,
pp.
1018–1021,
Feb.
2010.
[Online].
Available: http://www.sciencemag.org/content/327/
5968/1018.abstract
[7]
S.
Jiang et al.,
“A review of urban computing for mobile phone traces:
Current
methods,
challenges
and opportunities,” in Proc.
2nd ACM
SIGKDD Int. Workshop Urban Comput., 2013, p. 2.
[8]
F. Potier, P. Bovy, and C. Liaudat, “Big events: Planning, mobility man-
agement,” in Proc. ETC, Strasbourg, France, 2003.
[9]
“Planned special events: Checklists for practitioners,” U.S. Dept. Transp.,
Federal Highway Admin., Office Transp. Manag., Washington, DC, USA,
2006.
[10]
F.
Bolte,
“Transport
policy objectives:
Traffic management
as suitable
tool,” Federal Highway Res. Inst. (BASt), Bergisch-Gladbach, Germany,
2006, Tech. Rep.
[11]
F.
Middleham,
“Dynamic
traffic
management,”
Ministry
Transport,
Public Works,
Water
Manag.,
Directorate-Gener.
Public Works Water
Manag., AVV Transport Res. Centre, Rotterdam, The Netherlands, 2006,
Tech. Rep.
[12]
F.
Calabrese,
F.
Pereira,
G.
Di
Lorenzo,
L.
Liu,
and C.
Ratti,
“The ge-
ography of taste:
Analyzing cell-phone mobility and social
events,” in
Pervasive Computing,
vol.
6030,
Lecture Notes in Computer Science,
P.
Floréen,
A.
Krüger,
and M.
Spasojevic,
Eds.
Berlin,
Germany:
Springer-Verlag, 2010, pp. 22–37.
[13]
F.
C.
Pereira,
F.
Rodrigues,
and M.
Ben-Akiva,
“Internet
as a sensor:
Case study with special events,” in Proc. 91st Transp. Res. Board. Meet.,
Washington, DC, USA, 2012, pp. 1–11.
[14]
L. A. Schweitzer, “How are we doing? opinion mining customer sentiment
in U.S. transit agencies and airlines via twitter,” in Proc. 91st Transp. Res.
Board, Washington, DC, USA, 2012, pp. 1–12.
[15]
R. Li, S. Wang, H. Deng, R. Wang, and K. C.-C. Chang, “Towards social
user profiling:
Unified and discriminative influence model
for inferring
home locations,” in Proc. 18th ACM SIGKDD Int. Conf. Knowl. Discov.
Data Mining, 2012, pp. 1023–1031.
[16]
J. Yuan, Y. Zheng, and X. Xie, “Discovering regions of different functions
in a city using human mobility and POIS,” in Proc. 18th ACM SIGKDD
Int.
Conf.
Knowl.
Discov.
Data Mining,
New York,
NY,
USA,
2012,
pp. 186–194, ACM.
[17]
F.
Rodrigues et
al.,
“Estimating disaggregated employment
size from
points-of-interest
and census data:
From mining the web to model
im-
plementation and visualization,” Int.
J.
Adv.
Intell.
Syst.,
vol.
6,
no.
1/2,
pp. 41–52, 2013.
[18]
Z.
Yin,
L.
Cao,
J.
Han,
C.
Zhai,
and T.
Huang,
“Geographical
topic
discovery and comparison,” in Proc.
20th Int.
Conf.
World Wide Web,
2011, pp. 247–256.
[19]
Y. Sun, H. Fan, M. Bakillah, and A. Zipf, “Road-based travel recommen-
dation using geo-tagged images,” Comput.,
Environ.
Urban Syst.,
2013,
to be published.
[20]
A.
Madan,
K.
Farrahi,
D.
Gatica-Perez,
and A.
S.
Pentland,
“Pervasive
sensing to model
political
opinions in face-to-face networks,” in Proc.
Perv. Comput., 2011, pp. 214–231.
[21]
D. Phung, B. Adams, K. Tran, S. Venkatesh, and M. Kumar, “High accu-
racy context recovery using clustering mechanisms,” in Proc.
IEEE Int.
Conf. PerCom, 2009, pp. 1–9.
[22]
R.
ˇ
Reh
ˇ
rek and P.
Sojka,
“Software framework for topic modelling with
large corpora,” in Proc.
LREC Workshop New Challenges NLP Frame-
works, Valletta, Malta, May 2010, pp. 45–50.
[23]
S.
W.
Raudenbush,
Hierarchical Linear Models: Applications and Data
Analysis Methods, vol. 1.
Newbury Park, CA, USA: Sage, 2002.
[24]
W. E. Winkler, “String Comparator Metrics and Enhanced Decision Rules
in the Fellegi-Sunter Model
of Record Linkage,” Proc.
Section Survey
Res. 1990.
[25]
C.
M.
Bishop,
Pattern Recognition and Machine Learning (Information
Science and Statistics).
Secaucus, NJ, USA: Springer-Verlag, 2006.
[26]
D.
Kollar and N.
Friedman,
Probabilistic Graphical Models: Principles
and Techniques.
Cambridge, MA, USA: MIT, 2009.
Francisco C. Pereira (M’05) received the Ph.D. de-
gree from University of Coimbra, Coimbra, Portugal.
He is a Senior Research Scientist with Singapore–
Massachusetts
Institute of
Technology (MIT)
Al-
liance for Research and Technology,
Future Urban
Mobility
Integrated
Research
Group
(FM/IRG),
where he researches real-time traffic prediction, be-
havior modeling, and advanced data-collection tech-
nologies. In these projects, he applies his research on
data mining and pattern recognition applied to un-
structured contextual sources (e.g., web, news feeds,
etc.) to extract information that is relevant to mobility. He is also a Professor
with University of Coimbra and a cofounder of Ambient Intelligence Labora-
tory,
University of Coimbra.
He has led the development
of several
projects
that apply these context-mining principles, some with collaboration with other
MIT groups such as the Senseable City Laboratory, where he previously was a
Postdoctoral Fellow.
Filipe Rodrigues is working toward the Ph.D.
de-
gree in information science and technology at Uni-
versity of Coimbra, Coimbra, Portugal, where he is
researching probabilistic models for learning from
crowd-sourced and noisy data.
His research interests include machine learning,
probabilistic graphical models, natural language pro-
cessing, and intelligent transportation systems. He is
also involved in two research projects whose main
goal is to understand the effect of special events in
urban mobility.
Evgheni Polisciuc is working toward the Ph.D. de-
gree in information science and technology at Uni-
versity of
Coimbra,
Coimbra,
Portugal,
where he
researches state-of-the-art surveying and, in parallel,
on visualization models
that
highlight
areas
with
high degree of deviations from the routine in urban
mobility.
He specializes in information visualization, in par-
ticular
studies,
and develops
graphical
models
to
reveal and explain anomalies in urban mobility and
land use.
Moshe Ben-Akiva received the Ph.D.
degree in
transportation systems from Massachusetts Institute
of Technology (MIT),
Cambridge,
MA,
USA,
and
has received four honorary degrees.
He is the Edmund K. Turner Professor of Civil and
Environmental
Engineering,
the Director of MIT’s
Intelligent Transportation Systems (ITS) Laboratory,
and the coauthor
of
the textbook Discrete Choice
Analysis (MIT Press).
