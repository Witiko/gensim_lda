Research Policy 44 (2015) 1145–1159
Contents lists available at ScienceDirect
Research Policy
j o u r n a l
h o m e p a g e :
w w w . e l s e v i e r . c o m / l o c a t e / r e s p o l
Project selection in NIH: A natural experiment from ARRA
Hyunwoo Park
a
, Jeongsik (Jay) Lee
b,∗
, Byung-Cheol Kim
c
a
Georgia Institute of Technology, School of Industrial & Systems Engineering and Tennenbaum Institute, 755 Ferst Drive NW, Atlanta, GA 30332-0205,
United States
b
Drexel University, LeBow College of Business, 3141 Chestnut Street, Philadelphia, PA 19104, United States
c
Georgia Institute of Technology, School of Economics, 221 Bobby Dodd Way, Atlanta, GA 30332–0615, United States
a r t i
c l e
i
n f o
Article history:
Received 4 February 2014
Received in revised form 4 March 2015
Accepted 9 March 2015
Available online 11 April 2015
JEL classiﬁcation:
D2
H0
H5
O3
Keywords:
Public research funding
Project selection
Natural experiment
National Institutes of Health
American Recovery and Reinvestment Act
Revealed preference
a b s t r a c t
Using a natural
experiment in research funding by the National
Institutes of Health (NIH) following
the American Recovery and Reinvestment Act (ARRA) of 2009, we study the NIH’s revealed preference
in project selection. We do so by comparing the characteristics of the projects additionally selected for
funding due to an unexpected increase in resources under the ARRA with those supported through regular
NIH budget. We ﬁnd that the regular-funded projects are on average of higher quality, as measured by
the number of publications per project and the impact of these publications, than ARRA-funded projects.
Moreover, compared to ARRA projects, regular projects are more likely to produce highest-impact articles
and exhibit greater variance in research output. The output from regular projects also seems more closely
ﬁtting the purpose of funding.
The differences in project quality are largely explained by observable
attributes of the projects and research teams, suggesting that the NIH may use these attributes as cues
for discerning underlying project quality. In addition, ARRA projects are more likely than regular projects
to involve investigators with past grant experience. Many of these inter-group differences are speciﬁc to
R01 grants, the largest funding category in the NIH. Overall, these results suggest that the NIH’s project
selection appears generally in line with its purported mission. In particular, our results contrast starkly
with the frequent criticism that the NIH is extremely risk-averse and unwarrantedly favors experienced
investigators. We discuss the implications of our ﬁndings on the NIH’s behavior in project selection.
© 2015 Elsevier B.V. All rights reserved.
1.
Introduction
The National
Institutes of
Health (NIH) as a part of
the U.S.
Department of Health and Human Services is the largest public
source of funding for biomedical and health-related research in the
world.
Few disagree on the institution’s crucial role in improving
human health,
economic growth,
and job creation.
However,
the
adequacy of project selection in the NIH has been more contro-
versial.
For example,
Azoulay et al.
(2011) ﬁnd that the scientists
supported by the Howard Hughes Medical Institute (HHMI), a U.S.
non-proﬁt private biomedical research organization, produce high-
impact
articles at
a much higher rate than a control
group of
similarly accomplished NIH-funded scientists.
It has been argued
that the NIH tends to be extremely risk-averse and peer reviews
are too conservative,
putting greater weights on the likelihood of
success rather than the potential impact of the projects (Zerhouni,
∗
Corresponding author. Tel.: +1 (215) 571 4676.
E-mail addresses: hwpark@gatech.edu (H. Park), jaylee@drexel.edu (J. Lee),
byung-cheol.kim@econ.gatech.edu (B.-C. Kim).
2003; Nurse, 2006). These selection criteria thus tend to put young
and less experienced scientists at a disadvantage in securing grants
from the NIH (Weinberg, 2006). They also generate perverse incen-
tives for scientists to strategically submit proposals that are already
close to completion, rather than their most innovative applications
(Zerhouni,
2003; Nurse,
2006; Stephan,
2012).
To its credit,
the
NIH has actively sought to address these concerns in various ways.
For instance,
it recently began offering “High-Risk High-Reward
(HRHR)” Funding Opportunity Announcements (FOAs) and has ini-
tiated special programs such as the Director’s Pioneer Awards and
New Innovator Awards (Zerhouni,
2003; Austin,
2008).
As their
titles suggest, these initiatives are speciﬁcally designed to promote
highly innovative yet risky research ideas.
Nonetheless,
concerns
seem to remain unabated.
Given the NIH’s enormous inﬂuence on individual
scientists’
career as well as on the national-level innovation in the biomedical
ﬁeld,
it is imperative to ensure that the NIH supports the “right”
projects.
If the above-mentioned allegations were true,
the NIH
may not be doing their job and public resources may be used
inefﬁciently.
It is thus important to accumulate evidence on the
effectiveness of the NIH as a public institution,
particularly given
http://dx.doi.org/10.1016/j.respol.2015.03.004
0048-7333/© 2015 Elsevier B.V. All rights reserved.
1146
H. Park et al. / Research Policy 44 (2015) 1145–1159
their crucial role in biomedical research. However, despite the fre-
quent criticism on the NIH’s review criteria and selection process,
we know surprisingly little about the NIH’s “preference” in its
project selection.
Because the NIH does not disclose how it actu-
ally selects projects,
we simply have little way of conﬁrming or
disproving these allegations,
let alone assessing the institution’s
overall effectiveness.
Hence,
what seems still in order is to ﬁnd a
way to systematically investigate the nature of projects or scientists
that the NIH chooses to support.
This paper is our attempt to do just that.
Speciﬁcally,
we take
a revealed preference approach (Samuelson,
1938) to deduce the
NIH’s preference from the observed funding decisions.
Our logic
behind this approach is as follows. If choices x and y are both eli-
gible for selection and x is chosen over y, then x is revealed to be
at least as good as y. Thus, by comparing the characteristics asso-
ciated with choice x relative to those with choice y, one can infer
the decision-maker’s preference. Several challenges arise in apply-
ing this approach to our context because the following conditions
have to be satisﬁed: (i) projects x and y are both identiﬁed as eligi-
ble for funding when x is revealed preferred to y (i.e., both projects
belong to a feasible set); (ii) y is also funded eventually (so that the
research output from the projects are comparable); and (iii) under-
lying characteristics of x and y are independent of the nature of
the funding resource for y (i.e., the changes in funding resource are
exogenous to the attributes of the projects). The American Recovery
and Reinvestment Act (ARRA) in 2009 offers a natural experiment
that successfully meets these requirements.
In February 2009,
the 111th United States Congress signed on
the ARRA that stipulated outlays of later-revised $831 billion as
the economic stimulus package,
which is the largest single eco-
nomic stimulus program in the U.S.
history.
$10.4 billion of that
package was allocated to the NIH to be spent within two years
from the enactment. Considering the NIH’s annual budget of around
$25–30 billion,
this additional fund was substantial (Steinbrook,
2009).
The NIH accordingly disbursed most of the ARRA fund to
extramural scientiﬁc research in the form of grants ($8.97 billion
to 21,581 projects). In disbursing the ARRA fund, the NIH used two
distinct categories, on top of its regular NIH grants. The ﬁrst cate-
gory is “ARRA Solicited,” under which the NIH selected and funded
projects from competing applications that were newly submitted
in response to the ARRA FOA. Under the second category, “Not ARRA
Solicited,” the NIH selected and funded projects from a pool
of
past applications that “received meritorious priority scores from
the initial peer review process” and “received priority scores that
could not otherwise be paid in FY 2008 or 2009.”
1
In fact, the NIH
explicitly acknowledges that it “extended beyond payline to pick
them up.”
2
In other words,
these second-category projects were
evaluated as having sufﬁcient scientiﬁc merits but,
due to bud-
get constraints,
could not be selected for funding initially; absent
the ARRA enactment,
these would never have been awarded NIH
grants. This second category of projects forms the core of our study.
Under this category,
3869 projects ($1.41 billion) were awarded
NIH grants in FY2009 and 628 projects ($0.32 billion) in FY2010.
The ARRA thus disturbed the NIH funding mechanism temporarily
but substantially.
Given the nature of the event,
the ARRA experiment provides
several
important merits for our approach.
First,
the infusion of
the ARRA fund was exogenous to the NIH’s agenda (Chodorow-
Reich et al.,
2012; Wilson,
2012).
Thus,
the project selection is
unlikely to have been inﬂuenced by some NIH-speciﬁc policy initia-
tives. Second, the ARRA event has accidentally revealed the projects
that were deemed worthy of support but were not funded due to
1
http://grants.nih.gov/grants/guide/notice-ﬁles/not-od-09-078.html.
2
http://report.nih.gov/recovery/NIH ARRA Funding.pdf.
budget
constraints (for
convenience,
we label
them as “ARRA
projects”).
This set of projects thus belonged to the same risk set
as the projects that initially cleared the hurdle and were selected
for funding from the same pool of proposals (we label the latter
as “regular projects”). Third, ARRA projects were also funded later
by the NIH.
Hence,
both groups of projects are subject to a fair
comparison.
Lastly,
because the NIH requires all funded projects
to acknowledge their funding in all publications resulting from the
projects, we can precisely identify the research output and link it
to individual grants.
Exploiting this unusual
setting to study the NIH’s preference
in project selection, we examine the following questions: (i) Does
the NIH select higher quality projects?; (ii) Does the NIH prefer
riskier projects?; (iii) What are the cues that the NIH uses to iden-
tify fundable projects?; (iv) How close are the selected projects to
the intended purpose of funding?; and (v) Does the NIH favor expe-
rienced investigators? In addressing these questions, we also look
at variations across different types of grants.
We test
the ﬁrst
question by examining the differences in
research output
between the two groups,
in terms
of
their
impact (measured by the dollar-adjusted citation rates of
jour-
nal
publications per project) and productivity (measured by the
dollar-adjusted number of journal
publications per project).
We
ﬁnd that regular projects on average produce per-dollar publica-
tions of signiﬁcantly higher impacts (12.5%) than ARRA projects and
that this difference is primarily driven by R01 grants,
the largest
funding category among NIH grants.
We ﬁnd a similar result for
the (dollar-adjusted) number of publications,
and the gap is even
larger: regular projects produce 17.9% more per-dollar publications
than ARRA projects,
and R01 grants seem mainly responsible for
that difference. Overall, regular projects appear to be of consider-
ably higher quality than ARRA projects.
We next explore which group of projects exhibits higher “risk.”
In fact, the NIH claims that it considers risk as an important selec-
tion criterion along with impact (Austin,
2008).
If,
however,
the
NIH is indeed as risk averse as often criticized (Zerhouni,
2003;
Nurse, 2006), the projects selected ﬁrst from the application pool
(i.e., regular projects) would exhibit a smaller variance in the out-
put quality with a lower likelihood of producing tail outcomes than
the additionally selected projects (i.e.,
ARRA projects).
Our anal-
ysis shows that regular projects are signiﬁcantly more likely (by
3%p) than ARRA projects to generate highly successful articles,
as
deﬁned by the probability of belonging to the top 5% of the citation
distribution.
This pattern holds across different grant types.
Reg-
ular projects are also less likely (by 7.1%p) than ARRA projects to
“fail,” as deﬁned by producing zero publications. This, however, is
not simply a mean shift (to the left) under the ARRA because we fur-
ther ﬁnd that the distribution of impact—after accounting for the
differences in FOA,
project start time and mean quality—exhibits
a greater dispersion for regular projects than for ARRA projects.
This difference in variance is not speciﬁc to a particular grant type.
A similar pattern is found for the number of publications at the
aggregate level,
primarily driven by the difference in R01 grants.
Thus, on the whole, regular projects seem to exhibit greater varia-
tions in research output relative to ARRA projects. Taken together,
these ﬁndings contrast starkly with the criticism that the NIH is
too risk-averse (e.g., Zerhouni, 2003). We conﬁrm these results on
a subset of projects that started in the same time period and hence
are less subject to differences in time window.
The next question concerns potential cues that the NIH might
use to determine which projects are more promising and hence
warrant funding among other projects. Obviously, we do not have
data on the actual criteria or check points that the NIH uses to eval-
uate each project. Instead, we employ a set of observable attributes
of projects and investigators and relate them to research output to
see which factor better explains the characteristics of the output.
H. Park et al. / Research Policy 44 (2015) 1145–1159
1147
We ﬁnd that, among other factors, team size, recent grant money
and the institution-level grant award history positively explain the
impact of research output.
Team size is also strongly and posi-
tively related to the quantity-based productivity,
though project
cost seems inversely correlated with productivity.
Interestingly,
when these observable attributes are controlled for,
the differ-
ences in the mean research output between regular projects and
ARRA projects disappear.
In contrast,
the differences in the vari-
ance of research output survive these controls. These suggest that
project- and investigator-level attributes help predict reasonably
well the mean outcomes of the projects—though not the dispersion
thereof—and hence the NIH might be using them as useful cues for
identifying promising projects.
Each FOA has its own objectives of funding.
We thus compare
between the groups the extent to which the project produces an
output that is close to that funding purpose. For this,
we devise a
metric (“research ﬁt”) that quantiﬁes the proximity between the
objectives of an FOA and the content of the resulting publications
from the project funded under the FOA. On this metric, the regular
group of projects exhibits a signiﬁcantly greater research ﬁt than
the ARRA group.
This implies that,
in choosing projects of higher
quality and greater risk,
the NIH does not make a trade-off with
their ﬁt with the funding objectives.
Lastly, we examine if experienced investigators, deﬁned as those
with a record of past NIH grants,
are favorably treated in receiv-
ing NIH grants.
We do this by looking at the probability that the
applicant team with at least one principal
investigator (PI) who
previously received an NIH grant is awarded an ARRA grant,
as
opposed to receiving a regular grant. We ﬁnd that, controlling for
other project attributes, ARRA funding is signiﬁcantly more likely
given to experienced PIs.
This pattern is particularly observed for
R01 grants. The ﬂip-side of this result is that, in the regular fund-
ing cycle,
these experienced PIs are less likely to be selected for
funding, all else equal. This result contrasts with the frequent alle-
gation that the NIH favors PIs with proven records (e.g., Weinberg,
2006). In addition, the grant history of the PIs’ institutions has no
inﬂuence on the probability of an ARRA award, suggesting that the
so-called Matthew effect (Merton,
1968) does not apply to NIH
grants.
Reﬂecting the importance of NIH funding to scientiﬁc research,
the selection process at
the agency has been under academic
scrutiny on various aspects such as researcher ethnicity (Ginther
et al.,
2011) and expertise (Li,
2012),
political inﬂuences (Hegde
and Mowery, 2008; Hegde, 2009), and peer review (Hegde, 2009;
Azoulay et al.,
2012; Nicholson and Ioannidis,
2012).
We add to
this literature by documenting the agency’s revealed preference in
selecting projects; our unique research setting provides a natural
variation that allows for such an attempt. In spirit, our work is close
to Bisias et al. (2012) who assess the efﬁciency of the NIH’s funding
allocation across disease categories by applying modern portfolio
theory to analyze its risk attitude.
More broadly,
we join the recent
policy debate on scien-
tiﬁc research funding (Bourne and Lively,
2012; Fineberg,
2013;
McDonough, 2013). In doing so, our study complements the body of
literature on the effect of public funding on research output (Carter
et al.,
1987; Averch,
1989; Gordin,
1996; Arora and Gambardella,
2005; Jacob and Lefgren,
2011a,b; Benavente et al.,
2012).
3
Our
paper, however, is distinct from these studies in that our primary
focus is not on estimating the effect of funding on research out-
put per se.
Tangentially,
our study is also related to the literature
on the policy evaluation of the ARRA program,
which has so far
3
See Dorsey et al. (2010) and Moses et al. (2005) for overall trends of scientiﬁc
research funding, particularly those of the NIH.
focused almost exclusively on the program’s effect on employment
(Chodorow-Reich et al., 2012; Wilson, 2012).
2.
NIH grants and the ARRA program
2.1.
NIH grants
The NIH is the largest single funding source for biomedical
research in the world, accounting for 28% of the entire U.S. biomed-
ical research (Moses et al.,
2005).
An NIH-funded research led to
development of innovative technologies such as the magnetic res-
onance imaging (MRI),
and 138 NIH-supported researchers won
the Nobel Prize in chemistry,
economics,
medicine,
and physics.
4
Such evidence of achievements supports the view that the NIH is a
critical source of scientiﬁc development and economic growth by
sponsoring academic research in health and igniting private sector
innovation. For instance, Toole (2012) provides empirical evidence
that NIH-funded basic research helps new drug developments in
the pharmaceutical industry.
The NIH is a collective body of 27 institutes and centers (ICs)
such as the National Cancer Institute or the Center for Information
Technology.
Each individual
IC is responsible for administrating
and disbursing research funding focusing on a speciﬁc health prob-
lem domain. Of the $25–30 billion annual budget, more than 80%
is given to outside research communities (called “extramural”
research grants) such as universities, colleges, and private research
institutes.
Besides these extramural grants,
the NIH also operates
its own research laboratories and about 10% of the budget goes to
supporting these “intramural” research activities.
The overall process of NIH funding,
illustrated in Fig.
1,
is the
following.
When the need for study in a speciﬁc area or domain
is identiﬁed,
one of the ICs issues an FOA.
There are two major
types of FOAs: the Request for Applications (RFA) and Program
Announcements (PA). An RFA calls for research proposals in a nar-
rowly deﬁned area of study,
while a PA aims to support projects
researching in a broad area. A researcher (or a team of researchers)
applies for grants upon noticing an FOA. All proposals must be sub-
mitted in response to an FOA.
Researcher-initiated proposals are
also required to refer to a speciﬁc FOA number. Once proposals are
received, the NIH ﬁrst examines through a peer review process the
scientiﬁc merit that each proposal carries.
Reviewers,
selected to
have no conﬂict of interest,
grade each proposal using a 9-point
grading system (in which 1 denotes ‘exceptional’
and 9 ‘poor’).
The NIH provides reviewers with detailed guidelines for grading
proposals. A council meeting then reviews the scores, sets the pay-
line,
and prioritizes projects.
Proposals whose scores fall beyond
the payline are not funded for the term. There are three (occasion-
ally four) council meetings per ﬁscal year; accordingly,
there are
three standard due dates for proposals and review cycles. Depend-
ing on the timing within a ﬁscal year, the payline carries different
weights in selecting projects.
With the ﬁnal budget unapproved
at the beginning of ﬁscal year,
the council sets the payline con-
servatively to prepare for potential high-quality proposals in later
cycles of the year. Most projects are funded and initiated towards
the end of ﬁscal year when the ﬁnal budget is determined. When
the research output supported by NIH grants is published, the NIH
requires the authors to acknowledge the ﬁnancial support by citing
the grant number to their publications. The U.S. Congress mandates
the average length of NIH projects to be four years. Project end date
may be extended only with a prior approval of the NIH, even if the
extension request does not ask for additional funding.
4
http://www.nih.gov/about/ accessed on June 26, 2013.
1148
H. Park et al. / Research Policy 44 (2015) 1145–1159
Scientific Merit 
Review
FOA
(RFA, PA, PAR)
Application
Advisory 
Council Round
Project Start
Project End
Publications
acknowledging
NIH grant
Standard due date
varies across
activities and
application types.
Peer review process
scores applications
using 9-point system
under review guideline.
Payline is decided.
Projects are
selected and
prioritized.
Depending on budget,
NIH starts funding
projects with
high scores.
Average research project
grant duration is,
by mandate, 4 years.
Even no cost extension
requires prior approval.
All applications
must be submitted
in response to
a FOA.
Fig. 1.
Schematic illustration of NIH funding process.
Not all proposals deemed to have scientiﬁc merits are funded.
Although undisclosing to the public, the NIH internally keeps record
of the review scores of the proposals that earned meritorious scores
(i.e., scores that deserve funding) but fell beyond the payline that is
determined by funding availability. Once a proposal is selected and
funded, the applicant becomes the PI of the project responsible to
carry out the proposed research.
Multiple researchers can jointly
submit a proposal, in which case all the applicants can become PIs
with one of them designated as the contact PI.
Funded projects are classiﬁed by their activity and application
type. The activity type, an alphabet followed by two-digit number
or two alphabets followed by a single-digit number, characterizes
the purpose of fund and how it will be spent.
Examples include
R01, R03, and P01. R01, the oldest and largest funding mechanism
of
the NIH,
supports normal-size (∼$500,000) research projects
proposed by investigators. R03 provide relatively smaller amounts
of
fund (<$50,000 per year) to preliminary short-term research
projects with an explicit non-renewal term attached.
P01,
on the
other hand, funds the initiation of a program that addresses a broad
area of biomedical study. The application type is another dimension
of classiﬁcation.
Not all
projects are proposed as new or short-
term.
A funded project that spans more than a year is the norm,
not the exception.
Thus,
every year the PIs of an existing project
must submit a renewal application to secure continued funding.
Renewal applications may or may not go through a competitive
review process, depending on initial terms or other conditions. In
some cases,
a project can request additional funding as adminis-
trative supplements.
All these different types of applications are
labeled accordingly and recorded as separate projects for that ﬁscal
year. These ﬁne-grained classiﬁcation systems and detailed label-
ing information per project allow us to examine differences across
grant types and in some speciﬁcations to control for much of the
unobserved heterogeneity across types of
funding and research
activities.
2.2.
ARRA and NIH funding
In February 2009, the U.S. government earmarked $831 billion
for the economic stimulus package based on the ARRA enacted by
the 111th U.S. Congress. As a result, the US government raised more
than $800 billion and have paid out $290.7 billion for tax beneﬁts,
$254.5 billion for contracts, grants, and loans, and $250.8 billion for
entitlements.
5
One of the ﬁve main purposes of the ARRA stated in
the Act
6
is to make “investments needed to increase economic efﬁ-
ciency by spurring technological advances in science and health.”
The law also explicitly directs to “commence expenditures and
activities as quickly as possible.” This single largest fund ﬂowed into
5
http://www.recovery.gov/ as of May 31, 2013.
6
http://www.gpo.gov/fdsys/pkg/PLAW-111publ5/pdf/PLAW-111publ5.pdf.
the U.S. economy through many government agencies including the
NIH as part of the Department of Health and Human Services. Fig. 2
describes the ARRA timeline for NIH-related events, some of which
stress the urgency of expending the fund to stimulate the economy.
The NIH was appropriated to allocate $10.4 billion,
of which
$8.97 billion was spent
as extramural
research grants.
Among
these,
$2.71 billion (30.2%) were awarded through ARRA-speciﬁc
funding opportunities such as Challenge Grants and Grand Oppor-
tunity Grants.
$1.93 billion (21.5%)
were granted to existing
projects as administrative supplements and $2.31 billion (25.8%)
were awarded to ARRA-funded projects taking more than 2 years
via noncompeting continuation mechanism.
A notable awarding
mechanism,
which is the focus of our study,
that allocated $1.73
billion (19.3%) exclusively targeted the previously reviewed appli-
cations that had been submitted to funding opportunities unrelated
to the ARRA.
The NIH released the notice on April 3, 2009 stating that it would
consider funding proposals that had previously been reviewed and
earned meritorious scores, but had not been funded. All but a few
of these proposals had been submitted to an FOA unrelated to the
ARRA.
Thus,
these researchers had submitted proposals without
knowing that the NIH would soon obtain substantial
amount of
additional funding which must be expended “as quickly as possi-
ble.” The NIH awarded this fund to the proposals that had received
meritorious scores from the review but had fallen below the pay-
line in ﬁscal years of 2008 and 2009. In effect, the NIH temporarily
extended beyond the payline to “pick up” projects that would not
have been funded without payline extension,
and utilized some
of
the ARRA funding to support these projects.
This temporary
shift,
triggered by an exogenous event,
incidentally revealed the
proposals around the margin of the payline. Fig. 3 illustrates how
additional funding from the ARRA triggered payline extension and
which group of projects were affected. As detailed in the next sec-
tion, these additionally selected projects under the ARRA along with
the projects selected under regular funding cycles for the same
FOAs collectively form the subject of our empirical investigation.
3.
Data and sample
The project-level funding data come directly from the NIH. The
NIH makes its research activities publicly available in line with the
open government initiatives to ensure transparency in its opera-
tion. It provides a web interface for the public to browse the funded
projects, as well as a bulk download channel for those who want to
conduct a more systematic analysis. The entire project funding data
span from 1985 to 2012. Our main dataset focuses on the projects
funded in ﬁscal
years 2009 and 2010,
though we utilize project
records in previous years to construct some of our variables. Each
project record contains ﬁscal year, project number, administrating
IC, activity code, application type, indication of funding by the ARRA
H. Park et al. / Research Policy 44 (2015) 1145–1159
1149
Feb 17, 2009
Enactment of ARRA
Sep 30, 2009
Last project start date
for those funded
in FY 2009
FY 2009
Apr 3, 2009
NIH announces
that it will choose
proposals with
meritorious score
but falling
beyond payline.
(NOT-OD-09-078)
FY 2010
May 1, 2009
First day
when the projects
selected through
this mechanism
started
Sep 30, 2010
Last project start date
for all ARRA-funded
projects of any type
Mar 17, 2010
NIH reaffirms
the importance of
completing
projects within
two years.
(NOT-OD-10-067)
Sep 30, 2012
Ideal end date
for all ARRA-funded
projects
without extension
Sep 30, 2013
Compulsory
end date for
all ARRA-funded
projects
(OMB M-11-34)
Fig. 2.
NIH-speciﬁc ARRA timeline.
Research
Proposals
Meritorious
score cut-off
Proposals that falls beyond
meritorious score cut-off line.
Normal
payline
Funded projects
in normal times
Extended
payline
due to ARRA
Non-ARRA-solicited
but ARRA-funded projects
Funding
Sources
Usual NIH Budget for Research Grants
ARRA Fund
Proposals that would have not been funded
without payline extension.
Initial Peer Review Score
High
Low
Distributed by other mechanisms:
ARRA-specific FOAs,
administrative supplements, etc.
Fig. 3.
Graphic illustration of NIH-ARRA and payline extension (not drawn to scale).
appropriation, associated FOA number, project start and end dates,
list of PIs,
afﬁliation of the contact PI,
education institution type,
funding mechanism, and award amount. The NIH records the name
of each PI and assigns a serial number to each PI for identiﬁcation.
The education institution type is the category of the contact PI’s
afﬁliation (e.g., School of Medicine or School of Arts and Sciences).
The funding mechanism indicates the general purpose of the fund
such as Research Projects, Training, or Construction.
Not only does the NIH compile and disclose detailed project
records,
but it also keeps track of the list of publications gener-
ated from its funded projects and provides a linking table that
contains pairs of the publication identiﬁer and the project number.
The detailed bibliographic information of the publications listed in
this linking table comes from the PubMed database. Each publica-
tion record contains authors list,
afﬁliation of the corresponding
author,
journal title,
ISSN,
volume,
issue,
and year of publication.
We also use forward citations data from Scopus®.
By matching
these data with the project-level funding data, we can identify how
many publications result from a particular project and where and
when these articles are published.
The citations data allow us to
construct a measure of impact of a research project funded by the
NIH.
We collect all NIH projects funded in ﬁscal years 2009 and 2010.
Using application type and funding mechanism, we ﬁlter down to
only research projects initiated from a new application.
We then
drop grants for non-U.S.
institutions and projects funded by non-
NIH agencies. Because this sample only contains new application-
based projects, administrative supplements to existing projects and
continuation funding records are accordingly excluded.
Among ARRA-funded projects,
we are left with both ARRA-
solicited and non-ARRA-solicited projects.
The ARRA-solicited
projects are the ones that are selected from the applications asso-
ciated with ARRA-speciﬁc FOAs.
We remove these projects using
the corresponding FOA numbers. We also remove projects that are
missing the associated FOA number.
The purpose of our analysis
is to examine the NIH’s preference in project selection primarily
by comparing between the research output of projects supported
by the ARRA fund and that of projects supported by regular NIH
grants.
To that end,
we need to ensure that these two groups of
projects are intended to solve the same problem deﬁned by the
FOAs. Hence, among the projects supported by regular NIH grants,
we exclude all
projects that do not share an FOA number with
one of ARRA-funded projects in the sample. Our ﬁnal sample thus
contains 12,554 projects (2775 ARRA projects and 9779 regular
projects) associated with 288 distinct FOAs (i.e.,
43 projects per
FOA on average).
7
PA-type FOAs tend to contain more projects than
RFA-type FOAs as PAs typically aim to address broader classes of
problems.
Despite the differences in size and coverage,
each FOA
has an explicit research orientation and a goal to achieve. Such an
objective is clearly stated in “Research Objectives,” which appears
at the beginning of an FOA. To merge project data with the publi-
cation list, we search the link table with project numbers. A project
can have multiple publications. The entire set of matched publica-
tions thus contains 32,491 articles.
8
7
We remove two projects whose project start date is in FY2008,
which seem
apparent coding errors. We also remove four projects with only $1 as the funding
amount.
8
We remove articles published prior to 2009 as they seem obvious errors in the
raw data.
1150
H. Park et al. / Research Policy 44 (2015) 1145–1159
4.
Empirical analysis
4.1.
Project quality
We ﬁrst examine if the NIH selects higher quality projects on
average. We do this by comparing the impact and the quantity of
research output (i.e., journal publications) between regular projects
and ARRA projects.
Given that both groups of projects ultimately
received NIH grants but regular projects have been selected ﬁrst,
any superiority of the regular group on our quality measures would
indicate the NIH’s preference for (or its ability to select) higher
quality projects.
In addition to investigating this on the aggregate level, we also
examine if the pattern (if any) varies across grant types. Our sample
includes 14 activity categories but the top three categories are R01,
R21, and R03 (in a descending order of frequency), which together
account for over 90% of the projects.
According to the NIH
9
,
R01
is the classic type of research grant awarded to major research
projects, R03 is intended to support pilot or feasibility studies, and
R21 is for new and exploratory research that may involve high risk
but may lead to a breakthrough.
Both R03 and R21 projects are
encouraged to be followed up by R01 applications.
In sum,
R01
is a mechanism to support research that may show some initial
results,
while R03 and R21 are to seed-fund risky but potentially
highly impactful research initiatives.
Thus,
the nature of projects
and hence the characteristics of research output may well
vary
between R01 and the other two (R03 and R21) grant types.
For the measure of impact,
we use monthly citation rates of
publications. Speciﬁcally, we calculate for each funded project the
maximum number of citations of all journal publications that result
from the project (e.g., Benavente et al., 2012).
10
We then divide it
by the number of months since publication to account for the dif-
ferences in citation windows. For the quantity-based productivity
measure, we count the number of publications from a given project.
Because projects vary in the resources expensed for the research,
we also compute the normalized measures of impact and quantity
by dividing the raw measures by the total cost of project (in mil-
lion U.S.
dollars).
To distinguish between the groups,
we deﬁne a
dummy,
ARRA,
that equals one if a given project is funded under
the ARRA and zero if it is supported through regular NIH grants.
Table 1 provides the summary statistics of these variables,
along
with those of all other variables used in our study.
For this part of analysis, we regress the measures of project qual-
ity on the ARRA dummy,
with FOA- and project start year-ﬁxed
effects included to minimally control for variations across fund-
ing decision units at the NIH and the differences in timing across
grants.
11
Hence, our regression takes the following form:
y
ij
= ˛ + ˇ · ARRA
i
+ ϑ
j
+ 
i
+ ε
ij
(1)
where y
ij
is our measure of project i’s quality, ARRA
i
indicates the
ARRA status of project i,
ϑ
j
is a dummy for FOA j
that project i
belongs to,

i
is the year ﬁxed effects for the year that project i
started, and ε
ij
is an idiosyncratic error term. To examine the dif-
ferences between grant types, we interact the ARRA dummy with
R01
i
, which equals one if project i is supported by an R01 grant and
zero otherwise.
9
https://www.nichd.nih.gov/grants-funding/opportunities-mechanisms/
mechanisms-types/comparison-mechanisms/Pages/default.aspx.
10
For the project that has multiple publications with the same maximum citations,
we randomly select one from them.
11
By this,
we are essentially comparing the raw values of
quality measures
between the two groups, without taking account of project-level attributes. This is
because, given our purpose of examining if the projects funded under the ARRA are
fundamentally different from those funded through regular grants, controlling for
other observable attributes masks such differences in underlying quality of projects.
Table 2 reports the results.
Column 1 shows that the projects
funded under the ARRA on average generate publications with 6.8%
fewer monthly citations,
as compared to the projects supported
by regular grants.
When normalized by project cost,
the gap in
citation rate increases to 12.5% (column 3).
The inter-group dif-
ference in the number of publications (columns 5 and 7) is still
sizable (about 17.9% fewer publications per project-dollar for ARRA
projects). These results imply that regular-funded projects are not
only more productive than ARRA-funded projects but the research
output from regular projects also commands signiﬁcantly higher
impacts,
compared to that of ARRA projects.
Insofar as our meas-
ures represent the inherent quality of projects, our results suggest
that the NIH has a reasonable capacity to sort and prioritize grant
proposals based on the quality of the projects.
A breakdown analysis by grant type indicates that the differ-
ences in research impact between the groups, particularly in terms
of per-dollar citations,
come primarily from the R01 type grant
(columns 2 and 4). R01 also seems largely responsible for the dif-
ference in the quantity of research output (columns 6 and 8).
As
indicated by the coefﬁcients on the R01 dummy, projects supported
through R01 grants on average generate more and higher-impact
research output
than those supported through other
types of
grant.
Extending the logic above,
the differences between grant
types imply that the NIH is relatively good at identifying “better”
projects in the R01 category,
but may be less so in other cate-
gories that involve more exploratory research proposals on nascent
opportunities.
12
We have used the (normalized) maximum number of citations
per project as the measure of impact.
Alternatively,
one may be
interested in the total number of citations that a project has gen-
erated. While maximum citations measure the biggest impact of a
project, the sum of citations would capture the project’s aggregate
contribution to the ﬁeld. In our sample, however, we ﬁnd that the
sum of citations per project is highly correlated with the maximum
citations ( = 0.73) based on raw values. In terms of logged values,
which are what we use in the regression to minimize heteroskedas-
ticity, the two measures are even more highly correlated ( = 0.86).
The results using the sum of citations are also very similar to those
based on the maximum of citations.
13
4.2.
Project risk
The analysis in the previous section shows that regular projects
are on average of higher quality than ARRA projects, suggesting that
the NIH gives a priority to higher-quality proposals. As mentioned
earlier, the NIH is also interested in promoting high-risk projects,
in the hope that such projects will produce major breakthroughs
even if that also means higher chances of failure. We thus examine
in this section the NIH’s risk preference in project selection.
We
do this in two ways: ﬁrst,
by comparing between the two groups
of projects the likelihood of tail outcomes (i.e., extreme successes
and complete failures) and second, by contrasting the variances in
our measures of project quality.
For the ﬁrst part of analysis,
we
construct for each project two binary indicators of tail outcomes:
the Top 5% dummy,
which indicates if the project’s citation rate
makes into top 5% of all publications in our sample
14
; and the No
12
There may be two reasons for this. One is that exploratory research proposals are
inherently more difﬁcult to evaluate their potential even though the NIH attempts
to carefully prioritize projects based on the assessed quality.
The other is that in
these exploratory grant categories,
the NIH purposefully selects projects that are
not guaranteed to succeed, rather than based on quality assessment. We are unable
to discern which of the two is more likely.
13
The results are unreported here and are available from the authors upon request.
14
We would ideally want to construct this measure based on the entire publi-
cation pool of articles in biomedical research.
However,
deﬁning the boundary of
H. Park et al. / Research Policy 44 (2015) 1145–1159
1151
Table 1
Summary statistics.
Regular projects
ARRA projects
N
Mean
Std. dev.
Min
Max
N
Mean
Std. dev.
Min
Max
Citation rate
6743
0.77
1.19
0
24.04
1755
0.60
1.05
0
28.23
Citation rate per $M
6743
2.63
6.12
0
318.36
1755
2.84
16.29
0
641.74
(Dummy) Top 5%
6743
0.07
0.25
0
1
1755
0.04
0.19
0
1
# of publications
9779
2.73
3.95
0
63
2775
2.10
3.11
0
36
# of publications per $M
9779
9.66
26.23
0
1633.99
2775
9.27
34.32
0
1509.43
(Dummy) No publication
9779
0.31
0.46
0
1
2775
0.37
0.48
0
1
Research ﬁt (textual
similarity)
6739
3.06
6.15
0
82.71
1754
3.42
5.83
0
50.38
(Dummy) R01
9779
0.62
0.48
0
1
2775
0.37
0.48
0
1
(Dummy) R03 or R21
9779
0.31
0.46
0
1
2775
0.52
0.50
0
1
Total cost
9779
353,852.00
279,413.30
612
5,934,572
2775
321,628.10
286,513.30
3120
5,566,450
# of unique authors
9779
12.02
23.66
0
1022
2775
9.14
16.60
0
318
(Dummy) Within 2 years
9779
0.35
0.48
0
1
2775
0.97
0.18
0
1
Project start year
9779
2009.47
0.54
2008
2010
2775
2009.07
0.25
2009
2010
# of PIs
9779
1.11
0.36
1
6
2775
1.09
0.33
1
6
(Dummy) Existing PI
9779
0.75
0.43
0
1
2775
0.76
0.43
0
1
(Dummy) At least one PI
has a grant (2004–2008)
9779
0.69
0.46
0
1
2775
0.70
0.46
0
1
Mean cumulative $ grants
for PIs (2004–2008)
9779
200,365.60
236,191.20
0
3,502,544
2775
217,658.80
261,326.50
0
4,620,253
Thousand # of grants for
organization
(2004–2008)
9779
1.74
1.83
0
6.77
2775
1.67
1.79
0
6.77
Table 2
Analysis of project quality.
1
2
3
4
5
6
7
8
(Log) Citation rate
(Log) Citation rate per $M
(Log) N
pub
(Log) N
pub
per $M
(Dummy) ARRA
−0.068
**
−0.040
**
−0.125
**
−0.035
−0.124
**
−0.046
**
−0.179
**
−0.020
(0.013)
(0.010)
(0.032)
(0.026)
(0.045)
(0.015)
(0.068)
(0.028)
(Dummy) R01
0.101
**
0.201
**
0.328
**
0.444
**
(0.014)
(0.020)
(0.016)
(0.039)
ARRA × R01
−0.046
**
−0.155
**
−0.121
**
−0.267
**
(0.012)
(0.027)
(0.024)
(0.044)
Constant
0.519
**
0.450
**
1.091
**
0.953
**
1.576
**
1.352
**
2.602
**
2.299
**
(0.018)
(0.022)
(0.032)
(0.032)
(0.046)
(0.036)
(0.043)
(0.043)
N
8498
8498
8498
8498
8498
8498
8498
8498
F-stat
16.80
84.65
36.96
111.11
308.89
241.64
443.00
340.14
Adj. R
2
0.05
0.06
0.07
0.07
0.09
0.10
0.15
0.16
Note: FOA- and year-ﬁxed effects are included in all models. Robust standard errors, clustered by FOA, are in parentheses. All models are conditioned on the project having
at least one publication.
**
Statistical signiﬁcance at 1% level.
publication dummy,
which indicates if the project produces zero
publication. Hence, the former represents a right-tail outcome (i.e.,
extreme success) and the latter a left-tail outcome (i.e.,
complete
failure).
We estimate an analog of
Eq.
(1),
with the dependent vari-
able now being one of the two dummies of tail outcomes. Table 3
presents the results.
On average,
ARRA projects are signiﬁcantly
less (by 3%p) likely to produce a highly impactful publication than
regular projects (column 1).
ARRA projects are also signiﬁcantly
more likely than regular projects to have zero publication. By grant
type, the R01-supported projects in general are considerably more
prone to generating a top-5%-class research output and are less
likely to fail, relative to projects supported through other types of
grants. However, these distinctions hold equally for regular projects
biomedical research is challenging and thus collecting all publications in the ﬁeld
is practically infeasible. Instead, we use as the base all publications produced from
the projects in our sample. This in fact makes our deﬁnition of top 5% quite strin-
gent because the projects in our sample are already among a highly selective group
of projects that secured funding from the NIH through a rigorous scientiﬁc review
process. Therefore, the top 5% in our sample could well indicate an even higher rank
in the percentile distribution based on a (hypothetical) pool of full publications.
Table 3
Analysis of tail outcomes: proportions of top 5% citation and no publication.
1
2
3
4
(Dummy) Top 5%
(Dummy) No publication
(Dummy) ARRA
−0.030
**
−0.028
**
0.071
**
0.038
†
(0.004)
(0.007)
(0.025)
(0.019)
(Dummy) R01
0.027
**
−0.315
**
(0.009)
(0.018)
ARRA × R01
−0.002
0.027
(0.008)
(0.021)
Constant
0.098
**
0.080
**
0.182
**
0.374
**
(0.007)
(0.011)
(0.022)
(0.033)
N
8498
8498
12,554
12,554
F-stat
24.95
37.58
173.31
347.13
Adj. R
2
0.02
0.02
0.11
0.12
Note: FOA- and year-ﬁxed effects are included in all models. Robust standard errors,
clustered by FOA, are in parentheses. Models 1 and 2 are conditioned on the project
having at least one publication.
†
Statistical signiﬁcance at 10% level.
**
Statistical signiﬁcance at 1% level.
1152
H. Park et al. / Research Policy 44 (2015) 1145–1159
and ARRA projects, as indicated by the insigniﬁcant coefﬁcients on
the interaction terms (columns 2 and 4). Taken together, across all
grant types,
regular projects appear to command a higher risk of
producing right-tail outcomes than ARRA projects.
We further investigate the risk proﬁle of selected projects by
looking at the degree of dispersions in research output between
the two groups. We measure dispersions by the statistical variance
of our measures of project quality (i.e., citation-based impact and
quantity-based productivity). If the NIH were highly risk averse as
often criticized (e.g., Nurse, 2006), it would always prefer to choose
a set of projects whose research output as a group is more pre-
dictable over the one with less predictable outcomes.
Given that
regular projects were chosen ﬁrst over ARRA projects, one should
expect based on the portfolio theory that, controlling for the mean
output,
regular projects exhibit a smaller variance in the output
quality. Thus, we examine if the two groups distinctively differ in
the degree of dispersion on each of the quality measures in Table 2.
We ﬁrst demean the measure by the corresponding FOA,
project
start time and ARRA status, and perform a nonparametric test of the
equality of variance (Levene, 1960), which is robust to the potential
non-normality of the underlying distribution.
15
Columns 1 and 2 of Table 4 show that the projects supported
through regular
grants exhibit
a signiﬁcantly greater
variance
(p<0.01)
in citation-based research impact
than the projects
selected under the ARRA.
A similar pattern is found for the inter-
group difference in the quantity of research output (column 3),
though the difference is insigniﬁcant for the dollar-adjusted pro-
ductivity (column 4). The greater output dispersion of the regular
project
group holds regardless of
grant
types for the research
impact (columns 1 and 2),
but for the productivity R01 grants
appear to be driving the difference (column 3). On the whole, across
all types of grant,
regular NIH projects are considerably less pre-
dictable than ARRA projects in the quality of research output. This
also suggests that the results in Table 3 do not imply a simple mean
shift in the project quality under the ARRA. Therefore, to the extent
that the propensities of tail outcomes and the variances of research
output represent the relative “risk” of the projects, we can interpret
this section’s results as suggesting that, all else equal, the NIH gives
a priority to higher-risk proposals.
4.3.
Robustness checks on a cohort sample
Our analysis in the previous sections uses the full
sample of
projects.
However,
due to differences in the timing of cost dis-
bursement, the projects in our sample have different time windows
for producing research output.
In particular,
for the institutional
reasons explained in Section 2, even for the same ﬁscal year ARRA
projects systematically started later than regular projects (Fig. 4).
This difference in time window could affect
our measurement
of project output,
particularly the quantity side of it.
Though we
did control
for the timing effect through year-ﬁxed effects,
we
performed additional robustness checks by narrowing down the
sample to the projects that started in the same time period (May
1–September 30, 2009) and repeating the same analyses as we did
on the full sample.
On project quality, presented in Table 5(a), we ﬁnd results that
are qualitatively similar to those obtained from the full sample anal-
ysis (Table 2). Compared to ARRA projects, regular NIH projects on
average generate research output with signiﬁcantly higher (5–10%)
impacts and result in a greater (8–13%) number of publications per
15
For the implantation we use the Stata command robvar.
The results reported
here are robust to the test proposed by Brown and Forsythe (1974) that uses the
median instead of the mean.
project.
There is even stronger evidence that these differences in
project quality are largely speciﬁc to the R01 category.
We also ﬁnd similar patterns on project risk (Table 5(b)): regular
NIH projects exhibit greater variances in research output than ARRA
projects, in terms of both citation-based impact and quantity-based
productivity. On this cohort sample, the greater output dispersion
of the regular project group appears mostly driven by the R01 cat-
egory except in column 1, in which the difference in variance holds
across all grant types. On the whole, the previous sections’ results
on project quality and risk seem robust to the differential timing of
project start.
4.4.
Correlates of project quality
In this section we investigate the factors that help explain the
differences in research output between regular projects and ARRA
projects. This will allow us to speculate on the potential cues that
the NIH might be using to identify and select promising proposals.
Speciﬁcally, we re-estimate Tables 2 and 3 with a full set of controls
for available observable attributes of the project and the investiga-
tor(s). The regression model thus takes the following form:
y
i
= ˛ + ˇ · ARRA
i
+ x
i
 + ε
i
(2)
where y
i
is one of our measures of project i’s output characteris-
tics; ARRA
i
is a dummy indicating the ARRA status of project i; x
i
is a
vector of observable attributes of project i including (log) grant size
(in U.S. dollar), (log) number of unique authors, a dummy indicat-
ing whether project i ends within two years, a dummy indicating
whether project i is funded in FY2010,
the number of PIs associ-
ated with project i,
a dummy indicating whether any of the PIs
has an experience of any NIH grant in the past,
(log) mean grant
amount that the PIs have received in the preceding ﬁve years,
a
dummy indicating no grant in the preceding ﬁve years, the number
of grants awarded to the organization in the preceding ﬁve years,
and the activity-FOA-IC-education institution type-year dummies;
and ε
i
is an idiosyncratic error term. The number of unique authors,
identiﬁed from reported publications, captures the size of lab that
the PI(s) operate.
To identify a PI’s experience in NIH grants,
we
search the entire grant data between 1985 and 2008 (or 2009).
If
any of the PIs of projects funded in FY 2009 (or 2010) appears in
project records between 1985 and 2008 (or 2009),
then we mark
the project as including an experienced PI.
The amount of
NIH
grants of PIs in the preceding ﬁve years (2004–2008) measures
the PIs’ recent funding track record. For projects with multiple PIs,
we take the mean of all PIs’ recent grant amounts. Since this vari-
able is left-censored at zero,
we include an indicator of whether
the value is zero (i.e.,
no grant in 2004–2008).
The number of
grants that the contact PI’s institution received in the preceding ﬁve
years (2004–2008) represents the institution-level quality. Lastly,
the joint ﬁxed effects between activity type (R01,
R03,
etc.),
FOA
number (RFA or PA’s serial
number),
IC code (National
Institute
of Allergy and Infectious Disease,
National Cancer Institute,
etc.),
education institution type (School of Medicine, School of Arts and
Sciences,
etc.),
and project start year of proposal help control for
other potential unobserved heterogeneity across grants.
Table 6 reports
the results
from this
analysis.
Among the
explanatory variables,
team size,
measured by the number
of
unique authors, is estimated to be the strongest correlate of project
outcomes: projects with a greater team size on average generate
more and higher-impact publications (columns 1 and 2), are more
likely to produce the highest-impact research (column 3), and are
less likely to fail (column 4). In contrast, project cost, once team size
is controlled for,
performs poorly in explaining research output,
and in fact is negatively related to quantity-based output meas-
ures (columns 2 and 4).
Not surprisingly,
having more research
resources secured by the investigators recently (conditioning on
H. Park et al. / Research Policy 44 (2015) 1145–1159
1153
Table 4
Nonparametric tests of the equality of variances.
1
2
3
4
(Log) Citation rate
(Log) Citation rate per $M
(Log) N
pub
(Log) N
pub
per $M
All (N = 8498)
Regular
0.38
0.66
0.59
0.81
ARRA
0.33
0.63
0.54
0.80
F-stat
46.70
**
9.94
**
18.93
**
0.24
R01 (N = 5561)
Regular
0.40
0.64
0.61
0.77
ARRA
0.38
0.58
0.58
0.74
F-stat
9.93
**
14.20
**
4.01
*
1.49
R03 + R21 (N = 2436)
Regular
0.34
0.73
0.51
0.80
ARRA
0.29
0.69
0.50
0.79
F-stat
6.90
**
3.39
†
0.30
0.65
Note: F-stat is Levene (1960)’s robust test statistic for the equality of variances between ARRA and regular projects. All other cells report standard deviations. All variables
are demeaned by the corresponding FOA-year-ARRA status. All models are conditioned on the project having at least one publication.
†
Statistical signiﬁcance at 10% level.
*
Statistical signiﬁcance at 5% level.
**
Statistical signiﬁcance at 1% level.
having received some grants) helps produce signiﬁcantly more
impactful research output (column 1), though it does not increase
the likelihood of highest-impact output (column 3). It also has no
inﬂuence on the quantity of publication (column 2) or the likeli-
hood of project failure (column 4). The institution-level quality is
a positive and signiﬁcant correlate of research impact (in terms of
both the number of citations and the likelihood of receiving top
5% citation).
Interestingly,
when none of the PIs associated with
the project has a recent grant record, the project tends to produce
more impactful research output (column 1).
This implies that,
all
else equal,
new PIs might perform well
in generating impactful
research, if not more publications. Among other variables, projects
with a shorter time window produce fewer publications (columns
2),
while the impact of their output is generally comparable to
that of longer-term projects. Notice that when these project- and
investigator-level
attributes are accounted for,
the coefﬁcient of
the ARRA dummy loses signiﬁcance.
In other words,
once we
account for the observable attributes of the project,
regular NIH
projects on average appear qualitatively similar to ARRA projects.
This result implies that these project-level
attributes are collec-
tively highly correlated with the underlying quality of projects.
Hence,
they may indeed be useful cues for the NIH in identifying
and selecting promising projects among numerous grant proposals.
By contrast, these project-level attributes do not seem very effec-
tive for prioritizing projects based on the variability of research
output,
because the signiﬁcant differences in variance found in
Section 4.2 survive the control of these attributes. Speciﬁcally, we
perform Levene (1960)’s test of the equality of variances between
regular projects and ARRA projects using the residuals recovered
from the regressions in Table 6.
The test shows that the regular
group has a signiﬁcantly greater variance than the ARRA group in
both impact (F = 13.7,
p < 0.01) and productivity (F = 7.5,
p < 0.01).
The contrasting results on the mean and the variance with full con-
trols suggest that the cues used for assessing the return and the risk
of a project are likely to be different, and at least some of those cues
are unobservable to us.
4.5.
Research ﬁt
Each funding opportunity of the NIH is an attempt to address
a speciﬁc research problem.
We thus explore how closely the
selected projects ﬁt
the intended purpose of
the funding.
This
question is particularly relevant because selections of high-impact,
high-risk projects may be pursued at the expense of the original
0
200
400
600
800
# Projects
Oct 1, 2008
Apr 1, 2009
Oct 1, 2009
Apr 1, 2010
Oct 1, 2010
Project Start Date
Non-ARRA
ARRA
Fig. 4.
Histogram of project start date.
1154
H. Park et al. / Research Policy 44 (2015) 1145–1159
Table 5
Robustness checks: analysis on cohort sample.
(a) Mean comparison
1
2
3
4
5
6
7
8
(Log) Citation rate
(Log) Citation rate per $M
(Log) N
pub
(Log) N
pub
per $M
(Dummy) ARRA
−0.057
**
−0.027
†
−0.105
**
−0.001
−0.083
*
−0.025
−0.132
*
−0.001
(0.010)
(0.015)
(0.032)
(0.038)
(0.032)
(0.020)
(0.053)
(0.033)
(Dummy) R01
0.094
**
0.187
**
0.392
**
0.517
**
(0.016)
(0.031)
(0.021)
(0.061)
ARRA × R01
−0.048
*
−0.169
**
−0.088
**
−0.208
**
(0.019)
(0.040)
(0.026)
(0.043)
Constant
0.469
**
0.411
**
1.012
**
0.891
**
1.408
**
1.169
**
2.407
**
2.088
**
(0.004)
(0.011)
(0.014)
(0.025)
(0.014)
(0.013)
(0.023)
(0.038)
N
3912
3912
3912
3912
3912
3912
3912
3912
F-stat
31.40
34.97
10.85
71.94
6.72
279.89
6.21
44.02
Adj. R
2
0.06
0.06
0.09
0.09
0.08
0.09
0.18
0.19
(b) Variance comparison
1
2
3
4
(Log) Citation rate
(Log) Citation rate per $M
(Log) N
pub
(Log) N
pub
per $M
All (N = 3912)
Regular
0.40
0.67
0.60
0.81
ARRA
0.33
0.64
0.54
0.79
F-stat
37.62
**
4.34
*
20.20
**
0.35
R01 (N = 2305)
Regular
0.41
0.66
0.63
0.79
ARRA
0.38
0.58
0.58
0.74
F-stat
9.93
**
10.31
**
5.97
*
1.05
R03 + R21 (N = 1354)
Regular
0.35
0.73
0.52
0.78
ARRA
0.29
0.70
0.50
0.79
F-stat
4.31
*
1.39
0.75
1.00
Note: (a) FOA- and year-ﬁxed effects are included in all models. Robust standard errors, clustered by FOA, are in parentheses. (b) F-stat is Levene (1960)’s robust test statistic
for the equality of variances between ARRA and regular projects. All other cells report standard deviations. All variables are demeaned by the corresponding FOA-year-ARRA
status. All models are conditioned on the project having at least one publication.
†
Statistical signiﬁcance at 10% level.
*
Statistical signiﬁcance at 5% level.
**
Statistical signiﬁcance at 1% level.
Table 6
Correlates of project characteristics.
1
2
3
4
(Log) Citation rate
(Log) N
pub
Top 5%
No Pub
(Dummy) ARRA
−0.017
0.004
−0.014
−0.015
(0.019)
(0.021)
(0.010)
(0.009)
(Log) Total cost
−0.016
−0.066
**
−0.001
0.029
**
(0.018)
(0.024)
(0.011)
(0.009)
(Log) # of unique authors
0.237
**
0.561
**
0.076
**
−0.296
**
(0.009)
(0.011)
(0.007)
(0.007)
(Dummy) Within 2 years
−0.013
−0.056
*
0.003
−0.014
(0.019)
(0.027)
(0.012)
(0.011)
# of PIs
0.001
−0.013
0.003
0.016
*
(0.020)
(0.019)
(0.012)
(0.008)
(Dummy) Existing PI
−0.025
−0.031
0.003
−0.009
(0.017)
(0.022)
(0.011)
(0.010)
(Log) Mean cumulative $ grants
0.032
**
−0.003
0.006
0.007
for PIs (04–08)
(0.010)
(0.011)
(0.006)
(0.005)
(Dummy) No PI
0.362
**
−0.095
0.063
0.087
has a grant (04–08)
(0.120)
(0.128)
(0.078)
(0.064)
# grants for organization
0.012
**
−0.000
0.006
**
0.001
(04–08, thousands)
(0.003)
(0.003)
(0.002)
(0.002)
Constant
−0.316
0.904
**
−0.201
0.371
**
(0.214)
(0.310)
(0.140)
(0.127)
N
8498
8498
8498
12,554
F-stat
106.26
344.13
17.06
278.15
Adj. R
2
0.28
0.62
0.08
0.76
Note: Activity-FOA-IC-institution type-year ﬁxed effects are included in all models. Robust standard errors, clustered by activity-FOA-IC-institution type-year, are in paren-
theses. Models 1–3 are conditioned on the project having at least one publication.
*
Statistical signiﬁcance at 5% level.
**
Statistical signiﬁcance at 1% level.
H. Park et al. / Research Policy 44 (2015) 1145–1159
1155
Table 7
Analysis of research ﬁt.
1
2
(Dummy) ARRA
−0.401
**
−0.479
†
(0.130)
(0.243)
(Dummy) R01
0.314
(0.235)
ARRA × R01
0.175
(0.284)
Constant
3.454
**
3.244
**
(0.305)
(0.283)
N
8493
8493
F-stat
6.24
5.08
Adj. R
2
0.63
0.63
Note: FOA- and year-ﬁxed effects are included in all models. Robust standard errors,
clustered by FOA,
are in parentheses.
All
models are conditioned on the project
having at least one publication.
†
Statistical signiﬁcance at 10% level.
**
Statistical signiﬁcance at 1% level.
funding purposes. If such were the case, superiority of projects sim-
ply based on our measures of research output may not necessarily
indicate a quality work of project selection on the part of the NIH.
To examine this question, we measure the closeness of projects to
the purpose of a given FOA (“research ﬁt”) and compare it between
regular projects and ARRA projects.
If the NIH did not trade it off
with project quality in selecting projects, we should observe a sim-
ilar difference in research ﬁt between the two groups as found in
the previous analyses.
We construct a measure of research ﬁt by textually comparing
the FOA research objectives and the abstracts of publications from
the corresponding projects.
For the textual
analysis,
we use the
term frequency-inverse document frequency method (Manning
et
al.,
2008;
Bird et
al.,
2009;
Rehurek and Sojka,
2010).
Note
that
this measure is deﬁned ex post
because we use “publica-
tion” abstracts instead of grant proposal abstracts.
A publication
abstract is by deﬁnition written after the project is funded,
and
hence the investigators have much less incentive to intentionally
make it close to the FOA’s stated objectives. Thus, using publication
abstracts, rather than proposal abstracts, to capture the content of
projects better serves our purpose. We provide in Appendix A the
detailed process of constructing this variable. With this measure as
the dependent variable, we estimate an analog of Eq. (1).
As shown in Table 7,
ARRA projects on average exhibit a sig-
niﬁcantly lesser ﬁt with the FOA research objectives (column 1).
This implies that regular NIH projects as a group produce research
output that is more closely aligned with the purpose of funding
than the output from ARRA projects. There is no difference in the
pattern between grant types,
as the coefﬁcient of the interaction
term is insigniﬁcant (column 2).
Therefore,
across all grant types,
the NIH’s selection of high-quality high-risk projects, as evidenced
in previous sections,
seems achieved within the range of closely
meeting the objectives of the funding.
4.6.
Preference for experienced PIs
A ﬁnal piece of our analysis concerns if the NIH favors experi-
enced investigators in selecting projects. In fact, this is one of the
frequent allegations used for questioning the NIH’s effectiveness
in allocating resources (e.g.,
Weinberg,
2006).
If true,
this unwar-
ranted favoritism would indeed stiﬂe young scientists, potentially
miss opportunities of great promises,
and ultimately lead to an
overall decline of the biomedical ﬁeld. Even our results in the previ-
ous sections suggest that a favorable treatment for experienced PIs
would not be justiﬁable because the projects led by experienced
PIs do not result in superior research outcomes (Table 6).
Given
the importance of the question,
a systematic investigation of this
Table 8
Correlates of ARRA project selection.
1
2
3
Full
R01
R03 + R21
(Dummy) Existing PI
0.043
**
0.066
**
−0.008
(0.016)
(0.017)
(0.032)
# grants for organization
−0.002
−0.001
−0.002
(04–08, thousands)
(0.003)
(0.003)
(0.008)
(Log) Total cost
0.054
*
0.078
**
−0.038
(0.025)
(0.028)
(0.048)
(Log) # of unique authors
−0.029
**
−0.033
**
−0.002
(0.008)
(0.008)
(0.017)
Research ﬁt
−0.002
−0.005
*
0.002
(0.002)
(0.002)
(0.003)
Constant
−0.431
−0.810
*
0.804
(0.307)
(0.354)
(0.582)
N
8493
5556
2436
F-stat
4.50
7.39
0.17
Adj. R
2
0.28
0.18
0.29
Note: Activity-FOA-IC-institution type-year ﬁxed effects are included in all models.
Robust standard errors,
clustered by activity-FOA-IC-institution type-year,
are in
parentheses. All models are conditioned on the project having at least one publica-
tion.
*
Statistical signiﬁcance at 5% level.
**
Statistical signiﬁcance at 1% level.
aspect seems in order. Our approach to that question is to examine
the probability that the applicant team involving any PI who has
a history of past NIH grants is awarded an ARRA grant, relative to
that of receiving a regular grant.
We estimate a project-level regression model of the following
form:
y
i
= ˛ + z
i
 + ε
i
(3)
where y
i
is a dummy indicator of project i’s ARRA grant status
(i.e., one if ARRA-funded and zero otherwise); and z
i
is a vector of
explanatory variables including a dummy indicating whether any
of the PIs has an experience of any NIH grant in the past (“Existing
PI”), the number of grants awarded to the organization in the pre-
ceding ﬁve years, the number of unique authors, research ﬁt, and
the activity-FOA-IC-education institution type-year dummies. The
baseline represents a regular NIH grant,
hence a negative coefﬁ-
cient of the Existing PI dummy would suggest a greater chance of
grant award for experienced PIs in the regular funding cycle.
Table 8 presents the results.
Most notably,
the coefﬁcient of
the Existing PI dummy is signiﬁcantly positive (column 1). Projects
involving at least one experienced PI are 4.3%p more likely given
an ARRA grant than a regular grant,
controlling for other factors.
The ﬂip-side of this result is that in the regular funding cycle, these
experienced PIs are less likely to be selected for funding. This is in
stark contrast to the frequent allegation that the NIH gives a favor to
PIs with proven track records.
16
Moreover,
the institutional “rep-
utation,” represented by the institution-level grant record in the
preceding ﬁve years, seems to have no inﬂuence on the probability
of an ARRA grant.
Thus,
at least in our data,
there is no evidence
of the Matthew effect (Merton,
1968) that is often found in other
settings (e.g., Bhattacharjee, 2012).
Looking at other variables, ARRA projects on average have fewer
unique authors but have greater project costs than regular projects.
This suggests that the NIH may have given a priority to projects
that are smaller but could expend more money in research.
It is
intriguing that ARRA projects are smaller in team size than reg-
ular projects.
Considering that the ARRA was primarily aimed at
16
In fact, our result is consistent with the NIH’s own data: the investigators who
received the top 20% of funding in 2009 had an average of only 2.2 grants each
(Rockey, 2013).
1156
H. Park et al. / Research Policy 44 (2015) 1145–1159
boosting employment, granting larger amount of money to smaller-
size teams might have helped achieve the original purpose of the
ARRA. Recall that, in our previous analysis, team size was a strong
indicator of the quality of research output, whereas grant size was
not. This interpretation is thus consistent with the results in Table 2
that, without accounting for project-level attributes, ARRA projects
are on average associated with lower quality research output.
A split-sample analysis by grant types (columns 2 and 3) indi-
cates that
these results are entirely driven by R01 grants (the
regression model for the R03 and R21 sample is not even properly
speciﬁed).
5.
Discussion
How does the NIH select projects? Our analysis exploiting a
natural
experiment setting from the ARRA suggests that in the
regular funding cycle,
the NIH tends to opt for a high-risk,
high-
return portfolio with greater likelihoods of breakthrough research
outcomes.
The selected projects also seem aligned well with the
funding objectives. Some project- and investigator-level attributes
effectively explain the characteristics of research output from the
funded projects,
suggesting that these may be useful cues for the
NIH to identify high quality proposals. We ﬁnd no evidence that the
grant history of investigators or their afﬁliated institutions provides
an advantage in regular grants awards. There is some heterogeneity
across grant types, though R01 is generally the one driving most of
the observed patterns.
A robust ﬁnding from our analysis is that the projects selected
under the regular funding cycle are of higher quality than those
selected under the ARRA.
An obvious interpretation of this result
is that the NIH is reasonably good at picking “winners.” But it
could also imply that ARRA projects are marginal
ones and the
ARRA money went to less-deserving investigators.
This suggests
that additional funding to the agency may not necessarily help fund
projects that equally deserve such support. Though we cannot con-
clude from our analysis whether or not the level of current funding
is sufﬁcient, our result does indicate that the returns to additional
funding are likely to be lower.
Our mean-variance analysis of the projects suggests that the NIH
gives preference to promising yet uncertain projects, over the ones
with more certain output prospects though possibly with lower
returns. In other words, at least in our setting the NIH chose a set
of higher-risk higher-return projects even though it had an option
to choose a lower-risk lower-return portfolio. While underscoring
the uniqueness of our identiﬁcation strategy made possible by the
ARRA experiment (as this would not have been feasible without the
ARRA projects as a comparison group), this ﬁnding also implies that
the NIH is not as risk averse as it is often criticized. That is because
had the NIH been indeed highly risk averse in selecting projects, we
would have observed the opposite selection pattern.
17
Though we
cannot infer from our data the exact shape of the NIH’s risk attitude,
our ﬁnding clearly runs counter to the frequent allegation that the
NIH is too risk averse and prefers surer bets.
Interestingly, the inter-group differences in project quality dis-
appear when we control
for the project- and investigator-level
attributes. Though this may suggest that the NIH has no additional
insight beyond the possible cues from these observable attributes,
one could wonder if this is necessarily a bad thing.
Our ﬁnding
particularly suggests that team size and institutional prestige sig-
niﬁcantly predict the quality of projects. What that implies is that
17
With a standard risk-averse utility function and using the modern portfolio the-
ory, one can readily show that a revealed preference for a higher-risk higher-return
portfolio over a lower-risk lower-return portfolio implies that the degree of risk
aversion of the agent is bounded from above.
some of these attributes are essentially endogenous (i.e.,
success
begets success - the Matthew Effect in science).
One might argue
that to avoid an “association bias,” the NIH should implement a
completely blind review in selecting projects. However, our result
indicates that such a blind review might actually result in the selec-
tion of worse projects. The current two-stage selection process (i.e.,
blind peer reviews in the ﬁrst stage and the committee-based selec-
tion in the second stage) does no worse in terms of selecting good
projects without sacriﬁcing the risk proﬁle of the portfolio choice.
Thus, it is unclear if introducing greater “transparency” in the selec-
tion process would help improve the overall quality of selection.
The lack of inter-group quality differences with the full
con-
trol of observable attributes also helps rule out some alternative
accounts of our ﬁndings based on the comparison of raw values.
For instance,
one might say that the ARRA selection may simply
have been rushed, resulting in a poorer set of projects compared to
the regular ones. However, the result of indifference with full con-
trols strongly suggests that the NIH has maintained consistency
in its selection rule between different funding cycles.
One might
also argue that fewer publications from ARRA projects may result
from the ﬁxed capacity in biomedical journals.
If regular projects
were closer to completion than ARRA projects at the time of initial
selection and there is a ﬁxed capacity in journals,
then the out-
put of ARRA projects would be less likely to be published (or have
fewer publications). That is because, by the time the investigators
submit the papers, the ﬁxed set of journals may have reached the
limit of their capacity (including the allowance for backlogs).
18
Here
again, however, the disappearance of the difference with full con-
trols rejects this possibility: it is hardly imaginable that the NIH has
correctly anticipated solely based on these observables for which
of the projects the journal capacity would be more or less binding,
apart from the inherent quality of the projects.
19
Our analysis also shows that, in choosing projects of higher qual-
ity,
the NIH does not trade it off with the project’s ﬁt with the
funding objectives.
Compared to regular projects,
ARRA projects
were less relevant to the purported goals of the funding. If so, could
the money have been better spent on supporting other new projects
that would have a closer ﬁt with funding purposes,
instead of on
ARRA projects? In fact, the NIH did distribute the ARRA money on
new FOAs that called for new proposals (under the ARRA-solicited
FOAs). However, it is possible that these ARRA-solicited FOAs may
have targeted agenda that are less critical than those targeted by
the original non-ARRA-solicited FOAs, which our study exclusively
focuses on. Therefore, it is hard to determine which “inefﬁciency,”
if any, might be less costly from the social welfare standpoint—i.e.,
less ﬁt within non-ARRA-solicited FOAs (which is what our ﬁnding
suggests) versus less ﬁt (or less equality of urgency) between ARRA-
solicited FOAs and non-ARRA-solicited FOAs.
This is an intriguing
question that seems better left for future research.
Our analysis by grant categories hints at some inconsistencies in
the selection principle. In particular, there is little difference in the
risk proﬁle between regular grants and ARRA grants in the category
of R03 and R21. This is despite the fact that the ARRA group consists
18
Controlling for the project start time will
not completely address this issue
because the start time in our data is the timing of grant money disbursement, not the
actual start of the research, which we do not know. Even when the papers do get in
journals, ARRA-funded papers might have ended up in lower-tier journals because
of the capacity issue at higher-tier journals. This is in fact consistent with the result
of our additional analysis using journal impact factors (JIFs): regular projects on
average have higher JIFs than ARRA projects, though this difference disappears too
with controls of project-level attributes (unreported).
19
It is, however, plausible that the sudden—and sizeable—increase in funding to
the biomedical ﬁeld following the ARRA has increased the overall competition for
the ﬁxed slots in biomedical journals, thereby crowding out some of the work that
would otherwise have landed in one of those journals.
Though not a focus of our
study, this clearly appears an interesting topic for future research.
H. Park et al. / Research Policy 44 (2015) 1145–1159
1157
of disproportionately more projects in R03/R21 categories (52%)
compared to the regular group that is dominated by R01 projects
(62%). Both R03 and R21 grants are designed to promote less proven
scientists and projects. If the NIH were faithful to the stated goal of
these grant categories,
we would probably observe greater inter-
group differences for these grant types than for R01. While this may
be indicative of the potential gap between the goal and the practice
at the NIH, it may also reﬂect the inherently more volatile nature of
the proposals submitted under these categories. We simply do not
know which of these possibilities is more likely to lie behind our
results. Regardless, given that R01 is the largest funding category at
the NIH and is the major source of funding for biomedical scientists,
our ﬁndings in general appear quite representative and hence help
form policy implications for the practice.
Our
approach in this study is to deduce the NIH’s prefer-
ence from their revealed choices in project selection between two
rounds of
selection from the same pool
of
grant
proposals.
In
particular,
we take the projects selected under the ARRA as the
comparison group to characterize the underlying preference in the
regular funding cycle. One might argue that, given the special (and
unprecedented) circumstances that provoked the ARRA enactment,
the projects selected under that initiative are not appropriate for
a comparison group to deduce the NIH’s behavior under “normal”
circumstances. While this is a legitimate concern, we do not believe
our choice of this comparison group compromises our ﬁndings.
First of all,
the fairly low success rates for new proposals during
the period (17.3% in 2009–2010) imply that there must have been
many projects with sufﬁcient scientiﬁc merits. Thus, the differences
in project quality we document are unlikely to be orthogonal to the
differences in the underlying distribution of quality, unless the NIH
intentionally chose lower quality projects under the ARRA; we have
no reason to think they would have done so.
In terms of project
risk,
though we see a drop in variance for the ARRA group,
there
is no reason to expect that the NIH’s goal of expeditiously disburs-
ing the money should necessarily lead to a selection of less risky
projects (i.e.,
projects whose expected output is more “centered”
than those supported under regular funding). If the policy goal was
the main consideration in project selection under the ARRA,
the
NIH may have given priority to the expendability and employabil-
ity of the project,
as implied by one of our results (ARRA projects
had fewer team members; perhaps the NIH thought that these
teams had greater room for hiring people).
Regardless,
that need
not be correlated with lower risk. A similar argument can be made
for research ﬁt: the ARRA’s immediate policy goal need not have
forced the NIH to forgo the original funding objectives in selecting
projects.
The case of the experienced PI analysis is less clear.
One may
interpret the positive coefﬁcient of the ARRA dummy (Table 8) to
indicate that the NIH has given even more favor to experienced
PIs in selecting projects under the ARRA, over and above what they
normally give to them. While we cannot entirely rule out this possi-
bility, we also wonder why the NIH would have behaved that way
when they certainly knew that the temporary increase in money
would soon go away. What better opportunity than this will there
be for the NIH to make up for their adverse reputation by pur-
posefully selecting more young and new scientists with the ARRA
money? We therefore believe that ARRA projects serve as a rea-
sonable basis for comparison and hence our ﬁndings are unlikely
to suffer from any idiosyncrasy of the event.
20
20
Another plausible story is that more ARRA funding may have been allocated to
experienced PIs because these experienced researchers had better lobbying skills
and/or lobbied more aggressively for the ARRA funding. It would certainly be inter-
esting to study the link between investigator lobbying and project selection using
more detailed applicant-level data; however, that is beyond the scope of our study.
Another potential
issue with our analysis is that our sample
is conditional
on the project being selected.
That is,
we do not
observe the full set of projects with the opportunity to be selected.
Given the highly competitive nature of NIH grants, there must be
many projects that cleared the (more objective) threshold of scien-
tiﬁc merits but were still not selected under the ARRA. Also, many
promising but riskier proposals might not have made the cut in
scientiﬁc merits.
Thus,
the proﬁle of projects in our sample may
not be representative of the underlying distribution of projects.
For instance,
it is possible that our sample represents the lower-
risk group among the population of projects.
Then,
our ﬁnding of
the NIH’s preference for high-quality high-risk projects might lose
some signiﬁcance. However, we do not claim that the NIH uncon-
ditionally favors high-quality high-risk projects. Insofar as the peer
review process,
which is beyond the scope of our study,
is per-
formed objectively and that the NIH maintains their consistency
in prioritizing the risk-return portfolio among the eligible propo-
sals, our ﬁndings reasonably reﬂect the NIH’s preference in project
selection.
One might also argue that our analysis based on inter-group
mean comparison is unfair to the ARRA group, as at least some of
the projects in the regular group should be unambiguously supe-
rior ones. Thus, including these exceptional projects in calculating
the means can give a natural advantage to the regular group.
We
do not disagree.
In fact,
a more interesting and relevant analysis
would be to compare between the projects around the cutoff (prior
to the ARRA), i.e., projects that are marginally funded in the regu-
lar funding cycle and those that are marginally unfunded then but
funded later under the ARRA. Unfortunately, our data do not allow
for this approach. Nonetheless, even in this group-based compar-
ison,
employing a full set of observable attributes eliminates the
inter-group differences (see Table 6).
Hence,
while we acknowl-
edge this as a limitation,
we believe that our ﬁndings still claim
considerable validity.
On a related point, lack of data renders our analysis necessarily
crude.
Absent data limitations,
we would directly use the scores
given to each project in the selection process.
This would make
our analysis much more precise. We are convinced that the ques-
tion we are addressing in this paper is an extremely important one.
Surprisingly, however, this crucial issue has eluded scholars in the
ﬁeld,
even those who seem to have access to the full data of NIH
proposals and review scores (both funded and unfunded ones) (e.g.,
Li, 2012). We thus make our best attempt to examine this issue by
exploiting what is currently available to us. The ﬁndings we report
in this study suggest that such an attempt is a worthwhile exercise.
A few caveats to our study seem also in order.
First of all,
the
receipt of
a particular grant may have a limited impact on the
research productivity of
the grant awardees.
Prior studies have
shown that the productivity of funded applicants near the selection
cutoff is often fairly similar to that of unfunded applicants near the
cutoff (Carter et al., 1987; Jacob and Lefgren, 2011b). It may be that,
given the competitive nature of NIH funding, even researchers who
fail to receive an NIH grant can easily ﬁnd another source of fund-
ing to pursue their research (Jacob and Lefgren,
2011b).
If so,
the
research output we observe from funded projects may be only par-
tially inﬂuenced by NIH grants. This would limit our inference based
on all observed output from the projects. Moreover, in the analysis,
we use “realized” outcomes to infer the underlying characteristics
of the projects.
This approach admittedly ignores the uncertainty
and possible idiosyncrasies in the process of scientiﬁc research.
Some projects may have produced results that far exceeded their
initial
expectations.
Some high quality projects may have failed
to realize their full potential because of unexpected disturbances
in the process.
Thus,
it would be naive to regard research output
as a precise reﬂection of the underlying quality of the projects.
Nonetheless, to the extent that these idiosyncrasies and uncertainty
1158
H. Park et al. / Research Policy 44 (2015) 1145–1159
associated with scientiﬁc research equally apply to both groups
of projects,
our results should reasonably demonstrate the inter-
group differences in the underlying project quality.
We also note
that our ﬁndings should not be taken as suggesting that the NIH’s
selection scheme follows an efﬁciency trajectory because with the
sudden increase in funding resource they moved from higher-risk,
higher-return projects to lower-risk, lower-return projects. In gen-
eral, from an economic perspective, efﬁciency requires that grants
should be allocated to those researchers for whom the funding
would have the largest marginal
beneﬁt.
But,
to the extent that
researchers may have obtained similar research outcomes even
without the funding support,
high productivity does not neces-
sarily imply that the money was effectively spent.
This ex post
efﬁciency,
however,
is not the focus of
our study.
21
Lastly,
our
sample covers only very recent years (2009–2010).
Considering
the NIH’s aggressive push toward high-risk high-return projects
reﬂects fairly recent
policy initiatives,
our ﬁndings from these
recent data may not apply equally to the years preceding our sam-
ple period. While this may suggest that the NIH’s recent initiatives
might have been working,
it may also limit the generalization of
our ﬁndings to a broader time span during which the frequent crit-
icisms on the effectiveness of the NIH’s selection process have been
formed.
6.
Conclusion
Important matters tend to elicit greater attention. The NIH’s cen-
tral
role in promoting scientiﬁc research in the biomedical
ﬁeld
has drawn corresponding interests in the process by which the
institution selects projects and the effectiveness of the selection
mechanism.
Selection,
by deﬁnition,
means exclusion for at least
some. Complaints can thus arise accordingly. The decade-long stag-
nation in funding resources at the NIH may have added to such
tendency. With the recent effectuation of the U.S. budget seques-
tration,
which called for a spending cut of over $85 billion in the
ﬁscal year of 2013 only, the discordant voices may well be ampli-
ﬁed.
Allegations abound,
but the ﬁndings from our study do not
render support to such claims.
Perhaps the NIH will
want to be
more transparent in selection criteria and communicate them more
clearly to the interested audience. Without a doubt, a richer analy-
sis supported by more ﬁne-grained data will help bring additional
insight into this essential process.
Some limitations to our study
notwithstanding,
however,
we hope to be able to claim a modest
contribution to the policy discussion of this important institutional
arrangement.
Acknowledgement
We gratefully acknowledge the insightful comments and sug-
gestions from the Editor and two anonymous reviewers. All errors
remain ours.
Appendix A.
Construction of research ﬁt
Each FOA document starts with “Research Objectives,” which
describe the purpose of
the funding opportunity.
Two possible
inputs for comparison from the project side are the abstract of the
project proposal and abstracts of the publications resulted from the
project. Abstracts of the project proposal are subject to investiga-
tors’ deliberate efforts to make them as close as possible to the FOA
21
In contrast, if NIH grants are viewed as an award for research excellence, allocat-
ing the resource to the most productive researchers could be considered efﬁcient;
however, this ‘award’ view is fundamentally at odds with the premise that the NIH
funding is an essential input for research projects.
objectives in order to increase the chance of funding.
In contrast,
because the abstracts of the publications are in general ﬁnalized
after the funding decision, investigators have less incentive to make
these abstracts close to the FOA objectives.
Therefore,
we choose
to use publication abstracts for the comparison with FOA Research
Objectives.
We employ the natural language processing approach to com-
pute the similarity between FOA objectives and abstracts (Rehurek
and Sojka, 2010; Bird et al., 2009). The term frequency-inverse doc-
ument frequency (tf-idf) model is one of the classical vector space
models in natural language processing (Manning et al., 2008). The
key idea behind this model is that the more frequently a particular
term appears in a document (i.e., local property), the more repre-
sentative the term is of the document. However, if the term appears
in all documents (i.e., global property), the discerning power of that
term should be lower.
In our context,
suppose the term “medi-
cal” appears many times in an abstract.
Then,
we know that the
content of the paper is about some medical
topics.
However,
if
“medical” appears in all other FOA objectives and abstracts,
this
term offers little help in uniquely identifying the content relative
to other abstracts. By balancing between these local and global per-
spectives, we can identify nontrivial characterizing terms from the
collection of FOA objectives and abstracts.
We start by collecting 369 FOA objectives and 29,995 abstracts
of publications from the projects in our sample.
After removing
English stop words (e.g., a, an, the), we tokenize each document and
create a corpus (i.e., a formatted collection of documents). This cor-
pus provides us with the global perspective on determining which
terms have distinguishing power.
Based on the tf-idf model,
we
then compute for each project the similarity between the pub-
lication abstracts and the corresponding FOA objectives.
When a
project has multiple publications, we take the maximum value of
the computed similarities for the project research ﬁt.
References
Arora,
A.,
Gambardella,
A.,
2005.
The impact of NSF support for basic research in
economics. Ann. Econ. Stat., 91–117.
Austin, F., 2008. High-risk High-reward Research Demonstration Project. NIH Coun-
cil of Councils.
Averch, H.A., 1989. Exploring the cost-efﬁciency of basic research funding in chem-
istry. Res. Policy 18, 165–172.
Azoulay,
P.,
Zivin,
J.S.G.,
Manso,
G.,
2011.
Incentives and creativity: evidence from
the academic life sciences. RAND J. Econ. 42, 527–554.
Azoulay, P., Zivin, J.S.G., Manso, G., 2012. NIH Peer Review: Challenges and Avenues
for Reform.
National Bureau of Economic Research Working Paper Series No.
18116.
Benavente, J.M., Crespi, G., Figal Garone, L., Mafﬁoli, A., 2012. The impact of national
research funds: a regression discontinuity approach to the Chilean FONDECYT.
Res. Policy 41, 1461–1475.
Bhattacharjee,
Y.,
2012.
NSF’s ‘big pitch’
tests anonymized grant reviews.
Science
336, 969–970.
Bird, S., Klein, E., Loper, E., 2009. Natural Language Processing with Python. O’Reilly
Media.
Bisias,
D.,
Lo,
A.W.,
Watkins,
J.F.,
2012.
Estimating the NIH efﬁcient frontier.
PLoS
ONE 7, e34569.
Bourne, H.R., Lively, M.O., 2012. Iceberg alert for NIH. Science 337, 390.
Brown, M.B., Forsythe, A.B., 1974. Robust tests for the equality of variances. J. Am.
Stat. Assoc. 69 (346), 364–367.
Carter,
G.M.,
Winkler,
J.D.,
Biddle,
A.K.,
1987.
An Evaluation of the NIH Research
Career Development Award. RAND Corporation Report R-3568-NIH.
Chodorow-Reich,
G.,
Feiveson,
L.,
Liscow,
Z.,
Woolston,
W.G.,
2012.
Does state ﬁs-
cal relief during recessions increase employment? Evidence from the American
Recovery and Reinvestment Act. Am. Econ. J.: Econ. Policy 4, 118–145.
Dorsey, E.R., de Roulet, J., Thompson, J.P., Reminick, J.I., Thai, A., White-Stellato, Z.,
Beck, C.A., George, B.P., Moses III, H., 2010. Funding of US biomedical research,
2003–2008. J. Am. Med. Assoc. 303, 137–143.
Fineberg, H.V., 2013. Toward a new social compact for health research. JAMA 310,
1923–1924.
Ginther,
D.K.,
Schaffer,
W.T.,
Schnell,
J.,
Masimore,
B.,
Liu,
F.,
Haak,
L.L.,
King-
ton,
R.,
2011.
Race,
ethnicity,
and NIH research awards.
Science
333,
1015–1019.
Gordin, B., 1996. The Impact of Research Grants on the Productivity and Quality of
Scientiﬁc Research. Working Paper. INRS, Montreal, Quebec.
H. Park et al. / Research Policy 44 (2015) 1145–1159
1159
Hegde, D., 2009. Political inﬂuence behind the veil of peer review: an analysis of pub-
lic biomedical research funding in the United States. J. Law Econ. 52, 665–779.
Hegde, D., Mowery, D.C., 2008. Politics and funding in the US public biomedical R&D
system. Science 322, 1797–1798.
Jacob,
B.A.,
Lefgren,
L.,
2011a.
The impact of NIH postdoctoral training grants on
scientiﬁc productivity. Res. Policy 40, 864–874.
Jacob,
B.A.,
Lefgren,
L.,
2011b.
The impact of research grant funding on scientiﬁc
productivity. J. Public Econ. 95, 1168–1177.
Levene, H.,1960. Robust tests for equality of variances. In: Contributions to Probabil-
ity and Statistics: Essays in Honor of Harold Hotelling, vol. 2. Stanford University
Press, pp. 278–292.
Li,
D.,
2012.
Information vs.
Bias in Evaluation: Evidence from the NIH.
Working
Paper.
Manning, C.D., Raghavan, P., Schutze, H., 2008. Introduction to Information Retrieval.
Cambridge University Press, Cambridge.
McDonough, J.E., 2013. Budget sequestration and the U.S. Health Sector. N. Engl. J.
Med. 368, 1269–1271.
Merton, R., 1968. The Matthew effect in science. Science.
Moses III, H., Dorsey, E.R., Matheson, D.H.M., Thier, S.O., 2005. Financial anatomy of
biomedical research. J. Am. Med. Assoc. 294, 1333–1342.
Nicholson,
J.M.,
Ioannidis,
J.P.A.,
2012.
Research grants: conform and be funded.
Nature 492, 34–36.
Nurse, P., 2006. US biomedical research under siege. Cell 124, 9–12.
Rehurek,
R.,
Sojka,
P.,
2010.
Software framework for topic modelling with large
corpora. In: Proceedings of LREC 2010 Workshop New Challenges for NLP Frame-
works, pp. 46–50.
Rockey,
S.,
2013.
Transparency:
Two years of
blogging the NIH.
Nature 493,
298–299.
Samuelson, P., 1938. A note on the pure theory of consumer’s behaviour. Economica
5, 61–71.
Steinbrook, R., 2009. Health care and the American Recovery and Reinvestment Act.
N. Engl. J. Med. 360, 1057–1060.
Stephan, P., 2012. Research efﬁciency: perverse incentives. Nature 484, 29–31.
Toole, A.A., 2012. The impact of public basic research on industrial innovation: evi-
dence from the pharmaceutical industry. Res. Policy 41, 1–12.
Weinberg, R.A., 2006. A lost generation. Cell 126, 9–10.
Wilson, D.J., 2012. Fiscal spending jobs multipliers: evidence from the 2009 Ameri-
can Recovery and Reinvestment Act. Am. Econ. J. Econ. Policy 4, 251–282.
Zerhouni, E., 2003. The NIH roadmap. Science 302, 63–72.
