Public
Relations
Review
42
(2016)
952–961
Contents
lists
available
at
ScienceDirect
Public
Relations
Review
Full
Length
Article
Automated
content
analysis
and
crisis
communication
research
Toni
G.L.A.
van
der
Meer
(PhD.)
Amsterdam
School
of
Communication
Research,
University
of
Amsterdam,
Nieuwe
Achtergracht
166,
1018
WV
Amsterdam,
The
Netherlands
a
r
t
i
c
l
e
i
n
f
o
Article
history:
Received
18
January
2016
Received
in
revised
form
16
August
2016
Accepted
8
September
2016
Available
online
22
September
2016
Keywords:
Automated
content
analysis
Crisis
Computational
methods
Communication
Big
data
a
b
s
t
r
a
c
t
Communication
plays
a
central
role
in
how
crisis
events
evolve.
The
huge
collection
of
today’s
digital
available
content
from
actors
such
as
organizations,
news
media,
and
the
public
provides
scholars
with
the
opportunity
to
analyze
large-sized
collections
of
crisis-
related
communication
and
provide
supplementary
evidence
for
previous
ﬁndings
from
smaller
scaled
research.
However,
the
massive
costs
and
complexity
of
analyzing
these
large-scaled
data
sets
have
hindered
their
use
within
the
ﬁeld
of
crisis
research.
This
paper
aims
to
provide
an
overview
of
how
automated
content
analysis
can
potentially
simplify
and
complement
the
analysis
of
these
large
collections
of
texts.
Computational
methods
have
long
been
used
in
the
ﬁeld
of
computer
science
and
are
currently
gaining
momen-
tum
within
the
ﬁeld
of
crisis
communication.
This
paper
discusses
the
dictionary
method,
supervised
method,
and
the
unsupervised
method
as
potential
useful
tools
for
analyzing
crisis
communication.
©
2016
Elsevier
Inc.
All
rights
reserved.
1.
Introduction
Organizational
crisis
situations
1
and
their
societal
consequences
repeatedly
occupy
our
news
screens.
Correspondingly,
organizations
report
that
they
frequently
face
a
crisis
(Verhoeven,
Tench,
Zerfass,
Moreno,
&
Ver
ˇ
ci
ˇ
c,
2014).
This
omnipres-
ence
of
crisis
situations
and
the
potential
negative
societal
effects
of
these
critical
situations
have
increased
the
scholarly
attention
for
crisis
management.
Crisis
management
has
become
a
key
element
of
crisis
research,
mainly
because
multiple
stakeholders,
and
the
organization
itself,
will
suffer
when
the
management
regarding
a
crisis
fails
(Coombs,
2007).
Within
crisis
management,
communication
and
the
interaction
with
multiple
involved
actors
is
acknowledged
to
be
a
fundamental
factor;
when
the
communication
is
inefﬁcient,
so
will
be
the
crisis
management
efforts
(Coombs,
2015).
It
can
even
be
stated
E-mail
address:
G.L.A.vanderMeer@UvA.nl
1
Crisis
is
a
broad
term,
frequently
used
by
both
academics
and
practitioners
to
refer
to
a
wide
variety
of
events
and
issues.
In
general,
it
refers
to
a
breakdown
in
a
system,
creating
shared
stress
(Perry,
2007).
In
the
context
of
crisis
management,
Coombs
(2015)
divides
crisis
in
disaster
and
organizational
crisis.
Disasters
refer
more
to
disrupting
events
that
pose
great
societal
danger,
while
organizational
crisis
mainly
refers
to
the
threating
effects
of
an
unpredictable
event
on
important
expectations
of
stakeholders
and
the
negative
consequences
for
the
organization.
This
study
makes
no
explicit
difference
between
crisis
and
disaster.
It
needs
to
be
acknowledged
that
there
are
signiﬁcant
differences
in
communication
and
how
social
media
is
used
between
different
crisis
types.
These
differences
might
have
implications
for
the
applications
of
automated
content
analysis.
Nevertheless,
it
is
assumed
that
the
different
computational
methods
are
useful
for
studying
communication
in
the
context
of
all
types
of
crises
and
disasters.
The
methods
can
be
used
to
categorize
all
types
of
communication
based
on
different
starting
points
or
research
questions
related
to
crisis
research.
The
ﬁnal
results
of
the
analysis,
and
the
extent
to
which
they
are
theoretically
interesting,
depend
on
the
cases
that
are
studied,
however,
the
practical
application
of
the
methods
remains
comparable.
http://dx.doi.org/10.1016/j.pubrev.2016.09.001
0363-8111/©
2016
Elsevier
Inc.
All
rights
reserved.
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
953
that,
although
crises
have
real
origins,
they
are
constituted
in
the
communicative
interplay
between
several
actors,
whose
perceptions
produce
real
consequences
(Kleinnijenhuis,
Schultz,
&
Oegema,
2015).
Due
to
the
far-reaching
consequences
of
communicative
efforts
during
a
crisis,
a
signiﬁcant
body
of
research
and
numer-
ous
cases
about
crisis
communication
exist
today.
Scholars
and
practitioners
aim
to
understand
the
ﬂow
of
communication
in
times
of
crisis
and
how
the
communicative
interplay
can
affect
outcomes
such
as
public
panic
(e.g.,
Liu
&
Kim,
2011;
Van
der
Meer
&
Verhoeven,
2013),
crisis
escalation
(e.g.,
Seeger,
2002),
post-crisis
organizational
reputation
(e.g.,
Coombs
&
Holladay,
2008),
or
ﬁnancial
markets
(Kleinnijenhuis,
Schultz,
Utz,
&
Oegema,
2013).
The
increasing
body
of
crisis
research
in
the
ﬁeld
of
communication
has
adopted
multiple
methodological
approaches
to
unravel
the
dynamics
of
crisis
communication.
So
far,
crisis
literature
is
dominated
by
studies
applying
experimental
designs
to
understand
public
responses
towards
organizational
crisis
communication
(Coombs,
2007;
Kim
&
Cameron,
2011).
Additionally,
case
studies
are
still
the
majority
of
the
extant
crisis
research.
For
example,
multiple
scholars
have
analyzed,
under
different
conditions,
the
effectiveness
of
the
crisis-response
strategies
for
speciﬁc
cases
as
a
way
to
minimize
or
avoid
post-crisis
damage.
Based
on
Benoit’s
(1997)
speculative
image
restoration
strategies,
Coombs
(2007)
categorized
several
response
strategies
as
denial,
diminish,
and
rebuild.
Extensive
empirical
research
has
demonstrated
how
these
strategies,
for
various
crisis
situations,
differently
affect
several
outcome
variables
such
as
the
organization’s
post-crisis
reputation
(e.g.,
Coombs
&
Holladay,
2008)
and
secondary
crisis
communication
(Schultz,
Utz,
&
Göritz,
2011).
The
digital
age
has
brought
substantial
changes
to
the
ﬁeld
of
crisis
research.
In
general,
crisis
situations
set
in
motion
a
large
amount
of
messages
from
various
actors
(Thelwall
&
Stuart,
2007).
The
shift
towards
online
publication
and
archiving
of
different
news
outlets
–
e.g.,
online
news
websites
and
online
archiving
of
newspapers
–
and
organizations
–
e.g.,
online
press
releases
and
organizational
statements
on
corporate
websites
–
provides
crisis
researchers
with
new
opportunities
to
study
large
amounts
of
crisis
communication
data.
Furthermore,
social
media
has
become
an
integral
part
of
crisis
situations
(Madden,
Jansoke,
&
Briones,
2016;
Ott
&
Theunissen,
2015;
Van
der
Meer
&
Verhoeven,
2013),
increasing
the
accessibility
of
public
crisis
communication
via
online
platforms
such
as
blogs,
Facebook,
or
Twitter.
Analysing
the
huge
collection
of
content
and
understanding
the
complex
dynamics
of
this
contemporary
media
landscape
in
the
context
of
organizational
crisis
situations
requires
a
larger
scale
of
analysis.
Therefore,
an
emerging
research
avenue
in
the
ﬁeld
of
crisis
communication
applies
forms
of
automated
content
analysis
to
study
the
communicative
processes
and
effects
using
large
amounts
of
crisis
data.
As
scholars
have
recognized
that
much
of
the
crisis
is
constructed
and
formed
within
the
discourse
of
communication
among
different
domains
or
actors
(Kleinnijenhuis
et
al.,
2015;
Van
der
Meer,
Verhoeven,
Beentjes,
&
Vliegenthart,
2014),
the
use
of
automated
content
analysis
provides
opportunities
to
enrich
the
body
of
crisis
literature
using
large
data
sets.
In
other
words,
this
automated
approach
can
help
to
provide
supplementary
evidence
for
what
crisis
scholars
so
far
have
suspected
based
on
qualitative
or
small-scale
quantitative
research.
Academics
in
crisis
research,
just
like
academics
from
other
social
sciences,
have
just
started
to
recognize
the
opportunities
of
(newly)
available
automated
content
analysis.
The
general
aim
of
these
computational
methods,
which
commonly
ﬁnd
their
origin
in
computer
science,
is
to
automatically
identify
or
classify
certain
patterns
within
large
amounts
of
texts
with
reduced
costs
and
time
(Flaounas
et
al.,
2013).
With
the
use
of
computer-assisted
methods,
this
classiﬁcation
becomes
more
replicable
and
is
(likely)
to
be
without
bias
due
to
subjective
interferes
of
the
researcher
(Riff,
Lacy,
&
Fico,
2014).
This
paper
aims
to
map
the
available
and
applicable
automated
content
approaches
for
the
ﬁeld
of
crisis
research.
An
overview
of
such
techniques
is
provided
to
gain
practical
understanding
of
what
computational
methods
can
be
used
for
within
crisis
research,
guided
by
the
overview
paper
of
Grimmer
and
Stewart
(2013)
in
the
context
of
political
communica-
tion.
Both
deductive
and
inductive
computational
approaches
will
be
discussed.
Deductive
approaches
are
mainly
used
to
analyze
content
based
on
a
priori
deﬁned
categories
or
taxonomies
while
inductive
approaches
can
be
applied
to
explore
(new)
patterns
in
text.
Therefore,
these
different
methods
can
serve
different
purposes,
for
example
conﬁrmatory
analysis
of
expectations
regarding
content
or
consequences
of
communication
based
on
existing
theory
and
smaller
scale
analyses
or
more
exploratory
objectives
aiming
to
further
build
theory
based
on
population
samples.
Additionally,
for
each
method
an
example
will
be
provided
of
a
study
that
applied
this
approach
to
gain
insights
in
how
these
techniques
can
be
used
to
answer
questions
related
to
crisis
research.
2.
Principles
of
automated
content
analysis
Before
discussing
the
potential
useful
automated
content
methods,
some
principles
need
to
be
addressed
(Grimmer
&
Stewart,
2013).
First,
automated
content
analyses
are,
of
course,
not
free
from
drawbacks.
Automated
methods
are
not
equivalent
to
manual
methods.
The
computer-aided
part
makes
these
methods
more
systematically
reliable
and
therefore
more
replicable,
however,
it
cannot
replace
human
augment.
Due
to
the
complexity
of
language,
automated
content
analysis
might
only
amplify
careful
reading
of
text.
Most
automated
content
analysis
rely
on
the
bag
of
words
approach
where
word
frequencies
are
used
as
features
of
text
and
word
order
does
not
inform
the
analysis
(e.g.,
Hellsten,
Dawson,
&
Leydesdorff,
2010;
Miller,
1997).
All
these
automated
methods
might
fail
to
provide
an
accurate
account
to
actually
process
texts.
There-
fore,
it
is
argued
that
automated
content
analysis
should
be
solely
used
to
help
researchers
to
content
analyze
large
amounts
of
text
where
careful
thought,
reading,
and
interpreting
the
output
is
still
essential
and
should
be
guided
by
the
researcher.
Within
the
analysis
of
text,
a
wide
range
of
different
research
questions
and
designs
should
lead
to
the
use
of
different
methodological
and
statistical
approaches.
Therefore,
there
is
no
guarantee
that
certain
methods
will
be
applicable
to
each
study
design.
In
some
cases,
the
output
of
such
methods
may
be
simply
wrong
or
misleading.
Even
if
a
speciﬁc
automated
954
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
content
analysis
is
applicable
to
answer
or
test
certain
research
questions
of
hypotheses,
it
is
still
not
guaranteed
that
the
method
will
return
theoretically
interesting
results.
The
applicability
of
a
method
is
therefore
context
and
data
dependent.
For
that
reason,
the
validation
of
these
techniques
needs
to
be
tested
to
evaluate
the
applicability
in
the
context
of
a
study
design.
The
validation
of
the
use
of
automated
methods
can
take
multiple
forms,
dependent
on
the
type
of
method
(Grimmer
&
Stewart,
2013).
When
certain
categories
are
already
known
in
advance,
researchers
should
evaluate
whether
the
automated
methods
are
capable
of
replicating
the
human
coding.
However,
when
categories
are
not
known,
researchers
should
come
up
with
a
combination
of
theoretical,
experimental,
or
statistical
evidence
to
demonstrate
that
the
output
of
the
method
is
conceptually
accurate
and
valid
(Budge
&
Pennings
2007;
Slapin
&
Proksch,
2008).
Therefore,
it
is
important
for
researchers
who
plan
to
apply
computational
methods
to
consider
the
link
between
the
research
objective
and
type
of
automated
content
analysis.
3.
First
steps
Prior
to
conducting
the
automated
content
analysis,
the
data
need
to
be
acquired.
Within
crisis
research,
scholars
have
sparsely
applied
these
automated
methods
across
a
diverse
set
of
texts,
including
archives
of
media
content
about
an
organizational
crisis
(e.g.,
Schultz,
Kleinnijenhuis,
Oegema,
Utz,
&
Van
Atteveldt,
2012),
press
releases
of
the
organization
that
is
experiencing
a
crisis
from
the
website
of
the
organization
or
a
media
archive
(e.g.,
Van
der
Meer,
2014),
social
media
manifestation
of
online
public
such
as
blogs
or
twitter
data
(e.g.,
Van
der
Meer
&
Verhoeven,
2013),
or
transcription
of
the
communication
among
crisis
emergency
response
personnel
(Netten
&
van
Someren,
2006).
The
rapid
movement
to
electronically
stored
and
distributed
text
documents
often
enables
easy
access
to
required
data.
For
example,
certain
online
databases
as
Lexis
Nexis
or
ProQuest
facilitate
the
option
to
download
multiple
text
ﬁles
of
newspaper
or
press
wires
at
once.
However,
other
types
of
data
are
harder
to
acquire.
Sometimes
text
needs
to
be
transcribed
ﬁrst,
for
example
in
the
case
of
television
news
regarding
a
crisis
or
the
communication
among
emergency
response
personnel.
Additionally,
text
stored
on
websites
can
be
difﬁcult
to
access
as
websites
do
not
provide
an
option
to
batch
download
text
ﬁles.
Automated
scraping
techniques
(e.g.,
Jackman,
2006)
can
make
acquiring
texts
on
websites
easier
–
for
example
press
releases
from
an
organizational
website.
After
acquiring
the
data,
the
text
needs
to
be
simpliﬁed
and
transformed
into
quantitative
data
before
it
can
be
auto-
matically
analyzed.
In
order
to
effectively
analyze
text,
certain
textual
information
needs
to
be
discarded
that
is
either
unhelpful
or
too
complex
for
statistical
methods.
Certain
freely
available
software
can
help
in
simplifying
these
texts.
The
most
common
pre-processing
steps
are
discussed
below.
The
ﬁrst
pre-processing
steps
relates
to
the
bag
of
words
approach.
As
mentioned
before,
this
approach
discards
the
order
in
which
words
occur
in
documents
and
focuses
on
the
(co-)occurrence
of
words
within
texts.
It
is
assumed
that
a
simple
list
of
words
can
be
sufﬁcient
to
convey
the
general
meaning
of
the
text
of
analysis
(Jurafsky
&
Martin,
2009).
However,
certain
word
pairs
or
triples
can
be
retained.
Second,
the
vocabulary
of
the
words
in
the
text
needs
to
be
simpliﬁed.
Stemming
or
lemmatization
can
be
used
to
reduce
the
total
number
of
unique
words
in
the
data
set.
These
approaches
reduce
the
complexity
of
the
words
that
refer
to
the
same
basic
concept
by
removing
the
end
of
the
words
or
reduce
the
words
to
their
base
forms
(Manning,
Raghavan,
&
Schütze,
2008).
Additionally,
things
as
punctuation
and
capitalisation
are
typically
removed.
Third,
using
a
stop
words
list,
function
words
that
do
not
convey
meaning
but
primarily
have
a
grammatical
function
are
deleted
from
the
text.
Additionally,
by
removing,
for
example,
words
that
appear
in
more
than
99%
of
the
documents
and
in
less
than
1%,
other
very
common
words
and
uncommon
words
are
removed
(Hopkins
&
King,
2010;
Quinn,
Monroe,
Colaresi,
Crespin,
&
Radev
2010).
An
additional
option,
sometimes
applied
by
researchers,
is
to
weigh
the
words
in
the
document.
TF-IDF
weighted
word
frequencies
can
be
applied
to
evaluate
the
power
of
a
word
to
discriminate
between
articles
(Manning
et
al.,
2008).
If
a
word
is
rare,
it
is
assumed
to
be
more
distinguishing.
A
word
gets
assigned
the
number
of
times
it
occurs
in
the
document
(TF),
weighted
by
the
frequency
of
articles
in
the
dataset
containing
the
word
(IDF).
Finally,
the
texts
are
transformed
into
a
document-term
matrix,
indicating
how
often
words
occur
in
each
text
of
analysis.
This
matrix
is
often
the
input
for
the
different
types
of
automated
content
analysis.
Several
options
of
freely
available
software
exist
that
can
help
with
these
pre-processing
steps.
For
example,
JFreq
soft-
ware,
made
available
by
Will
Lowe
(Lowe,
2011),
and
the
software
programs
made
available
by
Loet
Leydesdorff
(Vlieger
&
Leydesdorff,
2011),
can
read
large
amounts
of
texts
to
create
documents
in
which
each
word
in
the
texts
is
a
row
and
each
text
is
a
column
(i.e.,
document-term
matrix).
Additionally,
this
type
of
software
also
allows
using
word
stems
as
opposed
to
whole
words
and
deleting
stopwords.
For
those
academics
familiar
with
programming
language,
they
can
use,
for
example,
Python
where
multiple
modules
are
available
for
these
pre-processing
steps
such
as
stemming,
lemmatizing,
TF-IDF,
and
creating
document-term
matrices.
4.
Automatically
categorizing
text
Within
the
ﬁeld
of
crisis
research,
as
well
as
in
other
social
and
communicative
ﬁelds,
assigning
texts
to
categories
is
the
most
common
application
of
automated
content
analysis.
The
categorizations
of
texts,
which
are
identiﬁed
by
the
analysis,
are
often
interpreted
as
the
topics
or
frames
used
in
the
texts
of
analysis.
In
general,
the
concept
of
framing
refers
to
an
emphasis
in
salience
of
different
aspects
of
an
issue
(De
Vreese,
2005).
In
the
context
of
crisis
research,
framing
refers
to
how
a
crisis
situation
is
presented.
The
basis
of
a
frame
is
formed
in
the
meaning
of
the
words
(Hellsten
et
al.,
2010).
More
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
955
speciﬁcally,
these
types
of
methods
build
upon
the
similarity
in
occurrence
patterns
of
words
(Hellsten
et
al.,
2010).
The
word
(co-)occurrences
of
related
words
can
specify
the
construction
of
crisis
meaning
and
represent
a
higher-order
structure
of
texts
(Leydesdorff
&
Hellsten,
2006).
Co-word
analysis
maps
the
strength
of
associations
between
key
words
in
texts,
which
enables
to
identify
clusters
or
frames
embedded
in
the
text.
Therefore,
content
analyzing
the
distribution
of
words
and
their
(co-)occurrences
can
help
to
automatically
identify
which
frames
are
used
within
a
certain
bulk
of
text
(Leydesdorff
&
Hellsten,
2006).
The
computational
methods
used
to
categorize
text
in
order
to
identify
frames
can
help
crisis
researchers
to
answer
multiple
questions.
How
a
crisis
is
framed
can
determine
whether
it
will
escalate
or
not
and
also
alter
the
severity
of
its
impact.
Accordingly,
public
relations
efforts
and
crisis
management
tend
to
be
regarded
as
successful
when
the
framing
of
organizational
press
releases
resonates
as
intended
in
the
news
and
hereby
affects
also
stakeholders’
perceptions
(Schultz
et
al.,
2012).
Therefore,
fundamental
questions
in
crisis
research
relate
to
how
a
crisis
situation
is
presented
or
framed
by
different
actors.
For
example,
how
does
the
news
media
frame
the
crisis?
Or
is
the
frame
provided
by
the
organization
that
is
undergoing
the
crisis
adopted
by
the
news
media
and
the
public?
Ergo,
the
theory
of
framing
in
combination
with
automated
content
analysis
can
play
a
key
role
in
crisis
research
to
provide
insights
in
how
the
situation
is
understood
and
framed
and
to
what
extent
the
framing
of
different
actors
is
comparable
or
different.
The
remainder
of
the
paper
addresses
different
types
of
automated
content
analysis.
An
overview
is
given
of
the
following
approaches:
Dictionary
methods,
supervised
methods,
and
unsupervised
methods.
Each
approach
generally
aims
to
describe
the
large
dataset
of
text
ﬁles
with
fewer
dimensions
by
automatically
mapping
a
group
of
correlated
words
or
text
that
together
form
a
distinctive
and
meaningful
classiﬁcation.
However,
the
computational
methods
serve
multiple
research
objectives.
The
techniques
differ
in
terms
of
being
either
deductive
or
inductive.
In
some
cases
the
classiﬁcation
(of
words)
is
a
priori
deﬁned
and
other
techniques
aim
to
inductively
identify
issue
speciﬁc
categories
in
order
not
to
overlook
meaningful
concepts
(Van
Atteveldt,
Kleinnijenhuis,
&
Ruigrok,
2008).
The
deductive
approaches
relate
more
to
conﬁrmatory
research
designs
while
the
inductive
approach
is
more
appropriate
for
exploratory
research.
For
each
technique
discussed
below
an
example
is
given
how
it
has
been
applied
within
the
context
of
crisis
research.
5.
Dictionary
methods
Dictionary
methods
are
most
in
line
with
manual
content
analysis
where
scholars
detect
certain
categories
–
e.g.,
frames
–
in
text
based
on
a
deductive
or
taxonomy
based-coding
strategy
using
for
example
a
codebook
with
indicators
questions
(e.g.,
Semetko
&
Valkenburg,
2000).
The
dictionary
methods
aims
to
automate
this
process
by
apply
advanced
search
strings.
In
this
sense,
dictionary
methods
are
the
most
intuitive
and
straightforward
automated
method
to
apply
(Grimmer
&
Stewart,
2013).
The
basis
behind
this
approach
is
counting
how
often
certain
words
occur,
words
that
determine
a
category
a
document
could
belong
to.
Thus,
the
dictionary
approach
uses
the
rate
at
which
key
words
appear
in
the
text
of
analysis
to
measure
to
what
extent
the
text
belongs
to
a
particular
predeﬁned
category.
A
frequently
used
categorization,
to
classify
text
using
dictionary
approach,
is
whether
a
text
conveys
information
pos-
itively
or
negatively.
When
the
goal
is
to
measure
tone
or
sentiment,
a
list
of
words
with
attached
tone
scores
are
used
to
measure
a
text’s
tone
based
on
the
relative
rate
at
which
these
speciﬁc
words
occur
(e.g.,
Eshbaugh-Soha,
2010).
Senti-
ment
analyses
are,
for
example,
commonly
used
within
the
ﬁeld
of
marketing
to
assess
how
organizations
and
their
brands
are
evaluated
by
the
public
online
(Mostafa,
2013).
The
dictionary
approach
to
analyze
sentiment
in
text
is
continuously
developing
and
improving
to
obtain
a
valid
tool
to
automatically
assess
the
tone
and
sentiment
in
text
for
all
languages.
For
example,
a
more
powerful
algorithm
called
SentiStrength
does
not
only
count
the
number
of
words
with
attached
tone
but
also
involves
other
features
of
the
text
such
as
negation
or
punctuation
(Thelwall,
Buckley,
Paltoglou,
Cai,
&
Kappas,
2010).
Tone
or
sentiment
is
just
one
type
of
text
categorization
that
can
be
performed
with
a
dictionary
approach.
Dictionaries
can
relatively
easy
be
applied
across
different
categories
or
issues.
One
needs
to
identify
the
words
that
separate
a
certain
category
and
measure
how
often
those
words
occur
in
the
text
of
analysis.
There
are
some
techniques
to
ﬁnd
these
separating
words.
A
variety
of
widely
applied
dictionaries
already
exist
that
provide
key
words
for
particular
categories
(e.g.,
Hart,
2000;
Pennebaker,
Francis,
&
Booth,
2001;
Turney
&
Littman,
2003).
Moreover,
certain
scholars
have
identiﬁed
how
well
words
actually
separate
already
identiﬁed
categories
(Diermeier,
Godbout,
Yu,
&
Kaufmann,
2011;
Taddy,
2010).
However,
the
dictionary
methods
should
be
used
with
caution.
Previously
applied
dictionaries
may
not
apply
in
other
contexts
or
domains.
For
example,
a
word
can
have
a
positive
connotation
in
one
context
and
refer
to
something
negative
in
another
situation.
The
domain
speciﬁcity
forces
scholars
to
evaluate
if
word
lists
created
in
another
context
are
indeed
applicable
to
their
ﬁeld
of
research
or
if
a
new
context-speciﬁc
dictionary
needs
to
be
created.
Again,
the
importance
of
validation
in
automated
methods
needs
to
be
emphasized.
Researchers
could
evaluate
whether
the
automated
clustering
of
text
using
dictionary
methods
is
comparable
with
human
coding
of
the
same
texts.
Unfortunately,
measures
from
dictionaries
are
rarely
validated
by
researches
(Grimmer
&
Stewart,
2013).
5.1.
Example
of
dictionary
method
An
illustrative
example
of
how
dictionary
methods
can
be
applied
within
the
ﬁeld
of
crisis
research
is
the
study
by
Schultz
and
colleagues
(2012).
In
this
paper,
Schultz
and
colleagues
investigated
the
communicative
interplay
between
public
relations
and
the
news
media
during
the
BP
oil
spill
crisis
in
the
Gulf
of
Mexico.
More
speciﬁcally,
via
an
automated
956
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
content
analysis
they
examined
frame
differences
between
BP’s
public
relations
and
UK
and
US
news
regarding
the
BP
crisis.
Using
a
dictionary
method,
the
salience
of
actors
and
the
use
of
frames
was
recorded.
For
their
analysis,
Schultz
and
colleagues
collected
126
press
releases
from
the
website
of
BP
and
used
LexisNexis
to
obtain
1.376
articles
from
American
newspapers
and
2.355
articles
from
UK
newspapers
about
the
BP
crisis.
Several
steps
were
taken
in
order
to
develop
a
keywords
list
with
descriptors
and
concrete
search
strings
to
identify
the
actors
and
frames
used
in
text.
First,
two
of
the
authors
read
a
sample
of
the
texts
to
gain
insights
into
the
central
actors
within
the
crisis
and
how
the
crisis
developed.
Second,
earlier
related
studies
were
consulted
to
gain
understanding
of
what
could
be
relevant
keywords
for
the
categories
of
actor
type
and
different
frames.
Third,
via
a
list
of
the
most
frequently
used
words,
the
list
with
keywords
was
reworked
to
select
the
most
important
descriptors
of
the
actors
and
frames.
Finally,
based
on
a
sample
of
the
texts,
an
iterative
test
of
these
keywords
was
used
to
ﬁlter
out
mistakes
of
the
ﬁrst
and
second
degree.
The
authors
provide
an
Appendix
with
the
ﬁnal
set
of
search
strings
that
was
used,
making
it
easier
for
other
researchers
to
replicate
the
study.
The
list
of
the
main
actors
within
the
BP
crisis
consisted
of
BP,
the
White
House,
politics,
the
court,
and
environmental
protests
actors.
Based
on
the
exploration
of
the
text
and
theory
regarding
diagnostic
and
prognostic
framing,
keywords
were
used
to
ﬁnd
the
categories
cause,
solution,
consequences,
and
oil
spill
problem.
Via
the
use
of
AmCat
(Amsterdam
Content
Analysis
Toolkit),
a
document
database
and
management
system,
keyword-based
analyses
were
conducted
to
measure
the
attention
for
actors
and
the
use
of
frames
in
each
text
ﬁle.
In
the
results,
the
mentioning
of
actors
and
use
of
frames
was
compared
among
the
press
releases
and
the
newspaper
articles
of
the
different
outlets.
Additionally,
the
frames
were
visualized
in
ﬁgures
were
associations
between
actors
and
the
frames
were
plotted.
6.
Supervised
methods
Supervised
methods
offer
a
useful
alternative
of
automated
content
method
for
assigning
texts
to
predetermined
cate-
gories
(Russell
&
Norvig,
2002).
Rather
than
dictionary
methods,
supervised
methods
do
not
require
researchers
to
identify
keywords
and
search
strings
that
separate
different
categories
to
deductively
code
text.
However,
this
more
advanced
auto-
mated
content
analysis
does
require
a
relatively
high
degree
of
initial
manual
labour.
The
idea
of
supervised
learning
methods
is
relatively
simple.
Based
on
a
set
of
texts
that
human
coders
categorize
by
hand,
a
computer
algorithm
learns
how
to
sort
texts
into
the
predetermined
categories
(Burscher,
Odijk,
Vliegenthart,
De
Rijke,
&
De
Vreese,
2014).
So
the
algorithm
iden-
tiﬁes
certain
characteristics,
or
the
lack
of
certain
characteristics,
that
separate
certain
categories
in
order
to
code
the
text
into
the
correct
category.
Thus,
this
technique
can
be
used
for
coding
latent
or
implicit
variables
in
a
dataset
so
large
that
it
is
not
feasible
to
hand
code
every
article.
The
method
starts
with
setting
up
a
codebook
for
the
manually
coding
of
a
training
set
based
on
a
sample
of
the
entire
dataset
of
texts.
As
this
type
of
supervised
learning
method
is
often
used
for
coding
latent
or
implicit
categories
the
reliability
of
the
manual
coding
scheme
is
essential.
Therefore,
it
is
suggested
to
iteratively
develop
a
codebook
that
will
guide
the
coders
how
to
categorize
the
text
of
analysis.
When
a
reliable
codebook
with
sufﬁcient
intercoder
reliability
(Krippendorff,
2004;
Neuendorf,
2002)
is
developed,
the
coding
of
the
training
set
can
begin.
The
more
articles
of
the
entire
dataset
are
coded,
the
more
reliable
the
automated
coding
will
be.
The
researchers
should
determine
how
large
their
random
sample
of
the
entire
dataset
should
be
for
manual
coding.
One
rule
of
thumb
states
that
one
hundred
texts
for
the
training
set
should
be
enough
in
case
of
a
total
of
500
texts
(Hopkins
&
King,
2010).
When
the
manual
coding
is
completed,
the
labelled
texts
are
used
to
train
the
supervised
learning
machine.
This
method
can
either
provide
the
proportion
of
a
category
for
each
text
or
assign
individual
texts
into
the
different
categories.
Basically,
the
learning
algorithm
assumes
that
there
is
a
(unobserved)
function
that
can
be
used
to
describe
the
relationship
between
the
words
in
texts
and
the
categories.
The
learning
method
attempts
to
learn
this
relationship
in
order
to
reproduce
the
manual
coding.
Different
types
of
supervised
methods
exist,
for
an
overview
see
the
article
by
Grimmer
and
Stewart
(2013).
A
clear
advantage
of
supervised
methods
over
the
dictionary
approach
is
that
they
are
easier
to
validate
with
statistical
indicators.
The
algorithm
can
be
used
to
redo
the
coding
that
was
already
done
manually.
If
the
supervised
learning
method
performs
well,
the
manual
coding
will
be
replicated
(Burscher
et
al.,
2014).
The
comparison
of
the
output
of
the
machine
coding
and
the
hand
coding
provides
a
clear
evaluation
that
is
free
from
researchers’
subjectivity.
6.1.
Example
supervised
method
In
the
context
of
framing
research,
the
application
of
supervised
learning
methods
have,
for
example,
been
tested
for
news
articles
(Burscher
et
al.,
2014)
and
policy
issues
(Burscher,
Vliegenthart,
&
De
Vreese,
2015a).
Burscher
and
colleagues
(2014)
explored
the
application
of
supervised
machine
learning
to
code
generic
frames.
They
focus
on
four
of
the
generic
news
frames
as
deﬁned
by
Semetko
and
Valkenburg
(2000):
conﬂict
frame,
economic
consequences
frame,
human-interest,
and
morality.
These
generic
frames
are
also
occasionally
used
to
explore
the
framing
of
organizational
crisis
(e.g.,
Kuttschreuter,
Gutteling,
&
de
Hond
2011).
An
indicator-based
and
holistic
approach
to
modelling
the
frame
coding
process
were
compared
to
a
random
baseline
model.
Burscher
and
colleagues
(2014)
collected
front-page
news
articles
of
three
national
Dutch
newspapers
between
1995
and
2011
about
political
issues
via
the
LexiNexis
database.
Based
on
11
dichotomous
indicator
questions
of
frame
indicators,
30
trained
coders
categorized
6030
articles
to
measure
the
extent
to
which
one
of
the
four
frames
appeared
in
a
text.
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
957
The
automated
coding
performance
was
evaluated
in
terms
of
classiﬁcation
accuracy,
receiver
operating
characteristics,
and
Krippendorff’s
Alpha
between
human
classiﬁcations
and
computer-based
classiﬁcations.
The
ﬁnal
results
indicate
high
coding
performance
for
all
four
generic
frames,
concluding
that
supervised
learning
methods
are
suitable
for
frame
coding.
Furthermore,
the
authors
show
that
the
performance
differs
between
frame
types
and
that
an
increase
in
the
number
of
training
texts
can
signiﬁcantly
improve
the
computer-based
classiﬁcation.
Besides
automatically
coding
frames,
supervised
methods
can
also
be
of
practical
help
in
times
of
emergency
or
crisis.
Netten
and
van
Someren
(2006)
investigated
the
application
of
supervised
learning
to
improve
the
distribution
of
information
between
emergency
response
personnel
involved
in
crisis
situations.
This
method
may
help
to
encounter
the
occurrence
of
communicative
errors
that
might
results
in
mistakes
and
subsequently
lead
to
more
damage
to
the
situation.
In
the
midst
of
a
crisis,
where
much
information
is
passed
by
means
of
speech
among
different
relevant
actors,
the
classiﬁcation
becomes
a
difﬁcult
task.
The
authors
found
a
way
to
use
supervised
machine
learning
techniques
to
classify
the
information
in
order
to
help
dynamic
information
distribution
and
optimize
the
information
ﬂow
among
collaborating
actors.
The
unsupervised
method
learns
from
a
collection
of
texts
that
is
manually
labelled
as
relevant
or
irrelevant
to
classify
texts
accordingly.
Furthermore,
a
prototype
system
for
information
distribution
was
developed,
called
Task-Adaptive
Information
Distributor.
To
test
their
prototype
system
they
focused
on
a
crisis
case
where
a
ﬁre
broke
out
in
a
church.
Due
to
several
mistakes
in
the
information
distribution
between
emergency
personnel,
three
ﬁremen
died.
The
information
that
the
police
went
inside
the
church
and
did
not
ﬁnd
anyone
was
not
communicated
to
the
ﬁre
truck
team.
If
this
had
been
communicated,
the
ﬁremen
would
not
have
gone
inside
at
their
arrival.
The
study
focuses
on
actors
such
as
control
rooms
of
the
police,
regional
ﬁre
department,
and
ﬁre-truck
team.
For
training
the
machine
learning
methods,
transcribed
simulated
scenarios
of
communication
among
emergency
per-
sonnel
in
crisis
management
operations
are
labelled.
Thus,
the
input
format
to
the
system
is
unstructured
text.
They
used
the
context
features
of
task
descriptions,
location
information,
and
emergency
phase
to
determine
whether
information
should
be
labelled
relevant
for
each
actor.
After
the
training
phase
the
system
was
able
to
classify
new
information,
in
combination
with
context
information,
accordingly
to
relevance
for
actors.
Finally,
the
Task-Adaptive
Information
Distributor
would
than
actually
distribute
the
message
to
the
designated
actor(s)
that
have
a
particular
role
for
which
the
message
is
relevant.
The
scenario
indicated
that
the
machine
learning
method
is
actually
able
to
correctly
identify
which
information
is
relevant
for
which
actor
and
hence
support
the
information
ﬂow
during
a
crisis.
7.
Unsupervised
methods
Contrary
to
dictionary
based
and
supervised
automated
content
analyses,
unsupervised
methods
inductively
identify
word
clusters
in
text.
In
other
words,
rather
than
searching
for
predeﬁned
categories
such
as
frames,
unsupervised
methods
provide
the
researcher
with
information
about
which
categories
can
been
found
in
the
texts
that
are
analyzed.
This
inductive
method
is
mainly
used
to
explore
texts
when
little
previous
empirical
research
or
theory
is
available
to
formulate
clear
expectations.
Thus,
this
type
of
method
can
be
useful
when
a
set
of
categories
is
difﬁcult
to
derive
beforehand,
as
is
often
the
case
for
speciﬁc
issues
or
when
the
aim
is
to
derive
issue-speciﬁc
frames
from
texts.
Moreover,
inductively
identifying
clusters
can
be
valuable
because
it
can
identify
organizations
of
text
that
are
theoretically
useful
but
still
understudied
or
previously
unknown
(Grimmer
&
Stewart,
2013).
In
short,
unsupervised
learning
methods
are
a
technique
that
automatically
learns
the
underlying
features
of
text
without
explicitly
imposing
certain
categories
beforehand.
Modeling
functions
and
properties
of
the
text
of
analysis
are
used
to
estimate
a
set
of
categories
and
assign
texts
to
those
categories.
This
unsupervised
automated
method
looks
for
word
repertoires
or
latent
patterns
of
words
co-occurrence
that
form
the
underlying
contexts
which
enables
to
highlight
how
a
certain
issue
can
be
categorized.
In
other
words,
the
(co-)occurrences
of
related
key
words
in
text
specify
the
construction
of
meaning
and
represent
a
higher-order
structure
of
texts
that
can
be
identiﬁed
as,
for
example,
a
frame
(Leydesdorff
&
Hellsten,
2006).
Two
broad
classes
of
relevant
unsupervised
learning
methods
for
the
classiﬁcation
of
text
can
be
distinguished:
Single
membership
and
mixed
membership
models.
Both
methods
are
considered
Fully
Automated
Clustering
approaches.
First,
single
membership
models
are
clustering
models
that
estimate
texts
into
mutually
exclusive
and
exhaustive
categories
or
clusters.
Groups
of
texts
are
clustered,
representing
an
estimate
of
a
category
(Grimmer
&
Stewart,
2013).
These
texts
score
the
highest
on
a
cluster
of
words
–
i.e.,
words
that
highly
correlated
with
each
other
in
and
among
texts.
The
groups
of
correlated
words
form
a
distinctive
meaningful
classiﬁcation
within
the
text
of
analysis.
These
word
groups
or
clusters
represent
the
higher-order
structure
in
texts
(Leydesdorff
&
Hellsten,
2006).
Amongst
the
most
common
Fully
Automated
Clustering
approaches
are
Principal
Component
Factor
Analysis
(Vlieger
&
Leydesdorff,
2012)
and
the
K-Means
algorithm
(Burscher,
Vliegenthart,
&
De
Vreese,
2015).
In
general,
these
algorithms
tend
to
identify
a
partition
of
the
text
that
minimizes
the
distance
within
the
cluster.
Each
text
gets
assigned
to
the
cluster
for
which
its
distance
to
the
cluster
center
is
the
smallest.
These
types
of
cluster
analysis
have
frequently
been
used
to
identify
issue-speciﬁc
frames
in
text.
Second,
mixed
membership
models
assume
that
each
text
is
a
mixer
of
different
categories
or
clusters,
there
for
each
text
exhibits
multiple
topics
in
different
proportions
(Grimmer
&
Stewart,
2013).
The
models
share
a
basic
hierarchical
structure.
Topic
models
have
been
proposed
as
the
method
of
including
this
structure
(Blei,
Ng,
&
Jordan,
2003;
ˇ
Reh
˚
u
ˇ
rek
&
Sojka,
2010).
The
most
widely
used
958
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
topic
model
is
the
Latent
Dirichlet
Allocation
(LDA).
So
far,
LDA
has,
for
example,
been
used
to
identify
important
news
items
(Krestel
&
Mehta,
2010).
7.1.
Arbitrary
steps
Despite
that
most
steps
within
the
unsupervised
methods
are
automated,
the
researcher
is
still
required
to
guide
the
analysis.
This
guidance
holds
that
parts
of
the
method
are
still
arbitrary
and
a
result
of
subjective
decisions.
First,
although
one
of
the
main
advantages
of
unsupervised
methods
is
that
no
a
priori
coding
schemes
need
to
be
supplied,
these
tools
often
do
require
setting
the
number
of
clusters
in
a
model.
For
example,
for
both
the
K-Means
clustering
and
the
LDA
topic
modeling
the
number
of
clusters
or
topics
in
the
ﬁnal
clustering
must
be
determined
beforehand.
However,
no
clear
or
default
rule
exists
for
determining
the
number
of
clusters.
The
trade-off
is
to
describe
the
data
with
fewer
categories
than
are
actually
present,
but
with
sufﬁcient
categories
so
all
relevant
information
is
included.
Some
statistical
equations
attempt
to
eliminate
this
decision
by
estimating
the
number
of
clusters
(Frey
&
Dueck,
2007).
For
example,
the
perplexity
measure
(Blei
et
al.,
2003)
and
alpha
hyperparameter
(Kim,
Kim,
&
Oh,
2014)
are
statistical
indicators
that
can
be
used
for
this
purpose.
However,
the
estimated
number
of
clusters
are
found
to
be
strongly
model
dependent.
Additionally,
having
mathematically
estimated
the
number
of
categories
does
not
say
anything
about
the
interpretability
of
the
produced
categories
(Grimmer
&
Stewart,
2013).
In
social
science
in
general,
the
categories
are
used
to
answer
substantial
theoretical
questions
about
the
texts
of
analysis.
Therefore,
the
interpretability
of
the
categories
is
considered
more
important
than
providing
the
best
prediction
of
the
data.
In
conclusion,
it
is
stressed
that
the
statistical
parameters
to
determine
the
amount
of
categories
should
only
be
used
to
make
an
initial
selection
of
models
and
the
interpretability
should
be
decisive
for
the
amount
of
categories.
Second,
not
relying
on
pre-deﬁned
categories
leaves
more
room
for
identifying
new
or
unexpected
frames.
These
gener-
ated
clusters
need
to
be
labelled
in
order
to
facilitate
the
communication
of
results
and
answering
certain
research
questions
(Van
der
Meer
et
al.,
2014).
This
labelling
is
an
interpretive
and
subjective
process
based
on
the
words
and/or
the
documents
that
form
the
clusters.
This
subjectivity
may
come
with
the
danger
of
the
fallacy
of
misplaced
concreteness.
Third,
the
validation
of
unsupervised
methods
is
not
as
clear
and
straightforward
as
it
is
for
supervised
methods.
The
best
way
of
proving
validity
of
Fully
Automated
Clustering
techniques
is
still
under
discussion
(DiMaggio,
Nag,
&
Blei,
2013;
Ramirez,
Brena,
Magatti,
&
Stella,
2012;
Quinn
et
al.,
2010).
Scholars
are
advised
to
start
using
statistical
parameters
to
gain
an
indication
of
the
amount
of
clusters,
and
then
manually
inspect
how
many
of
the
clusters
are
meaningful.
Afterwards,
manual
coders
can
check
whether
the
automated
coding
was
done
correctly
for
a
subsample
of
all
texts.
Additionally,
if
a
cluster
is
actually
valid,
then
external
events
should
explain
sudden
increases
in
attention
to
a
cluster
(Grimmer,
2010).
For
a
more
elaborate
review
of
validity
and
Fully
Automated
Clustering
models,
see
DiMaggio
et
al.
(2013)
and
Quinn
et
al.
(2010).
7.2.
Example
unsupervised
methods
The
unsupervised
method
approach
enables
to
highlight
frame
development
over
time
and
compare
framing
among
different
domains
(Leydesdorff
&
Hellsten,
2005).
For
example,
Jonkman
and
Verhoeven
(2013)
investigated
the
public
debate
in
The
Netherlands
regarding
third-party
airport
risk
using
unsupervised
methods.
To
explore
how
the
discourse
regarding
the
airport
evolved
over
time,
the
content
of
two
quality
newspapers
from
May
1992
to
May
2009,
a
total
of
579
relevant
news
articles,
was
content
analyzed.
Different
periods
were
identiﬁed
within
the
data
based
on
previous
research
on
third-party
airport
risk
and
the
publicity
pattern
detected
in
the
data
set.
To
study
the
development
of
discourse
and
frames
over
time,
the
news
articles
were
analyzed
for
each
research
period
separately.
An
unsupervised
method
was
applied
to
obtain
the
frames
that
were
used
in
each
period.
With
the
open
source
software
from
De
Vlieger
and
Leydesdorff
(2011)
document-word
matrices,
based
on
the
75
most
frequently
used
words
in
text,
were
constructed
for
each
period.
Principal
Component
Factor
analyses
were
run
on
the
matrices
to
ﬁnd
a
maximum
of
six
word
clusters
per
period.
These
word
clusters
were
interpreted
as
the
implicit
frames
used
in
the
newspapers
that
formed
the
discourse
regarding
the
airport
risk.
The
clusters
were
provided
with
a
frame
label
so
the
frame
could
be
compared
between
the
different
time
periods.
Moreover,
the
frames
were
visualized
in
word
networks
or
clouds
using
Pajek
software
based
on
a
cosine-normalized
matrix
(see
e.g.,
Leydesdorff
&
Hellsten,
2005).
In
the
two-dimensional
word
network
each
node
represented
a
word
and
the
size
of
the
node
was
proportional
to
the
frequency
of
the
word.
The
lines
in
the
pictures
represented
the
correlation
among
the
words.
Each
word
in
the
network
was
assigned
a
different
colour
depending
on
the
frame
it
belongs
to.
The
central
conclusion
of
the
study
was
that
the
economic
frame
dominated
the
third-party
airport
discourse.
In
1990s
these
economic
frames
were
ﬂanked
by
accidents
and
risk
frame
and
in
the
2000s
by
accidents
and
safety
frames.
Other
researchers
in
the
ﬁeld
of
crisis
research
applied
a
comparable
unsupervised
method
to
explore
the
interplay
between
different
domains.
Van
der
Meer
and
colleagues
(2014)
investigated
whether
the
crisis
frames
of
the
domains
public
relations
of
the
organization
experiencing
the
crisis,
news
media,
and
the
public
aligned
over
time.
The
authors
collected
press
releases,
newspaper
articles,
and
public
social
media
messages
of
four
organizational
crisis
cases
to
compare
the
frame
usage
over
time.
To
explore
the
development
of
implicit
framing
over
time,
the
data
were
analyzed
separately
for
several
research
periods:
Initial
phase,
period
of
extensive
communication,
and
ﬁnal
crisis
phase.
Using
the
software
from
De
Vlieger
and
Leydesdorff
(2011)
and
Principal
Component
factor
analysis,
unsupervised
learning
methods
were
used
to
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
959
obtain
twelve
frames
for
each
actor,
time
period,
and
crisis
situation.
To
obtain
a
statistical
indicator
of
frame
alignment
among
the
different
actors,
the
application
of
factor
analysis
was
further
elaborated
on
the
level
of
the
words
that
form
the
clusters.
In
a
factor
analysis
each
word
gets
a
factor
loading
assigned
which
indicates
its
substantive
importance
to
the
given
cluster
or
frame.
Hence,
the
factor
loadings
are
interpreted
as
the
extent
to
which
a
speciﬁc
word
represents
a
frame.
To
statistically
assess
the
level
of
frame
alignment
among
domains,
the
factor
loadings
of
mutual-used
words
by
the
separate
domains
are
compared.
This
process
provided
the
research
with
the
level
of
frame
alignment
among
the
domains
PR,
news
media,
and
the
public
in
each
crisis
period
for
each
crisis
case.
The
overall
results
documented
the
dynamic
character
of
crisis
framing
over
time
among
actors.
First,
in
the
initial
crisis
phase,
the
frames
generally
varied
across
the
domains.
Second,
in
the
phase
of
extensive
communication
the
frames
of
the
domains
actually
did
align.
However,
this
level
of
frame
alignment
was
only
temporary
as
in
the
ﬁnal
crisis
phase
the
framing
de-aligned.
8.
Conclusion
and
discussion
This
article
provides
a
brief
overview
of
recent
automated
tools
to
analyze
crisis
communication.
The
discussed
automated
techniques
can
help
researchers
in
the
ﬁeld
of
crisis
communication
and
management
as
a
valuable
tool
in
any
large-scale
content
analysis
project.
As
a
lot
of
data
is
becoming
available
online,
the
tools
can
help
to
understand
the
evolvement
of
crisis
communication
and
how
to
manage
the
ﬂow
of
communication.
Thus,
these
automated
methods
will
make
possible
inferences
that
were
previously
impossible.
If
crisis
researchers
are
capable
to
effectively
use
large-scale
content
analysis
in
their
inferences,
then
many
substantial
research
questions
are
likely
to
be
answered
(Grimmer
&
Stewart,
2013).
The
automated
approach
to
content
analysis
provides
a
wide
range
of
tools
to
answer
diverse
questions
within
the
ﬁeld
of
crisis
research.
The
available
tools
actually
extend
beyond
the
methods
discussed
in
this
paper.
However,
this
overview
provides
a
ﬁrst
outline
of
what
is
broadly
available
and
potential
useful
for
crisis
researchers.
The
dictionary
method,
supervised
method,
and
unsupervised
method
provide
especially
interesting
tools
for
classifying
texts
and
identifying
frames
within
crisis
communication.
However,
it
needs
to
be
emphasized
that
the
selection
of
these
tools
is
context
speciﬁc.
The
selection
of
an
automated
tool
should
depend
the
input
data
that
will
be
used
as
well
as
on
the
initial
interest
and
questions
of
the
researcher.
Therefore,
a
debate
of
which
automated
method
is
the
“best”
is
considered
a
misplaced
debate
(Grimmer
&
Stewart,
2013).
For
example,
a
debate
existed
that
casted
unsupervised
and
supervised
method
as
competitors
(e.g.,
Hillard,
Purpura,
&
Wilkerson
2008;
Quinn
et
al.,
2010).
However,
both
methods
have
different
objectives
and
should
rather
be
seen
as
complementary
methods.
Practically,
supervised
methods
are
most
applicable
when
predeﬁned
categories
exist
for
the
texts
that
need
to
be
categorized.
If
no
categorization
scheme
is
available
beforehand,
unsupervised
method
are
useful
by
inductively
ﬁnding
new
categories.
In
some
cases
a
combination
between
these
methods
can
be
insightful,
especially
for
new
research
projects
with
recently
collected
data.
For
example,
unsupervised
methods
could
complement
supervised
methods
by
contributing
to
new
coding
schemes,
while
supervised
methods
could
validate
and
generalize
the
ﬁndings
of
unsupervised
methods.
Besides
the
input
data,
the
initial
research
objective
should
guide
the
selection
of
the
type
of
computational
method.
The
deductive
methods,
the
dictionary
method
and
supervised
method,
are
mainly
applicable
when
researchers
have
clear
indications
of
what
to
expect.
It
can
be
used
to
conﬁrm
how
often
different
actors
use
certain
predeﬁned
categories
in
crisis
communication.
For
example,
the
supervised
method
used
in
the
discussed
article
by
Burscher
and
colleagues
(2014)
can
be
used
to
conﬁrm
what
generic
frames
different
actors
or
media
outlets
use
when
communicating
about
different
types
of
crisis.
Furthermore,
in
the
other
discussed
article
Schultz
and
colleagues
(2012)
used
previous
theory
and
a
qualitative
analysis
of
the
text
to
come
up
with
multiple
categories
that
were
than
used
to
categorize
text
with
the
help
of
a
dictionary
method.
Accordingly,
these
deductive
methods
are
also
highly
applicable
for
validating
ﬁndings
based
on
small-scale
and
qualitative
research.
The
inductive
unsupervised
method
is
mainly
useful
for
the
purpose
of
exploring
new
areas
or
complement
existing
ﬁndings.
The
discussed
articles
(Jonkman
&
Verhoeven,
2013;
Van
der
Meer
et
al.,
2014)
show
how
this
method
can
be
applied
to
explore
crisis
communication
processes
when
little
clear-cut
assumption
can
be
formulated
beforehand.
Because
most
computational
methods
addressed
in
this
paper
are
related
to
the
concept
of
framing,
its
theoretical
limitations
need
to
be
mentioned.
Framing
has
emerged
as
one
of
the
most
popular
research
areas
in
communication
science.
Yet
despite
the
popularity
of
framing
theory,
the
conceptualization
has
arguably
become
less
clear
over
the
years.
The
framing
literature
is
characterized
by
considerable
disagreement
over
the
exact
constitution,
both
in
terms
of
theoretical
and
methodological
operationalization,
and
a
strong
overlap
exists
with
other
conceptual
models
such
as
agenda-setting
and
priming
(Cacciatore,
Scheufele,
&
Iyengar,
2016;
Scheufele
&
Iyengar,
2014).
Academics
who
use
the
automated
content
analysis
to
identify
frames
in
texts
should
acknowledge
these
limitations
and
place
their
studies
along
the
lines
of
existing
framing
traditions
(e.g.,
psychology-rooted
or
sociology-rooted
framing
and
equivalency
framing
or
emphasis
framing).
Despite
the
theoretical
limitations
of
framing,
the
use
of
computational
methods
(in
combination
with
framing
theory)
is
still
considered
a
valuable
approach
to
automatically
categorize
large-sized
collections
of
texts
and
understand
what
is
emphasized
within
the
communication
of
crisis
events.
The
suggested
automated
methods
also
have
their
limitations.
The
tools
are
consistently
being
improved
and
researchers
look
for
new
ways
to
statistically
analyze
texts.
For
example,
one
could
argue
that
the
bag
of
word
approach,
that
forms
the
basis
for
all
discussed
methods,
has
its
limitations.
By
assuming
that
text
documents
are
a
bag
of
words,
the
word
order
does
not
inform
the
analysis.
Only
looking
at
word
(co)occurrences
signiﬁcantly
reduces
the
amount
of
information.
The
bag
of
word
approach
is
commonly
found
to
infer
substantively
interesting
properties
of
texts
(Hopkins
&
King,
2010),
960
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
however;
there
are
certain
other
approaches
that
are
capable
of
taking
more
elements
of
language
into
account.
For
example,
techniques
such
as
part
of
speech
tagging
and
named
entity
recognition
can
take
the
syntactic
structure
into
account.
Van
Atteveldt
and
colleagues
(2008)
build
a
model
to
automatically
code
semantic
relationships
and
disentangle
the
syntactical
function
of
the
elements
of
a
sentence.
This
type
of
methods,
developed
in
the
ﬁeld
of
computational
linguistics,
might
help
to
advance
the
current
automated
analysis
to
categorize
and
understand
content.
In
the
end
there
will
always
be
room
for
improvement
if
it
comes
to
using
algorithms
to
understand
texts.
Therefore,
for
automated
content
analysis
to
become
a
standard
tool
for
crisis
research,
academics
have
to
contribute
to
the
development
of
new
computational
methods
and
ways
to
validate
the
ﬁndings
of
computer-assisted
analyses.
However,
the
automated
content
analyses
as
discussed
in
this
paper
do
seem
to
serve
their
purpose
at
this
point.
Hopefully
this
brief
overview
provides
understanding
of
how
automated
tools
can
be
applied
within
the
ﬁeld
of
crisis
research
and
inspires
researchers
to
apply
them
to
large-scale
data
sets.
References
Benoit,
W.
L.
(1997).
Image
repair
discourse
and
crisis
communication.
Public
Relations
Review,
23(2),
177–186.
Blei,
D.
M.,
Ng,
A.
Y.,
&
Jordan,
M.
I.
(2003).
Latent
dirichlet
allocation.
the
Journal
of
Machine
Learning
Research,
3,
993–1022.
Budge,
I.,
&
Pennings,
P.
(2007).
Do
they
work?
Validating
computerised
word
frequency
estimates
against
policy
series.
Electoral
Studies,
26,
121–129.
Burscher,
B.,
Odijk,
D.,
Vliegenthart,
R.,
De
Rijke,
M.,
&
De
Vreese,
C.
H.
(2014).
Teaching
the
computer
to
code
frames
in
news:
Comparing
two
supervised
machine
learning
approaches
to
frame
analysis.
Communication
Methods
and
Measures,
8(3),
190–206.
Burscher,
B.,
Vliegenthart,
R.,
&
De
Vreese,
C.
H.
(2015).
Using
supervised
machine
learning
to
code
policy
issues:
Can
classiﬁers
generalize
across
contexts?
Annals
of
the
American
Academy
of
Political
and
Social
Science,
659(1),
122–131.
Burscher,
B.,
Vliegenthart,
R.,
&
de
Vreese,
C.
H.
(2015).
Frames
Beyond
Words
Applying
Cluster
and
Sentiment
Analysis
to
News
Coverage
of
the
Nuclear
Power
Issue.
Social
Science
Computer
Review,
in
press.
Cacciatore,
M.
A.,
Scheufele,
D.
A.,
&
Iyengar,
S.
(2016).
The
end
of
framing
as
we
know
it.
.
.
and
the
future
of
media
effects.
Mass
Communication
and
Society,
19(1),
7–23.
Coombs,
W.
T.,
&
Holladay,
S.
J.
(2008).
Comparing
apology
to
equivalent
crisis
response
strategies:
Clarifying
apology’s
role
and
value
in
crisis
communication.
Public
Relations
Review,
34,
252–257.
Coombs,
W.
T.
(2007).
Protecting
organization
reputations
during
a
crisis:
The
development
and
application
of
situational
crisis
communication
theory.
Corporate
Reputation
Review,
10(3),
163–176.
Coombs,
W.
T.
(2015).
On
going
crisis
communication;
planning,
managing,
and
responding.
London:
Sage
Publications,
Inc.
De
Vlieger,
E.,
&
Leydesdorff,
L.
(2011).
Content
analysis
and
the
measurement
of
meaning:
The
visualization
of
frames
in
collections
of
messages.
Public
Journal
of
Semiotics,
1,
28.
De
Vreese,
C.
H.
(2005).
News
framing:
Theory
and
typology.
Information
Design
Journal
+
Document
Design,
13(1),
51–62.
DiMaggio,
P.,
Nag,
M.,
&
Blei,
D.
(2013).
Exploiting
afﬁnities
between
topic
modeling
and
the
sociological
perspective
on
culture:
Application
to
newspaper
coverage
of
u.s
government
arts
funding.
Poetics,
41,
570–606.
Diermeier,
D.,
Godbout,
J.,
Yu,
B.,
&
Kaufmann,
S.
(2011).
Language
and
ideology
in
Congress?
British
Journal
of
Political
Science,
42(1),
31–55.
Eshbaugh-Soha,
M.
(2010).
The
tone
of
local
presidential
news
coverage?
Political
Communication,
27(2),
121–140.
Flaounas,
I.,
Ali,
O.,
Lansdall-
◦
©Welfare,
T.,
De
Bie,
T.,
Mosdell,
N.,
Lewis,
J.,
&
Cristianini,
N.
(2013).
Research
methods
in
the
age
of
digital
journalism.
Digital
Journalism,
1(1),
102–116.
Frey,
B.,
&
Dueck,
D.
(2007).
Clustering
by
passing
messages
between
data
points?
Science,
315(5814),
972–976.
Grimmer,
J.,
&
Stewart,
B.
M.
(2013).
Text
as
data:
The
promise
and
pitfalls
of
automatic
content
analysis
methods
for
political
texts.
Political
Analysis,
21(3),
267–297.
Grimmer,
J.
(2010).
A
Bayesian
hierarchical
topic
model
for
political
texts:
Measuring
expressed
agendas
in
senate
press
releases?
Political
Analysis,
18(1),
1–35.
Hart,
R.
P.
(2000).
Diction
5.0:
The
text
analysis
program.
Thousand
Oaks,
CA:
Sage-Scolari.
Hellsten,
I.,
Dawson,
J.,
&
Leydesdorff,
L.
(2010).
Implicit
media
frames:
Automated
analysis
of
public
debate
on
artiﬁcial
sweeteners.
Public
Understanding
of
Science,
19,
590–608.
Hillard,
D.,
Purpura,
S.,
&
Wilkerson,
J.
(2008).
Computer-assisted
topic
classiﬁcation
for
mixed-methods
social
science
research?
Journal
of
Information
Technology
&
Politics,
4(4),
31–46.
Hopkins,
D.,
&
King,
G.
(2010).
Extracting
systematic
social
science
meaning
from
text?
American
Journal
of
Political
Science,
54(1),
229–247.
Jackman,
S.
(2006).
Data
from
Web
into
R?
The
Political
Methodologist,
14(2),
11–16.
Jonkman,
J.,
&
Verhoeven,
P.
(2013).
From
risk
to
safety:
Implicit
frames
of
third-party
airport
risk
in
Dutch
quality
newspapers
between
1992
and
2009.
Safety
Science,
58,
1–10.
Jurafsky,
D.,
&
Martin,
J.
(2009).
Speech
and
natural
language
processing:
An
introduction
to
natural
language
processing,
computational
linguistics,
and
speech
recognition.
Upper
Saddle
River,
NJ:
Prentice
Hall.
Kim,
H.
J.,
&
Cameron,
G.
T.
(2011).
Emotions
matter
in
crisis:
The
role
of
anger
and
sadness
in
the
publics’
response
to
crisis
news
framing
and
corporate
crisis
response.
Communication
Research,
38(6),
826–855.
Kim,
Y.,
Kim,
S.,
Jaimes,
A.,
&
Oh,
A.
(2014).
A
computational
analysis
of
agenda
setting.
Proceedings
of
the
companion
publication
of
the
23rd
international
conference
on
world
wide
web
companion,
323–324.
Kleinnijenhuis,
J.,
Schultz,
F.,
Utz,
S.,
&
Oegema,
D.
(2013).
The
mediating
role
of
the
news
in
the
BP
oil
spill
crisis
2010:
How
us
news
is
inﬂuenced
by
public
relations
and
in
turn
inﬂuences
public
awareness,
foreign
news,
and
the
share
price.
Communication
Research,
1–21.
Kleinnijenhuis,
J.,
Schultz,
F.,
&
Oegema,
D.
(2015).
Frame
complexity
and
the
ﬁnancial
crisis:
A
comparison
of
the
United
States,
the
United
Kingdom,
and
Germany
in
the
period
2007–2012.
Journal
of
Communication,
65(1),
1–23.
Krestel,
R.,
&
Mehta,
B.
(2010).
Learning
the
importance
of
latent
topics
to
discover
highly
inﬂuential
news
items.
In
KI
2010:
Advances
in
artiﬁcial
intelligence.
pp.
211–218.
Berlin,
Germany:
Springer.
Krippendorff,
K.
(2004).
Content
analysis:
An
introduction
to
its
methodology.
New
York:
Sage.
Kuttschreuter,
M.,
Gutteling,
J.
M.,
&
de
Hond,
M.
(2011).
Framing
and
tone-of-voice
of
disaster
media
coverage:
The
aftermath
of
the
Enschede
ﬁreworks
disaster
in
the
Netherlands.
Health,
Risk
&
Society,
13(3),
201–220.
Leydesdorff,
L.,
&
Hellsten,
I.
(2005).
Metaphors
and
diaphors
in
science
communication
mapping
the
case
of
stem
cell
research.
Science
Communication,
27(1),
64–99.
Leydesdorff,
L.,
&
Hellsten,
I.
(2006).
Measuring
the
meaning
of
words
in
contexts:
An
automated
analysis
of
controversies
about
‘Monarch
butterﬂies’,
‘Frankenfoods’
and
‘stem
cells’.
Scientometrics,
67,
231–258.
Liu,
B.
F.,
&
Kim,
S.
(2011).
How
organizations
framed
the
2009
H1N1
pandemic
via
social
and
traditional
media:
Implications
for
US
health
communicators.
Public
Relations
Review,
37,
233–244.
Lowe,
W.
(2011).
JFreq:
Count
words,
quickly&rsquo
java
software
version
0.5.4..
URL.
http://www.conjugateprior.org/software/jfreq/
Madden,
S.,
Janoske,
M.,
&
Briones,
R.
L.
(2016).
The
double-edged
crisis:
Invisible
Children’s
social
media
response
to
the
Kony
2012
campaign.
Public
Relations
Review,
42(1),
38–48.
T.G.L.A.
van
der
Meer
/
Public
Relations
Review
42
(2016)
952–961
961
Manning,
C.,
Raghavan,
P.,
&
Schütze,
H.
(2008).
Introduction
to
information
retrieval.
Cambridge,
UK:
Cambridge
University
Press.
Miller,
M.
M.
(1997).
Frame
mapping
and
analysis
of
news
coverage
of
contentious
issues.
Social
Science
Computer
Review,
15(4),
367–378.
Mostafa,
M.
M.
(2013).
More
than
words:
Social
networks’
text
mining
for
consumer
brand
sentiments.
Expert
Systems
with
Applications,
40(10),
4241–4251.
Netten,
N.,
&
van
Someren,
M.
(2006).
Automated
support
for
dynamic
information
distribution
in
incident
management.
In
Proceedings
of
the
3
International
ISCRAM
Conference.
Neuendorf,
K.
A.
(2002).
The
content
analysis
guidebook.
Thousand
Oaks,
CA:
Sage
Publications,
Inc.
Ott,
L.,
&
Theunissen,
P.
(2015).
Reputations
at
risk:
Engagement
during
social
media
crises.
Public
Relations
Review,
41(1),
97–102.
Pennebaker,
J.,
Francis,
M.,
&
Booth,
R.
(2001).
Linguistic
inquiry
and
word
count:
LIWC
2001.
Mahway,
NJ:
Erlbaum
Publishers.
Perry,
R.
W.
(2007).
What
is
a
crisis?
In
H.
Rodriguez,
E.
L.
Quarantelli,
&
R.
R.
Dynes
(Eds.),
Handbook
of
disaster
research
(pp.
1–15).
Ney
York,
NY:
Springer.
Quinn,
K.
M.,
Monroe,
B.
L.,
Colaresi,
M.,
Crespin,
M.
H.,
&
Radev,
D.
R.
(2010).
How
to
analyze
political
attention
with
minimal
assumptions
and
costs.
American
Journal
of
Political
Science,
54(1),
209–228.
ˇ
Reh
˚
u
ˇ
rek,
R.,
&
Sojka,
P.
(2010).
Software
framework
for
topic
modelling
with
large
corpora.
In
Proceedings
of
the
LREC
2010
Workshop
on
New
Challenges
for
NLP
Frameworks
(pp.
45–50).
Ramirez,
E.
H.,
Brena,
R.,
Magatti,
D.,
&
Stella,
F.
(2012).
Topic
model
validation.
Neurocomputing,
76(1),
125–133.
Riff,
D.,
Lacy,
S.,
&
Fico,
F.
(2014).
Analyzing
media
messages:
Using
quantitative
content
analysis
in
research.
New
York,
NY:
Routledge.
Russell,
S.,
&
Norvig,
P.
(2002).
Artiﬁcial
intelligence:
A
modern
approach.
Upper
Saddle
River,
NJ:
Prentice
Hall.
Scheufele,
D.
A.,
&
Iyengar,
S.
(2014).
The
state
of
framing
research.
In
The
oxford
handbook
of
political
communication.
Schultz,
F.,
Utz,
S.,
&
Göritz,
A.
(2011).
Is
the
medium
the
message?
Perceptions
of
and
reactions
to
crisis
communication
via
twitter,
blogs
and
traditional
media.
Public
Relations
Review,
37(1),
20–27.
Schultz,
F.,
Kleinnijenhuis,
J.,
Oegema,
D.,
Utz,
S.,
&
Van
Atteveldt,
W.
(2012).
Strategic
framing
in
the
BP
crisis:
A
semantic
network
analysis
of
associativeframes.
Public
Relations
Review,
38,
97–107.
Seeger,
M.
W.
(2002).
Chaos
and
crisis:
Propositions
for
a
general
theory
of
crisis
communication.
Public
Relations
Review,
28(4),
329–337.
Semetko,
H.
A.,
&
Valkenburg,
P.
M.
(2000).
Framing
European
politics:
A
content
analysis
of
press
and
television
news.
Journal
of
Communication,
50(2),
93–109.
Slapin,
J.,
&
Proksch,
S.
(2008).
A
scaling
model
for
estimating
time-series
party
positions
from
texts?
American
Journal
of
Political
Science,
52(3),
705–722.
Taddy,
M.
A.
(2010).
Inverse
regression
for
analysis
of
sentiment
in
text.
Arxiv
Preprint
ArXiv,
1012,
2098.
Thelwall,
M.,
&
Stuart,
D.
(2007).
RUOK?
Blogging
communication
technologies
during
crises.
Journal
of
Computer-Mediated
Communication,
12(2),
523–548.
Thelwall,
M.,
Buckley,
K.,
Paltoglou,
G.,
Cai,
D.,
&
Kappas,
A.
(2010).
Sentiment
strength
detection
in
short
informal
text.
Journal
of
the
American
Society
for
Information
Science
and
Technology,
61(12),
2544–2558.
Turney,
P.,
&
Littman,
M.
L.
(2003).
Measuring
praise
and
criticism:
Inference
of
semantic
orientation
from
association?
ACM
Transactions
on
Information
Systems
(TOIS),
21(4),
315–346.
Van
Atteveldt,
W.,
Kleinnijenhuis,
J.,
&
Ruigrok,
N.
(2008).
Parsing,
semantic
networks,
and
political
authority
using
syntactic
analysis
to
extract
semanticrelations
from
Dutch
newspaper
articles.
Political
Analysis,
16,
428–446.
Van
der
Meer,
T.
G.
L.
A.,
&
Verhoeven,
P.
(2013).
Public
framing
organizational
crisis
situations:
Social
media
versus
news
media.
Public
Relations
Review,
39,
229–231.
Van
der
Meer,
T.
G.
L.
A.,
Verhoeven,
P.,
Beentjes,
H.,
&
Vliegenthart,
R.
(2014).
When
frames
align:
The
interplay
between
PR,
news
media:
and
the
public
in
times
of
crisis.
Public
Relations
Review,
40,
751–761.
Van
der
Meer,
T.
G.
L.
A.
(2014).
Organizational
crisis-denial
strategy:
The
effect
of
denial
on
public
framing.
Public
Relations
Review,
40,
537–539.
Verhoeven,
P.,
Tench,
R.,
Zerfass,
A.,
Moreno,
A.,
&
Ver
ˇ
ci
ˇ
c,
D.
(2014).
Crisis?
What
crisis?;
How
European
professionals
handle
crises
and
crisis
communication.
Public
Relations
Review,
40,
107–109.
Vlieger,
E.,
&
Leydesdorff,
L.
(2011).
Content
analysis
and
the
measurement
of
meaning:
The
visualization
of
frames
in
collections
of
messages.
Public
Journal
of
Semiotics,
1,
28.
