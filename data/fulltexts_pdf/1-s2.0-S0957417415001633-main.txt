An analysis of the coherence of descriptors in topic modeling
Derek O’Callaghan
⇑
,
Derek Greene,
Joe Carthy,
Pádraig Cunningham
School of Computer Science & Informatics,
University College Dublin,
Ireland
a r t i
c l e
i
n f o
Article history:
Available online 9 March 2015
Keywords:
Topic modeling
Topic coherence
LDA
NMF
a b s t r a c t
In recent years,
topic modeling has become an established method in the analysis of text corpora,
with
probabilistic techniques such as latent Dirichlet allocation (LDA) commonly employed for this purpose.
However,
it might be argued that adequate attention is often not paid to the issue of topic coherence,
the semantic interpretability of the top terms usually used to describe discovered topics.
Nevertheless,
a number of studies have proposed measures for analyzing such coherence,
where these have been lar-
gely focused on topics found by LDA, with matrix decomposition techniques such as Non-negative Matrix
Factorization (NMF) being somewhat overlooked in comparison. This motivates the current work, where
we compare and analyze topics found by popular variants of both NMF and LDA in multiple corpora in
terms of both their coherence and associated generality,
using a combination of existing and new mea-
sures, including one based on distributional semantics. Two out of three coherence measures ﬁnd NMF to
regularly produce more coherent topics,
with higher levels of generality and redundancy observed with
the LDA topic descriptors.
In all cases,
we observe that the associated term weighting strategy plays a
major role.
The results observed with NMF suggest that this may be a more suitable topic modeling
method when analyzing certain corpora,
such as those associated with niche or
non-mainstream
domains.
Ó 2015 Elsevier Ltd.
All rights reserved.
1. Introduction
Topic modeling is a key tool for the discovery of latent semantic
structure within a variety of document collections,
where proba-
bilistic models such as latent Dirichlet allocation (LDA) have effec-
tively become the de facto standard method employed (Blei, Ng, &
Jordan,
2003).
The discovered topics are usually described using
their
corresponding top N highest-ranking terms,
for
example,
the top 10 most probable terms from an LDA / topic distribution
over terms.
In the case of probabilistic topic models,
a number of
metrics
are used to evaluate model
ﬁt,
such as
perplexity or
held-out
likelihood (Wallach,
Murray,
Salakhutdinov,
& Mimno,
2009).
At the same time,
it might be argued that less attention is
paid to the issue of topic coherence, or the semantic interpretability
of the terms used to describe a particular topic,
despite the obser-
vation that evaluation methods such as perplexity are often not
correlated with human judgements
of
topic
quality
(Chang,
Boyd-Graber,
Gerrish,
Wang,
& Blei,
2009).
However,
a number of
measures have been proposed in recent years for the measurement
of coherence, based on approaches that include co-occurrence fre-
quencies
of
terms
within a
reference
corpus
(Newman,
Lau,
Grieser,
& Baldwin,
2010;
Mimno,
Wallach,
Talley,
Leenders,
&
McCallum,
2011;
Lau,
Newman,
& Baldwin,
2014)
and dis-
tributional
semantics (Aletras & Stevenson,
2013).
The intuition
is that pairs of topic descriptor terms that co-occur frequently or
are close to each other within a semantic space are likely to con-
tribute to higher levels of coherence.
Non-probabilistic methods based on matrix decomposition are
also used for topic modeling,
such as Latent Semantic Analysis
(LSA) (Deerwester,
Dumais,
Landauer,
Furnas,
& Harshman,
1990)
or Non-negative Matrix Factorization (NMF) (Lee & Seung,
1999;
Arora, Ge, & Moitra, 2012). Here, topic term descriptors can be gen-
erated in a similar fashion to those of
probabilistic models,
for
example, using the top N highest-ranked terms from an NMF topic
basis vector. In our previous work, we generated topics using both
LDA and NMF with two particular corpora,
where a qualitative
analysis of the corresponding term descriptors found the most read-
ily-interpretable topics to be discovered by NMF (O’Callaghan,
Greene, Conway, Carthy, & Cunningham, 2013). An example of the
issues we encountered can be illustrated with the following topics
that were discovered by LDA and NMF for the same value of k within
a corpus of
online news articles (described in further detail
in
Section 5):
http://dx.doi.org/10.1016/j.eswa.2015.02.055
0957-4174/Ó 2015 Elsevier Ltd.
All rights reserved.
⇑
Corresponding author.
E-mail
addresses:
derek.o-callaghan@ucdconnect.ie
(D.
O’Callaghan),
derek.
greene@ucd.ie (D.
Greene),
joe.carthy@ucd.ie (J.
Carthy),
padraig.cunningham@
ucd.ie (P.
Cunningham).
Expert Systems with Applications 42 (2015) 5645–5657
Contents lists available at ScienceDirect
Expert Systems with Applications
j o u r n a l
homepag e:
w w w . e l s e v i e r . c o m / l o c a t e / e s w a
 LDA: iran, syria, syrian, iraq, weapon, president, war, nuclear, mili-
tary,
iranian.
 NMF: syria, syrian, weapon, chemical, assad, damascus, rebel, mili-
tary,
opposition,
lebanon.
At a glance, both topics appear both relevant and coherent, with
no identiﬁable irrelevant terms,
where the topics may be inter-
preted as being associated with the ongoing Syria conﬂict. A closer
inspection of the terms suggests that the LDA topic is in fact a gen-
eral topic about the Middle East,
while the NMF topic is far more
speciﬁcally concerned with Syria (including the lebanon term in
this context), which could also be interpreted as being more coher-
ent depending on the end user’s expectations. This issue regarding
the possibility for LDA to over-generalize has been raised pre-
viously by Chemudugunta,
Smyth,
and Steyvers (2006).
However,
a study by Stevens,
Kegelmeyer,
Andrzejewski,
and Buttler (2012)
of the coherence of topics discovered by LSA, NMF and LDA within
a single corpus composed of online New York Times articles from
2003 (Sandhaus,
2008),
concluded that NMF produced the more
incoherent topics.
As our previous ﬁndings suggest that this issue
is unresolved,
we perform an evaluation of LDA and NMF using a
range of corpora, where our two major objectives are the measure-
ment and comparison of (1) topic coherence, and (2) topic general-
ity.
The latter
is considered at
two levels;
the tendency for
a
method to generate topics containing high-frequency descriptor
terms from the underlying corpus,
and also the presence of terms
in multiple descriptors for a particular model,
signifying the exis-
tence of overlap or dependence between the topics.
To this end, we compiled six new and existing corpora contain-
ing documents that had been (manually) annotated with classes,
including online news articles from the BBC,
the Guardian,
and
the New York Times, in addition to Wikipedia project page content.
A consistent set of pre-processing steps was applied to these,
and
topics were discovered with LDA and NMF. Although multiple vari-
ants exist
for
both topic modeling methods,
we restricted the
experiments to those that are commonly used, with popular imple-
mentations being run accordingly (McCallum,
2002;
Pedregosa
et
al.,
2011),
in addition to recommended parameter
values
(Steyvers & Grifﬁths, 2006). Two out of three coherence measures,
including a new measure based on word2vec (Mikolov,
Chen,
Corrado,
& Dean,
2013) term vector similarity,
ﬁnd NMF to regu-
larly produce more coherent topics, while higher levels of general-
ity and redundancy are observed with the LDA topic descriptors.
However,
we observe that the associated term weighting strategy
plays a major role,
as modiﬁcations to both document term pre-
processing (NMF) and descriptor term post-processing (LDA) can
produce markedly different results.
Separately,
we also ﬁnd that
LDA produces more accurate document-topic memberships when
compared with the original class annotations.
2. Related work
2.1.
Topic modeling
Topic modeling is concerned with the discovery of latent seman-
tic structure or topics within a set of
documents,
which can be
derived from co-occurrences of words in documents (Steyvers &
Grifﬁths, 2006). This strategy dates back to the early work on latent
semantic indexing by Deerwester et al. (1990), which proposed the
decomposition of term-document matrices for this purpose using
Singular
Value Decomposition.
Probabilistic topic models
have
become popular in recent years,
having been introduced with the
Probabilistic Latent Semantic Analysis (PLSA) method of Hofmann
(2001),
also known as
Probabilistic
Latent
Semantic
Indexing
(PLSI).
Here,
a topic is a probability distribution over words,
with
documents being mixtures of topics, thus permitting a topic model
to be considered a generative model
for documents (Steyvers &
Grifﬁths, 2006). With this process, a document is generated by ﬁrst
sampling a topic z from the document-topic distribution h, followed
by a word w from the corresponding topic-word distribution /. The
extension of
this model
by Blei
et
al.
(2003),
known as latent
Dirichlet allocation (LDA),
suggested using a Dirichlet prior on h
with an associated hyperparameter
a
.
Grifﬁths
and Steyvers
(2004) proposed also using a Dirichlet prior on /, with correspond-
ing hyperparameter b.
The plate notation for this model
can be
found in Fig. 1.
Grifﬁths and Steyvers (2004) also used collapsed Gibbs sam-
pling to indirectly estimate these distributions,
by iteratively
estimating the probability of
assigning each word to the topics,
conditioned on the current topic assignments of all
other words,
using count
matrices of
topic-word (C
WT
)
and document-topic
(C
DT
) assignments:
Pðz
i
¼ jjz
i
; w
i
; d
i
; :Þ
/
C
WT
w
i
;j
þ b
P
W
w¼1
C
WT
w;j
þ Wb
C
DT
d
i
;j
þ
a
P
T
t¼1
C
DT
d
i
;t
þ T
a
ð1Þ
Following this process,
the distributions for sampling a word i
from topic j (/
j
),
and topic j for document d (h
d
) are estimated as:
/
j
¼
C
WT
ij
þ b
P
W
w¼1
C
WT
wj
þ Wb
h
d
¼
C
DT
dj
þ
a
P
T
t¼1
C
DT
dt
þ T
a
ð2Þ
There have been a number of additional
variants of LDA pro-
posed in recent years.
However,
in this paper,
we are primarily
concerned with the coherence of topic modeling in general,
and
so the discussion here is accordingly restricted to (a)
popular
LDA variants,
and (b) those used by the topic coherence experi-
ments described in Section 2.2. Two popular toolkits that are often
used for topic modeling with LDA are MALLET (McCallum,
2002),
which provides
a fast
implementation of
the Gibbs
sampling
method described above,
and gensim (R
ˇ
ehu
˚
r
ˇ
ek & Sojka,
2010),
which
implements
the
online
variational
Bayes
method
of
Hoffman,
Blei,
and Bach (2010).
The motivation for
the latter
method was the application of LDA to data streams or large data-
sets.
In addition to the method implementations
provided by
MALLET and gensim,
other prominent methods featuring in the
topic coherence experiments that have not been discussed so far
include the Correlated Topic Model
(CTM)
of
Blei
and Lafferty
(2006),
which attempts to directly model
correlation between
the latent topics themselves,
and the Pólya Urn method proposed
by Mimno et al.
(2011),
which extended Gibbs sampling to incor-
porate information used in the corresponding coherence metric.
Non-negative Matrix Factorization (NMF)
is a technique for
decomposing a non-negative matrix V 2 R into two non-negative
factors W and H,
where V  WH (Lee & Seung,
1999).
Although it
has been used in multiple domains,
it is also applicable to topic
modeling (Arora et al.,
2012).
In this context,
V is an n  m term-
document matrix,
and W and H are reduced rank-k factors whose
product is an approximation of V,
with dimensions W ¼ n  k and
H ¼ k  m. This enables a parts-based representation, where W con-
tains a set of k topic basis vectors, and H provides the coefﬁcients for
the additive linear combinations of these basis vectors to generate
the corresponding document vectors in V. The weights in a W topic
basis vector can be used to generate a topic descriptor consisting of
high-ranking terms (analogous to the most probable terms in an
LDA / distribution),
while a H vector of coefﬁcients can be inter-
preted as the k topic membership weights for the corresponding
document.
Two common objective functions (Lee & Seung,
2001)
used to generate W and H are the Euclidean squared error:
X
n
i¼1
X
m
j¼1
ðV
ij
 ðWHÞ
ij
Þ
2
¼ kV  WHk
2
F
ð3Þ
5646
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
and the Kullback–Leibler (KL) divergence, when V and WH both sum
to 1 (thus acting as normalized probability distributions):
DðVjjWHÞ ¼
X
n
i¼1
X
m
j¼1
V
ij
log
V
ij
ðWHÞ
ij
!
ð4Þ
NMF with KL divergence was previously shown to be equivalent
to PLSA by Gaussier and Goutte (2005).
As an alternative to the
multiplicative update rules approach of Lee and Seung (2001) for
determining W and H,
Lin (2007) proposed the use of a projected
gradient
method with alternating non-negative least
squares.
Separately,
to address the instability introduced by standard ran-
dom initialization of W and H, Boutsidis and Gallopoulos (2008) pro-
posed
deterministic
initialization
with
Non-Negative
Double
Singular
Value Decomposition (NNDSVD),
which is particularly
suitable for sparse matrices. As with LDA, here we are primarily con-
cerned with popular NMF variants,
where the focus is upon the
implementation of the method proposed by Lin (2007),
with the
squared error objective function and NNDSVD initialization, as pro-
vided by the scikit-learn machine learning library (Pedregosa et al.,
2011).
2.2.
Topic coherence
Although perplexity (held-out likelihood) has been a common
method for the evaluation of
topic models,
the study of
Chang
et al.
(2009) found that this was often negatively correlated with
human judgements of topic quality (using topics discovered with
PLSI,
CTM,
and LDA),
and suggested that
evaluation should be
focused upon real-world task performance that includes human
annotation.
This has led to a number of studies that have focused
upon the development of topic coherence measures, which capture
the semantic interpretability of discovered topics based on their
corresponding descriptor
terms
(for
example,
the top N most
probable
terms
from a
/ distribution
estimated
by
LDA).
Newman et al.
(2010) calculated the correlation between human
judgements and a set
of
proposed measures,
and found that
a
Pointwise Mutual
Information (PMI)
measure achieved best
or
near-best out of all evaluated measures.
This was based on co-oc-
currence frequency of each set of top 10 (LDA) topic terms within a
reference corpus (Wikipedia), using a sliding window of 10 words,
with the mean pairwise term PMI used as an individual topic score,
where the intuition was that terms that regularly co-occurred were
likely to produce coherent topic descriptors.
A similar co-occur-
rence measure was suggested by Mimno et al. (2011),
which used
log conditional probability (LCP) rather than PMI (conditioned on
the higher-ranking term in each term pair), and was found to pro-
duce higher correlation with human judgements than that of the
latter.
In contrast to Newman et al.
(2010),
the co-occurrence fre-
quencies were calculated using the corpus being modeled,
rather
than relying upon a reference corpus.
Here,
LDA topics were dis-
covered using the Gibbs sampling method along with their pro-
posed extension.
Both of these measures were employed in the study of Stevens
et al. (2012), which compared the coherence of topics generated by
LSA,
NMF and LDA using a model-level summarization.
Although
they found that each of these methods had certain strengths,
they
also concluded that
NMF tended to produce more incoherent
topics
than
either
of
the
other
two
methods.
Aletras
and
Stevenson (2013) proposed measuring LDA topic coherence using
distributional similarity (DS) between the top terms,
where each
term was represented as a vector in a semantic space,
with topic
coherence calculated as mean pairwise vector similarity; Cosine
similarity,
Jaccard similarity,
and the Dice coefﬁcient were used.
As before, these were correlated with human judgements, in addi-
tion to the PMI
measure,
a normalized variant of
PMI
(Bouma,
2009) (NPMI, range = ½1; 1), and LCP. They found the term vector
similarity measures to compare favorably with those based on PMI,
in particular, Cosine similarity, while LCP performed poorly in gen-
eral, where they suggested that the latter is sensitive to the size of
the modeled corpus. Lau et al. (2014) performed an empirical com-
parison of these four PMI,
NPMI,
LCP and DS measures in the con-
text of the original
evaluation tasks used by Chang et al.
(2009)
(using PLSI, LDA, and CTM), where the NPMI and DS measures were
those most strongly correlated with the human raters at the topic
level.
To address the sparsity issue with using the corpus being
modeled to calculate term co-occurrence for LCP,
as pointed out
by Aletras and Stevenson (2013)
(and also an earlier
work by
Lau,
Baldwin,
& Newman,
2013),
they proposed instead using the
same reference corpus as required by the PMI measures.
More recently,
Röder,
Both,
and Hinneburg (2015) proposed a
unifying framework that
represented coherence measures as a
composition of
parts,
with the objective
of
achieving higher
correlation with human judgements.
This
was
an attempt
to
address certain issues raised in their earlier work in relation to
coherence
measures
based
on
term co-occurrence
(Rosner,
Hinneburg,
Röder,
Nettling,
& Both,
2013).
The emphasis was lar-
gely on topics discovered by LDA,
as they followed the evaluation
schemes of Newman et al.
(2010),
Aletras and Stevenson (2013)
and Lau et al. (2014). One clear observation that can be made about
these previous works is the attention given to LDA or
similar
probabilistic topic modeling methods,
where matrix factorization
methods such as NMF are very much in the minority.
The current
work aims to address this issue with a comparison of topics discov-
ered with NMF and LDA across multiple corpora,
particularly in
light of our previous ﬁndings that the former produces more read-
ily-interpretable topics (O’Callaghan et al.,
2013).
3. Data
A range of corpora were analyzed in this evaluation,
where we
were focused upon both new and existing corpora containing
documents that had been (manually) annotated with classes.
The
ﬁrst new corpus contained news articles from the BBC website
1
.
At
the start
of
January 2014,
we retrieved all
tweets up to the
3,200 Twitter
REST API
2
limit
for
71 manually selected Twitter
accounts afﬁliated with the BBC or
its journalists (for
example,
@BBCNews,
@BBCSport,
@bbcscitech),
which yielded a
total
of
91,616 tweets.
All
unique URLs containing the domains bbc.co.uk
or bbc.com were extracted from these tweets, and the corresponding
Fig. 1.
Plate notation for the graphical LDA topic model.
1
http://www.bbc.com
2
https://dev.twitter.com/docs/api
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
5647
web pages (where still
accessible)
were then retrieved,
with the
exception of
a set of
blacklisted URL preﬁxes (for example,
those
related to careers,
advertising and other non-news content).
From
the retrieved pages,
we extracted and fetched web pages for all
unique URLs containing either of
the two BBC domains that had
not
been previously fetched;
this process was performed twice.
We then ﬁltered all
retrieved articles that were published outside
the time period 2009-01-01 to 2013-12-31,
or whose publication
date could not be ascertained from the corresponding page meta-
data.
The article body text was extracted using the Java Boilerpipe
library
3
(Kohlschütter, Fankhauser, & Nejdl, 2010), with articles con-
taining empty body text being ﬁltered. The ﬁnal corpus consisted of
all
articles in the top 40 sections,
as annotated by the BBC and
extracted from the page metadata,
where each article is assigned
to exactly one section.
A similar process was used to generate a corpus containing news
articles from the Guardian website,
4
where 29 manually selected
Twitter accounts afﬁliated with the Guardian or its journalists (for
example,
@guardian,
@GdnPolitics,
@guardianﬁlm)
yielded a total
of 184,284 tweets.
Unique URLs containing the domains guardian.-
co.uk or theguardian.com were used to fetch the article text,
with
the ﬁnal corpus consisting of articles found in the top 24 annotated
sections as extracted from the metadata,
with each article featuring
one section annotation. Two additional news corpora were extracted
from the New York Times Corpus,
which contains over 1.8 million
articles written and published by the New York Times (NYT) between
January 1,
1987 and June 19,
2007,
in addition to article metadata
annotations (Sandhaus,
2008).
The ﬁrst of these corpora consisted
of articles from 2003, as also analyzed by Stevens et al. (2012) in their
coherence evaluation of multiple topic modeling methods,
with the
second containing a stratiﬁed 10% sample of articles from 2000 to
2007.
In contrast to our extracted BBC and Guardian corpora,
the
NYT articles
can contain multiple section annotations.
For
both
extracted corpora, identiﬁable meta-sections covering multiple topics
were excluded, such as ‘‘Front Page’’, ‘‘Corrections’’, and ‘‘Magazine’’.
The ﬁnal two corpora consisted of pages found on Wikipedia. We
initially selected eight top-level
categories from the WikiProject
Council Directory
5
that had active sub-categories and/or task forces,
and also selected the set of active sub-categories.
As the sub-cate-
gories found in each top-level category directory are often not peers
of
each other in the hierarchy,
sub-categories at the highest level
were chosen,
for example a sport category was selected as opposed
to a category for a particular team.
For each category,
a list of page
titles in the classes ‘FA’,
‘FL’,
‘A’,
‘GA’,
‘B’,
‘C’
was retrieved from the
corresponding category class page,
and the page text was extracted
from a Wikipedia dump from January 2014 by means of a wrapper
around the WikiExtractor utility.
6
This resulted in 42,170 page titles
for 194 categories containing text with P10 terms, with some titles
belonging to multiple categories.
The two corpora consisted of
a
selection of
top-level
categories
and lower-level
sub-categories
respectively.
Details of all six corpora used in the evaluation can be
found in Table 1,
and pre-processed versions are made available
online for further research.
7
4. Methodology
4.1.
Topic discovery and descriptor generation
A set of common pre-processing steps were applied to all six cor-
pora.
We
initially compiled a
‘‘standard’’
set
of
671 English
stopwords from a variety of
sources including those featured in
the machine learning library scikit-learn (Pedregosa et al.,
2011),
the Natural Language Toolkit (NLTK) (Loper & Bird,
2002),
and the
MALLET toolkit (McCallum, 2002), in addition to English honoriﬁcs
and contractions documented on Wikipedia.
8,9
These stopwords
were ﬁltered,
along with any terms containing common top-level-
domains such as ‘‘.com’’ or ‘‘.co.uk’’. URLs and terms containing digits
were also ﬁltered, and ascii normalization was performed to remove
diacritics.
Terms were converted to lowercase,
and a lemmatizer
was applied (NLTK wrapper around WordNet’s built-in morphy func-
tion). Stemming was not performed as it often led to the subsequent
generation of topic descriptor terms that were not interpretable by
an end user.
Finally,
low-frequency terms occurring in fewer than l
of the total m documents were also excluded,
where the l threshold
was set to maxð10; m=1000Þ.
Although multiple variants exist for both LDA and NMF (for
example,
Blei,
Grifﬁths,
Jordan,
& Tenenbaum,
2004;
Rosen-Zvi,
Grifﬁths,
Steyvers,
& Smyth,
2004; Saha & Sindhwani,
2012),
we
restricted the experiments to those that are popular and/or were
used
by
the
topic
coherence
papers
discussed
earlier
in
Section 2.2.
In the case of LDA,
we used the fast Gibbs sampling
implementation provided by
the
MALLET
toolkit
(McCallum,
2002), with the same parameters as found in the coherence evalua-
tion code provided by Stevens et al. (2012)
10
apart from an increased
number
of
iterations.
This
included the
recommended values
(Steyvers & Grifﬁths,
2006) for the Dirichlet hyperparameters
a
and
b of 50=k (k = number of topics) and 0.01 respectively (these are actu-
ally the
default
parameter
values
in MALLET),
in addition to
hyperparameter optimization for an asymmetric Dirichlet prior over
the document-topic distribution h (Wallach,
Mimno,
& McCallum,
2009). LDA operates on bag-of-words document representations, and
the corresponding feature sequences used by MALLET were created
for each corpus following the pre-processing steps described above.
For
NMF,
the same pre-processed corpus
documents
were
transformed to
log-based Term Frequency-Inverse
Document
Frequency (TF-IDF) vectors (Salton & Buckley,
1988),
and subse-
quently normalized to unit
length.
We used the squared error
NMF variant as provided by scikit-learn (Pedregosa et al.,
2011),
which is an implementation of the fast alternating least squares
method proposed by Lin (2007).
The Kullback–Leibler objective
function was not considered for this evaluation due to its equiva-
lence to PLSA (Gaussier
& Goutte,
2005),
itself
a probabilistic
Table 1
The six corpora used in the topic coherence evaluation,
including the number of
documents and terms following pre-processing (described in Section 4.1).
Documents
Terms
Classes
(a) Corpus size
BBC
161,469
17,079
40
Guardian
194,153
22,141
24
NYT 2003
70,134
20,429
20
NYT 2000+ (10%)
65,335
21,461
20
Wikipedia (high-level)
5682
28,699
6
Wikipedia (lower-level)
4970
24,265
10
Min Max
Mean
Median Standard deviation
(b) Document length statistics (terms)
BBC
10
7241 242.00 167
362.53
Guardian
10
19080 299.08 250
276.65
NYT 2003
10
7816 282.40 260
239.42
NYT 2000+ (10%)
10
16997 302.88 275
298.26
Wikipedia (high-level)
10
7510 897.35 607
901.03
Wikipedia (lower-level)
10
7520 924.31 604
929.62
3
https://code.google.com/p/boilerpipe/
4
http://www.theguardian.com
5
http://en.wikipedia.org/wiki/Wikipedia:WikiProject_Council/Directory
6
http://medialab.di.unipi.it/wiki/Wikipedia_Extractor
7
http://mlg.ucd.ie/topiccoherence/
8
http://en.wikipedia.org/wiki/English_honoriﬁcs
9
http://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions
10
https://github.com/fozziethebeat/TopicModelComparison
5648
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
precursor to LDA. To address the instability introduced by random
initialization in standard NMF,
the deterministic NNDSVD initial-
ization method was also employed (Boutsidis & Gallopoulos, 2008).
For each topic found by applying these variants of
LDA and
NMF,
a descriptor was created as follows:
1.
NMF
w
: the top 10 highest-ranking terms from the topic’s basis
vector in W
k
,
a factor of V
TFIDF
.
2.
LDA
u
:
the top 10 most
probable terms
from the topic’s
/
distribution.
The w (weighted) and u (unweighted) notation reﬂects the term
weighting strategy employed,
in addition to the simplest bag-of-
words
weighting strategy based on term frequencies.
The IDF
pre-processing step used by NMF
w
down-weights the contribution
of
TF in the case of
frequent
(more general)
terms,
while also
boosting the contribution of
rarer terms that may be more dis-
criminating.
As the analogous term weighting with LDA
u
is effec-
tively that of TF,
we refer to it as unweighted for the purpose of
this evaluation.
Although it is customary to generate LDA term
descriptors
using the most
probable terms,
Blei
and Lafferty
(2009) state their preference for selecting the top terms ranked
using the score deﬁned in Eq.
(5):
term-score
k;
v
¼
b
b
k;
v
log
b
b
k;
v
Q
K
j¼1
b
b
j;
v


1
K
0
B
B
@
1
C
C
A
ð5Þ
This is inspired by TF-IDF weighting, where the ﬁrst expression
b
k;
v
,
the probability of term
v
for topic k,
is analogous to TF,
while
the second expression down-weights terms that have high proba-
bility across all k topics, somewhat similar to IDF. As this operation
mirrors the weighted nature of NMF
w
, we also discovered topics by
applying NMF to TF input vectors V
TF
(minus the IDF component),
which in turn mirrors the unweighted nature of LDA
u
. Thus, the fol-
lowing topic descriptors are also generated:
1.
NMF
u
: the top 10 highest-ranking terms from the topic’s basis
vector in W
k
, a factor of V
TF
. As this is based on a pre-processing
operation,
these topics are entirely separate to those of NMF
w
.
●
●
●
●
●
●
●
●
●
●
0.05
0.10
0.15
0.20
0.25
10
30
50
70
90
k
TC−NPMI
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.05
0.10
0.15
0.20
0.25
10
30
50
70
90
k
TC−NPMI
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.05
0.10
0.15
0.20
0.25
10
30
50
70
90
k
TC−NPMI
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.05
0.10
0.15
0.20
0.25
10
30
50
70
90
k
TC−NPMI
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.05
0.10
0.15
0.20
0.25
10
30
50
70
90
k
TC−NPMI
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.05
0.10
0.15
0.20
0.25
10
30
50
70
90
k
TC−NPMI
●
NMF
w
NMF
u
LDA
u
LDA
w
Fig. 2.
Mean topic TC-NPMI coherence scores for k 2 ½10; 100.
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
5649
2.
LDA
w
: the top 10 highest-ranking terms, weighted using Eq. (5).
As this is based on a post-processing operation, these topics are
the same as those used by LDA
u
.
At this point, we note that other work that measured coherence
of topics found by LDA is largely focused upon the LDA
u
topic term
descriptors of option 2 (Newman et al.,
2010; Mimno et al.,
2011;
Stevens et al.,
2012; Aletras & Stevenson,
2013; Lau et al.,
2014).
4.2.
Measuring topic coherence and generality
Having generated a set
of
topic models,
the following topic
coherence measures were calculated for each of
the four term
descriptor methods NMF
w
,
LDA
u
,
NMF
u
,
and LDA
w
,
with N ¼ 10:
1.
TC-NPMI
– mean pairwise normalized PMI
(Aletras
&
Stevenson,
2013; Lau et al.,
2014) (the unnormalized ver-
sion originally proposed by Newman et al. (2010) was also
calculated,
but
the normalized version is reported here
given its superior
performance as demonstrated by Lau
et al.
(2014)):
TC-NPMI ¼
1
N
2


X
N
j¼2
X
j1
i¼1
log
Pðw
j
;w
i
Þþ

Pðw
i
ÞPðw
j
Þ
 log Pðw
i
; w
j
Þ þ

ð6Þ
2.
TC-LCP – mean pairwise log conditional
probability
(Mimno et al.,
2011):
TC-LCP ¼
1
N
2


X
N
j¼2
X
j1
i¼1
log
Pðw
j
; w
i
Þ þ

Pðw
i
Þ
ð7Þ
3.
TC-W2V – as an analog to the DS measures of Aletras
and Stevenson (2013),
we propose the creation of term
vectors w
v
using a word2vec model
(Mikolov et
al.,
2013).
This tool
provides two neural
network-based
algorithms
for
estimating word representations
in a
vector space; Continuous Bag-of-Words (CBOW), where
the current word is predicted based on its context,
and
Skip-gram,
which predicts context words based on the
current
word.
These approaches have been found to
generate word vectors that explicitly encode linguistic
●
●
●
●
●
●
●
●
●
●
−2.2
−2.0
−1.8
−1.6
−1.4
10
30
50
70
90
k
TC−LCP
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
−2.2
−2.0
−1.8
−1.6
−1.4
10
30
50
70
90
k
TC−LCP
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
−2.2
−2.0
−1.8
−1.6
−1.4
10
30
50
70
90
k
TC−LCP
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
−2.2
−2.0
−1.8
−1.6
−1.4
10
30
50
70
90
k
TC−LCP
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
−2.2
−2.0
−1.8
−1.6
−1.4
10
30
50
70
90
k
TC−LCP
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
−2.2
−2.0
−1.8
−1.6
−1.4
10
30
50
70
90
k
TC−LCP
●
NMF
w
NMF
u
LDA
u
LDA
w
Fig. 3.
Mean topic TC-LCP coherence scores for k 2 ½10; 100.
5650
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
regularities from large amounts of
unstructured text
data
(Mikolov,
Sutskever,
Chen,
Corrado,
& Dean,
2013), and so are appropriate for use with a large refer-
ence corpus in the analysis of topic coherence. Here, the
coherence score is the mean pairwise Cosine similarity
of two term vectors generated with a Skip-gram model:
TC-W2V ¼
1
N
2


X
N
j¼2
X
j1
i¼1
similarityðw
v
j
; w
v
i
Þ
ð8Þ
For each coherence measure,
we generated an aggregate score
for a particular (descriptor method,
k) model
by taking the mean
of
the constituent
topic scores.
Similar
model-level
coherence
scores were also used in the evaluation of Stevens et al.
(2012).
As suggested by Mimno et al.
(2011),
a smoothing count

¼ 1
was included as required to avoid taking the logarithm of zero.
In addition to measuring topic coherence, we also analyzed the
generality of
the topic descriptors for the four methods NMF
w
,
LDA
u
, NMF
u
, and LDA
w
. Here, generality is considered at two levels;
(1) the overlap or dependence between topics,
based on the pres-
ence of terms in multiple descriptors for a particular model,
and
(2)
the tendency for
a method to generate topics
containing
high-frequency descriptor terms from the underlying corpus.
As
discussed by Arora et al.
(2012),
some level of similarity is to be
expected,
but
lower
numbers
of
unique
terms
across
topic
●
●
●
●
●
●
●
●
●
●
0.1
0.2
0.3
0.4
10
30
50
70
90
k
TC−W2V
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.1
0.2
0.3
0.4
10
30
50
70
90
k
TC−W2V
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.1
0.2
0.3
0.4
10
30
50
70
90
k
TC−W2V
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.1
0.2
0.3
0.4
10
30
50
70
90
k
TC−W2V
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.1
0.2
0.3
0.4
10
30
50
70
90
k
TC−W2V
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.1
0.2
0.3
0.4
10
30
50
70
90
k
TC−W2V
●
NMF
w
NMF
u
LDA
u
LDA
w
Fig. 4.
Mean topic TC-W2V coherence scores for k 2 ½10; 100.
Table 2
Examples of
pairwise differences between TC-LCP and the other
two coherence
measures.
TC-LCP tends to produce higher scores when one or both terms are more
general, this can be seen with the score from the pair containing the general term year
(Pðw
j
Þ ¼ 0:08)
compared to that
of
the other
pair
with the more speciﬁc term
education (Pðw
j
Þ ¼ 0:01).
w
i
¼ school,
w
j
¼ year
w
i
¼ school,
w
j
¼ education
Pðw
j
Þ
0.08
0.01
Pðw
i
; w
j
Þ
0.005
0.004
TC-NPMI
0.12
0.38
TC-LCP
0.85
1.01
TC-W2V
0.20
0.54
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
5651
descriptors can be an indication of less useful models. The follow-
ing steps were performed for each (descriptor method,
k) model:
1.
The mean pairwise Jaccard similarity between the topic
descriptors
TD was
calculated.
Higher
similarity values
indicate increased topic dependency:
MPJ
m;k
¼
1
k
2
 
X
k
j¼2
X
j1
i¼1
jTD
i
\ TD
j
j
jTD
i
[ TD
j
j
ð9Þ
2.
The probability distribution of descriptor term occurrences
across all topics was generated.
Terms having high occur-
rence frequencies often appear to be general
terms from
the underlying corpus.
Only the top N (10) descriptor terms are considered, as these are
the terms that would be presented to an end user. The generality of
the remaining terms for a particular topic is effectively irrelevant.
In addition, we do not consider the raw term values due to the dif-
ferent methods being evaluated.
5. Evaluation
In this section,
we provide an evaluation of
the four
topic
descriptor methods NMF
w
, NMF
u
, LDA
u
, and LDA
w
in terms of their
corresponding coherence and generality,
using the model-level
measures described in Section 4.
We also analyze the associated
document-topic memberships by referencing the underlying cor-
pus class labels.
As we ﬁnd similar patterns across all six corpora,
we illustrate the differences in coherence and generality between
the methods by focusing speciﬁcally on the BBC corpus results.
5.1.
Model coherence
Tokenized versions of the documents belonging to each of the
six
corpora
were
generated
using
the
pre-processing
steps
described in Section 4.1,
where the ﬁnal statistics can be found in
Table 1.
For each corpus,
the documents were transformed to log-
based TF-IDF unit vectors, topics were discovered using the scikit-
learn implementation of NMF (including NNDSVD initialization),
and the corresponding NMF
w
topic descriptors were generated
from the highest-ranking top 10 terms found in each topic basis
●
●
●
●
●
●
●
●
●
●
0.01
0.02
0.03
0.04
0.05
10
30
50
70
90
k
Mean pairwise Jaccard
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.00
0.02
0.04
0.06
0.08
10
30
50
70
90
k
Mean pairwise Jaccard
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.01
0.02
0.03
0.04
0.05
10
30
50
70
90
k
Mean pairwise Jaccard
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.01
0.02
0.03
0.04
10
30
50
70
90
k
Mean pairwise Jaccard
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.005
0.010
0.015
10
30
50
70
90
k
Mean pairwise Jaccard
●
NMF
w
NMF
u
LDA
u
LDA
w
●
●
●
●
●
●
●
●
●
●
0.010
0.015
0.020
0.025
0.030
10
30
50
70
90
k
Mean pairwise Jaccard
●
NMF
w
NMF
u
LDA
u
LDA
w
Fig. 5.
Mean pairwise Jaccard similarity of topic descriptors (using top 10 topic terms) for k 2 ½10; 100.
5652
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
vector. Similarly, NMF was also applied to TF vector representations
of
the documents
to generate the corresponding NMF
u
topic
descriptors.
In the case of LDA,
the MALLET implementation was
applied to the sets of
document feature sequences,
from which
the LDA
u
and LDA
w
topic descriptors were generated respectively
from the top 10 most probable topic terms,
and the top 10 topic
terms following the Blei
and Lafferty normalization described in
Eq.
(5).
In
all
cases,
topics
were
discovered
for
values
of
k 2 ½10; 100 (intervals of 10), where this seemed to be a reasonable
range given the number of annotated classes (see Table 1), using the
parameters for NMF and LDA as described in Section 4.1.
Coherence
scores
were
then calculated for
the
four
topic
descriptor
methods,
with co-occurrence frequencies
generated
for the unique descriptor terms across all models using a reference
corpus for TC-NPMI and also for TC-LCP,
due to the issues associ-
ated with using the corpus being modeled for the latter that were
highlighted by Aletras and Stevenson (2013) and Lau et al. (2014).
This reference corpus consisted of a Wikipedia dump from January
2014,
where the tokenization process included ﬁltering the same
671 stopwords as used in pre-processing of the six corpora used
for
evaluation,
term lemmatization,
with all
remaining terms
retained. Following tokenization,
the term co-occurrence frequen-
cies were calculated using a sliding window of 20 terms. This tok-
enized Wikipedia corpus was also used to create the word2vec
model as required by TC-W2V,
using the same parameters as the
demo-word.sh script provided with revision 37 of the source code,
11
i.e.
the Skip-gram model
with word vector dimensions = 200,
max
context window skip length = 5, hierarchical softmax enabled, nega-
tive sampling disabled,
and sample threshold = 1e3. The word2vec
Cosine similarity between each pair of unique topic descriptor terms
was calculated at this point.
Mean model-level coherence scores for the four topic descriptor
methods
are presented for
TC-NPMI,
TC-LCP and TC-W2V in
Figs.
2–4 respectively.
The coherence score scale is less important
here,
where the relative difference between the methods is more
interesting. For TC-NPMI and TC-W2V, a certain level of separation
is observable between the weighted (w) and unweighted (u) topic
descriptor methods.
The weighted methods are producing more
0
30
60
90
2
3
4
>=5
Topic Descriptor occurrences (>1 descriptor)
Number of terms
NMF
w
NMF
u
LDA
u
LDA
w
0
50
100
2
3
4
>=5
Topic Descriptor occurrences (>1 descriptor)
Number of terms
NMF
w
NMF
u
LDA
u
LDA
w
0
25
50
75
100
125
2
3
4
>=5
Topic Descriptor occurrences (>1 descriptor)
Number of terms
NMF
w
NMF
u
LDA
u
LDA
w
0
25
50
75
100
125
2
3
4
>=5
Topic Descriptor occurrences (>1 descriptor)
Number of terms
NMF
w
NMF
u
LDA
u
LDA
w
0
30
60
90
2
3
4
>=5
Topic Descriptor occurrences (>1 descriptor)
Number of terms
NMF
w
NMF
u
LDA
u
LDA
w
0
20
40
60
80
2
3
4
>=5
Topic Descriptor occurrences (>1 descriptor)
Number of terms
NMF
w
NMF
u
LDA
u
LDA
w
Fig. 6.
Frequency of terms occurring in X (multiple) topic descriptors, for X 2 ½2; 3; 4; P 5 from all four methods with k ¼ 100. LDA
u
consistently generates higher frequencies
of terms occurring in X P 5 descriptors.
11
https://code.google.com/p/word2vec/
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
5653
coherent
topics,
where NMF
w
is
regularly the most
coherent
method with LDA
w
also performing strongly, while the model-level
coherence of the LDA
u
topic descriptors (generated from the most
probable terms for a particular topic) is always lower. This pattern
is replicated across all six corpora. However, the situation seems to
be somewhat reversed in the case of TC-LCP,
where LDA
u
is found
to be most coherent, with the NMF methods performing poorly and
LDA
w
positioned in-between.
We now illustrate the differences between TC-LCP and the other
two measures with an analysis of the coherence scores of two pairs
of terms that were included in one of the BBC topic descriptors that
is related to education, where these scores can be found in Table 2.
The term pair (school,
year) yields a higher TC-LCP score than that
of (school,
education),
while the reverse is true for the other mea-
sures.
At a glance,
both term pairs appear coherent.
However,
it
might be argued that (school,
education) is somewhat more coher-
ent than (school,
year),
due to the general nature of the year term.
TC-NPMI,
being based on PMI,
considers the probability of
both
terms in a particular
term pair,
where frequent
terms will
be
down-weighted with respect to those that occur less frequently,
such as education from this example.
Lau et al.
(2014) discuss the
bias of the unnormalized version of this measure (TC-PMI) towards
lower frequent terms,
which should be corrected by TC-NPMI.
At
the same time,
this example clearly demonstrates that
the TC-
NPMI score is higher for the (school,
education) pair,
which is fur-
ther supported by the corresponding word2vec Cosine similarity.
As the TC-LCP measure only considers the probability of one (the
highest-ranking) term for a particular term pair,
the presence of
general
terms is less of
an issue,
particularly when both terms
are general.
This behavior,
coupled with the tendency for LDA to
generate
high-ranking
topic
terms
that
are
more
general
(Chemudugunta et al.,
2006)
is likely the reason for the higher
LDA
u
TC-LCP scores.
5.2.
Model generality
We also analyzed the generality of topic descriptors produced
by the NMF
w
,
NMF
u
,
LDA
u
,
and LDA
w
methods,
where we were
speciﬁcally interested in the overlap or
dependence between
topics,
based on the presence of terms in multiple descriptors for
a particular model,
and the tendency for a method to generate
topics containing high-frequency descriptor terms from the under-
lying corpus.
The mean Jaccard similarity between the topic
descriptors generated by all
four
methods was calculated,
and
the results for values of k 2 ½10; 100 are presented in Fig. 5. As with
the coherence scores discussed in Section 5.1,
a pattern is observ-
able across all six corpora, where the highest levels of similarity are
always found with the LDA
u
descriptors,
with NMF
w
producing
those that are least similar in most cases. LDA
w
also generates rela-
tively low levels of similarity,
while it is interesting to note that
there is a separation between the unweighted methods LDA
u
and
NMF
u
,
where the similarity of the latter is closer to those of the
weighted methods.
The overlap in topics produced by LDA
u
due
to lower
numbers of
unique terms across its topic descriptors
may be an indication of less useful (coherent) models, as suggested
by Arora et
al.
(2012).
It
is likely that
the different
behavior
observed with the Wikipedia corpora is related to the correspond-
ing smaller
number
of
annotated classes.
Here,
the pattern at
k ¼ 10 (the value of
k that
is closest
to the actual
number of
classes) is similar to that of the other corpora.
This overlap can also be demonstrated by looking at the fre-
quency of
terms
occurring in X (multiple)
descriptors,
where
Fig.
6 contains the results for X 2 ½2; 3; 4; P 5 from all four meth-
ods, with k ¼ 100. Here, it can be seen that LDA
u
consistently gen-
erates higher frequencies of terms occurring in X P 5 descriptors.
Further investigation ﬁnds that these are often general terms from
the underlying corpus.
For example,
Table 3 contains the top ten
most
frequent
topic descriptor
terms for
the BBC corpus with
k ¼ 100.
Although certain terms are occurring frequently for all
four methods (also seen in Fig. 6), those of LDA
u
appear to be gen-
eral terms that may be less discriminating when coherence is con-
sidered,
with year and people occurring in 30% and 18% of topic
descriptors
respectively.
In the
case
of
LDA
w
,
people
is
also
highly-ranked, albeit with lower probability. Both it and NMF
u
fea-
ture football-related terms,
indicating the existence of topic over-
lap.
A number
of
general
terms such as uk and year
are also
present with NMF
w
.
However,
their relatively lower probability
suggests that this is less of an issue in comparison to the other
methods.
5.3.
Document-topic membership
Our main objectives in this work were the analysis of
topic
descriptor coherence and generality.
In addition,
we completed
our evaluation with a brief
look at the agreement between the
document-topic
membership and the underlying corpus
class
labels.
Although both LDA and NMF permit
the assignment
of
documents
to multiple
topics,
we
focused solely on disjoint
Table 3
Top ten most frequent topic descriptor terms for the BBC corpus with k ¼ 100.
Term
Probability
(a) NMF
w
manager
0.06
uk
0.05
nation
0.05
ﬁnal
0.05
country
0.05
year
0.04
win
0.04
team
0.04
market
0.04
championship
0.04
(b) NMF
u
league
0.13
involved
0.10
ball
0.09
team
0.08
wicket
0.07
shot
0.06
point
0.06
world
0.05
minute
0.05
match
0.05
(c) LDA
u
year
0.30
people
0.18
goal
0.12
team
0.11
world
0.10
uk
0.09
league
0.08
bbc
0.08
england
0.07
match
0.06
(d) LDA
w
people
0.11
league
0.08
goal
0.08
england
0.07
world
0.06
team
0.06
player
0.06
ball
0.06
season
0.05
match
0.05
5654
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
analysis where membership was assigned using the highest-rank-
ing topic for each document.
This can be justiﬁed due to the fact
that the classes in the six corpora are also largely disjoint. All docu-
ments
assigned to multiple classes
were excluded.
We used
Adjusted Mutual
Information (AMI) to measure this agreement,
which produces results in the range ½0; 1
and corrects for chance
agreement,
while also accounting for the fact that MI tends to be
larger with clusterings containing higher numbers of clusters.
AMI
scores for NMF
w
,
NMF
u
,
and LDA
u
(k 2 ½10; 100)
can be
found in Fig.
7.
The LDA
w
topic descriptor method is not included
here as its descriptors are derived from the post-processed LDA
topic-term distributions;
it
has
the same document-topic dis-
tributions as LDA
u
.
The agreement scores are relatively low for
the non-Wikipedia corpora,
where LDA
u
produces slightly higher
scores than NMF
w
,
with NMF
u
performing poorly in all
cases.
Higher agreement scores with little difference between the meth-
ods are observed with Wikipedia.
It is likely that these results are
related to
the
smaller
number
of
annotated classes
in the
Wikipedia corpora.
They may also suggest the existence of a cer-
tain level of inaccurate document annotations.
5.4.
Discussion
For all six corpora,
we have observed differences between the
scores generated by the weighted (NMF
w
,
LDA
w
) and unweighted
(LDA
u
, NMF
u
) topic descriptor methods. In the case of the aggregate
model-level
coherence scores,
the weighted methods
perform
strongly for all
measures apart from TC-LCP.
This contrasts with
the evaluation of Stevens et al.
(2012),
where they found that the
TC-LCP scores were often in agreement with those of the unnor-
malized version of
TC-NPMI
(the two coherence measures used
in their evaluation),
with the TC-LCP scores for NMF matching or
exceeding those of LDA for k 6 100. Lau et al. (2014) also observed
strong correlation between the human coherence ratings and those
of TC-LCP and TC-NPMI.
However,
as they did not compare topics
discovered by the multiple methods they used with each other,
it
is unclear whether we can draw many parallels between their
correlation-based ﬁndings and those of our own evaluation.
The measures related to topic generality ﬁnd higher levels of
similarity between the descriptors generated by LDA
u
,
along with
the promotion of
general
high-frequency corpus terms among
0.25
0.50
0.75
10
30
50
70
90
k
AMI
NMF
w
NMF
u
LDA
u
0.25
0.50
0.75
10
30
50
70
90
k
AMI
NMF
w
NMF
u
LDA
u
0.25
0.50
0.75
10
30
50
70
90
k
AMI
NMF
w
NMF
u
LDA
u
0.25
0.50
0.75
10
30
50
70
90
k
AMI
NMF
w
NMF
u
LDA
u
0.25
0.50
0.75
10
30
50
70
90
k
AMI
NMF
w
NMF
u
LDA
u
0.25
0.50
0.75
10
30
50
70
90
k
AMI
NMF
w
NMF
u
LDA
u
Fig. 7.
AMI scores for NMF
w
,
NMF
u
,
and LDA
u
,
for k 2 ½10; 100.
LDA
w
is not included as it is derived from the post-processed LDA topic-term distributions (it has the same
document-topic distributions as LDA
u
).
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
5655
multiple descriptors. This effect is less noticeable with LDA
w
, and is
signiﬁcantly lower with NMF
w
. Wallach et al. (2009) have pointed
out that it is often customary to create a corpus-speciﬁc stopword
list to address this issue,
even if some of these play meaningful
semantic roles.
They also suggest that using an asymmetric prior
over the LDA document-topic distribution h can result in topics
that
are unaffected by stopwords,
with stopwords
themselves
being isolated in a small
number of
topics.
However,
although
we have enabled this particular option in MALLET, we still observe
the presence of general terms in multiple descriptors, which is only
decreased when the Blei and Lafferty (2009) normalization of LDA
w
is applied.
We also note the differences in generality scores when
the value of k is considerably different to the number of underlying
corpus
classes,
as
observed for
both Wikipedia corpora with
k > 10.
Steyvers and Grifﬁths (2006) suggested the use of probability
distribution divergence measures such as Jensen–Shannon diver-
gence when calculating the similarity between terms or
docu-
ments following topic discovery;
this is also applicable to the
topics themselves. Although such measures can be applied to both
LDA topics and those discovered by NMF, they were not employed
here as
we were speciﬁcally concerned with the top N topic
descriptors that
could ultimately be presented to an end user.
Separately, Stevens et al. (2012) also evaluated the impact of differ-
ent

values on the calculation of the two coherence measures they
employed.
They found that
using a small
value of

¼ 10
12
resulted in a decrease in coherence scores for NMF compared to
LDA,
particularly in the case of the PMI-based measure.
We also
calculated the TC-NPMI
and TC-LCP scores using this

value,
where a similar decrease in coherence was observed for a subset
of
the NMF models.
However,
a close inspection of
some of
the
coherence score distributions found that this small

value pro-
duced signiﬁcant outliers for term pairs that did not occur together
in the reference Wikipedia corpus. The fact that an individual topic
descriptor’s score was calculated using the mean of the constituent
term pairwise scores meant that it was sensitive to such outliers,
which could lead to a low score even in the case of a single term
pair not occurring together while the remaining term pairs were
highly coherent.
It might in fact be argued that taking the median
of these pairwise scores is more appropriate. However, we felt that
the presence of descriptor terms that do not occur together must
be acknowledged,
where the use of

¼ 1,
as originally suggested
by Mimno et
al.
(2011),
acts
as
a compromise between both
extreme cases.
6. Conclusions
In this work,
we have described an analysis of
the semantic
interpretability,
also known as topic coherence,
of the sets of top
terms generally used to describe topics discovered by a particular
algorithm.
This has been achieved with an evaluation of popular
variants of
both probabilistic (LDA)
and matrix decomposition
(NMF)
topic modeling techniques on multiple corpora,
using a
combination of
existing and new measures that
focus on topic
coherence and generality.
A common pre-processing procedure
has been employed for both techniques where possible,
without
relying on particular actions such as the generation of
corpus-
speciﬁc stopword lists.
We have found that NMF regularly pro-
duces more coherent topic descriptors than those generated by
the standard approach used with LDA,
with higher levels of topic
generality and redundancy observed with the latter.
It
can be
observed that a key role is played by the associated term weighting
strategy,
where modiﬁcations to document term pre-processing
and descriptor term post-processing can produce markedly differ-
ent results.
This evaluation has provided insight into the characteristics and
differences between the topic models produced by NMF and LDA.
While LDA may offer good general descriptions of broader topics,
our results indicate that the higher coherence and lower generality
associated with NMF topics mean that the latter method is more
suitable
when
analyzing
niche
or
non-mainstream content.
Similarly,
although improvements in topic coherence have been
found with the use of n-gram terms (Lau et al., 2013), here we have
restricted the
evaluation to use
the
common unigram-based
approach.
Regardless of the topic modeling technique employed,
it is clear that close reading of any generated topics is essential.
As certain issues have been raised in relation to coherence mea-
sures that are based on individual term pair co-occurrence within a
reference corpus (Rosner et al.,
2013) (albeit,
where NMF not was
considered),
in future work,
we would like to investigate alterna-
tive measures.
We would also hope to perform a user survey in
order to correlate human judgements with our automated results,
although this would likely be different to prior coherence studies
that requested ratings of individual topics,
(Newman et al.,
2010;
Mimno et al., 2011; Lau et al., 2014), where descriptor comparisons
would instead need to be considered.
Acknowledgements
This
research was
supported by 2CENTRE,
the EU funded
Cybercrime Centres of
Excellence Network,
Science Foundation
Ireland (SFI)
under
Grant
No.
SFI/12/RC/2289,
and the EU FP7
funded VOX-Pol Network of Excellence.
References
Aletras,
N.
& Stevenson,
M.
(2013).
Evaluating topic coherence using distributional
semantics.
In Proceedings of
the 10th international
workshop on computational
semantics IWCS-10 (pp.
13–22).
Arora,
S.,
Ge,
R.,
Halpern,
Y.,
Mimno,
D.
M.,
Moitra,
A.,
Sontag,
D.,
et al.
(2012).
A
practical
algorithm for topic modeling with provable guarantees.
<arXiv:abs/
1212.4777>.
Arora,
S.,
Ge,
R.,
& Moitra,
A. (2012). Learning topic models – going beyond SVD. In
FOCS (pp.
1–10).
IEEE Computer Society.
Blei, D. M., Grifﬁths, T. L., Jordan, M. I., & Tenenbaum, J. B. (2004). Hierarchical topic
models
and the nested chinese restaurant
process.
In Advances
in neural
information processing systems,
NIPS.
MIT Press.
Blei, D. M., & Lafferty, J. D. (2006). Correlated topic models. In Proceedings of the 23rd
international conference on machine learning (pp.
113–120).
MIT Press.
Blei,
D.,
& Lafferty,
J.
(2009).
Topic models.
In Text mining: Theory and applications.
Taylor and Francis.
Blei,
D.
M.,
Ng,
A.
Y.,
& Jordan,
M.
I.
(2003).
Latent dirichlet allocation.
Journal
of
Machine Learning Research,
3,
993–1022.
Bouma,
G.
(2009).
Normalized pointwise
mutual
information in collocation
extraction.
In Proceedings of
the international
conference of
the German society
for computational linguistics and language technology,
GCSL ’09.
Boutsidis,
C.,
& Gallopoulos,
E.
(2008).
SVD based initialization: A head start for
nonnegative matrix factorization.
Pattern Recognition,
41(4),
1350–1362.
Chang,
J.,
Boyd-Graber,
J.,
Gerrish,
S.,
Wang,
C.,
& Blei,
D.
M.
(2009).
Reading tea
leaves: How humans interpret topic models.
In Advances in neural
information
processing systems NIPS.
Chemudugunta,
C.,
Smyth,
P.,
& Steyvers,
M.
(2006).
Modeling general and speciﬁc
aspects of documents with a probabilistic topic model.
In Advances in neural
information processing systems (pp.
241–248).
Deerwester,
S.
C.,
Dumais,
S.
T.,
Landauer,
T.
K.,
Furnas,
G.
W.,
& Harshman,
R.
A.
(1990).
Indexing by latent semantic analysis.
Journal of the American Society of
Information Science,
41(6),
391–407.
Gaussier, E., & Goutte, C. (2005). Relation between PLSA and NMF and implications.
In Proceedings of the 28th annual international ACM SIGIR conference on research
and development in information retrieval, SIGIR ’05 (pp. 601–602). New York, NY,
USA: ACM.
Grifﬁths,
T.
L.,
& Steyvers,
M.
(2004).
Finding scientiﬁc topics.
Proceedings of
the
National
Academy of
Sciences
of
the United States
of
America,
101(Suppl.
1),
5228–5235.
Hoffman,
M.
D.,
Blei,
D.
M.,
& Bach,
F.
R.
(2010).
Online learning for
latent
dirichlet allocation.
In Advances in neural
information processing systems (pp.
856–864).
Hofmann,
T.
(2001).
Unsupervised learning
by
probabilistic
latent
semantic
analysis.
Machine Learning,
42(1–2),
177–196.
Kohlschütter,
C.,
Fankhauser,
P.,
& Nejdl,
W.
(2010).
Boilerplate detection using
shallow text features. In Proceedings of the third ACM international conference on
web search and data mining (pp.
441–450).
ACM.
5656
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
Lau, J. H., Baldwin, T., & Newman, D. (2013). On collocations and topic models. ACM
Transactions on Speech and Language Processing,
10(3),
10:1–10:14.
Lau,
J.
H.,
Newman,
D.,
& Baldwin,
T.
(2014).
Machine
reading tea
leaves:
Automatically
evaluating
topic
coherence
and
topic
model
quality.
In
Proceedings
of
the
European
chapter
of
the
association
for
computational
linguistics,
EACL-14 (pp.
530–539).
Lee,
D.
D.,
& Seung,
H.
S.
(1999).
Learning the parts of
objects by non-negative
matrix factorization.
Nature,
401,
788–791.
Lee, D. D., & Seung, H. S. (2001). Algorithms for non-negative matrix factorization. In
T. Leen, T. Dietterich, & V. Tresp (Eds.). Advances in neural information processing
systems (Vol.
13,
pp.
556–562).
MIT Press.
Lin,
C.-J.
(2007).
Projected gradient methods for nonnegative matrix factorization.
Neural Computation,
19(10),
2756–2779.
Loper, E., & Bird, S. (2002). NLTK: The natural language toolkit. In Proceedings of the
ACL-02 workshop on effective
tools
and methodologies
for
teaching
natural
language
processing
and
computational
linguistics.
ETMTNLP
’02
(Vol.
1,
pp.
63–70).
Stroudsburg,
PA,
USA: Association for Computational Linguistics.
McCallum,
A.
(2002).
MALLET:
A
machine
learning
for
language
toolkit.
<http://mallet.cs.umass.edu>.
Mikolov,
T.,
Chen,
K.,
Corrado,
G.,
& Dean,
J.
(2013).
Efﬁcient estimation of word
representations in vector space.
<arXiv:abs/1301.3781>.
Mikolov,
T.,
Sutskever,
I.,
Chen,
K.,
Corrado,
G.,
& Dean,
J.
(2013).
Distributed
representations of words and phrases and their compositionality. In Advances in
neural information processing systems,
NIPS.
Mimno,
D.,
Wallach,
H.
M.,
Talley,
E.,
Leenders,
M.,
& McCallum,
A.
(2011).
Optimizing semantic coherence in topic models. In Proceedings of the conference
on empirical
methods in natural
language processing,
EMNLP ’11 (pp.
262–272).
Stroudsburg,
PA,
USA: Association for Computational Linguistics.
Newman,
D.,
Lau,
J.
H.,
Grieser,
K.,
& Baldwin,
T.
(2010).
Automatic evaluation of
topic coherence. In Human language technologies: The 2010 annual conference of
the north American chapter of the association for computational linguistics, HLT ’10
(pp. 100–108). Stroudsburg, PA, USA: Association for Computational Linguistics.
O’Callaghan,
D.,
Greene,
D.,
Conway,
M.,
Carthy,
J.,
& Cunningham,
P.
(2013).
An
analysis of
interactions within and between extreme right
communities in
social media.
In M.
Atzmueller,
A.
Chin,
D.
Helic,
& A.
Hotho (Eds.),
Ubiquitous
social media analysis.
Lecture notes in computer science (Vol.
8329,
pp.
88–107).
Berlin Heidelberg: Springer.
Pedregosa,
F.,
Varoquaux,
G.,
Gramfort,
A.,
Michel,
V.,
Thirion,
B.,
Grisel,
O.,
et al.
(2011).
Scikit-learn: Machine learning in python.
Journal
of
Machine Learning
Research,
12,
2825–2830.
R
ˇ
ehu
˚
r
ˇ
ek,
R.,
& Sojka,
P.
(2010).
Software framework for topic modelling with large
corpora.
In Proceedings of
the LREC 2010 workshop on new challenges for NLP
frameworks (pp.
45–50).
Valletta,
Malta: ELRA.
Röder,
M.,
Both,
A.,
& Hinneburg,
A.
(2015).
Exploring the space of topic coherence
measures. In Proceedings of the eighth ACM international conference on web search
and data mining,
WSDM ’15 (pp.
399–408).
New York,
NY,
USA: ACM.
Rosen-Zvi, M., Grifﬁths, T., Steyvers, M., & Smyth, P. (2004). The author-topic model
for authors and documents.
In Proceedings 20th conference on uncertainty in
artiﬁcial intelligence,
UAI ’04 (pp.
487–494).
Arlington,
VA,
United States: AUAI
Press.
Rosner, F., Hinneburg, A., Röder, M., Nettling, M., & Both, A. (2013). Evaluating topic
coherence measures.
In Advances in neural information processing systems,
NIPS.
Saha,
A.,
& Sindhwani,
V.
(2012).
Learning evolving and emerging topics in social
media: A dynamic NMF approach with temporal regularization.
In Proceedings
of
the
ﬁfth ACM international
conference
on web search and data mining
(pp.
693–702).
ACM.
Salton,
G.,
& Buckley,
C.
(1988).
Term-weighting approaches in automatic text
retrieval.
Information Processing Management,
24(5),
513–523.
Sandhaus,
E.
(2008).
The New York times annotated corpus.
Philadelphia: Linguistic
Data Consortium.
Stevens,
K.,
Kegelmeyer,
P.,
Andrzejewski,
D.,
& Buttler,
D.
(2012).
Exploring topic
coherence over many models and many topics.
In Proceedings of the 2012 joint
conference on empirical methods in natural language processing and computational
natural
language learning,
EMNLP-CoNLL ’12 (pp.
952–961).
Stroudsburg,
PA,
USA: Association for Computational Linguistics.
Steyvers,
M.,
& Grifﬁths,
T.
(2006).
Probabilistic topic models.
In T.
Landauer,
D.
Mcnamara,
S.
Dennis,
& W.
Kintsch (Eds.),
Latent semantic analysis:
A road to
meaning.
Laurence Erlbaum.
Wallach,
H.
M.,
Mimno,
D.,
& McCallum,
A.
(2009).
Rethinking LDA: Why priors
matter.
In Advances in neural information processing systems,
NIPS.
Wallach,
H.
M.,
Murray,
I.,
Salakhutdinov,
R.,
& Mimno,
D.
(2009).
Evaluation
methods
for
topic models.
In Proceedings
of
the 26th annual
international
conference on machine learning,
ICML ’09 (pp.
1105–1112).
New York,
NY,
USA:
ACM.
D.
O’Callaghan et al. / Expert Systems with Applications 42 (2015) 5645–5657
5657
