See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/287878556
A Tool for Addressing Construct Identity in Literature Reviews and Meta-
Analyses
Article in MIS Quarterly · September 2016
DOI: 10.25300/MISQ/2016/40.3.01
CITATIONS
29
READS
2,121
2 authors:
Some of the authors of this publication are also working on these related projects:
Papers Employing Latent Semantic Analysis (LSA)
View project
Construct Taxonomies
View project
Kai R. Larsen
University of Colorado Boulder
75
PUBLICATIONS
1,907
CITATIONS
SEE PROFILE
Chih How Bong
University Malaysia Sarawak
27
PUBLICATIONS
169
CITATIONS
SEE PROFILE
All content following this page was uploaded by Kai R. Larsen on 14 February 2017.
The user has requested enhancement of the downloaded file.
M
ETHODS 
A
RTICLE
A
T
OOL FOR 
A
DDRESSING 
C
ONSTRUCT 
I
DENTITY IN
L
ITERATURE 
R
EVIEWS AND 
M
ETA
-A
NALYSES
1
Kai R. Larsen
Leeds School of Business, University of Colorado, Boulder, 995 Regent Drive,
Boulder, CO 80309-0419 U.S.A. {kai.larsen@colorado.edu}
Chih How Bong
Faculty of Computer Science and Information Technology, Universiti Malaysia Sarawak,
94300 Kota Samarahan, Sarawak, MALAYSIA {chbong@unimas.my}
The problem of detecting whether two behavioral constructs reference the same real-world phenomenon has
existed for over 100 years. Discordant naming of constructs is here termed the construct identity fallacy (CIF). 
We designed and evaluated the construct identity detector (CID), the first tool with large-scale construct
identity detection properties and the first tool that does not require respondent data. Through the adaptation
and combination of different natural language processing (NLP) algorithms, six designs were created and
evaluated against human expert decisions. All six designs were found capable of detecting construct identity,
and a design combining two existing algorithms significantly outperformed the other approaches. A set of
follow-up studies suggests the tool is valuable as a supplement to expert efforts in literature review and meta-
analysis. Beyond design science contributions, this article has important implications related to the taxonomic
structure of social and behavioral science constructs, for the jingle and jangle fallacy, the core of the Informa-
tion Systems nomological network, and the inaccessibility of social and behavioral science knowledge. In sum,
CID represents an important, albeit tentative, step toward discipline-wide identification of construct identities.
Keywords: Construct identity fallacy (CIF), construct identity detector (CID), construct validity, nomological
networks, synonymy and polysemy, inter-nomological network, jingle and jangle fallacy, natural language
processing (NLP), latent semantic analysis (LSA), design science, ontologies
The first step of science is to know one thing from another. This knowledge consists in their
specific distinctions; but in order that it may be fixed and permanent, distinct names must
be given to different things, and those names must be recorded and remembered.
Carolus Linnaeus, Systema Naturae (1738)
Introduction
1
The large-scale detection of construct identity has remained
a challenging puzzle in the behavioral and social sciences for
over 100 years, with identically named constructs sometimes
representing 
different 
real-world 
phenomena 
(hereafter
referred to as phenomena) and differently named constructs
often representing the same phenomenon (Kelley 1927;
Thorndike 1904). Construct identity fallacy (CIF) is the term
coined here to signify discordant naming practices. Sug-
gesting that such puzzles may be the norm rather than the
exception, Venkatesh et al. (2003) united eight major theories
of technology acceptance after determining that the construct
perceived usefulness appeared under five different names.
1
Andrew Burton-Jones was the accepting senior editor for this paper. Yulin
Fang served as the associate editor.
The appendices for this paper are located in the “Online Supplements”
section of the MIS Quarterly’s website (http://www.misq.org).
MIS Quarterly Vol. 40 No. 3, pp. 529-551/September 2016
529
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Addressing unrecognized construct overlaps is “essential for
true scientific progress” (Noar and Zimmerman 2005, p. 275)
and holds the potential to spark new ideas and connections
crucial for the development and integration of theories in
social sciences research (Cozby 1993).
This article introduces natural language processing (NLP) for
large-scale detection of construct identity and eventual input
into 
finer-meshed 
factor 
analyses, 
and 
it 
addresses 
the
research question: How do NLP designs compare to human
experts in detecting whether or not two constructs signify the
same phenomenon? We developed a tool for use of latent
semantic analysis (LSA)—a mathematically based theory of
meaning (Deerwester et al. 1990; Landauer 2007)—for con-
struct identity detection, and then tested whether LSA can
detect construct correspondence above that of random assign-
ment. We then evaluated LSA against three state-of-the-art
designs, which we gave the nomenclature LI, an existing
sentence-based similarity algorithm named by our team after
its originator (Li et al. 2006); MI, a short-answer grading
algorithm, likewise named after its creator (Mihalcea et al.
2006); and Latent Dirichlet Allocation (LDA), a commonly
used alternative to LSA (Aggarwal and Zhai 2012).
Tests of the four techniques generated two new approaches,
which we termed construct identity detector one and two
(CID
1+2
). CID
1
utilizes the combined principles of LSA and
LI. CID
2
utilizes the combined principles of LSA and MI. 
CID
1
outperformed the other five designs on the tasks of
overall construct correspondence detection and on the jangle
fallacy 
detection, 
whereas 
LSA 
outperformed 
the 
other
designs for the jingle fallacy detection. After evaluations, we
combined the best designs into a tool, which we termed the
construct identity detector (CID). The design science para-
digm (Hevner et al. 2004) applies to research that creates and
evaluates tools intended to improve performance in a specific
research domain. Specifically, we followed the NLP design
science process as described by Abbasi et al. (2012), among
others, in framing this article as exaptation research: applying
known solutions to new problems (Gregor and Hevner 2013).
CID provides a baseline for future research on construct
identity detection as well as statistical support for the general
hypothesis that NLP-based designs have the potential to per-
form on par with human experts in detecting whether two
constructs signify the same phenomenon.
We assessed CID
1
’s usefulness empirically by conducting an
experiment and a follow-up meta-analytic exercise. We found
the tool to be competitive with participants’ ability to detect
constructs, particularly if the participants were not previously
knowledgeable of the constructs that were being evaluated. 
The study provides the first evidence that large-scale con-
struct identity evaluations are possible, suggesting a path
forward for addressing the longstanding jingle (Thorndike
1904) and jangle (Kelley 1927) fallacies that arguably impede
the progress of the behavioral sciences (Block 1995).
The next section examines the historical need for and justifi-
cation of construct identity detection. The following three
sections address (1) the proposed methods and evaluation
criteria for addressing construct identity; (2) the findings; and
(3) discussion and conclusion of the study.
The Construct Identity Fallacy
Thorndike (1904) first described the jingle fallacy as the
occurrence of two constructs with identical names referencing
different real-world phenomena. Two decades later, Kelley
(1927) named the companion jangle fallacy, referencing cases
when different construct names are is used to refer to the to
the same real-world phenomena. In this article, we combine
the two fallacies into the construct identity fallacy (CIF),
wherein a construct pair references the same or different
phenomena and are given dissimilar or identical names,
respectively. Construct names are crucial when addressing
the CIF, and they provide a common criterion used to find
articles for theory reviews (Larsen et al. 2014), one key area
in which the CIF impinges upon research progress in the
social and behavioral sciences.
Researchers have criticized the
ever-increasing proliferation of labels that are some-
times offered as synonyms, sometimes presented as
specific aspects of the subsuming construct, or, more
often, simply loosely used to refer to the related con-
structs without self-conscious attempts at a more
precise or consensual usage (Alexander et al. 1991,
p. 315).
Complaints arise from almost every well-researched area in
the social and behavioral sciences (Larsen et al. 2013),
including motivation (Marsh et al. 2003; Murphy and Alex-
ander 2000), emotional intelligence (Van Rooy et al. 2005),
mental toughness (Crust and Swann 2010), disability (Pollard
et al. 2007), personality (Block 1995; Peck 2007), sensation
seeking (Zuckerman 2008), user satisfaction (Zmud et al.
1994), and virtually every subarea related to IS success
(Larsen 2003). The field of personality psychology as a
whole has been urged to “confront its severe, even crippling,
terminological problems” (Block 1995, p. 209). Block (1995)
evinced the many difficulties in personality assessment that
“derive from the hasty, hazy, lazy use of language” and sug-
530
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 1. Construct Identity Fallacy Matrix
Construct Names
Same
Different
Phenomena
Correspondent
Correct outcome:
True positive
Jangle fallacy:
False negative
Independent
Jingle fallacy:
False positive
Correct outcome:
True negative
gested that “psychologists have tended to be sloppy with
words” (p. 209).
A summary of the CIF can be seen in Table 1. Identification
of a correspondent pair of constructs with the same names, or
identification of an independent pair of constructs with
different names leads to correct evaluation outcomes (true
positive and true negative, respectively). Alternatively,
labeling a pair differently when they are, in fact, corre-
spondent 
leads 
to 
a 
false 
negative––the 
jangle 
fallacy. 
Labeling a construct pair the same when they are, in fact,
independent leads to a false positive––the jingle fallacy. The
next two sections examine the jangle fallacy and the jingle
fallacy, respectively.
Jangle Fallacy
If you commit a jangle fallacy your work is akin to
the reinvention of the wheel. Either a lack of knowl-
edge or of concern about the work of other scientists
or a lack of required data demonstrating divergent
validity between new and established measures/
constructs is the cause of the fallacy (Wilhelm 2009,
p. 146).
The literature is replete with findings of jangle fallacies. For
example, Whiteside and Lynam (2001) have suggested Tel-
legen’s (1982) control and Zuckerman's (1994) disinhibition
scales “seem to measure similar constructs despite bearing
different labels” (p. 670). Likewise, the field of self-control
research is “significantly handicapped” by jangle fallacies,
according to a meta-analysis of 282 studies (Duckworth and
Schulze 2009, p. 23). Block (1995, p. 210) also weighed in
on the jangle fallacy, stating that it abounds with and is exem-
plified by instances where constructs are
put forward without recognition of the earlier and
alternative, differently named personality constructs
to which they are intrinsically linked or which they
blend and confound.
These problems exist in the IS discipline as well. Perceived
usefulness (Lim and Benbasat 2000) and switching benefits
(Kim and Kankanhalli 2009) are two constructs used among
a large set of very similar IS-discipline constructs with a
multitude of names. Assessing both constructs via their
measurement items (hereafter referred to as items), both
constructs focus on evaluation of a new technology in relation
to an existing technology, a distinction that is implicit in Lim
and Benbasat’s (2000) construct definition and explicit in
Kim and Kankanhalli’s (2009) definition. For a reader not
well-versed in the IS-specific adoption literature, the construct
names voluntariness (Venkatesh and Davis 2000) versus
extrinsic motivation: external perceived locus of causality
(Malhotra et al. 2008) and their definitions may sound quite
different. When evaluating these constructs’ items, however,
we determined that the two research teams operationalized the
constructs inversely (voluntary versus mandatory usage). We
argue that the above examples are similar enough that
researchers using one construct should be aware of the other. 
Jingle Fallacy
The jingle fallacy applies to any case where two
constructs with identical names measure different
latent constructs. If you commit a jingle fallacy you
transfer available evidence to a new measure al-
though in reality what you conceive as being an
instance 
of 
an 
established 
construct 
somehow
diverges (Wilhelm 2009, p. 146).
The renaming of an existing construct (jangle fallacy) may
increase the perceived novelty of a construct, whereas the
reuse of an existing construct name to represent a different
phenomenon (jingle fallacy) may increase the perceived
novelty of a relationship, as results are likely to diverge from
past research. Block (1995) found that,
within the field of personality psychology, the Big
Five factors, as they have evolved and become
differently understood while remaining similarly
MIS Quarterly Vol. 40 No. 3/September 2016
531
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
labeled by different Big Fivers, represent striking
instances of the jingle fallacy (p. 209).
While jangle fallacies were more common in our sample of IS
constructs (as shown later), the IS discipline has not been
immune to the jingle fallacy. For example, perceived useful-
ness has been used to describe both the perceived importance
of skill proficiency on job performance (Nelson 1991) and the
perceived belief that a system can enhance job performance
(Davis 1989). 
Consequences of the Construct
Identity Fallacy
There 
are 
numerous 
consequences 
of 
the 
CIF. 
When
researchers search for a construct in an online repository of
journal articles, the name of that construct can neither be
trusted to elicit high recall
2
(a consequence of jangle) nor high
precision
3
(a consequence of jingle). An incomplete under-
standing of the many names for a construct will lead to litera-
ture searches that are incomplete, requiring astute care in
guarding 
against 
constructs 
with 
identical 
names 
that
reference different phenomena. This added work may, under
some conditions, contribute to (1) confusion, misunder-
standings, and conceptual messes in research (Alexander et al.
1991; Bong 1996); (2) waste of scientific time (Block 1995;
Zuckerman 2008); and (3) difficulty in theory unification and
knowledge cumulation (Block 1995; Duckworth and Schulze
2009). Taken together, these consequences hold the potential
to stunt progress in the social and behavioral sciences.
Definition and Operationalization
of Construct Correspondence
We designate a construct, C, to be correspondent to another
construct, C', when both refer to the same real-world phenom-
enon (Borsboom et al. 2009; Loevinger 1957). Operationally,
a construct C will be judged as correspondent to another
construct C' if the domain experts determine C could also be
used to refer to the real-world phenomenon referenced by C'. 
The basis for determination might include similarity between
indicators such as items, definitions, names, citations, units of
analysis, and other evidence for the two constructs. For
example, the construct named perceived performance was
defined by Kim et al. (2009) as “the consumer’s perception of
how the transaction, including product/ service performance,
fulfills his or her needs, wants and desires” (p. 243), and it
was operationalized through four measurement items such as
“using this website improved my performance in shopping”
(p. 254). The construct utilitarian outcomes was defined by
Hseih et al. (2008) as “the extent to which performing the
behavior 
enhances 
the 
effectiveness 
of 
personal 
related
activity” (p. 102), and it was operationalized through the four
items, 
including 
“using 
the 
Internet 
TV 
improves 
my
performance for communication and information search” (p.
122). Both of these constructs were operationalized at the
individual level of analysis, and for both constructs Davis
(1989) was cited as a construct source. The domain experts
decided that the two constructs were correspondent, and, more
specifically, that they provide an instance of the jangle fallacy
due to the different names.
Past discussions of the jingle and jangle construct fallacies
have strongly depended on whether the two construct names
in question were identical or different. While the use of
construct names is crucial in framing the historical context of
this article’s contributions, for methodological purposes, we
initially dispense with the notion of name as the identifying
feature of a phenomenon in the CIF. Our name dispensation
aligns with the role names play in other construct identity
methods, such as factor analysis and multitrait–multimethod
(MTMM) analysis where construct names serve only as item
“containers” (Preacher and MacCallum 2003). Thus, to
develop methods that address the CIF, construct names are
not strictly needed. We return to construct names during the
evaluation phase to separately evaluate design performance
for jingle and jangle.
Proposed Methods for Construct
Identity Detection
We formulate construct identity detection as a problem of
finding constructs that share semantically similar measure-
ment items. We evaluate and use two schools of thought
related to semantic and lexical similarity—the corpus model
and the knowledge-based model––and use these to develop
new hybrid designs (see Table 2). Although the pure corpus
and knowledge-based approaches have been described exten-
sively 
in 
the 
literature, 
their 
application 
to 
large-scale
construct identity detection is novel. Details on all six
designs are available in Appendix A.
On average and when derived from the same academic
domain, the designs should assign a higher similarity score to
a correspondent pair than they would to an independent pair. 
2
Precision = true positives / (true positives + false positives).
3
Recall = true positives / (true positives + false negatives).
532
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 2. Summary of Approaches to Construct Identity Detection
Approach
Design
Summary of Design
Justification for Inclusion
Corpus
Models
Latent
Semantic
Analysis
(LSA)
A text-similarity method based on a
low-rank approximation of a set of
context-related texts. Paragraphs from
construct articles used for training.
A long history of use in the IS field (e.g.,
Evangelopoulos et al. 2010; Larsen and
Monarchi 2004; Siderova et al. 2008). LSA has
proven to be both versatile and effective.
Latent
Dirichlet
Allocation
(LDA)*
A topic model for texts based on
extraction of topics that the examined
texts hold in common. Paragraphs
from construct articles used for
training.
Also a dimensionality reduction technique, but
LDA has been found to be superior to other
dimensionality reduction approaches (Tirunillai
and Tellis 2014), and LDA can be embedded in
more complex models (Blei et al. 2003).
Knowledge-
based Models
Li et al. (LI)
A sentence-similarity method built upon
length and depth distances between
words in an existing knowledge base
where word order in a sentence is
taken into account. WordNet used for
training.
Li’s method is conspicuous among other
methods, as it has been reported to work very
well on short texts (Li et al 2006). One of the
main reasons is that it preserves word-order
information in sentences, which is often lost in
other methods. 
Mihalcea et
al. (MI)
A sentence-similarity method built upon
an existing knowledge base where the
similarity score is an aggregation of a
number of word-word similarity
measures. WordNet used for training.
The method combines different word-similarity
metrics and incorporates directionality of word
similarity. The method gives specific meaning
to words with higher weight than generic
concept words. Expected to better differentiate
abstract concepts. 
Hybrid
Models
Construct
Identity
Detector 1
(CID
1
)
Integration of LI and LSA designs. 
Paragraphs from construct articles
used for training.
LI relies on a general English literature knowl-
edge base to derive similarity scores. We
speculated that CID
1
would boost LI’s perfor-
mance through use of an LSA domain- specific
knowledge base to detect construct properties.
Construct
Identity
Detector 2
(CID
2
)
Integration of MI and LSA designs. 
Paragraphs from construct articles
used for training.
As the majority of construct items are short, the
word specificity in items may convey important
information. We expected that narrowing down
the specificity to a LSA domain-specific knowl-
edge base would enable CID
2
to distinguish the
differences between important concepts.
*LDA was tested based on a reviewer suggestion, and as it did not outperform all other designs, no LDA hybrid model was developed and neither
was it included in CID. Future versions of CID may be updated to include LDA. We employed online LDA (Hoffman et al. 2010) through Gensim
(Řehůřek and Sojka 2010). 
Such designs allow for detection of correspondent construct
pairs regardless of the names of these constructs. At each
similarity score threshold, we assign any construct pair with
a similarity score equal to or higher than the threshold as a
correspondent-pair prediction by the design. In the following
sections, we briefly discuss the background of the four
existing designs and then present the two hybrid designs.
The Corpus-Based Models
Corpus-based models, sometimes known as distributional
models, are extensions of vector space models (Salton et al.
1975). Unfortunately, short texts, such as items, do not have
sufficient word overlap to create appropriate models (Deer-
wester et al. 1990; Dumais et al. 1988). Therefore, the
corpus-based designs and the two hybrid designs were trained
on all paragraphs in 193 academic articles that originated the
sample of constructs used in this study.
Corpus-Based Models: LSA
LSA has performed almost as well as humans on complex
knowledge management and integration tests (Foltz 2007;
Graesser et al. 2007; Landauer 2002). Therefore, LSA repre-
MIS Quarterly Vol. 40 No. 3/September 2016
533
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
sents a reasonable starting point for evaluation of NLP’s
potential for coarse evaluation of construct correspondence. 
Sidorova et al. (2008) and Larsen et al. (2008) detail its
application in IS. After creation of an appropriate semantic
space, each construct item included in the study was projected
into the semantic space to generate its mathematical represen-
tation. These representations—item vectors with the same
dimensionality as the semantic space—were used to compute
cosine angles between all item vectors, with higher cosines
indicating higher similarity between items.
Corpus-Based Models: LDA
Latent Dirichlet Allocation (LDA), also known as topic
modeling, is a generative algorithm that allows posterior
distribution of large collections of documents to reveal the
latent semantics of each word in the collections. The under-
lying idea of LDA is that documents are composed of a
mixture of topics, which are represented by words with proba-
bilities related to the topics. For example, a fruit topic likely
has a high probability of generating food-related words such
as banana, orange, and grape. Likewise, these words have
high probabilities of being related to the topic of fruit.
The Knowledge-Based Models
Knowledge-based models compute sentence similarity from
component-word similarities that are derived from human-
crafted knowledge bases like WordNet (Miller 1995; Poli et
al. 2010). Many knowledge-based models also use corpus
statistics and probability theory to measure information con-
tent and to indicate whether or not a word is common or
specific. Word-order information enables differentiation be-
tween such concepts as information systems and systems
information. We appropriate two algorithms (Li et al. 2006;
Mihalcea et al. 2006) that are heavily cited and have been
reported to be effective in predicting text similarity (Bailey
and Meurers 2008; Mitchell and Lapata 2008). Both algo-
rithms use WordNet, which contains a substantial set of
English literature words; hence, WordNet does not always
cover domain-specific vocabulary, such as that of the IS
discipline.
While we limit our scope to the use of WordNet, behavioral
researchers interested in developing ontologies or extensions
and alternatives to WordNet could use tools such as Protege,
the open-source ontology editor. Other ontology-related
resources are also emerging that may serve as starting points
for discipline-wide ontology development in the future. The
controlled vocabulary is a central ontology resource, and
constitutes an agreed-upon reduced set of words and phrases
that can be used to tag information units or to develop a
shared understanding of the entities of interest. Controlled
vocabularies 
are 
designed 
to 
avoid 
correspondence 
and
independence problems in a domain. Relevant controlled
vocabularies include the psychology ontology available on the
NIH Bioportal, as well as the National Cancer Institute’s
Thesaurus (NCIt). Parts of the Systematized Nomenclature of
Medicine—Clinical Terms (SNOMED-CT), especially the
social context section, also qualify, as do the Medical Subject 
headings (MeSH)’s Psychiatry and Psychology sections, and
the 
Semanticscience 
Integrated 
Ontology 
available 
at 
BioPortal. Iivari et al. (2004), as well as Alter (2005, 2012),
propose practical approaches to developing similar content in
the IS field.
Knowledge-Based Models: LI
We chose the LI knowledge-based model proposed by Li et
al. (2006) because it reportedly works well on short texts. 
One of the main reasons for its efficacy is the preservation of
sentence word-order information, which is normally lost with
LDA and LSA. The word order is crucial because it repre-
sents syntactic information which, when combined with
semantic information, conveys the central meaning of sen-
tences. The LI similarity score is derived from the semantic
similarity between words, word-order similarity, and the
information content of the words.
Knowledge-Based Models: MI
MI (Mihalcea et al. 2006) computes the similarity score
through 
the 
combination 
of 
word 
similarity 
and 
word
specificity. In doing so, it borrows similarity metrics from
applications such as malapropism detection.
4
MI also uses
word-sense disambiguation (distinguishing between different
meanings of a word, such as the word bank in the context of
river versus finance).
The Hybrid Models
We propose that hybrid models combining the IS-specific
domain knowledge of LSA and the word-word similarity
components of LI and MI (named CID
1
[LI and LSA] and
CID
2
[MI and LSA]) outperform the original designs, because
WordNet lacks IS-specific domain knowledge. LI and MI
both purport to detect embedded semantics in English-
4
Detection of the unintentional replacement of one word for another similar-
sounding but incorrect word––such as “there’s no stigmata connected with
going to see a psychologist.”
534
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
language sentences. Yet, whereas the original LI sentence-
similarity measure is derived from word order and similarity,
MI factors in word similarity and word specificity. Further-
more, both designs use quite different techniques to measure
word similarity collectively. Benefiting from specific features
of the LI and MI designs, CID
1
and CID
2
use domain-specific
and semantically specialized knowledge bases derived from
LSA 
(rather 
than 
those 
from 
WordNet) 
to 
capture 
the
semantic contexts more accurately and thus better detect
construct correspondence.
Hybrid Models: Construct Identity
Detector 1 (CID
1
)
CID
1
is based on combining the LI algorithm with LSA. CID
1
computes item-similarity scores by using LSA’s cosine of
vectors in the semantic space for word-word similarity rather
than the semantic distance of the words in the hierarchical
structure of WordNet. We retain the ideas of joint word sets,
score normalization, and word ordering. The highly domain-
specific and semantically specialized nature of behavioral
constructs suggests that replacing the WordNet knowledge
base with the LSA-derived domain-specific knowledge base
in LI will capture the semantic context of constructs more
accurately 
and 
therefore 
better 
detect 
construct 
corre-
spondence.
Hybrid Models: Construct Identity
Detector 2 (CID
2
)
CID
2
is based on combining the MI sentence-similarity algo-
rithm with LSA. CID
2
computes item similarity by finding
the most similar words in the items and weighing the words
according to word rarity and commonality. Unlike MI, CID
2
does not rely on WordNet to compute word similarity. CID
2
relies instead on the domain-specific corpus of LSA. This
hybrid 
design 
replaces 
MI’s 
six 
different 
word-word
similarity measures with LSA’s singular cosine value.
Evaluation and Findings
Following the collection and categorization of a set of
constructs to create an evaluative standard, we evaluated the
designs as follows:
1.
All six designs (LSA, LI, MI, LDA, CID
1
, and CID
2
)
were evaluated against a gold standard at the construct
level, using standard evaluative measures, and then tested
for 
design 
efficacy 
under 
jingle 
and 
jangle 
fallacy
conditions.
2.
The usability of the best design, CID
1
, was evaluated in
an experimental setting on four tasks against two groups
of Ph.D. student evaluators (participants) as they per-
formed a construct retrieval task using two different
systems. This evaluation included an evaluation of com-
mon versus uncommon constructs.
3.
The usefulness of CID
1
for researching common con-
structs was evaluated by considering the meta-analytic
implications of differences in constructs retrieved.
4.
The usefulness of CID
1
for researching uncommon con-
structs was evaluated by considering the conceptual
implications of differences in constructs retrieved.
Design Science Process
One goal for this article was to employ a design science pro-
cess that should not only serve the purposes of this project,
but should enable future evaluation of construct technologies.
This process should also allow evaluation of other algorithms
proposed for inclusion in CID or of related technologies
designed to address the CIF. Addressing the CIF in all the
behavioral and social disciplines will require a concerted
community effort. We employed two steps, which should
also allow future research to move forward with expediency. 
Step 1. Procuring constructs and items. The first step in
evaluation should be to assess designs against an existing test
bed. Because no test bed existed at the time of this project,
we had to create one. To simplify future research, we made
the construct test bed for this article publicly available. This
dataset consists of the construct names, definitions, and all
measurement items, as well as construct-level classification
information. Future designs should show an ability to out-
perform the designs tested in this article, either on the test bed
from this article, a larger or different IS test bed, or on a test
bed developed for another discipline. The designs tested
would not necessarily need to outperform existing designs for
all available test beds since, over time, designs should be
optimized for specific settings. For effective progress, it is
important for new test beds or reanalysis of existing test beds
to be shared as public resources. Because no two researchers
will ever completely agree on the structure of a sufficiently
complex taxonomy, improving taxonomies themselves should
be considered worthy research efforts—as long as the im-
proved taxonomy is shared in full, in machine-readable
format.
Step 2. Developing new designs or importing existing
designs. Following Gregor and Hevner (2013), design
science work addressing the CIF should fall into the exapta-
MIS Quarterly Vol. 40 No. 3/September 2016
535
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 3. Design Science Contributions 
Guideline (Hevner et al. 2004)
Contribution
Design as an Artifact
The research outcomes:
1.
CID methodology.
2.
The extension and operationalization of jingle and jangle fallacies.
3.
The CID tool for integration of theories.
Problem Relevance
Large-scale construct identity problem was identified over 100 years ago with no viable
solution to date, in spite of increasing relevance due to publication volume. Providing
tools for recognition of overlaps between theories is essential for true scientific progress
(Noar and Zimmerman 2005). 
Design Evaluation
Utility and efficacy of design outcome evaluated using expert gold standard, experiment
with Ph.D. students, a meta-analytic evaluation of common constructs, and a con-
ceptual analysis of uncommon constructs.
Research Contributions
The first design tool, showing promise for detecting construct correspondence and
independence. Expanded knowledge about IS constructs.
Research Rigor
Comparison of six designs using a carefully developed gold standard, as well as other
evaluations, resulting in triangulation.
Design as a Search Process
Discovery of an effective solution in the form of CID
1
. Evaluation of two different types
of similarity designs and the creation of hybrid designs.
Communication of Research
Detailed information on how to build replicable CID technologies and implications for
behavioral research.
tion or improvement categories. When addressing a new
discipline where CID designs have not been used before,
exaptation research should suffice for the first article, and
research findings would be valuable even if only existing
designs are used. In such cases, the contribution will consist
of examining the extent to which semantic and lexical designs
have the potential to work in the focal discipline, and the
designs discussed herein may represent a reasonable first step.
For improvement research, the focus will be on ensuring that
these designs perform better than existing designs applied to
construct identity detection in the focal discipline. Table 3
outlines the design science process and contributions of this
article in the context of Hevner et al.’s (2004) criteria.
Gold Standard Evaluation
Evaluating the six designs in this article required a gold
standard. A gold standard—“data that is hand-annotated by
domain experts” (Cohen and Hunter 2004, p. 153) who share
an understanding of the task—is used as the “solution”
against which the design performance is assessed and repre-
sents a taxonomy’s use for machine learning training or evalu-
ation. As is common in design science research (Gregory and
Muntermann 2014; Simon 1981), we utilized several heu-
ristics during gold standard creation. First, our experience has
shown that occasionally, construct definitions and items
imperfectly mirror each other. Given that items undergo
rigorous testing and are “closer” to real-world phenomena
than are definitions, we asked experts, in discrepant cases, to
attend primarily to items. Second, we have found no discern-
able rules for whether to explicitly include perceived as part
of a construct name. When supported by the definition and
items, experts were asked not to use the term perceived in a
construct name as a distinguishing feature for determining
construct correspondence.
Three final heuristics addressed construct ambiguity, in-
cluding congeneric methods effects; mono-method bias (Ortiz
de Guinea et al. 2013); items signifying more than one con-
cept (Abbott 1997); ambiguity of an item’s meaning (Abbott
1997); nonlinear and complex relationships between con-
structs (Abbott 1997; Ortiz De Guinea et al. 2014); the
linguistic nature of questionnaire processing (Arnulf et al.
2014); complex constructs encompassing emotions, cogni-
tions, and behaviors (Ortiz de Guinea and Webster 2013); and
use of individual-level items to evaluate group-level con-
structs (Burton-Jones and Gallivan 2007; Easley et al. 2003). 
First, because the jingle fallacy cannot conclusively be
assessed at the item level, domain experts were asked to rigor-
ously treat overlapping but ambiguous constructs as indepen-
dent, rather than as correspondent, resulting in a conservative
evaluation (Burton-Jones and Gallivan 2007; Easley et al.
2003). Second, while any individual indicator or subset of
indicators may be ambiguous, we included only constructs
with three or more items reported. Third, we averaged item
similarities to aggregate results to the construct level.
536
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 4. The 20 Most Common Construct Clusters
Cluster Name
Number of
Constructs
Cluster Name
Number of
Constructs
Individual-level usefulness 
67
IS project success and performance 
11
Ease of use
46
Social influence 
10
Intention to use 
29
Actual user participation 
10
Affect towards technology use 
28
Technology compatibility with work style/values
10
Facilitating conditions 
23
Job overload 
9
Individual-level trust: Trustworthiness 
19
Information quality 
9
General concerns about information privacy 
16
Credibility: Trust in ability and benevolence 
9
Satisfaction with technology 
15
Individual-level trust: Trust propensity 
9
Technology self-efficacy 
12
Trust in others’ ability 
8
Organization-level usefulness 
12
Satisfaction with decision-making outcome 
8
The constructs for this article were collected from two
journals, MIS Quarterly and Information Systems Research,
which were selected for their broad nature and the higher-
than-average 
expectation 
for 
research 
novelty 
in 
highly
ranked journals. The research team considered these journals
likely to contain a broad set of constructs consistent with the
interests of many researchers in the discipline. After exam-
ining articles published during the 1983 to 2009 period—and
after selecting articles containing at least one construct that
explicitly reported at least three items for inclusion—193
articles were found, with a total of 1,004 constructs.
The 1,004 constructs were categorized into 347 correspondent
clusters with an average of 2.89 correspondent constructs in
each cluster (S.D. = 5.32). In this dataset, 178 constructs
(17.7%) were one-offs that never gained traction in these
journals. Correspondent constructs may exist for most of
these singlets outside of the two journals examined. The
1,004 constructs yielded 503,506 construct pairs, including
5,851 correspondent construct pairs. Table 4 lists the 20
largest construct clusters, and the entire gold standard is
reported in Appendix B.
To evaluate the extent of the construct identity problem in the
context of the combinatorics necessary to address the prob-
lem, we separately categorized names for the 1,004 constructs
as same or different and compared these results to the gold
standard categorization of the same constructs. The term
“perceived” appeared in many construct names, which indi-
cates that the construct is ontologically subjective, whereas
the lack of “perceived” could indicate a case of an onto-
logically objective construct (Burton-Jones 2009). This gold
standard set contained predominantly ontologically subjective
constructs. Therefore, we removed the word “perceived”
from all the constructs names to avoid falsely inflating the
number of unique construct names. Doing so led to removal
of 115 instances of “perceived.” Table 5 shows the results.
For the understanding of problem significance, the true nega-
tives (497,236 pairs) are misleading. In fact, any classifica-
tion task with hundreds of categories will predominantly
generate true negative pairs by the nature of the combina-
torics. After all, researchers generally do not start with a
construct such as usefulness and evaluate it against all other
constructs. Literature reviews are generally based on locating
constructs that correspond to the constructs in which one is
interested. As displayed in Table 5, researchers looking for
correspondent constructs will, when attending to names,
return independent constructs 28% of the time. Seldom will
researchers find the jangle fallacy constructs, which consti-
tuted 82% of the total set of correspondent pairs in the gold
standard.
NLP Design Findings
We used each of the six designs to generate similarity scores
for the 11,270,591 relationships between 4,751 items, which
were then aggregated into construct-level relationships (see
Appendix C) and evaluated against the expert-generated gold
standard. We report the outcome using precision, recall, and
F
1
-scores and receiver operating characteristic (ROC) curves,
which are plots that show the true positive rate against the
false positive rate at multiple cutoff points, for which we
conducted statistical significance tests. Finally, we calculated
the area under the curve (AUC) for each ROC curve.
The ROC plotting (Figure 1) was based on the information
collected at 21 similarity-cutoff thresholds in 0.05 increments
MIS Quarterly Vol. 40 No. 3/September 2016
537
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 5. Construct Identity Fallacy Occurrences
Construct Names
Same
Different
Correspondent
True positive:
1,053 pairs
Jangle fallacy:
4,798 pairs
Independent
Jingle fallacy:
419 pairs
True negative:
497,236 pairs
Figure 1. ROC Curves for All Designs
ranging from 0.0 to 1.0. At each cutoff threshold, we con-
sider any construct pair with a similarity score at or above the
cutoff to be correspondent, whereas, we consider any pair
below the cutoff to be independent. Therefore, we can simply
sort the construct pairs according to the similarity score and
count the numbers of true positives (TPs) and false positives
(FPs), depending on whether the predicted relationships are
in agreement with the gold standard.
Comparing the AUCs of the six designs (see Table 6) using
the concentrated ROC framework (CROC) (Swamidass et al.
2010) confirms that LSA does indeed perform significantly
better than random assignment as well as LI and MI, and that,
as expected, CID
1
significantly outperforms LSA, LDA, LI,
and MI at p < 0.01.
Focusing on the best original design (LSA) along with CID
1
and LDA, Figure 2 summarizes precision and recall evalua-
tion by presenting the cumulative frequency distribution
histogram of the correspondent constructs at 21 different
cutoffs, with the X-axis showing various intervals of simi-
larity scores (e.g., the interval labeled 0.50 includes any score
from 0.500 to 0.549). The few negative LSA cosines were
truncated to zero. The rightmost Y-axis of the figure repre-
sents the number of correspondent constructs discovered. The
precision of correspondent constructs obtained at different
cutoff scores is plotted as lines and overlaid on the histogram
in the left Y-axis.
Looking at Figure 2, we see that CID
1
has the highest
precision at higher cutoffs (as compared to LSA and LDA).
538
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
0
1000
2000
3000
4000
5000
6000
Similarity
Number of correspondent constructs (bars)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision (lines)
CID
1
LSA
LDA
CID
1
LSA
LDA
Table 6. Area Under the Curve of ROC and CROC Tests
Design
AUC*
CID
1
0.814
LSA
0.791**
LI
0.728**
MI
0.718**
LDA
0.680**
CID
2
0.643**
*CROC significance evaluations were conducted for all algorithms against CID
1
.
**
α
< 0.01 on the ratio of true positive rate versus false positive rate compared to best design (CID
1
).
Figure 2. Cumulative Number of Correspondent Constructs Found
Although its precision decreases at a faster rate after the
similarity cutoff of 0.50 when compared to LSA, CID
1
’s
precision remains higher than that of the other designs down
to a cosine threshold of 0.15. LSA’s strong initial perfor-
mance, compared to LDA on both precision and detected
correspondent pairs, leads to a better overall performance and
a second rank on AUC (Table 6).
From Figure 3, we can see that LI performed nearly on par
with LSA and outperformed MI and CID
2
, with CID
2
as the
weakest performer. Thus, with respect to finding corre-
spondent constructs, CID
1
is a better design because it has
higher precision and results in more correspondent pairs. 
Significantly, random assignment would yield a precision of
0.012, which, by comparison, CID
1
outperforms by a factor of
48 or higher all the way down to the .50 similarity cutoff. In
fact, CID
1
finds almost all correspondent construct pairs
before nearing the random assignment precision level.
Both CID
1
and CID
2
frequencies were normally distributed,
5
whereas LSA’s distribution is skewed to the right and not
normal. Figures 2 and 3 demonstrate that a better design is
one that has high precision as well as a high number of
correspondent pairs above higher cutoff points. The actual
cutoff value is of little interest here, as one can assume that a
5
Normalities were tested with the Kolmogorov–Smirnov test.
MIS Quarterly Vol. 40 No. 3/September 2016
539
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
0
1000
2000
3000
4000
5000
6000
Similarity
Number of correspondent constructs (bars)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision (lines)
MI
LI
CID
2
MI
LI
CID
2
Figure 3. Cumulative Number of Correspondent Constructs Found (Continued)
future user of CID has a fixed level of patience with false
positives and will continue to review results until the preci-
sion rate gets too low. CID
1
’s performance is favorable due
to its better balance between precision and number of
returned correspondent constructs. Despite CID
2
’s capacity
for identifying a high proportion of correspondent pairs at
higher cutoff levels than the other designs, its high false
positive rate renders it an inferior design.
Jingle and Jangle Fallacy Findings
Moving into the CIF components, a test of the overall
performance of the six designs for addressing the jingle and
jangle fallacies was conducted in the same way by plotting
ROC charts. Doing so presented a problem because, per
Table 1, a case of jingle fallacy represents a false positive.
The existence of true negatives complicates the detection of
these false positives. In this case, true negatives represent
more pairs in excess of two orders of magnitude (see
Table 5), which would severely reduce the diagnosticity of
any test. We therefore removed all true negative pairs and
evaluated each design’s ability to provide low similarity
scores for jingle fallacy pairs (pairs with same names for
independent pairs). We inverted similarity scores (1-sim) to
simplify 
presentation 
of 
ROC 
chart 
and 
the 
AUC
calculations. 
The jangle fallacy provides an equivalent, if smaller, prob-
lem in that for every five jangle fallacy pairs (false nega-
tives) there is about one true positive pair. Starting with the
original dataset, we therefore removed all true positive pairs
and reevaluated all designs for their ability to provide high
similarity scores for jangle fallacy pairs. As demonstrated
in Table 7, we were able to address the jangle fallacy
through the designs we examined. Removing the true posi-
tives (constructs with the same names measuring the same
phenomenon) would likely retain the construct pairs that are
hardest to detect. In spite of this, CID
1
retained the vast
majority of its ability to detect correspondent pairs. As may
be seen by comparing jangle fallacy AUC scores (Table 7)
to overall AUC results (Table 6), the order of performance
remained consistent, and CID
1
lost less ability than the other
top designs,
6
only 0.0158 AUC. CROC evaluation showed
that CID
1
’s performance was significantly better than all
other designs. Further, Table 7 shows that we were able to
address the jingle fallacy using CID designs. While AUC
performance is lower than that for the jangle fallacy and
lower than overall performance, the performance is still
respectable. In this case, LSA significantly outperformed
all other designs; CID
1
came in second and also signi-
ficantly outperformed all other designs.
6
Only CID
2
lost less ability, but this is likely due to the poor initial
performance of CID
2
.
540
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 7. Area Under the Curve (AUC) of ROC and CROC Tests for Jingle and Jangle Fallacies
Jangle Fallacy
Jingle Fallacy
Design
AUC
Design
AUC*
CID
1
0.7982
LSA
0.7328
LSA
0.7735**
CID
1
0.6719**
LI
0.6986**
LI
0.6424**
MI
0.6939**
MI
0.6050**
LDA
0.6565**
LDA
0.5809**
CID
2
0.6380**
CID2
0.5767**
*CROC significance evaluations were for the jingle fallacy conducted for all algorithms against LSA.
**
α
< 0.01 on the ratio of true positive rate versus false positive rate compared to best design (CID
1
).
While the above evaluations clearly showed the efficacy of
the CID
1
and LSA designs, the practical implications of such
designs bear evaluation. Therefore, we conducted a construct
retrieval experiment in which CID
1
was evaluated against 36
doctoral students who were randomly assigned to use either
a full-text search engine that is commonly available in
libraries worldwide (EBSCO), or a newer search engine con-
taining extracted constructs (INN). The experimental evalua-
tion section contains the full evaluation procedure and results.
Experimental Evaluation
To evaluate the usability and usefulness of CID
1
, we con-
ducted a randomized experiment with students from a wide
range of Ph.D. programs in the United States and around the
world. After evaluating design performance by participants
on four construct-detection tasks, we evaluated the perfor-
mance 
of 
CID
1
for 
common 
constructs 
and 
then 
for
uncommon constructs.
Experimental Process
To evaluate CID
1
’s ability to perform large-scale retrieval of
constructs, we selected EBSCO HOST’s full-text article
search because it is widely used in practice, and equivalent or
better performance by CID
1
would suggest practical merit. At
the 
time 
of 
the 
experimental 
process, 
EBSCO 
further
represented the longest uninterrupted period of full-text
coverage for both MIS Quarterly and Information Systems
Research available to the research team (1996–2009). The
Inter-Nomological Networks (INN)—an online Web portal
enabling direct search for constructs (Larsen and Bong 2015)
in journals such as MIS Quarterly and Information Systems
Research—also covered the same period with a total of 2,908
variables, including 1,730 behavioral constructs that we
classified according to the process outlined in the evaluation
section. INN provides the opportunity to do a theoretical
comparison because it shows what a search platform like
EBSCO could achieve if designed more effectively, and it
enables comparison of expert judgment versus automatic
design results on similar database content, favoring the
experts. Expert advantages came from having access to
construct names, definitions, and citation information as well
as the item information used by CID
1
. We built a special-
purpose version of INN that enabled keyword search only for
the selected period, using the Lucene open-source search
engine (Hatcher et al. 2004). Arguably, removing all the
misleading full-text around constructs will account for much
of the ability of the designs to outperform experts using full-
text search.
Using t-tests, the two experimental groups were evaluated
based on age, years of work experience, years in Ph.D.
program, have/do not have Master’s degree, experience with
construct-based research, and task experience, none of which
was significant at p < .05, suggesting no major differences
between groups. Two variables failed the Poisson test and
were analyzed via ranked scores using a Mann–Whitney test.
The self-reported number of articles published in journals
was not significantly different between the groups, whereas
the number of articles published in conferences was signi-
ficantly higher for the group assigned to the EBSCO system
(z = -2.10, p = 0.036), suggesting that any difference between
the groups favored the traditional approach to construct
retrieval. One additional indicator suggested an advantage for
the traditional system: the average participant of experi-
mental group 1 (EBSCO) indicated that s/he had used the
system more than six times before, whereas participants in
experimental group 2 (INN) had, for the most part, never used
the system (χ² = 15.451, df = 4, p-value = 0.004), suggesting
a significant advantage for the traditional system.
MIS Quarterly Vol. 40 No. 3/September 2016
541
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 8. Constructs for Tasks 1 through 4
Task/Construct*
Definition
Items
1.
Social Influence (Hsieh
et al. 2008) (32
correspondents)
The perceived expectation from
family, relatives, friends, and
peers for an individual to perform
the behavior of interest
•
My family thinks that I should use the Internet TV
•
My relatives think that I should use the Internet TV
•
My friends think that I should use the Internet TV
•
People I work with think I should use the Internet TV
2.
Perceived usefulness
(Chin et al. 2003) (71
correspondents)
The degree to which a system is
perceived to enhance one’s
performance
•
Using Electronic Mail in my job enables (would enable) me to
accomplish tasks more quickly
•
Using Electronic Mail improves (would improve) my job performance
•
Using Electronic Mail in my job increases (would increase) my
productivity
•
Using Electronic Mail enhances (would enhance) my effectiveness
on the job
•
Using Electronic Mail makes it (would make it) easier to do my job
•
I find (would find) Electronic Mail useful in my job
3.
Familiarity [with
website] (Gefen 2000)
(4 correspondents)
Consumer’s familiarity with the
site
•
Overall, I am familiar with this site
•
I am familiar with searching for items on this site
•
I am familiar with the process of purchasing from this site
•
I am familiar with buying products from this site
4.
Errors [in personal
data held by organiza-
tion] (Smith et al. 1996)
( 3 correspondents)
Consumers’ opinions about
errors in organizational practices
•
All the personal information in computer databases should be
double-checked for accuracy—no matter how much this costs
•
Online companies should take more steps to make sure that the
personal information in their files is accurate
•
Online companies should have better procedures to correct errors in
personal information
•
Online companies should devote more time and effort to verifying
the accuracy of the personal information in their databases
*The number of correspondent constructs differs from those in Table 4 due to different inclusion criteria imposed by the period of full-text availability
for EBSCO.
We evaluated CID
1
’s performance against that of the experi-
mental groups using precision, recall, and F
1
-measures. CID
1
design answers cannot be considered equivalent to individual
human responses; furthermore, the design produces only one
answer, meaning that statistical significance tests comparing
the design and the groups of human participants were not
possible. In spite of this, we believe the results (1) allow
evaluation of the usability of the designs for retrieval of con-
structs and (2) offer suggestions as to circumstances under
which a specific design may best approximate human capa-
bilities (keeping in mind the impossibility of human-coder
processes for the problem of construct-identity detection,
given the sheer volume of published constructs).
Tasks
Because construct-retrieval tasks are very time consuming,
four tasks were provided for the participants, two common
tasks (commonly used and likely known constructs) and two
uncommon tasks (infrequently used and likely unknown
constructs). The tasks were randomly selected from the pools
of classified constructs, two from the set of common con-
structs and two from the pools with only three to five
synonymous constructs (uncommon constructs) (see Table 8).
For each task, the participants were given, as the starting
point, an example of a construct name along with a definition
and a set of items. Participants were then asked to find all
synonymous constructs in the given database. The starting
construct example was randomly picked from each pool of
synonymous constructs. Participants were randomly assigned
to start with common or uncommon tasks; we found no statis-
tical difference in performance based on this order at p < .05.
On average, participants self-reported that each task took 29.7
minutes, with common tasks at 32.8 minutes and uncommon
tasks at 26.6 minutes.
The first author and a chief research assistant with four years
of experience in construct extraction and evaluation assessed
the first half of all non-categorized constructs, which were
those answers by the participants that could not immediately
be resolved as correct (primarily an issue with the EBSCO
542
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Figure 4. Results (Average Precision, Recall, and F
1
-Scores)
responses). For example, if a participant suggested the exis-
tence of a usefulness construct in an article where past coding
had not suggested such a construct, that article was re-
examined. There were no disagreements between the two
coders for the first half of the sample. As a result, the first
author coded the second half of the sample alone. Figure 4
shows results for common and uncommon construct search,
and is discussed in the following subsections.
Common Construct Tasks Findings
For the common tasks, the study participants were likely
familiar with the literature. Given the attention in IS to (1) the
technology acceptance model (TAM; Davis 1989) and the
unified theory of acceptance and use of technology (UTAUT;
Venkatesh et al. 2003) and (2) perceived usefulness and social
influence in these theories, the participant performance on
these tasks suggests a keen understanding of the constituent
content of these constructs. When evaluating precision in the
social influence retrieval task (Figure 4a), participants as-
signed to the INN search condition correctly recognized the
social influence construct 86% of the time. While that recog-
nition rate dropped to 55% for users of EBSCO, it should still
be considered quite high. The CID
1
algorithm outperformed
10 of the 18 participants in the EBSCO group, with a pre-
cision of 60%, but it clearly lagged behind the INN, outper-
forming only 1 of 18 participants in that group.
As may be seen from Figure 4a, all three approaches—INN
keyword (24%), EBSCO (6%), and CID
1
(20%)—performed
poorly on the recall evaluation, no approach retrieving even
25% of the known instances of social influence in the dataset. 
Here, CID
1
outperformed 6 of 18 doctoral students using INN
and all doctoral students using EBSCO. The F
1
-score is
shown here in the same order as the independent precision
MIS Quarterly Vol. 40 No. 3/September 2016
543
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
and recall metrics, with scores of .36, .11, and .26, respec-
tively. For F
1
, which represents the overall evaluation for the
task, CID
1
outperformed 3 of 18 participants using INN and
17 of 18 participants using EBSCO.
These patterns repeated themselves for the second task,
perceived usefulness, with the exception that, for precision,
the EBSCO experimental group outperformed CID
1
, sug-
gesting just how well Ph.D. students know this construct—
perhaps the best-known—in the IS discipline. Per Figure 4b,
precision for INN was the best of any task at 88%, followed
by EBSCO at 59%, and CID
1
at 39%. In this case, CID
1
out-
performed no Ph.D. students using INN but 7 of 18 students
using EBSCO. The story for recall was quite a bit better for
CID
1
, and CID
1
’s performance (26%) was again close to that
of the INN group (31%), outperforming 4 of 17 group parti-
cipants as well as every participant in the EBSCO group (7%)
by a wide margin. The surprise here is how hard it is to find
all relevant articles containing the desired constructs in a full
text search. Finally, the F
1
-scores confirm that INN keyword
outperformed 
the 
other 
two 
approaches, 
.45 
versus 
.13
(EBSCO) and .23 (CID
1
). In this case, CID
1
outperformed
only one Ph.D. student using INN and 13 of 18 using EBSCO. 
Uncommon Construct Tasks Findings
Because of the drive toward novelty, there are many more
small, or uncommonly used, construct categories. Most new
projects will require the researcher to venture into unknown
territory in search of novelty. The third task, website
familiarity (Figure 4c) provides an example of such territory. 
For this task, CID
1
’s precision (100%) outperformed INN
(22%) and EBSCO (7%); on recall, CID
1
(24.5%) out-
performed EBSCO (3%) but underperformed INN (45%). For
this task, CID
1
outperformed all but three INN users and all
EBSCO users. For the F
1
-score, CID
1
(.40) outperformed
both INN (.27) and EBSCO (.04). Here, CID
1
outperformed
all 36 human participants.
For the final task, per Figure 4d, for precision, CID
1
(100%)
again outperformed both INN (71%) and EBSCO (14%), in
the process surpassing 15 of 17 INN users who completed the
task and all EBSCO users. For recall, CID
1
(67%) barely
surpassed INN (66%) but clearly outperformed EBSCO
(19%). For recall, CID
1
was only able to surpass one INN
user, but did outperform 14 of 16 EBSCO users who com-
pleted the task. For the overall F
1
evaluation, CID
1
(.80)
outperformed INN (.54) and EBSCO (.16). CID
1
outper-
formed 10 of 17 INN users and 15 of 16 EBSCO users. 
Because design performance will vary based on the construct
used as a starting point, Appendix D shares an evaluation of
CID
1
with 20 randomly drawn starting points, resulting in
further support for CID
1
’s performance.
We are aware of no other assessments in the literature
regarding the human ability to find constructs, and our results
indicate poor human capability to locate constructs in full-
text. On average, the participants assigned to EBSCO
retrieved 9% of the relevant constructs, even in such a small
sample of articles. Much has been said about the “reinvention
of the wheel” in the world of construct research. This assess-
ment evinces that the problem may be one of human inability
to find existing research. We believe the participants in this
experiment were quite thorough and motivated to do a good
job, as shown by their performance on the large/known tasks,
and as such, the experiment only scratches the surface of how
incredibly difficult it is to find existing research in an
environment plagued by the CIF.
It is encouraging that with this initial algorithm, even experi-
enced doctoral students can be outperformed on some tasks.
The CID
1
algorithm performs slightly better than participants
using INN keyword search for average F
1
value over the four
tasks of .42 versus .40, and outperforms the participants on
the unknown tasks. If the goal is high recall, as we believe it
should be, there simply are too many constructs now in
existence for human beings to be able to play much more of
a role than that of creating, training, and evaluating designs.
Given 
that 
the 
participants 
self-reported 
spending 
29.7
minutes per task, the existence of hundreds, if not thousands,
more relevant journals would likely degrade human perfor-
mance appreciably due to eventual satisficing.
Implications of CID
1
for Understanding of
Common Constructs: A Meta-Analysis
The preceding results show that CID
1
and INN each perform
well in different scenarios, but both appear to have clear
benefits over strict reliance by participants searching EBSCO. 
To understand these results in more detail, we now leave INN
aside and examine whether the difference between using CID
1
and EBSCO can actually have a material effect on what
researchers can learn by using CID
1
rather than current prac-
tices. This section focuses on the case of common constructs.
We selected the meta-analytic approach (Glass et al. 1981;
Stroup et al. 2000) because it enables evaluation the ability of
various search processes to produce the best possible results
within a corpus. In this case, we conducted the analysis to
examine material differences in performance of CID
1
versus
full-text search. Based on the gold standard, we found and
examined the articles containing both perceived usefulness
544
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
and 
social 
influence 
constructs 
within 
the 
experimental
corpus, finding 12 articles that contained at least one pair of
these constructs. Upon examination, one article (Karahanna
et al. 1999) focused on pre- and post-adoption effects and did
not provide relationship statistics for the focal constructs,
leaving 11 articles that reported on 29 statistical relationships
between the two constructs.
We selected the fixed-effect model (Hedges and Vevea 1998),
because we examined all papers available in the experimental
time period. We evaluated each participant (as well as CID
1
)
and his/her average correlation and confidence interval (C.I.)
against the population findings, depending on which papers
a participant was able to find. The participant was required to
discover both constructs in the same paper.
From each paper, we collected the correlation coefficients
between the variables of interest—perceived usefulness and
social influence—and sample size. For each correlation coef-
ficient obtained, we calculated the standard error from the
sample size as n-3. Using a Fisher’s Z-transform, the corre-
lation coefficients were transformed to get r
z
, and the fixed
correlation coefficient was computed from the transformed
value, using a weighted mean average.
We computed the 95% C.I. using the standard error. For each
participant, we also (1) calculated the correlation coefficient
using just the papers that were found, and then (b) we com-
pared this correlation coefficient to the expected correlation
coefficient by noting whether it was within the 95% C.I.
obtained for each participant. Table 9 is ordered from best
performance to worst performance, and the significance
column identifies participants whose answers deviated signi-
ficantly from the population answer. The first data row in
Table 9, entitled Population, contains the population calcula-
tion using all 29 correlations and their sample sizes, a
weighted sample size of 6,843, with an average correlation of
0.303 and 95% C.I. between 0.282 and 0.325.
Per Table 9, only three participants found one or more of the
11 articles (participants 4, 8, and 10), and for one of these, the
retrieved articles would have misled him/her into concluding
erroneously that the relationship between perceived usefulness
and social influence was significantly different from its popu-
lation score. By evaluating the z-scores, we see that CID
1
performed between the top two participants with correlation
estimates not significantly different from the population and
it outperformed the average of the three best participants in
spite of having access to only construct items, whereas the
participants also had access to construct names, definitions,
citations, and the literature reviews inside full-text articles. It
should be noted that given the small set of articles found by
human participants and algorithm alike, arriving at the right
answer in this exercise likely contained elements of luck.
While the precision and recall for this experiment clarified the
incumbent need to develop alternatives to full-text search, it
was not clear that missing a few papers would make a measur-
able difference in evaluating relationships between constructs.
The meta-analytic approach employed in this article shows
that 83% of participants would arrive at the conclusion that
the relationship had not been tested at all; of the remaining
three participants, only two would arrive at a correct conclu-
sion. If incorrect conclusions and underestimations are pos-
sible for well-known construct pairs, such conclusions could
be quite common during searches for lesser-known constructs.
Implications of CID for Understanding of
Uncommon Constructs: The Error Construct
Common constructs, such as perceived usefulness and social
influence, are crucial to the success of several major theories,
including TAM (Davis 1989), UTAUT (Venkatesh et al.
2003), and diffusion of innovations (Rogers 2003), and are,
consequently, well known by researchers in the discipline. At
times, expert researchers can recall several correspondent
construct names. Hence, experts dealing with common con-
structs sometimes have multiple starting points for their
search (for example, a researcher may search for relative
advantage, usefulness, or performance expectancy).
In contrast, researchers searching for uncommon constructs
are less likely to know of the existence of past work applying
the construct of interest. When they are aware of past work,
they are much more likely to proceed from only one starting
point. In such cases, one possibility is that the researcher
knows that a construct exists, has access to its name, defini-
tion, and previously used measurement items, but does not
know of related correspondent constructs. Given our findings
of 
low 
recall 
for 
uncommon 
constructs, 
what 
are 
the
behavioral implications to the IS discipline?
Our sample of 1,004 constructs was categorized into 347
distinct construct categories, of which 314 (90.5%) fit our
definition of uncommon—defined here as occurring five or
fewer times in our dataset. The large proportion of uncom-
mon constructs could be seen as indicative of the IS field’s
focus on theoretical extensions of existing theories. Uncom-
mon constructs serve a crucial function in validating common
constructs (Cronbach and Meehl 1955) and to better under-
standing what drives behavior. Common constructs are some-
times extended in thousands of articles (Larsen et al. 2014),
yielding many new constructs, the overwhelming majority of
which remain uncommon.
MIS Quarterly Vol. 40 No. 3/September 2016
545
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table 9. Meta-Analytic Evaluation of Participant and Algorithm Performance
Each Individual Meta-Analysis
Comparison Versus Population
Name
Articles
Found
~Sample
Size
Correlation
Low 
[95% C.I.]
High 
[95% C.I.]
z-score
Significant
Difference
Population
11
6,843 
0.303
0.282
0.325
n/a
n/a
Best participant
2
957 
0.306
0.248
0.363
0.095
CID
1
1,335 
0.268
0.217
0.317
1.292
Second best participant
1
339 
0.370
0.275
0.458
1.352
Third best participant
3
1,743 
0.379
0.339
0.419
3.216
***
Participants 4 - 18
0
—
Results significantly different from population answer. *p < .05, **p < .01, ***p < .001.
The small corpus used in our experiment should have com-
pensated for the lower motivation likely in an experimental
setting. Therefore, our experimental findings on low expert
ability to find constructs in full-text articles were surprising. 
If indeed researchers are able to find only a small proportion 
of the relevant construct articles, what are the implications for
research practice? Ceteris paribus, missed constructs will
disproportionately impede domain sampling and content
validity (Churchill 1979) for uncommon constructs relative to
common constructs.
We focus here on the specific case wherein recall under the
full-text search condition (EBSCO) was at its peak, 19%
(relative to 3%, 6%, and 7%). This experimental task, to find
the construct named errors [in personal data held by organi-
zations]—is a privacy-focused construct associated with items
such as “Companies should take more steps to make sure that
the personal information in their files is accurate.” This was
where both CID
1
and all the participants (regardless of system
use) performed the best for recall. When evaluating the
reason for this performance, we found this task, relative to all
others, to be easy. Specifically, the experimental task was to
find three identically named constructs with almost identical
measurement items on the topic of privacy, a context that was
quite unique within the sample of articles. This was a case
where experimental participants had a great advantage over
CID
1,
in that the three articles containing this construct were
heavily connected through citations, and no knowledge of
correspondent construct names was needed.
The errors construct was first proposed by Smith et al. (1996)
in an article cited no less than 20 times in Stewart and Segars
(2002) and 21 times in Malhotra et al. (2004). Malhotra et al.
(2004) also cited Stewart and Segars (2002) 11 times. While
it is hard to imagine any researcher missing these citations
when sitting down to read the found articles, the 19% recall
should be taken as an indication of how many articles remain
unfound in this initial step of the review process. Such an
implication must also take into account that, in a setting of
hundreds of journals and as many relevant conferences, exam-
ination of citation links in the articles found is only possible
to the extent that the number of retrieved articles is below the
threshold set by a researcher’s available time. Even for
uncommon constructs such as errors, and even if the preci-
sion could be perfect, the number of papers found in a search
may consist of dozens or even hundreds of articles, each with
dozens of citations to explore.
Smith et al. proposed an information privacy instrument mea-
suring the “primary dimensions of individuals’ concern about
organizational information privacy practices.” The resulting
instrument contained a first-order model containing four
constructs; collection, errors, unauthorized secondary use,
and improper access. These constructs focus on attitudes
toward company collection of private data, the existence of
errors in the resulting company database, company use of data
for unauthorized purposes, and access to the data by unauth-
orized individuals, respectively. Accessing this paper pro-
vides the researcher with the knowledge that the error
construct exists and also provides understanding of constructs
closely related to error. However, without finding Stewart
and Segars, the researcher will not know the four constructs
may be presented as a second-order model, which thereby
provides stronger evidence of the relationship between the
second-order model, computer anxiety, and behavioral inten-
tion. Malhotra et al. (2004) reimagined privacy concern after
examining the error construct along with other constructs. 
They suggested that the error construct was no longer neces-
sary when privacy is recontextualized within social contract
theory (Donaldson and Dunfee 1994). Malhotra et al.’s path
values suggested that further examination was necessary, but
they introduced trusting beliefs, risk beliefs, information
sensitivity, 
and 
additional 
evidence 
for 
relationships 
to
behavioral intention, all relationships that would be missed by
the vast majority of the EBSCO participants.
546
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Swanson (1986) convincingly showed that academia’s divide-
and-conquer strategy (into academic fields), necessitated by
the increasing complexity and sheer mass of research, at times
left even seemingly obvious or “public” knowledge undis-
covered. He suggested that most units into which we split
academia are undoubtedly connected to other units, that there
are far more potential relationships between units than there
are units themselves, and that the system of organization is
not organized to cope with combinations. Our experiment
shows that even within the narrowest definitions of such units
(two highly selective journals in one relatively narrow field),
existing relationships are likely lost during literature reviews
due to the complexities of full-text search. In essence, the
experimental participants would have missed most of the
nomological network tied to the error construct. We provide
evidence that the search for knowledge is more complex than
suggested by Swanson. While Swanson addressed undis-
covered knowledge, we found the vast majority of discovered
knowledge is hidden from the individual researcher by the
very nature of the search process itself.
Discussion
Researchers are not confined to gathering empirical evidence
in order to support hypotheses but can also substantiate
hypotheses by fitting them to existing theories (Kaplan 1964).
When hypothesizing a relationship between constructs A and
B, detecting constructs A' and B' in another theory arguably
provides as much evidence for the relationship as a new test
of the relationship (Bacharach 1989). Similarly, in meta-
analysis, finding a published relationship between X' and
dependent variable Y can be considered a conceptual replica-
tion of the relationship between X and Y (Hunter and Schmidt
2004, p. 436). As long as the constructs reflect the same
phenomenon, such analysis arguably improves construct
validity (Hunter and Schmidt 2004, p. 448). Given the
enormous corpora of published research, attention should
begin to shift from generation of new research to integration
of past research and the tools necessary to detect corre-
spondent relationships.
We confirm that NLP-based designs can indeed address the
construct identity fallacy, including its jingle and jangle com-
ponents. Such designs can support meta-analysis, reviews,
and construct validity evaluations. In addition to this major
implication, the work has several secondary findings. First,
we establish that construct renaming reduces construct recall,
especially during full-text search. Second, we show the
relative frequencies of jingle and jangle fallacies in a sample
of IS journals. Third, extending Swanson’s (1986) conclu-
sions, we demonstrate that relationships need not be hidden in
order to be inaccessible to researchers. Fourth, while use of
INN in many cases outperformed CID
1
, INN still requires the
active involvement of human experts, whereas CID tools can
examine large sets of constructs automatically. Further, CID
should be relatively invariant in terms of reflective and
formative constructs (see the second section of Appendix C).
The construct identity fallacy represents an enormous chal-
lenge for researchers laboring to connect their research to the
past and incrementally build a future for their discipline. 
During evaluation of designs, CID
1
was found to be most
efficacious for addressing overall CIF and the jangle fallacy,
and second best for the jingle fallacy, after LSA. Human
participants using INN during the common construct tasks
outperformed CID
1
, but CID
1
outperformed participants using
INN and those using EBSCO full-text search during uncom-
mon construct tasks. When applying a meta-analytic evalua-
tion of the results for the common construct tasks in which
participants outperformed CID
1
when evaluated against the
gold standard, we found that CID
1
outperformed the vast
majority of participants using full-text search and that most
participants would erroneously conclude that no past research
had addressed the relationship between the two constructs.
By selecting only constructs where three or more items were
available, one-item and two-item constructs were excluded,
leaving an open question about how well the tool would work
for such constructs. While one- and two-item constructs may
not be as common as three- or more-item constructs, the issue
may be pervasive because authors often include only one or
two sample items for each construct in the published paper. 
This case, however, is no different from that faced by
researchers doing factor-analytic research, as items must be
procured before analysis is possible.
Our experiment and meta-analytic evaluation show not only
the importance of such research due to the herculean task of
finding constructs using full-text search, but also the useful-
ness of NLP-based designs in detecting and integrating con-
structs based on linguistic features. In fact, the experimental
meta-analysis showed that the majority of Ph.D. student parti-
cipants retrieved results that would have resulted in incorrect
conclusions about the relationships between constructs. This
concealment of past research discloses a worrisome question: 
What is the value of research if only a fraction of past
research is accessible to any individual researcher when
needed? We suggest here that because such knowledge is
available in the plain text of papers, ontology learning ap-
proaches are necessary in order for researchers to access a
larger proportion of our shared knowledge heritage. 
MIS Quarterly Vol. 40 No. 3/September 2016
547
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Conclusion
The construct identity fallacy has existed for over 100 years
across a number of behavioral and social disciplines with no
viable large-scale solution hitherto proposed. CIF has major
implications 
for 
the 
retrieval 
of 
relevant 
research. 
In
evaluations of participants’ recall performance in full-text
search, we found that participants could detect relevant
articles containing a specific construct on average only 9% of
the time, and relevant articles containing a pair of common
constructs on average only 3% of the time. The experiment
also found that while Ph.D. student participants were able to
recognize familiar constructs when these constructs were
already extracted from the full-text, this ability decreased
markedly 
when 
participants 
were 
asked 
to 
detect 
and
delineate those constructs from a corpus of full-text papers. 
The construct identity detector tool presents a technical solu-
tion to CIF that diverges from existing approaches because it
does not depend on survey respondents, and therefore allows
large-scale investigation of constructs. CID could enable
science to function better by encouraging creative research
unrestrained from the need for naming conventions, while
simultaneously identifying construct identity fallacies, under-
standing 
their 
effects, 
and 
allowing 
improved 
research
integration.
This work is part of a larger Design Science research program
designed to address construct identity fallacies, and it takes a
step toward providing the construct identity detection capa-
bilities necessary to make ontology learning possible and
useful through automatic creation of construct knowledge
bases, as proposed by Li and Larsen (2011, 2013). Our first
attempt to evaluate the various designs provides a direction
for further exploration and refinement of a design that can
help identify correspondent constructs. While creation of
construct knowledge bases will require work, the tool outlined
in this article is notable because items from thousands of
constructs may be evaluated, and construct pairs with high
similarity scores are more likely to be correspondent. Finding
additional relationships between constructs are shown to have
major behavioral implications for our understanding of con-
structs and real-world phenomena.
Finding a shared construct in two theories may enable the
connection of those two theories. While the concept of fitting
hypotheses to existing theories is deceptively simple, the
successful implementation of this concept has the potential to
change the conduct of research in the behavioral sciences. 
Success in creating technologies able to address the construct
identity puzzle for all the behavioral disciplines—as this
article found possible for the IS discipline—has the potential
for discipline-wide transformation across the social and
behavioral sciences.
Acknowledgments
We thank the U.S. National Science Foundation for research support
under grant NSF 0965338 and the National Institutes of Health
through Colorado Clinical & Translational Sciences Institute for
research support under NIH/CTSI 5 UL1 RR025780. We are
grateful for the unwavering support from MIS Quarterly Senior
Editor Andrew Burton-Jones, Associate Editor Yulin Fang, and four
anonymous reviewers. We appreciate Julia I. Lane and Randy Ross
for their support and for the feedback from Lee Jun Choi, Justo
Amador Diaz, James Endicott, Brent Gallupe, Dirk Hovorka, Kishen
Iyengar, Damien Joseph, Kenneth Kozar, Tor Larsen, Jintae Lee,
Jingjing Li, Sid Saleh, Felix Tan, and Zoya Voronovich. We are
grateful to faculty at the University of Colorado, University of
Queensland, 
Australia, 
Bond 
University, 
Australia, 
and 
the
Norwegian 
Business 
School 
for 
invitations 
to 
present 
earlier
versions of this work and the following feedback. We also would
like to thank Universiti Malaysia Sarawak for supporting this work
and the research team members who work under IRIS/2011/360 for
providing computing resources.
We are also thankful for the research and data collection efforts
headed by Connor Williams, Lauren Durkee, Heather Witte, Lauren,
Krowl, 
Leslie 
Grush, 
Jocelyn 
Mulkey, 
Victoria 
Gershuny,
Alexandra Morgan, and all of the Human Behavior Project research
team at the Leeds School of Business, University of Colorado at
Boulder.
References
Abbasi, A., Zhang, Z., Zimbra, D., Chen, H., and Nunamaker, J. F. 
2012. “Detecting Fake Websites: The Contribution of Statistical
Learning Theory,” MIS Quarterly (34:3), pp. 1-28.
Abbott, A. 1997. “Seven Types of Ambiguity,” Theory and Society
(26:2), pp. 357-399.
Aggarwal, C. C., and Zhai, C. 2012. “A Survey of Text Clustering
Algorithms,” in Mining Text Data, C. C. Aggarwal and C. Zhai
(eds.), New York: Springer, pp. 77-128.
Alexander, P. A., Schallert, D. L., and Hare, V. C. 1991. “Coming
to Terms: How Researchers in Learning and Literacy Talk about
Knowledge,” 
Review 
of 
educational 
research 
(61:3), 
pp.
315-343.
Alter, S. 2005. “Architecture of Sysperanto: A Model-Based
Ontology of the IS Field,” Communications of the Association for
Information Systems (15:1), p. 1-40.
Alter, S. 2012. “The Knowledge Cube: Scaffolding for a Body of
Knowledge about Information Systems,” in Proceedings of the
20
th
European Conference on Information Systems, Barcelona,
Spain, 2012, Paper 22.
Arnulf, J. K., Larsen, K. R., Martinsen, Ø. L., and Bong, C. H. 
2014. “Predicting Survey Responses: How and Why Semantics
Shape Survey Statistics on Organizational Behaviour,” PloS One
(9:9), p. e106361.
Bacharach, S. 1989. “Organizational Theories: Some Criteria for
Evaluation,” 
Academy 
of 
Management 
Review 
(14:4), 
pp.
496-515.
548
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Bailey, S., and Meurers, D. 2008. “Diagnosing Meaning Errors in
Short Answers to Reading Comprehension Questions,” in EANL
’08 Proceedings of the Third Workshop on Innovative Use of
NLP for Building Educational Applications, Association for
Computational Linguistics, pp. 107-115.
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. “Latent Dirichlet
Allocation,” Journal of Machine Learning Research (3), pp.
993-1022.
Block, J. 1995. “A Contrarian View of the Five-Factor Approach
to Personality Description,” Psychological Bulletin (117:2), pp.
187-215.
Bong, M. 1996. “Problems in Academic Motivation Research and
Advantages and Disadvantages of Their Solutions,” Contem-
porary Educational Psychology (21:2), pp. 149-165.
Borsboom, D., Cramer, A. O. J., Kievit, R. A., Scholten, A. Z., and
Franic, S. 2009. “The End of Construct Validity,” in The
Concept of Validity: Revisions, New Directors, and Applications,
R. W. Lissitz (ed.), Charlotte, NC: Information Age Publishing,
pp. 135-170.
Burton-Jones, A. 2009. “Minimizing Method Bias Through
Programmatic Research,” MIS Quarterly (33:3), pp. 445-471.
Burton-Jones, A., and Gallivan, M. J. 2007. “Toward a Deeper
Understanding of System Usage in Organizations: A multilevel
Perspective,” MIS Quarterly (31:4), pp. 657-679.
Chin, W. W., Marcolin, B. L., and Newsted, P. R. 2003. “A Partial
Least Squares Latent Variable Modeling Approach for Measuring
Interaction Effects: Results from a Monte Carlo Simulation
Study 
and 
an 
Electronic-Mail 
Emotion/Adoption 
Study,”
Information Systems Research (14:2), pp. 189-217.
Churchill Jr., G. A. 1979. “A Paradigm for Developing Better
Measures 
of 
Marketing 
Constructs,” 
Journal 
of 
Marketing
Research (16:1), pp. 64-73.
Cohen, K. B., and Hunter, L. 2004. “Natural Language Processing
and Systems Biology,” in Artificial Intelligence Methods and
Tools for Systems Biology, W. Dubitzky and F. Azuaje (eds.),
New York: Springer, pp. 147-173.
Cozby, P. 1993. Methods in Behavioral Research (5
th
ed.),
Houston, TX: Mayfield Publishing Company.
Cronbach, L. J., and Meehl, P. E. 1955. “Construct Validity in
Psychological Tests,” Psychological Bulletin (52:4), pp. 281-302.
Crust, L., and Swann, C. 2010. “Comparing two Measures of
Mental Toughness,” Personality and Individual Differences
(50:2), pp. 217-221.
Davis, F. D. 1989. “Perceived Usefulness, Perceived Ease of Use,
and 
User 
Acceptance 
of 
Information 
Technology,” 
MIS
Quarterly (13:3), pp. 319-340.
Deerwester, 
S., 
Dumais, 
S., 
Furnas, 
G., 
Landauer, 
T., 
and
Harshman, R. 1990. “Indexing by Latent Semantic Analysis,”
Journal of the American Society for Information Science (41:6),
pp. 391-407.
Donaldson, T., and Dunfee, T. W. 1994. “Toward a Unified Con-
ception of Business Ethics: Integrative Social Contracts Theory,”
Academy of Management Review (19:2), pp. 252-284.
Duckworth, A. L., and Schulze, R. 2009. “Jingle Jangle: A Meta-
Analysis of Convergent Validity Evidence for Self-Control
Measures,” unpublished manuscript, Department of Psychology,
University of Pennsylvania, Philadelphia, PA.
Dumais, S. T., Furnas, G. W., Landauer, T. K., Deerwester, S., and
Harshman, R. 1988. “Using Latent Semantic Analysis to
Improve Access to Textual Information,” in Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems,
New York: ACM, pp. 281-285.
Easley, R. F., Devaraj, S., and Crant, J. M. 2003. “Relating
Collaborative 
Technology 
Use 
to 
Teamwork 
Quality 
and
Performance: An Empirical Analysis,” Journal of Management
Information Systems (19:4), pp. 247-265.
Evangelopoulos, N., Zhang, X., and Prybutok, V. R. 2010. “Latent
Semantic Analysis: Five Methodological Recommendations,”
European Journal of Information Systems (21:1), pp. 70-86.
Foltz, P. W. 2007. “Discourse Coherence and LSA,” in Handbook
of Latent Semantic Analysis, T. K. Landauer, D. S. McNamara,
S. Dennis, and W. Kintsh (eds.), Mahwah, NJ: Lawrence
Erlbaum Associates, pp. 167-184.
Gefen, D. 2000. “E-Commerce: The Role of Familiarity and
Trust,” OMEGA—The International Journal of Management
Science (28:6), pp. 725-737.
Glass, G. V., McGaw, B., and Smith, M. L. 1981. Meta-Analysis
in Social Research, Beverly Hills, CA: SAGE Publications.
Graesser, A., Penumastsa, P., Ventura, M., Cai, Z., and Hu, X. 
2007. “Using LSA in AutoTutor: Learning Through Mixed-
Initiative Dialogue in Natural Language,” in Handbook of Latent
Semantic Analysis, T. K. Landauer, D. S. McNamara, S. Dennis,
and 
W. 
Kintsh 
(eds.), 
Mahwah, 
NJ: 
Lawrence 
Erlbaum
Associates, pp. 243-262.
Gregor, S., and Hevner, A. R. 2013. “Positioning and Presenting
Design Science Research for Maximum Impact,” MIS Quarterly
(37:2), pp. 337-355.
Gregory, R. W., and Muntermann, J. 2014. “Heuristic Theorizing: 
Proactively Generating Design Theories,” Information Systems
Research (25:3), pp. 639-653.
Hatcher, E., Gospodnetic, O., and McCandless, M. 2004. “Lucene
in Action,” Greenwich, CT: Manning Publications.
Hedges, L. V., and Vevea, J. L. 1998. “Fixed-and Random-effects
Models in Meta-Analysis,” Psychological Methods (3:4), pp.
486-504.
Hevner, A., March, S., Park, J., and Ram, S. 2004. “Design Science
in Information Systems Research,” MIS Quarterly (28:1), pp.
75-105.
Hoffman, M., Bach, F. R., and Blei, D. M. 2010. “Online Learning
for Latent Dirichlet Allocation,” Advances in Neural Information
Processing Systems 23 (NIPS 2010), J. D. Lafferty, C. K. I.
Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.),
Vancouver, Canada, pp. 956-864.
Hsieh, J. J. P.-A., Rai, A., and Keil, M. 2008. “Understanding
Digital 
Inequality: 
Comparing 
Continued 
Use 
Behavioral
Models of the Socio-Economically Advantaged and Disad-
vantaged,” MIS Quarterly (32:1), pp. 97-126.
Hunter, J. E., and Schmidt, F. L. 2004. Methods of Meta-Analysis: 
Correcting Error and Bias in Research Findings, Thousand
Oaks, CA: SAGE Publications.
Iivari, J., Hirschheim, R., and Klein, H. K. 2004. “Towards a
Distinctive Body of Knowledge for Information Systems Experts: 
Coding ISD Process Knowledge in Two IS Journals,” Infor-
mation Systems Journal (14), pp. 313-342.
MIS Quarterly Vol. 40 No. 3/September 2016
549
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Kaplan, A. 1964. The Conduct of Inquiry: Methodology for
Behavioral Science, San Francisco: Chandler.
Karahanna, E., Straub, D. W., and Chervany, N. L. 1999. “Infor-
mation Technology Adoption Across Time: A Cross-Sectional
Comparison of Pre-Adoption and Post-Adoption Beliefs,” MIS
Quarterly (23:2), pp. 183-213.
Kelley, T. L. 1927. Interpretation of Educational Measurements,
Oxford, UK: World Book Company.
Kim, D. J., Ferrin, D. L., and Rao, H. R. 2009. “Trust and Satisfac-
tion, Two Stepping Stones for Successful E-Commerce Relation-
ships: 
A 
Longitudinal 
Exploration,” 
Information 
Systems
Research (20:2), pp. 237-257.
Kim, H. W., and Kankanhalli, A. 2009. “Investigating User
Resistance to Information Systems Implementation: A Status
Quo Bias Perspective,” MIS Quarterly (33:3), pp. 567-582.
Landauer, T. K. 2002. “On the Computational Basis of Cognition: 
Arguments from LSA,” in The Psychology of Learning and
Motivation, B. H. Ross (ed.), New York: Academic Press, pp.
43-84.
Landauer, T. K. 2007. “LSA as a Theory of Meaning,” in
Handbook of Latent Semantic Analysis, T. K. Landauer, D. S.
McNamara, S. Dennis, and W. Kintsch (eds.), Mahwah, NJ: 
Lawrence Erlbaum Associates, pp. 3-34.
Larsen, K. R. 2003. “A Taxonomy of Antecedents of Information
Systems 
Success: 
Variable 
Analysis 
Studies,” 
Journal 
of
Management Information Systems (20:2), pp. 169-246.
Larsen, K. R., and Bong, C. H. 2015. “Inter-Nomological Network
Search Engine,” Human Behavior Project: Boulder, Colorado.
Larsen, K. R., Hovorka, D. S., West, J., Birt, J., Pfaff, J. R.,
Chambers, T. W., Sampedro, Z. R., Zager, N., and Vanstone, B. 
2014. “Theory Identity: A Machine-Learning Approach,” in
Proceedings of the 47
th
Hawaii International Conference on
System Sciences, Los Alamitos, CA: IEEE Computer Society
Press.
Larsen, K. R., and Monarchi, D. E. 2004. “A Mathematical
Approach to Categorization and Labeling of Qualitative Data: 
The Latent Categorization Method,” Sociological Methodology
(34:1), pp. 349-392.
Larsen, K. R., Monarchi, D. E., Hovorka, D. S., and Bailey, C. 
2008. “Analyzing Unstructured Text Data: Using Latent Cate-
gorization to Identify Intellectual Communities in Information
Systems,” Decision Support Systems (18:1), pp. 23-43.
Larsen, K. R., Voronovich, Z. A., Cook, P. F., and Pedro, L. W. 
2013. “Addicted to Constructs: Science in Reverse?,” Addiction
(108:9), pp. 1532-1533.
Li, J., and Larsen, K. R. 2011. “Establishing Nomological Net-
works for Behavioral Science: A Natural Language Processing
Based Approach,” in Proceedings of the 32
nd
International
Conference on Information Systems, Shanghai, China.
Li, J., and Larsen, K. R. 2013. “Tracking Behavioral Construct Use
Through Citations: A Relation Extraction Approach,” in Pro-
ceedings of the 34
th
International Conference on Information
Systems, Milan, Italy.
Li, Y., McLean, D., Bandar, Z. A., O’Shea, J. D., and Crockett, K. 
2006. “Sentence Similarity Based on Semantic Nets and Corpus
Statistics,” 
IEEE 
Transactions 
on 
Knowledge 
and 
Data
Engineering (18:8), pp. 1138-1150.
Lim, K. H., and Benbasat, I. 2000. “The Effect of Multimedia on
Perceived Equivocality and Perceived Usefulness of Information
Systems,” MIS Quarterly (24:3), pp. 449-471.
Loevinger, J. 1957. “Objective Tests as Instruments of Psych-
ological Theory: Monograph Supplement 9,” Psychological
Reports (3:3), pp. 635-694.
Malhotra, N. K., Kim, S. S., and Agarwal, J. 2004. “Internet Users’
Information Privacy Concerns (IUIPC): The Construct, the
Scale, and a Causal Model,” Information Systems Research
(15:4), pp. 336-355.
Malhotra, Y., Galletta, D. F., and Kirsch, L. J. 2008. “How
Endogenous Motivations Influence User Intentions: Beyond the
Dichotomy of Extrinsic and Intrinsic User Motivations,” Journal
of Management Information Systems (25:1), pp. 267-300.
Marsh, H. W., Craven, R., Hinkley, J., and Debus, R. L. 2003. 
“Evaluation 
of 
the 
Big 
Two-Factor 
Theory 
of 
Motivation
Orientation: An Evaluation of Jingle-Jingle Fallacies,” Multi-
variate Behavioral Research (38:2), pp. 189-224.
Mihalcea, R., Corley, C., and Strapparava, C. 2006. “Corpus-Based
and Knowledge-Based Measures of Text Semantic Similarity,”
in The 21
st
National Conference 
on Artificial Intelligence,
Boston: MIT Press, pp. 775-780.
Miller, G. 1995. “WordNet: A Lexical Database for English,”
Communications of the ACM (38:11), pp. 39-41.
Mitchell, J., and Lapata, M. 2008. “Vector-Based Models of
Semantic Composition,” in Proceedings of the 46
th
Annual
Meeting 
of 
the 
Association 
for 
Computational 
Linguistics: 
Human Language Technologies, Columbus, Ohio, pp. 236-244.
Murphy, P. K., and Alexander, P. A. 2000. “A Motivated
Exploration 
of 
Motivation 
Terminology,” 
Contemporary
Educational Psychology (25:1), pp. 3-53.
Nelson, R. R. 1991. “Educational Needs as Perceived by IS and
End-User 
Personnel: 
A 
Survey 
of 
Knowledge 
and 
Skill
Requirements,” MIS Quarterly (15:4), pp. 503-525.
Noar, S. M., and Zimmerman, R. S. 2005. “Health Behavior
Theory and Cumulative Knowledge Regarding Health Behaviors: 
Are We Moving in the Right Direction,” Health Education
Research (20:3), pp. 275-290.
Ortiz de Guinea, A., Titah, R., and Leger, P.-M. 2013. “Measure
for Measure: A Two Study Multi-Trait Multi-Method Investi-
gation of Construct Validity in IS Research,” Computers in
Human Behavior (29:3), pp. 833-844.
Ortiz de Guinea, A., Titah, R., and Leger, P.-M. 2014. “Explicit
and 
Implicit 
Antecedents 
of 
Users’ 
Behavioral 
Beliefs 
in
Information Systems: A Neuropsyhological Investigation,”
Journal of Management Information Systems (30:4), pp. 179-209.
Ortiz de Guinea, A., and Webster, J. 2013. “An Investigation of
Information Systems Use Paterns: Technological Events as
Triggers, the Effect of Time, and Consequences for Perfor-
mance,” MIS Quarterly (37:4), pp. 1165-1188.
Peck, S. C. 2007. “TEMPEST in a Gallimaufry: Applying Multi-
level Systems Theory to Person-in-Context Research,” Journal
of Personality (75:6), pp. 1127-1156.
Poli, R., Healy, M., and Kameas, A. 2010. “WordNet,” in Theory
and Applications of Ontology: Computer Applications, C. Fell-
baum (ed.), New York: Springer, pp. 231-243.
550
MIS Quarterly Vol. 40 No. 3/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Pollard, B., Johnston, M., and Dixon, D. 2007. “Theoretical Frame-
work and Methodological Development of Common Subjective
Health Outcome Measures in Osteoarthritis: A Critical Review,”
Health and Quality of Life Outcomes (5:14), pp. 1-9.
Preacher, K. J., and MacCallum, R. C. 2003. “Repairing Tom
Swift’s 
Electric 
Factor 
Analysis 
Machine,” 
Understanding
Statistics (2:1), pp. 13-43.
Řehůřek, R., and Sojka, P. 2010. “Software Framework for Topic
Modelling with Large Corpora,” in LREC 2010: Workshop New
Challenges for NLP Frameworks, Valletta, Malta, pp. 46-50.
Rogers, E. M. 2003. Diffusion of Innovations (5
th
ed.), New York: 
Free Press
Salton, G., Wong, A., and Yang, C. 1975. “A Vector Space Model
for Automatic Indexing,” Communications of the ACM (18:11),
pp. 613-620.
Siderova, A., Evangelopoulos, N., Valacich, J. S., and Ramakrish-
nan, T. 2008. “Uncovering the Intellectual Core of the Infor-
mation Systems Discipline,” MIS Quarterly (32:3), pp. 467-482.
Simon, H. A. 1981. The Sciences of the Artificial, Cambridge, MA: 
The MIT Press.
Smith, H. J., Milberg, S. J., and Burke, S. J. 1996. “Information
Privacy: Measuring Individuals’ Concerns about Organizational
Practices,” MIS Quarterly (20:2), pp. 167-196.
Stewart, K. A., and Segars, A. H. 2002. “An Empirical Exam-
ination of the Concern for Information Privacy Instrument,”
Information Systems Research (13:1), pp. 36-49.
Stroup, D. F., Berlin, J. A., Morton, S. C., Olkin, I., Williamson, G.
D., Rennie, D., Moher, D., Becker, B. J., Sipe, T. A., and
Thacker, S. B. 2000. “Meta-Analysis of Observational Studies
in Epidemiology,” JAMA: The Journal of the American Medical
Association (283:15), pp. 2008-2012.
Swamidass, S. J., Azencott, C.-A., Daily, K., and Baldi, P. 2010. 
“A CROC Stronger than ROC: Measuring, Visualizing and Opti-
mizing Early Retrieval,” Bioinformatics (26:10), pp. 1348-1356.
Swanson, D. R. 1986. “Fish Oil, Raynaud’s Syndrome, and Undis-
covered 
Public 
Knowledge,” 
Perspectives 
in 
Biology 
and
Medicine (30:1), pp. 7-18.
Tellegen, A. 1982. “Brief Manual for the Multidimensional Person-
ality Questionnaire,” unpublished manuscript, University of
Minnesota, Minneapolis, 1031-1010.
Thorndike, E. 1904. An Introduction to the Theory of Mental and
Social Measurements, New York: Science Press.
Tirunillai, S., and Tellis, G. J. 2014. “Mining Marketing Meaning
from Online Chatter: Strategic Brand Analysis of Big Data
Using 
Latent 
Dirichlet 
Allocation,” 
Journal 
of 
Marketing
Research (51:4), pp. 463-479.
Van Rooy, D. L., Viswesvaran, C., and Pluta, P. 2005. “An Eval-
uation of Construct Validity: What Is this Thing Called Emo-
tional Intelligence?,” Human Performance (18:4), pp. 445-462.
Venkatesh, V., and Davis, F. D. 2000. “A Theoretical Extension of
the Technology Acceptance Model: Four Longitudinal Field
Studies,” Management Science (46:2), pp. 186-204.
Venkatesh, V., Morris, M. G., Davis, G. B., and Davis, F. D. 2003. 
“User Acceptance of Information Technology: Toward a Unified
View,” MIS Quarterly (27:3), pp. 425-478.
Whiteside, S. P., and Lynam, D. R. 2001. “The Five Factor Model
and Impulsivity: Using a Structural Model of Personality to
Understand Impulsivity,” Personality and Individual Differences
(30:4), pp. 669-689.
Wilhelm, O. 2009. Issues in Computerized Ability Measurement: 
Getting out of the Jingle and Jangle Jungle, European Commis-
sion, Joint Research Centre: Institute for the Protection and
Security of the Citizen.
Zmud, R. W., Sampson, J. P., Reardon, R. C., Lenz, J. G., and Byrd,
T. A. 1994. “Confounding Effects of Construct Overlap: An
Example 
from 
IS 
User 
Satisfaction 
Theory,” 
Information
Technology & People (7:2), pp. 29-45.
Zuckerman, M. 1994. Behavioral Expressions and Biosocial Bases
of Sensation Seeking, New York: Cambridge University Press.
Zuckerman, M. 2008. “Rose Is a Rose Is a Rose: Content and
Construct Validity,” Personality and Individual Differences
(45:1), pp. 110-112.
About the Authors
Kai R. Larsen is an associate professor of Information Systems at
the Leeds School of Business, University of Colorado at Boulder.
He holds a courtesy appointment in the Information Science
Department, College of Media, Communication and Information. 
As director of the Human Behavior Project, he is conducting
research to create a transdisciplinary “backbone” for theoretical
research. He applies text mining technologies to create an inte-
grating framework for predictors of human behavior. The research
has 
implications 
for 
our 
understanding 
of 
human 
behaviors,
including technology utilization, investor decisions, and cancer
prevention behaviors.
Chih How Bong is a senior lecturer of Computer Science at
Universiti Malaysia Sarawak. Chih How received his Ph.D. from
the University of Colorado Boulder in 2011. His research interests
include data mining, computational semantics, and intelligence in
education. His work has been published or is forthcoming in jour-
nals such as MIS Quarterly, Expert Systems with Applications, and
Software Quality Professional.
MIS Quarterly Vol. 40 No. 3/September 2016
551
552
MIS Quarterly Vol. 40 No. 3/September 2016
M
ETHODS 
A
RTICLE
A
T
OOL FOR 
A
DDRESSING 
C
ONSTRUCT 
I
DENTITY IN
L
ITERATURE 
R
EVIEWS AND 
M
ETA
-A
NALYSES
Kai R. Larsen
Leeds School of Business, University of Colorado, Boulder, 995 Regent Drive,
Boulder, CO 80309-0419 U.S.A. {kai.larsen@colorado.edu}
Chih How Bong
Faculty of Computer Science and Information Technology, Universiti Malaysia Sarawak,
94300 Kota Samarahan, Sarawak, MALAYSIA {chbong@unimas.my}
Appendix A
Natural Language Processing Designs
This appendix provides background details on the NLP designs used in our paper. For each, the steps necessary for implementation are
outlined.
Latent Semantic Analysis
The underlying idea of latent semantic analysis (LSA) (Deerwester et al. 1990) is that the aggregate of all of the contexts in which a given word
does and does not appear provides a set of constraints that determines the similarity of meanings for words, and sets of words, to each other
(Landauer et al. 1998). Thus, when two terms occur in contexts of similar meaning, even if they never occur in the same passage, LSA
represents them as having similar meanings. 
LSA Steps
Step 1. Preparing term-document matrix. For the LSA process, we employed minimal pre-processing. Starting with the paragraphs from
the MIS Quarterly and Information Systems Research articles that contained the sample of constructs, we removed non-alphanumeric characters
and converted all words to lower case; we then stemmed the words using the Porter (1980) algorithm and weighted the term-document matrix
using log-entropy and normalization.
Step 2. Creating semantic space. After weighting and normalization, the matrix A is decomposed using singular value decomposition (SVD),
a mathematical algorithm similar to a factor analysis, with the result being a semantic space containing term vectors, each infused with an
“understanding” of the term.
Step 3. Projecting items into the semantic space. Given the query q, which is a construct item, a query vector q
÷
is obtained through an
aggregation of term vectors relevant to the item. In our research, we project every item into the semantic space as a query vector q
÷
, and that
vector is saved as q
÷
n
for future item-item analysis, where n is the total number of items. This improves speed of the analytics and enables
separately stored solutions for search-engine purposes.
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A1
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Step 4. Finding similar items. To find similar items to q
÷
, the query vector is compared against all the items stored inside the semantic space,
q
÷
n
, using the cosine similarity measurement. We deem vectors in q
÷
n
that yield the highest cosine scores corresponding to q
÷
to be more
semantically relevant; we deem those with low cosine scores to be relatively irrelevant.
Semantic Space Selection
“The selection of dimensionality in LSA is fraught with problems” (Kakkonen et al. 2008, p. 280), and in fact, researchers have yet to find a
generalizable solution to dimensionality selection (see, e.g., Bingham and Mannila 2001; Globerson and Tishby 2003; Landauer et al. 1998). 
In IS, this issue was prominently featured in a very thorough LSA exposition by Siderova et al. (2008), where domain-specific 5-, 13-, and 100-
dimensional solutions were proposed and analyzed. While research has suggested that 300-dimensional solutions work well (Landauer and
Dumais 1997), that result was not based in theory and is not likely to generalize perfectly into the psychometric domain. The purpose of our
article is to show that natural language processing works to address the construct correspondence problem rather than to fully optimize such
designs. This work was simply conducted using a 300-dimensional solution. Our own examinations have shown that when used for construct
items, LSA semantic spaces between 200 and 500 are roughly equivalent, but higher dimensions do marginally better (although with associated
increases in processing time).
Latent Dirichlet Allocation
We translated all words in each item into k topical probabilities according to the posterior distribution. Hence, each item was depicted as a
k-dimensional vector representing its topic distribution. To compute the similarity of the items, we computed the cosine of the item vector. 
The following list of steps illustrates the process.
Step 1. Building the LDA model. We used the same paragraphs used for LSA to build the LDA model.
Step 2. Preprocessing. We preprocessed paragraphs by removing stop words,
1
punctuation, and non-alphabet characters.
Step 3. Computing topic distribution. An advantage of online LDA is its ability (1) to process the whole document collection in a single,
full pass; (2) to converge rapidly; and (3) to yield an accurate topic estimation. In this step, we defined k to present the number of topics. We
set k to 300 after examination of values from 50 to 500 in increments of 50. That examination follows in the next section.
Step 4. Building an item subspace. Once the LDA model was ready, we projected each item into the model and transformed it into the k-topic
distribution format. Similar to LSA, this represented the model subspace for all of the items.
Step 5. Finding similar items. It is our assumption that two items are semantically similar if they share identical topic distribution. Thus,
we defined the similarity as the angle cosine of the two item vectors in the subspace.
Topical Probabilities
We evaluated 10 LDA designs using different numbers of topical probabilities, where k = (200, 250, 300, 350, 400, 450, 500). Improvements
are apparent when increasing k from 50 to 300, with the greatest improvements seen in the change from 150 to 200. However, increasing k
beyond 300 yielded only insignificant improvements along with longer convergence time. A close examination revealed that the k values
around 300 were able to transform marginal false negatives to true positives and eliminate a high number of false positives. LDA should work
well theoretically for the problem at hand, but the nature of the texts used to measure a construct (the construct measurement items) is quite
different from the majority of paragraphs in an academic paper. In addition, we are aware of no other research that uses LDA in quite this way. 
Unlike LSA, the representation of items in LDA is not able to take into account synonymy effectively. While LDA may be better fit for
detecting latent topics, our experiments show that LDA did not perform well for establishing similarity. One of the reasons for this discrepancy
is the representation of topic distribution, as the short vectors may diminish its performance during calculation of similarity. For example, LDA
similarity for “task” and “job” yielded 0.00, whereas in LSA, it yielded over 0.9.
1
We used the University of Glasgow stop word list (http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words). A pdf copy of this list is available from the
authors for archival purposes.
A2
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Knowledge-Based Approaches: LI 
The semantic similarity of item pairs is computed by taking the path length and the depth of two words in a hierarchical semantic knowledge
base, i.e., WordNet (Miller 1995; Poli et al. 2010). WordNet is a hierarchical lexical reference system consisting of “English nouns, verbs,
and adjectives…organized into synonym sets, each representing one underlying lexical concept” (Miller et al. 1993, p. 235). Later work added
adjectives to WordNet. LI relies on WordNet to measure word similarity because it is one of the richest and most accurate lexical dictionaries
ever crafted, correlates well with human judges (Miller 1995), is readily available, and does not adhere to a specific domain.
Word-order similarity is factored in, turning each sentence into a vector by ordering the words as they appear and computing the difference
of the word orders. Finally, in order to separate the informative words from those that are not, information content of each word is normalized
onto each similarity score from the Brown University Standard Corpus of Present-Day American English (Marcus et al. 1993).
Knowledge-Based Approaches: MI 
Building on WordNet, MI’s (Mihalcea et al. 2006) word specificity refers to the specific meaning of words (e.g., collie and sheepdog) versus
generic concept words (e.g., animal and mammal). The similarity measure gives higher weight to precise, specific-meaning words than to
abstract, generic-concept words. Word specificity is computed using the inverse document frequency (IDF) algorithm (Jones 1986), which
was applied to the British National Corpus. IDF assumes that rare words have greater discriminatory weight than common words. Additionally,
IDF similarity scores are normalized to reflect word specificity in the sentence. MI averages word-similarity metrics from six different sources: 
Jiang and Conrath (1997), Leacock and Chodorow (1998), Lesk (1986), Lin (1998), Resnik (1995), and Wu and Palmer (1994). These metrics
were created to measure word relatedness and similarity by calculating the shortest distance between given words’ synsets (sets of synonymous
words) in the WordNet hierarchy; the shorter the distance between words, the higher the similarity score.
MI was designed to measure short text similarity and has been used at the sentence level by Mihalcea et al. (2006). Like LI, the MI similarity
score is a number between 0 and 1, where 0 indicates no semantic overlap and 1 indicates exact match. The MI sentence similarity measure
is computed for two candidate sentences, S
1
and S
2 
as follows:
Step 1. Identifying part-of-speech (POS). The process begins with tokenization and part-of-speech tagging of all the words in the sentence
into their respective word classes (noun, verb, adverb, and adjective, as well as cardinal numbers).
Step 2. Calculating word similarity. Each word in the sentence is evaluated against all the words from the other sentence to find the highest
semantic similarity (maxSim) from the six word similarity metrics: Jiang and Conrath (1997), Leacock and Chodorow (1998), Lesk (1986),
Lin (1998), Resnik (1995), and Wu and Palmer (1994). These metrics were originally created to measure concept likeness rather than word
likeness, but they are adapted in our study to compute word similarity, by computing the shortest distance between given words’ synsets in
the WordNet hierarchy. The word–word similarity is computed only on the words from the same word class, which are either from noun or
verb word classes, because WordNet contains separate semantic trees for nouns and verbs; it is thus not possible to obtain similarity between
nouns and verbs using WordNet distance. For other word classes such as adverb, adjective, cardinal, and unknown words, whole word matching
is used instead. The word-word similarity measure is directional. It begins with each word in S
1
being computed against each word in S
2 
and
then vice versa.
Step 3. Calculating sentence similarity. Once the highest semantic similarity (maxSim) for each word in the sentences is computed, it is
normalized with inverse document frequency (IDF) to weight rare and common terms. The normalized scores are then totaled for a sentence
similarity score, Sim
MI
, as follows:
(1)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
Sim
S
S
w S
IDF
IDF
w S
IDF
IDF
MI
w S
w S
w S
w S
1
2
2
1
2
1
1
,
,
,
=
×
×
+
×
∈
∈
∈
∈
maxSim
w
w
maxSim
w
w
1
2
2
where maxSim(w, S
2
) is the score of the most similar word in S
2
to w and IDF (w) is the inverse document frequency of word w.
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A3
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
The CID Hybrid Approaches: The CID
1
Model
The process steps for calculating CID
1
similarities of two given items are as follows:
Step 1. Forming both items into a joint text. The comparing item and the joint item are formed into a matrix to allow each word from
different items to be compared. Table A1 shows that process for the items, “RAM keeps things” and “the CPU uses RAM.” The word-word
similarity here is computed using LSA cosine similarity scores. 
Step 2. Identifying the maximum similarity score. For each column (the columns represent the element of the lexical word vector), the
maximum similarity score is identified, which, in turn, is normalized using information content from the Brown Corpus. 
Step 3. Forming a lexical word vector for the first item. Only the similarity scores above the preset threshold of 0.2 (developed for LI) are
selected to form a lexical word vector. Scores below the threshold are set to zero, based on empirical findings that such words are too dissimilar
to score (Li et al. 2006). In this study, we did not test whether a different threshold from LI’s 0.2 would work better for CID
1
. Extensive
differences between the LI and LSA similarity measures suggest that testing alternative thresholds might improve the CID
1
algorithm, but we
did not expect it to change the outcome of the evaluations. Thus the lexical word vector for the first item is 
÷
S
1
= [0.39, 0.33, 0.179, 0.00,
0.074, 0.23].
Step 4. Forming a lexical word vector for the second item. The step is identical to step 3, but using Table A2. This yields 
÷
S
2
= [0.19,
0.00, 0.16, 0.03, 0.389, 0.04].
Step 5: Deriving the word-order vectors using joint word. The word-order vectors are also derived using joint word, such that words in
the vector are assigned with unique indices according to the order in which they appear in candidate items. For example, the word-order vector
for the joint words “RAM keeps things The CPU uses ” is [1,2,3,4,5,6].
To derive the word-order vector for “RAM keeps things,” we assign the word-order index to the corresponding words in the candidate item. 
The first word “RAM” has an index of 1, and “keeps” has 2, etc. However, since the item does not have the words “The,” “CPU,” and “uses,”
the index will be the most similar word from the index that is computed in Step 2. For example, according to Table D1, “The” is similar to
no other word and thus it has an index value of 0, whereas “CPU” and “uses” are most similar to “things,” which has an index of 3. Thus the
word-order vector for the first item is 
. The same process applied to the second item yields, 
. The
[
]
O
1
1 2 3 0 3 3
=
,
,
,
,
,
[
]
O
2
1 0 3 4 5 6
=
,
,
,
,
,
minimum similarity threshold also applies here. For any word that has similarity less than the preset threshold, the value is zero. Likewise,
the other comparing item is also derived using the same approach.
Step 6. Computing the similarity of word-order information to distinguish the meaning of two items attributed to syntactic differences. 
The word-order similarity score is then computed as follows:
(2)
(
)
Sim
S
S
order
O
O
O
O
1
2
1
1
2
1
2
,
||
||
|
||
=
−
−
+
Step 7. Obtaining items’ similarity score. Similarity scores are obtained by calculating the cosine angle of the lexical word vectors,
(3)
(
)
Sim
S
S
semantic
S
S
S
S
1
2
1
2
1
2
,
,
||
||,||
||
=
Step 8. Calculating overall sentence similarity score. Finally, the overall item similarity is defined as the combination of lexical semantic
similarity and word-order similarity, which are operationalized in in the following: 
(4)
(
)
(
)
(
)
Sim
S
S
Sim
S
S
Sim
Li
semantic
order
1
2
1
2
1
,
,
=
+
−
γ
γ
where γ is a relative contribution of semantic similarity and word-order similarity to the overall similarity. According to Li et al, γ is a value
greater than 0.5. 
A4
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table A1. Deriving Semantic Vector for Item 1 (S
1
)
S
1
RAM
keeps
things
the
CPU
uses
RAM
1
0.05
0
0
0
0
keeps
0.15
1
0
0
0
0
things
0.33
0.39
1
0
0.34
0.83
Sim
1
1
1
0
0.34
0.83
Weight
I(RAM)
I(RAM)
I(keeps)
I(keeps)
I(things)
I(things)
I(CPU)
I(things)
I(uses)
I(things)
÷
S
1
0.390
0.330
0.179
0
0.074
0.23
Table A2. Deriving Semantic Vector for Item 2 (S
2
)
S
1
RAM
keeps
things
the
CPU
uses
The
0
0
0
1
0
0
CPU
0
0
0
0
1
0
uses
0
0
0.25
0
0
1
RAM
1
0
0
0
0.33
0
Sim
1
0
0.25
1
1
1
Weight
I(RAM)
I(RAM)
I(things)
I(uses)
I(The)
I(The)
I(CPU)
I(RAM)
I(uses)
I(uses)
÷
S
2
0.190
0
0.16
0.03
0.389
0.04
The CID Hybrid Approaches: The CID
2
Model
While based on the MI sentence-similarity algorithm, CID
2
does not perform part-of-speech (POS) tagging, and word similarity is directly
calculated using LSA. Similarity scores are then normalized with inverse document frequency (IDF) to reflect rare and common terms
(Mihalcea et al. 2006). The following steps illustrate the sentence-similarity score process developed for CID
2
.
Step 1. Computing word similarity using LSA. The process begins directly with word-word similarity. Given two words, the cosine
similarity is calculated based on the LSA semantic space.
Step 2. Computing sentence similarity. Once the word-similarity scores are obtained for each sentence, they are normalized with
correspondent IDF, and the sentence-similarity scores are computed by replacing the original equation with the LSA similarity measure, which
yields the following equation for CID
2
:
(5)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
CID S
S
Sim LSA w S
IDF w
IDF w
Sim LSA w S
IDF w
IDF w
w S
w S
w S
w S
2
1
2
1
2
2
1
1
1
2
2
,
_
,
_
,
=
×
×
+
×
∈
∈
∈
∈
While we had expected CID
2
to outperform LSA, careful consideration of the evidence suggests a good reason for CID
2
’s performance. We
examined a number of variables using two separate linear regressions on the relative ranking of item pairs for CID
2
versus MI, split on whether
the item pairs were correctly or incorrectly classified. In both analyses, the max(IDF) for each item pair emerged as the most important negative
influence on CID
2
design performance. In a separate experiment, we removed the IDF weighting from CID
2
and compared its performance
against all designs on a sample set of item pairs, which resulted in CID
2
moving up from the bottom to the third best performer (just behind
CID
1
and LSA, respectively). Detailed examination also indicated that the weighting of IDF in CID
2 
did not scale well with LSA (between
common and informative words) because the geometrical scaling of word occurrences is less meaningful in LSA; the virtue of its similarity
scores is maneuvered through algebraic operations in the semantic space. CID
2
was not included in the final CID tool in lieu of LI, because
LI serves the function of providing a WordNet knowledge-based design for the final tool, which is important for future contexts of use.
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A5
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Appendix B
Overview of Gold Standard
Data collection was performed by a team assembled for a larger project funded by the National Science Foundation. The team consisted of
advanced undergraduate and master’s level graduate students at a large, research university in the western United States. Students were selected
for interviews based on previous research experience, GPA, academic awards, and interest in research. Selected applicants received a two-hour
training session. This training was followed by an hour-long construct-extraction task consisting of the extraction of all variables, definitions,
and items from an academic article. Student performance on the task was then graded against a gold standard of previously extracted variables,
definitions, and items, and validated by our Chief Senior Research Assistant. The top scorers were hired as research assistants (RAs), resulting
in the top 3 percent of applicants accepting positions. Hired RAs received an additional 10 hours of training along with careful monitoring
of their first 100 hours of extraction. After 200 to 300 hours of extraction experience, the best 20 to 30 percent of RAs were promoted to senior
RAs. All articles were first extracted by a junior or senior RA and then audited by a senior RA.
Categorization Process
Categorization was carried out by a research team, which consisted of one experienced faculty researcher, three doctoral students, and four
experienced research assistants (RAs with at least 500 hours of experience in construct extraction).
Overall, the categorization task was divided into two stages: rough categorization and refined categorization. Rough categorization involved
grouping constructs with highly similar properties into the same cluster to make the task cognitively tractable. The refined categorization
process resulted in hierarchies, sub-hierarchies, and categories of the groups generated by rough categorization. The refined categorization
process began by labeling the hierarchical clusters. The taxonomy in this study was restricted to two hierarchical levels. For example, the main
category, “Trust,” had the subcategories, “Trust in Benevolence,” “Credibility,” and “General Trust.” The hierarchical structure was used solely
to reduce the cognitive load on the participants. Ultimately, the only important consideration was whether a pair of constructs should be placed
in the same category (correspondence) or different categories (independence).
Once each construct was assigned to one of the categories, annotator teams took turns examining all categories to ensure that constructs had
similar contexts and fit the definition of correspondence. Some clusters—usually those with a high number of constructs—required several
examinations by different teams. Each category was examined by at least two teams, with the team containing the faculty member always doing
the final examination. If any team did not fully agree with the previous team, that team would reorder and move constructs, and a third team
would reexamine the category. Constructs were assigned only to one category. Both the rough and refined categorization tasks were carried
out in stages across six days, and they consumed about 200 person-hours.
Evaluation
Finally, in recognition of the complexity of constructing a taxonomy for such an intricate dataset, two researchers, having a combined 15+ years
of research experience and no exposure to the taxonomy process, were chosen to evaluate the dataset. The experts were given a semi-
randomized set of 300 construct pairs, where each pair had a 0.5 probability of being either correspondent or independent. The inter-rater
agreements of correspondence versus independence between the two experts and the taxonomy were at 85 percent and 90 percent. The resulting
Cohen’s (1960) kappas were 0.68 and 0.79, indicating agreement levels that are considered “substantial” and close to “almost perfect” by Landis
and Koch (1977).
The Taxonomy
We share this taxonomy with the caveat that it represents only a partial view of constructs in the two journals MIS Quarterly and Information
Systems Research. It consists of constructs reported with a minimum of three items during the period from 1983 to 2009. The taxonomy is
irrelevant to the construct-identity-detection designs in this article, as the designs operated at the lowest level of the taxonomy and did not
consider the hierarchical structure of the taxonomy. However, the hierarchical structure was invaluable in reducing the cognitive strain of the
categorization exercise reported in the evaluation section.
A6
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
In the case of the present taxonomy, a bottom-up process was employed, which tends to guide the taxonomy toward the larger topics. In other
words, initial groups such as adoption and development will tend to grow faster and attract constructs earlier, before other established groups
have a chance to spawn on their own. These originally smaller groups will become even smaller hierarchies as their remaining constructs are
organized. For example, constructs related to top management involvement, which were categorized into the IS development hierarchy, may
have ended up in the leadership hierarchy in other categorization processes where such seemingly related constructs as leader/subordinate
relationships exists. Further, the lean hierarchy named academic could have attracted more constructs under other circumstances as well as
a larger hierarchical structure if a classification exercise had been undertaken with a preexisting top-level structure. Nevertheless, any topic
of reasonable size is likely to be represented as a hierarchy, and the hierarchies and their construct categories represent a potentially useful step
toward the development of an ontology in which such construct categories would need to be recreated with links to related construct categories. 
With these caveats, we describe here the taxonomy of IS constructs created for this project.
By size, the hierarchies of the taxonomy are IT adoption (412 constructs), IS development (102), trust (63), information/data (48), task/job (45),
interorganizational (43), IS function (43), communications (40), organizational (39), learning (23), purchase (22), group (19), knowledge (17),
judgment and decision making (13), leadership (10), strategy (10), general psychology (7), ethics/morals (6), and academics (4).
That the IS field is focused on IS adoption (41%)—perhaps unsurprisingly to anyone involved in IS research—is among the most immediate
findings. Within the adoption hierarchy, the largest sub-hierarchy by far is the one we termed technology factors, constructs that focus on user
perceptions about technology features such as ease of use and usefulness. The second largest sub-hierarchy, which could have fit in another
taxonomy under technology factors, was named affective factors, which focuses on user affect and attitude, including immersion in a tech-
nology. The third largest, use factors, focuses on constructs related to amount and extent of use, intended or self-reported. The last sub-
hierarchies, social/external factors and efficacy factors, round out the hierarchy with a focus on facilitating conditions and technology self-
efficacy.
The second largest hierarchy, IS development (10.2%), focuses on the process of developing information systems, primarily parti-
cipation/support, which is about (1) the perceived level of user participation during the process and (2) (top-)management involvement in the
same. Process methodology includes methodology, along with risk factors such as requirement-focused constructs. Less obvious is the focus
on trust (6.3%), perhaps suggesting a level of uncertainty or even insecurity about the interface between technology and users, as well as
between technology users and other individuals/organizations mediated through technology. The information/data (4.7%) hierarchy speaks
to a focus on the data and their transformation process, with quality and understanding representing key foci. Other hierarchies of interest
included communication, privacy, learning, and purchase. Smaller hierarchies are not discussed in this analysis.
What stands out in this gold standard is the great variety in level of analysis within the construct set (Burton-Jones and Gallivan 2007). The
majority of research is plainly at the individual level (IS adoption being a typical example), but we found research directed at the task/job
(4.5%), group (1.9%), IS function (4.3%), organization (3.9%), and interorganizational (4.3%) levels of analysis. In Table B1, we share the
whole set of hierarchies. Readers are warned against the spurious belief that construct items and definitions of constructs are predictable based
on construct, category, sub-hierarchy, and hierarchy names alone.
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A7
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy
Hierarchy
Sub-hierarchy
Category
#
Constructs
Academic
Masculinity/femininity values
1
Revising activities 
1
Writing importance for tenure/promotion
2
Communications
Amount of communications
Individual Information quality
2
Channel characteristics
Communication channel characteristics
4
Virtual co-presence 
1
Communication knowledge 
Knowledge sharing to/with groups outside of the organization
6
Knowledge sharing obligation
1
Loss of knowledge power
1
Communications psychology
Conflict 
4
Self-worth from knowledge sharing 
1
Communications
relationship
Anticipated reciprocal relationships through knowledge sharing
1
Expectation of reciprocity in knowledge sharing
1
Shared understanding of role within organization
2
Strength of ties
1
Communication
satisfaction/rewards
Economic incentive
1
Communications quality
Communication quality
7
Interaction quality
1
Quality of communication interface
1
Etc.
Communication ease
1
Communication seeking and reception 
1
Experimental similarity 
1
Governmental contention 
1
Vertical coordination 
1
Ethics/morals
Ethical behavior
1
Internet ecology
1
Moral intention
3
Predisposition to justice
1
General psychology
Negative
Negative affectivity
2
Trait anxiety
1
Neutral
Effort to change mental model
1
Long-term orientation
1
Positive
Need for cognition
1
Self-monitoring
1
Group
Control culture
1
Dynamic capabilities
1
Employee versus job orientation culture
2
Group cohesion
7
Inclusivity culture
1
Innovative culture
2
Normative versus pragmatic work norms
1
Perceived outcome quality
1
Relational capital
1
Satisfaction with group
1
Team member accountability
1
A8
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy (Continued)
Hierarchy
Sub-hierarchy
Category
#
Constructs
Information/Data
Accessibility 
1
Argument quality persuasiveness
2
Compatibility 
4
Completeness of information and breadth of information exchange 
4
Currency and availability of information 
2
Data collection 
1
Information quality 
9
Information quality satisfactory 
3
Information quality specific yet broad satisfaction 
3
Information quality importance 
2
Information reliability 
4
Information reliability importance
1
Information that meets needs
2
Information understanding
5
Information usefulness 
1
Locatability 
2
Understandability and reliability 
2
Interorganizational
Extent/strength of
collaboration/relationship
Collective sanctions
1
Cooperative norms expectations to work together
1
Degree of IS outsourcing 
2
Interorganizational process integration 
6
Mutual adoption 
1
Supplier responsiveness to customer needs 
2
Market factors
Asset specificity 
3
Contracting flexibility 
1
Coordination costs between firm & vendor
2
Cost/benefit of external service provider 
1
Exchange safeguards 
1
Interorganizational relationship standards 
1
Interorganizational relationship efficiency 
1
Network effectiveness 
1
Supplier choice 
1
Transaction cost
1
Psychological factors
Motivation to comply in general 
1
Persistent expectation of subordinance due to previous contract
1
Relationship satisfaction 
1
Success
Effectiveness of product development work unit 
1
Relationship performance benefits from interaction 
2
Task/process factors
Interorganizational process modularity 
1
Trading partner readiness 
1
Etc.
Customer obligation 
1
Interorganizational pressures to adopt technology 
2
Obligation for accurate project scoping 
1
Obligation for building effective team 
1
Obligation for clear authority structure 
1
Obligation for taking charge
1
Relationship management skills
1
Restricted network access for strategic purposes 
1
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A9
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy (Continued)
Hierarchy
Sub-hierarchy
Category
#
Constructs
IS development 
Participation/support 
Perceived user participation
10
Breadth of involvement in planning
1
Client formal control over vendor 
1
Desire to participate in process/group 
1
Functional capability of the application service provider 
1
Management involvement in planning 
7
Obligation to ensure top management support 
1
Organizational support 
1
Perceived provider performance 
1
Planning cooperation 
1
Technical service guarantees 
1
Top management involvement - quality policy and goals 
1
Top management technology attitudes 
3
Process methodology
Amount of SDM use
2
Barriers to implementing the methodology 
1
Frequency of use of coordination technology 
1
Methodology provides an effective planning process SDM 
5
Methodology provides a high-quality end product SDM
4
Process standardization 
3
Quality of interactions 
1
Rewards for reuse
1
Software development methodology type
2
Risk factors
Performance estimation risk process 
3
Requirements instability 
2
Requirements unanalyzability 
2
Requirements uncertainty 
1
Requirement understanding 
1
Risk factors due to task size and team member’s experience 
1
Risk perception 
1
Technology uncertainty 
1
Etc.
Absorptive capacity 
2
Code reusability 
2
Competitive advantage of development team 
1
Group coordination 
1
IS project success and performance 
11
Obligation to communicate clearly 
1
Open source software beliefs 
1
Open source software values 
1
Planning analysis 
1
Product/service portfolio differentiation/competitiveness in the
market 
1
Project resource availability 
4
Staff skills team and leadership - knowledge & expertise 
4
System outcome 
4
Technology: customized versus canned/generic 
1
Technology evaluation competence 
1
User-IS function relationship 
4
A10
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy (Continued)
Hierarchy
Sub-hierarchy
Category
#
Constructs
IT adoption
Affective factors
Affect towards technology use 
28
Attitude towards technology usefulness 
7
Computer alienation 
5
Computer playfulness 
4
Focused immersion 
6
Importance of entertainment 
1
Personal Innovativeness
4
Result demonstrability 
4
Technology escapism 
1
Technology invasion 
1
Technology loyalty 
2
Technology overload 
1
Technology visibility 
2
Efficacy factors 
Computer anxiety 
4
Effort requirement 
2
Knowledge self-efficacy 
1
Lack of adequate skills meaninglessness 
1
Perceived control 
4
Technology-complexity inadequacy 
2
Technology knowledge 
3
Technology self-efficacy 
12
Social/ external factors
Absorptive Capacity
1
Facilitating conditions 
23
Image 
7
Mimetic pressures 
1
Normative pressure 
3
Readiness of suppliers to do e-business 
1
Social influence 
10
Technology factors
Cognitive challenge of technology 
1
Ease of use
46
E-shopping versus in-store shopping similarity 
1
Expectations from use of technology 
5
Flexibility, Organization
2
IT support for contextualization 
1
IT performance 
2
Perceived monetary value 
1
Satisfaction with technology 
15
Social presence of technology 
2
System reliability 
3
Technology availability/accessibility 
2
Technology compatibility with prior experience 
2
Technology diagnosticity 
5
Technology encourages innovation 
2
Flexibility, Individual
4
Technology functionality perceived 
7
Technology speed 
4
Technology quality 
1
Technology typicality 
1
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A11
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy (Continued)
Hierarchy
Sub-hierarchy
Category
#
Constructs
IT adoption
(continued)
Technology factors
(continued)
Trialability 
3
Usefulness to individual 
67
Usefulness to organization 
12
Visual appeal 
5
Use factors
Attitude toward IT
1
Consensus on appropriation 
1
Faithfulness of appropriation 
2
Habit 
1
Individual use
1
Intention to use 
29
IT use for specific purposes 
7
Resistance to use 
1
Search scope 
2
Self-reported use 
5
Switching costs 
3
Voluntariness 
2
Etc.
Convenience and confidence 
2
Importance of ease of use 
1
Importance of technology quality 
1
Organizational adoption enabling factors 
2
Technology compatibility with work style & morals/values
10
Technology confirmation 
5
Time resources/constraints 
1
IS function 
Adequacy, quality, and amount of support 
1
Complexity evaluation of decision/process
1
Departmental shared knowledge 
1
Features that determine service quality 
2
Importance of IS department attributes 
3
Importance of IT use in meeting organizational objectives 
1
IS strategic alignment 
2
IT strategy process uniqueness 
1
Mutual influence 
1
Organizational IT capability 
7
Organizational systems integration 
2
Process alignment 
2
Product design maturity 
1
Service quality: Empathy 
5
Service quality: General 
3
Service quality: Reliability 
4
Service quality: Responsiveness 
4
Technology standards 
1
Technology uncertainty 
1
Judgement and decision making
Satisfaction with brainstorming outcome 
3
Satisfaction with decision making outcome 
8
Team performance 
2
A12
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy (Continued)
Hierarchy
Sub-hierarchy
Category
#
Constructs
Knowledge
Diversity of knowledge 
1
Importance of IS knowledge/skills in business 
2
Importance of IS knowledge in technology 
4
Interorganizational knowledge creation 
2
IS/IT management knowledge of business practices
4
Knowledge of access to knowledge 
1
Knowledge of team’s abilities 
1
Top management IT/IS knowledge 
2
Leadership
Importance of leadership skills 
2
Leader effectiveness 
1
Leader/subordinate relationship 
1
Organizational skills 
1
Personal responsibility for leadership 
3
Power 
1
Presence of leadership skills 
1
Learning 
Attitude toward knowledge sharing 
1
Course enjoyment 
1
Group work contributes to learning
2
Learning frequency 
1
Learning orientation 
1
Motivation to learn
2
Observational learning process 
3
Organizational learning
1
Perceived learning 
6
Relative advantage 
1
Self-efficacy for doing school work 
1
Self-regulation during learning 
1
Skill development 
1
Subjective norm toward knowledge sharing 
1
Organizational 
Financials 
Business performance 
3
Task non-contractability quality 
2
Money
Market orientation 
1
Organization-level
characteristics 
Business-process specificity: Human capital 
2
Business-process specificity: Intellectual capital 
3
Formalization 
2
People
Impact of system on organization 
1
Interactions between CIO and TMT
2
IT hiring and retainment practices
1
Perceptions of organization
Ambiguity 
1
Benefits uncertainty 
1
Fairness of labor division 
1
Information need fulfillment 
1
Norm of cooperation 
1
Organizational commitment 
4
Organizational uncertainty: Market turbulence 
4
Organizational uncertainty: Technological uncertainty 
1
Organizational uncertainty: Production volume 
1
Perceived competitive advantage 
4
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A13
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy (Continued)
Hierarchy
Sub-hierarchy
Category
#
Constructs
Organizational
(continued)
Perceptions of organization
(continued)
Perceived strategic risks
1
Perceived supportive affect within work groups 
1
Work environment satisfaction 
1
Privacy
Benefit overriding privacy concerns 
2
Concern over unauthorized access
3
Concern over unauthorized secondary use 
2
Errors in private data/info
3
General concerns about information privacy 
16
Importance of transparency info use policies 
2
Perceived protection by companies 
2
Personal control of information 
2
Risk of loss
2
Technology security risk
2
User awareness of security policies
3
Purchase
Buyer’s intention to purchase
2
Consumer’s opinions of traveling to make a purchase 
1
Importance of shopping convenience 
1
Internet customer relation 
2
Monetary resources 
1
Need for uniqueness from purchasing 
1
Perceived product choice on the internet 
1
Product value 
2
Purchasing skills 
1
Satisfaction with purchasing experience 
6
Seller’s past performances 
3
Societal benefit from complaining 
1
Strategy
Business-process strategic criticality 
2
Locus of authority for strategic planning 
1
Strategic aggressiveness 
1
Strategic defensiveness 
1
Strategic intent 
1
Strategic investment rationale 
1
Strategy comprehensiveness 
1
Strategy proactiveness 
1
Strategy: Risk aversion 
1
Task/job
Employment
Job turnover intention 
3
Job specifics
Fairness 
2
Improved job performance due to autonomy 
2
Job overload 
9
Outcomes/Performances/
Rewards
Benefit of efforts 
1
Determination of rewards 
1
Satisfaction
Individual rewards present 
1
Job satisfaction 
2
Rewards 
2
Task characteristics
Business process modularity 
1
Job autonomy 
3
Role ambiguity 
2
Role/task conflicts
3
A14
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Table B1. The Gold Standard Taxonomy (Continued)
Hierarchy
Sub-hierarchy
Category
#
Constructs
Task/job
(continued)
Task characteristics
(continued)
Task difficulty 
1
Task interdependence 
1
Task uncertainty 
1
Task variety 
4
Uncertainty with information
1
Etc.
Emotion toward customer 
2
Work-family conflict 
3
Trust 
Credibility Trust in ability and benevolence 
9
Individual-level trust: Other’s ability 
1
Individual-level trust: Other’s ability and trustworthiness 
3
Individual-level trust: Other’s Benevolence
1
Individual-level trust: Structural assurances 
5
Individual-level trust: Trust propensity 
9
Individual-level trust: Trustworthiness 
19
Organization-level trust: Other’s ability 
2
Organization-level trust: Other’s benevolence 
2
Organization-level trust: Other’s integrity 
2
Organization-level trust: Reputation 
2
Trust in others’ ability 
8
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A15
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Appendix C
Item to Construct Solutions
This appendix begins by examining approaches to the transformation of similarity scores from the item level to the construct level. It then
discusses how such transformations may affect the analysis of formative versus reflective items.
Generalized Functions for Transforming Similarity Scores
from the Item Level to the Construct Level
Regardless of the designs used, construct relationships can be predicted by comparing two items, where each construct item is treated as natural
text and a similarity score is produced to indicate their degree of similarity. Item similarities are computed for all inter-construct item pairs,
and the item scores are transformed to indicate a construct relationship. Two functions are formulated to subsume item similarities into the
construct relationship. While future work should examine all possibilities, the first measure (sec) tested for this article used the second highest
score among all inter-construct item relationships. This function is based on the notion that the two constructs’ highest inter-item relationship
can be misleading, while the use of the second highest relationship score results in a compromise between the rate of false positives and false
negatives. The second function used to represent the construct similarity (avg) takes the average of the similarity of the most similar item pair
scores. This function tends to compensate for item pairs that are constituted of both highly similar and very dissimilar constituent parts. 
Although we examined both measures, we anticipated that avg could better represent the construct relationship without the skewedness
demonstrated by sec. 
An analysis evaluating sec versus avg for LSA showed that both generalized functions were above the diagonal line (> 0.5), indicating that
both functions sec (AUC = 0.759) and avg (AUC = 0.767) used after LSA performed better than chance. CROC tests, with α < 0.01 against
both generalized LSA functions, showed that both functions performed significantly better than chance and that the avg function significantly
outperformed the sec function. This pattern was confirmed for tests on all algorithms. We show further evaluations only with the avg function
results.
Reflective Versus Formative Constructs
The majority of constructs that we included in this study were reflective constructs such as ease of use, where the items are assumed to reflect
an underlying latent unobservable phenomenon (Petter et al. 2007). Whether examining reflective constructs, composites of multiple measures
(Petter et al. 2007), or formative constructs, different construct types may be measured using identical items. In our research, we came across
several examples of this, including the measurement of privacy concerns (defined as “subjects’ concerns with their privacy over the Internet”)
(Hui et al. 2007) versus improper access (“consumers’ opinions about improper access in organizational practices”) (Malhotra et al. 2004). 
Both of these constructs are measured by multiple items, including the shared item, “Computer databases that contain personal information
should be protected from unauthorized access—no matter how much it costs.” 
Figure C1 illustrates how CID operates to overcome some of these problems. In the figure, reflective constructs A and B (for example, two
different ease of use constructs) have three items each. After evaluating all nine potential inter-item relationships for the two constructs, CID
detects and links the three highest-similarity item pairs, starting with the most similar pairs. CID then uses the average similarity score as an
evaluator of the construct relationship. CID conducts the same evaluations between all four constructs, resulting in the detection of high-
similarity item pairs between two of the three items for formative constructs X and Y (for example, user information satisfaction [UIS], which
often contains an ease of use item and otherwise taps into a number of different phenomena). Of the formative constructs in the example, only
construct X references an ease-of-use item (X
1
).
In the example, X
1
is highly similar to all items for reflective constructs A and B, but it is allowed to establish a relationship only to its highest-
similarity item within each alternative construct. For example, for both reflective and formative constructs the most similar items are connected. 
What this means is that even when a formative construct contains items that are quite dissimilar from other items within the construct, if another
formative construct, contains identical or similar items (also dissimilar within the construct), the relationship between the two formative
constructs will still be high, because only the most similar items are connected between the two. If the two formative constructs focus on
different phenomenon, while any linked item pair will be the most similar of the pairs, it will still be very dissimilar, leading to a low inter-
construct relationship. Per Figure C1, the relationship between items in X and B may be X
1
—B
1
= 1, X
2
—B
3
= 0, X
3
—B
2
= 0). If each high-
A16
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Figure C1. CID on Reflective Versus Formative Constructs
similarity pair (double lines) is scored as 1.0 and each low-similarity pair (dotted lines) is scored 0, the construct–construct scores become A—B
= 1, X—Y = .66, X—A = .33, X—B = .33, Y—A = 0, and Y—B = 0. In short, the process is relatively invariant in terms of reflective and
formative constructs. That is, it should work equally well for either construct type while exhibiting low-to-moderate relationships between
reflective and formative constructs.
Appendix D
Evaluation of Automatic Cutoffs
To evaluate the potential impact of using the automatic F
1
cutoffs when evaluating CID
1
, we conducted a separate assessment, in which a
random draw resulted in 20 constructs that were used as starting points to find all constructs in the same category. We used CID
1
in the same
manner as the other tests documented in this article, whereas two approaches were used to query EBSCO for each of the 20 construct categories. 
In the first approach, the words from the constructs’ measurement items were weighted using log-entropy and the weights combined across
all items in the construct; the top three words were then used to create queries. In the second approach, the name of the construct was used
as a search query in up to two combinations for longer construct names. These procedures produced up to four queries for each EBSCO
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A17
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
construct search task, of which the best performing query, as measured by maximum F
1
-score, was elected to represent expert queries. The
randomly selected tasks were expected to return between 1 and 56 constructs (average = 18.1 and median = 6.5, suggesting a few large tasks
and several smaller tasks). CID
1
outperformed EBSCO on every assessment measure: average precision (.68 versus .46), average recall (.58
versus .39), average F
1
(.60 versus .32), average constructs found (7.50 versus 3.55), and percent of constructs found (52.82 versus 28.85),
thereby supporting the original findings.
Appendix E
Supervised Machine Learning for CID
To examine the potential of supervised machine learning for CID, the six similarity designs were used to create input features for supervised
machine learning using the gold standard as the target. Using the five-fold sampling process on construct pairs leads to problems because any
randomly selected 80% of pairs (36 pairs) would still leave the network almost intact and result in a test set that would incorrectly represent
the algorithm’s predictive abilities. Therefore, a fully categorized sample of constructs was not an option. For a relatively simple test, we
started with the gold standard of 1,004 constructs and randomly split the sample into five sets of 504 constructs for training (126,756 pairs)
and 504 constructs for testing (again with 126,756 pairs). For each pair, the six similarity measures were used as features in the training and
evaluation, and the synonymy status of each pair as the target variable.
Table E1. Evaluative Measures for Supervised Learning
J48
Precision
Recall
F
1
Correspondent
Constructs Found
Total Constructs
Returned
Sample 1
0.715
0.182
0.29
246
344
Sample 2
0.594
0.199
0.298
259
436
Sample 3
0.781
0.121
0.21
210
269
Sample 4
0.605
0.159
0.252
187
309
Sample 5
0.603
0.135
0.22
173
287
Average
0.659
0.159
0.254
215
329
Random Forest
Sample 1
0.783
0.196
0.313
314
401
Sample 2
0.638
0.169
0.267
220
345
Sample 3
0.696
0.127
0.215
220
316
Sample 4
0.596
0.145
0.233
170
285
Sample 5
0.651
0.148
0.241
190
292
Average
0.672
0.157
0.253
222.8
327.8
Naïve Bayes
Sample 1
0.146
0.416
0.216
636
4348
Sample 2
0.12
0.441
0.188
575
4799
Sample 3
0.147
0.407
0.216
705
4799
Sample 4
0.098
0.417
0.159
490
4982
Sample 5
0.111
0.45
0.178
579
5234
Average
0.124
0.426
0.191
597
4832.4
CID
1
Sample 1
0.322
0.259
0.287
397
1232
Sample 2
0.411
0.258
0.317
336
818
Sample 3
0.4111
0.24
0.303
416
1011
Sample 4
0.373
0.239
0.291
281
754
Sample 5
0.409
0.255
0.314
328
803
Average
0.385
0.250
0.302
351.6
923.6
A18
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
For each training set, a predictive model was created using three Weka version 3.7.9 algorithms with default parameters: Naïve Bayes, J48,
and Random Forest (Bouckaert et al. 2013; Hall et al. 2009).
2
The test datasets were used to evaluate the success of the three algorithms against
the success of the CID
1
design. As may be seen from Table E1, while CID
1
outperforms the other algorithms––especially Naïve
Bayes––substantially on average F
1
-scores, the precision of both J48 and Random Forest is quite impressive. Unfortunately, the recall scores
for both algorithms are low. Nevertheless, the results suggest that work on supervised machine learning holds great potential for future
improvements.
References
Bingham, E., and Mannila, H. 2001. “Random Projection in Dimensionality Reduction: Applications to Image and Text Data,” in KDD ’01
Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , New York: ACM, pp.
245-250.
Bouckaert, R. R., Frank, E., Hall, M., Kirkby, R., Reutemann, P., Seewald, A., and Scuse, D. 2013. “WEKA Manual for Version 3-7-8,”
University of Waikato, New Zealand.
Burton-Jones, A., and Gallivan, M. J. 2007. “Toward a Deeper Understanding of System Usage in Organizations: A multilevel Perspective,”
MIS Quarterly (31:4), pp. 657-679.
Cohen, J. 1960. “A Coefficient of Agreement for Nominal Scales,” Educational and Psychological Measurement (20:1), pp. 37-46.
Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R. 1990. “Indexing by Latent Semantic Analysis,” Journal of the
American Society for Information Science (41:6), pp. 391-407.
Globerson, A., and Tishby, N. 2003. “Sufficient Dimensionality Reduction,” The Journal of Machine Learning Research (3:7/8), pp.
1307-1331.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., and Witten, I. H. 2009. “The WEKA Data Mining Software: An Update,”
ACM SIGKDD Explorations Newsletter (11:1), pp. 10-18.
Hui, K.-L., Teo, H. H., and Lee, S.-Y. T. 2007. “The Value of Privacy Assurance: An Exploratory Field Experiment,” MIS Quarterly (31:1),
pp. 19-33.
Jiang, J. J., and Conrath, D. W. 1997. “Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy,” in Proceedings of the
International Conference on Computatioal Linguistics (ROCLING X), Taiwan.
Jones, K. S. 1986. Synonymy and Semantic Classification, Edinburgh, Scotland: Edinburgh University Press.
Kakkonen, T., Myller, N., Sutinen, E., and Timonen, J. 2008. “Comparison of Dimension Reduction Methods for Automated Essay Grading,”
Educational Technology & Society (11:3), pp. 275-288.
Landauer, T. K., and Dumais, S. T. 1997. “A Solution to Plato’s Problem: The Latent Semantic Analysis Theory of the Acquisition, Induction,
and Representation of Knowledge,” Psychological Review (104:2), pp. 211-240.
Landauer, T. K., Foltz, P. W., and Laham, D. 1998. “An Introduction to Latent Semantic Analysis,” Discourse Processes (25:2-3), pp. 259-
284.
Landis, J. R., and Koch, G. G. 1977. “The Measurement of Observer Agreement for Categorical Data,” Biometrics (33:1), pp. 159-174.
Leacock, C., Miller, G. A., and Chodorow, M. 1998. “Using Corpus Statistics and Wordnet Relations for Sense Identification,” Computational
Linguistics (24:1), pp. 147-165.
Lesk, M. 1986. “Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone,”
in Proceedings of the 5
th
Annual International Conference on Systems Documentation, New York: ACM, pp. 24-26.
Li, Y., McLean, D., Bandar, Z. A., O’Shea, J. D., and Crockett, K. 2006. “Sentence Similarity Based on Semantic Nets and Corpus Statistics,”
IEEE Transactions on Knowledge and Data Engineering (18:8), pp. 1138-1150.
Lin, D. 1998. “An Information-theoretic Definition of Similarity,” in Proceedings of the 15
th
International Conference on Machine Learning,
Madison, WI, pp. 296-304.
Malhotra, N. K., Kim, S. S., and Agarwal, J. 2004. “Internet Users’ Information Privacy Concerns (IUIPC): The Construct, the Scale, and
a Causal Model,” Information Systems Research (15:4), pp. 336-355.
Marcus, M. P., Marcinkiewicz, M. A., and Santorini, B. 1993. “Building a Large Annotated Corpus of English: The Penn Treebank,”
Computational Linguistics (19:2), pp. 313-330.
Mihalcea, R., Corley, C., and Strapparava, C. 2006. “Corpus-Based and Knowledge-Based Measures of Text Semantic Similarity,” in The
21
st
National Conference on Artificial Intelligence, Boston: MIT Press, pp. 775-780.
Miller, G. 1995. “WordNet: A Lexical Database for English,” Communications of the ACM (38:11), pp. 39-41.
Miller, G. A., Beckwith, R., Fellbaum, C., Fross, D., and Miller, K. 1993. “Introduction to WordNet: An On-Line Lexical Database,”
International Journal of Lexicography (3:4), pp. 235-244.
2
Weka 3.7.9 default parameters used: Naïve Bayes: All False; J48: -C 0.25 –M 2; Random Forest: -I 10 –K 0 –S 1 –num –slots.
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
A19
Larsen & Bong/Construct Identity in Literature Reviews and Meta-Analyses
Petter, S., Straub, D., and Rai, A. 2007. “Specifying Formative Constructs in Information Systems Research,” MIS Quarterly (31:4), pp. 623-
656.
Poli, R., Healy, M., and Kameas, A. 2010. “WordNet,” in Theory and Applications of Ontology: Computer Applications, C. Fellbaum (ed.),
New York: Springer, pp. 231-243.
Porter, M. F. 1980. “An Algorithm for Suffix Stripping,” Program (14:3), pp. 130-137.
Resnik, P. 1995. “Using Information Content to Evaluate Semantic Similarity in a Taxonomy,” in IJCAI-95: Proceedings of the 14
th
International Joint Conference on Artificial Intelligence–Volume 1, C. S. Mellish (ed.), San Francisco: Morgan Kaufmann Publishers
Inc.
Siderova, A., Evangelopoulos, N., Valacich, J. S., and Ramakrishnan, T. 2008. “Uncovering the Intellectual Core of the Information Systems
Discipline,” MIS Quarterly (32:3), pp. 467-482.
Wu, Z., and Palmer, M. 1994. “Verbs Semantics and Lexical Selection,” in ACL ‘94 Proceedings of the 32
nd
Annual Meeting on Association
for Computational Linguistics, Stroudsburg, PA, pp. 133-138.
A20
MIS Quarterly Vol. 40 No. 3—Appendices/September 2016
View publication stats
View publication stats
