Applied Intelligence
https://doi.org/10.1007/s10489-018-1299-7
Hybrid attribute based sentiment classiﬁcation of online reviews
for consumer intelligence
Barkha Bansal
1
· Sangeet Srivastava
1
© Springer Science+Business Media, LLC, part of Springer Nature 2018
Abstract
Rich online consumer reviews (OCR) can be mined to gain valuable insights, beneficial for both brands and future buyers.
Recently,
aspect based sentiment classification have shown excellent results for fine grained sentiment analysis of OCR.
However,
there are only few studies so far that
rely on both explicitly deriving sentiment
using syntactic features,
and
capturing implicit contextual word relations for the task of aspect based sentiment classification. In this paper, we propose
a novel method: Hybrid Attribute Based Sentiment Classification (HABSC) with the aim to derive sentiment orientation of
OCR by capturing implicit word relations and incorporating domain specific knowledge. First, we detect the most frequent
bigrams and trigrams in the corpus,
followed by POS tagging to retain aspect
descriptions and opinion words.
Then,
we employ TFIDF (term frequency inverse document frequency) to represent each document,
followed by automatically
extracting optimal number of topics in the given corpus. All the adjectives and adverbs are labelled using domain specific
knowledge and pre-existing lexicons. Lastly, we find sentiment orientation of each review under the assumption that each
review is a mixture of weighted and sentiment labelled attributes. We test the efficiency of our method using datasets from
two different domains: hotel reviews from TripAdvisor.com and mobile phone reviews from Amazon.com.
Results show
that,
the classification accuracy of HABSC significantly exceeds various state-of-the-art methods including aspect-based
sentiment classification and supervised classification using distributed word and paragraph vectors. Our method also exhibits
less computational time as compared to distributed vectorization schemes.
Keywords
Consumer intelligence · Topic sentiment · Semantic sentiment · Distributed vector · Lexicons · n gram ·
Part-of-speech
1 Introduction
The systematic growth in e-retailing has rendered buyers to
express their feedbacks and reviews of the products online,
making online consumer reviews (OCR) a valuable resource
for both future consumers and brands [42]. OCR immensely

Sangeet Srivastava
sangeetsrivastava@ncuindia.edu
Barkha Bansal
barkha.bansal20@yahoo.com
1
Department of Applied Sciences, The NorthCap University,
Gurugram, Haryana, India
influence other buyers before they purchase these products
[49]
and play a significant
role in customer
relationship
management (CRM), brand and product management, com-
parative analysis and effective advertising,
to name a few.
However,
it
is
challenging to analyse large amount
of
unstructured or semi-structured data quickly and efficiently.
One of the ways to gain insight
from OCR is sentiment
classification that includes explicitly finding average senti-
ment of either a person,
review,
product,
service or brand.
Conventional approach to sentiment classification includes
quantifying emotional
content
of a given text
data based
on annotated lexical
features [25].
Similarly,
majority of
existing sentiment analysis methods rely on explicitly find-
ing lexical
and syntactic features and hence overlook the
semantic word relations as well as contextual information
in the text.
Recently,
studies have shown the significance
B. Bansal, S. Srivastava
of employing semantic relations and word co-occurrences
for
superior
classification accuracy [13].
Moreover,
few
fine grained methods have also tried aspect
based senti-
ment analysis, by considering sentiment of each aspect in a
review. In other words, the overall sentiment of a product is
observed to be comprised of sentiments of its aspects [37].
So far,
there are very few studies
that
could combine
contextual and semantic information with aspect based sen-
timent analysis. Hence, this study proposes hybrid attribute
based sentiment classification (HABSC),
which associates
a review with one or more attributes and is capable of cap-
turing contextual information for extracting corresponding
attribute-sentiment. One of the advantages of the proposed
method is that, frequently occurring ambiguous words in the
corpus are efficiently handled. For instance, adjective long,
if used in context
of battery exhibits a positive sentiment
whereas,
if
used with mobile updates,
exhibits negative
sentiment.
In
this
paper,
we
first
extract
aspect
descriptions
and sentiment
bearing words using POS (part-of-speech
tagging),
then,
LDA (Latent
Dirichlet
Allocation)
[7]
is
employed to regroup very similar
aspects
together
to
form an attribute.
To find attribute specific sentiments,
topic
content
analysis
[19]
is
performed using lexicon
based approach. Following two different lexicons are used:
first, pre-existing lexicon: AFINN-111 [35] and second by
incorporating pre-existing lexicon with domain and attribute
specific customised words.
Main reasons
to incorporate
attribute specific words are as follows:
–
While analysing attribute specific sentiment
bearing
words,
it
was
observed that
pre
existing lexicons
does not
incorporate all
the sentiment
bearing words.
For
instance,
for
an attribute Physical
Appearance,
component
(word)
New has
a
positive
sentiment,
however,
it
is
difficult
to
infer
from pre-existing
lexicons.
–
Also,
the sentiment
polarity of
an ambiguous com-
ponent
can be different
for
different
attributes.
For
instance, adjective Cheap holds a negative polarity for
attribute Physical
Appearance but
a positive polarity
for
attribute Price.
Therefore,
to produce more reli-
able results we use both:
pre-existing and customised
lexicons.
The main contributions of the paper are as follows:
–
A novel
framework HABSC to mine OCR,
capable
to capture contextual
word relations and incorporate
attribute specific knowledge for
increased sentiment
classification accuracy.
–
Exhibit
the
capability
of
handling
most
frequent
ambiguous words by capturing word correlations and
context.
–
Demonstrate
the
superior
accuracy
of
HABSC as
compared to other computationally expensive methods,
without
feeding
prior
polarity
labels
or
consumer
ratings.
The
paper
is
structured as
follows.
Section 2 reviews
related research work such as state-of-the-art
distributed
representations and aspect
based sentiment
classification
using
topic
modelling.
Section
3
describes
data
sets
used and methodology for HABSC.
Section 4 details the
experimental
results and discussion.
Section 5 concludes
our work and finally Section 6 describes the limitations of
our approach and future research work.
2 Related work
2.1 Distributed vector representations: word2vec
and doc2vec
Recently,
distributed word vector representations [22,
32,
33] have exhibited outstanding capability to capture seman-
tic word relations.
Using these representations,
semanti-
cally related words can be measured quantitatively in con-
tinuous space [44].
For
instance,
when properly trained,
DV (Madrid) − DV (Spain) + DV (F rance) gives vector
representation of P aris,
where DV represents distributed
vector [32–34]. By capturing such semantic properties, dis-
tributed vector representations have outperformed previous
models for the task of both coarse and fine grained senti-
ment classifications [22].
Specifically,
word2vec is a shal-
low neural network, with two different model architectures
i.e. CBOW (continuous bag of words) and continuous skip-
grams (SG) [32–34]. CBOW estimates a word semantically
when context
is given,
whereas,
skip-gram gives context
that
is,
surrounding words
in a particular
range of
the
word. Larger range for context can give superior classifica-
tion accuracy but increases computational complexity [32].
Skip-gram is capable of learning from small training data
and is known to correctly represent
infrequent
words and
phrases whereas,
CBOW learns faster and performs better
with frequently recurring words [12]. These algorithms for
semantic word representations were naturally followed by
distributed representations of paragraphs (or documents).
Paragraph Vector representations are fixed length repre-
sentations of variable length pieces of texts to preserve word
order, similar to an n-gram model for a very large value of n.
Hybrid attribute based sentiment classiﬁcation of online reviews for consumer intelligence
PV-DM (paragraph vector - distributed memory) when com-
bined with PV-DBOW (Distributed Bag of Words) shows
marginal
classification accuracy,
as compared to PV-DM
alone [22].
Distributed word vector
representations have
been applied in many papers for the task of sentiment clas-
sification [2, 12, 15, 34] but not extensively across various
domains.
Some of
the recent
works in this direction are
as follows.
Word vector representations have shown their
capability to efficiently classify texts in many languages
including English [1,
3,
8,
12,
34,
43].
Zhang et
al.
[56]
classified Chinese consumer reviews using word2vec and
SVM-perf.
Giatsoglou et
al.
[12]
used hybrid of
lexicon
and word embeddings for classification of online reviews in
Greek and English languages. Other studies including sen-
timent classification of citations have also been performed
using distributed word vectors [27].
However,
distributed representations
may not
always
give significantly superior classification accuracy as com-
pared to non-distributed methods. Jiang et al. [15] observed
only
a
small
improvement
in
classification
accuracy
when word vectors were combined using TF-IDF (Term
frequency-Inverse Document Frequency) and IDF (Inverse
Document Frequency) to represent a document from Yelp
and Trip Advisor datasets.
Although both word and para-
graph vector
representations efficiently capture semantic
word relations in text, they need prior polarity labelling for
the task of supervised learning. Further, distribution vector
representation models are computationally expensive and
more time consuming as compared to proposed HABSC. In
this paper,
we extensively study and apply state-of-the-art
methods of distributed vector representations and machine
learning algorithms for classification of OCR and compare
these models with HABSC.
2.2 Topic-sentiment models
Another popular way for aspect
based sentiment
analysis
employs topic-sentiment
models,
to simultaneously detect
the mixture of latent topics and their associated sentiments
in a review [31].
Lin and He [24]
proposed fully unsu-
pervised model called JST (Joint Sentiment/ Topic Model)
that employs small set of domain-independent positive and
negative seed words instead of employing pre-existing lex-
icons for finding aspect based sentiments. However, JST is
not
capable of capturing word ordering.
For instance,
not
good might give a positive sentiment, considering only the
sentiment bearing word good. Therefore, we propose to han-
dle this problem by detecting the most frequently occurring
bigrams and trigrams. Xianghua et al. [50] proposed multi-
aspect
sentiment
analysis using LDA and HowNet
lexi-
con for Chinese social reviews that outperformed JST and
other supervised classification techniques. First, they found
global topics using all the reviews and then identified local
topics in the sliding windows using the trained LDA model.
Finally,
the lexicon was used to analyse the sentiment
of
a sliding window.
Hence,
in this paper,
we employ pre-
existing sentiment lexicon, with the incorporation of domain
knowledge. One advantage to this proposed approach is that,
we efficiently handle ambiguous words (ambiguity with
respect to different attributes). Further, Jo and Oh [17] pro-
posed Sentence-LDA (SLDA) and Aspect
and Sentiment
Unification Model (ASUM) frameworks that automatically
discover aspects and associated sentiments in online con-
sumer
reviews.
One of
the downsides to their
proposed
models is the assumption,
that all words in a single docu-
ment describe only one aspect.
Wang et al.
[47] proposed
two novel semi supervised methods FL-LDA (Fine grained
labelled
LDA )
and
UFL-LDA (Unified
fine
grained
labelled LDA), that use seed words extracted from product
descriptions on E-commerce websites for the task of aspect
detection in OCR.
FL-LDA was applied using the seeds
and UFL-LDA was applied to unlabelled data for extracting
the aspects which were related to seeding aspects or other
high frequency words in the reviews. However, performance
of
proposed model
depends on quality and comprehen-
siveness of seed word lists.
Zirn and Stuckenschmidt [57]
employed two variants of
LDA:
LogicLDA and Labeled
LDA to compare positions
of
different
political
parties
towards particular topics by measuring the distance between
corresponding topics.
Joint
Multi-grain Topic Sentiment
(JMTS) model was proposed by Alam et al.
[4] to extract
aspect,
and does not
require labelled data,
rating based
reviews
or
any other
supervised setting.
They fed the
model
with two small
domain independent
sets of
posi-
tive and negative word lists. The classification accuracy by
JMTS outperformed ASUM (Aspect-Sentiment Unification
Model) [17] and JST (Joint-Sentiment-Topic Model) [24].
We compare our
model
with JST [24],
ASUM [17]
and
JMTS [4].
Although our model
requires priori knowledge
of domain and attributes, it exhibits significant increase in
classification accuracy.
Liu et al. [28] presented a framework that automatically
detects brand topics using LDA in 1.7 million user generated
tweets followed by sentiment analysis. They showed prac-
tical implication of their study by answering brand related
research questions with the aim to equip brand managers
with significant
findings
from brand-related user
gener-
ated content (UGC) on Twitter. However, drawback to their
study is that,
LDA is not
suitable for short
texts such as
tweets [52].
Yu et al.
[54] proposed aspect modelling and
B. Bansal, S. Srivastava
sentiments
classification,
called Ratings
Are Sentiments
(RAS).
They combined users’ sentiments in reviews and
corresponding ratings in the model.
However,
not all con-
sumer reviews come with consumer ratings.
For instance,
Twitter
is an important
platform for
consumer
generated
content
or
consumer-brand interaction but
tweets do not
involve a rating based criteria.
Hence,
we compare our
experimental
results to ratings given in the mobile phone
review data set
instead of using the ratings as a form of
customer sentiment.
Recently, product defect mining methods have been pro-
posed using contextual features and word co-relations [29,
39].
Qiao et al.
[39] extended LDA model to Latent Prod-
uct
Defect
Mining model
that
is able to recognise prod-
uct
defects.
They specified domain-specific attributes to
describe the defects.
Their
research was
aimed towards
product quality analytics with important managerial impli-
cations.
Another way to detect
aspects of a product
is by
employing PoS (part of speech) tagging.
Wang et al.
[48]
adopted methods
proposed by [14,
26]
to extract
most
frequent
nouns as possible candidates for product
aspects
from PoS tagged corpus.
The extracted items were large
in number,
so they employed dimensionality reduction.
Then,
they employed an econometric model
to analyse
the impact of changing opinion on sales volume by using
aspect
and corresponding sentiment.
Again,
limitation to
their model
is that
it
does not
capture implicit
contextual
information.
For example,
CHEAP when used in conjunc-
tion with Screen,
expresses a negative sentiment,
whereas,
with (Price) expresses positive sentiment. POS-tagging and
other explicit features are not capable of capturing correct
sentiments in such cases.
3 Data and methodology
3.1 Data Description and Pre-processing
Proposed model has been demonstrated using two datasets
from two different domains (Table 1). First dataset is taken
from Amazon.com, referred as MRD (mobile phone reviews
Table 1
Dataset description. User review datasets from two different
domains: hotel and mobile phones
Dataset
Reviews
Sentences
Words per
per review
review
MRD (Mobile Phone Reviews)
413840
3.035
40.53
HRD (Hotel Reviews)
20491
12.03
104.36
Table 2
Class labels based on consumer ratings, for both balanced and
unbalanced datasets
Consumer
Class label in t
Class label in
rating
balanced datase
unbalanced dataset
1
−1
−1
2
−1
−1
3
−
1
4
1
1
5
1
1
data),
containing more than 400,000 reviews for
various
international brands [11]. Second dataset contains more than
20,000 hotel
reviews
(HRD)
from TripAdvisor.com [4].
Both datasets
include
user
ratings,
which we
use
for
calculating classification accuracy and not
for
training
purposes.
We perform experiments on both balanced and
unbalanced datasets (Table 2). However, balancing datasets
reduces the size of data (Table 3) and hence,
reduces the
classification accuracy in the case of
distributed vectors,
as larger
datasets produce quality vector
representations.
Hence, we report results for unbalanced datasets. Text pre-
processing involves following standard steps:
white space
elimination,
stop words
removal,
stemming,
converting
into lower
case and punctuation symbols
removal.
Pre-
processing is done using spaCy library in python [45].
3.2 Hyper-parameter setup: LDA, word2vec and
doc2vec
Latent Dirichlet Allocation (LDA) [7] is a generative prob-
abilistic model,
capable to automatically segregate topics
from documents (reviews) in a corpus. Each document is a
mixture of weighted topics, where each topic can be either
positive (green),
negative (red),
or can contain ambiguous
adjectives (yellow) (Fig.
1).
LDA assumes generative pro-
cess as shown in Algorithm (Phase 2).
LDA gives two
outputs,
first,
all
the topics
with list
of
weighted com-
ponents,
second,
mixture of
topics associated to a docu-
ment,
with different
degrees of memberships [18].
These
Table 3
Number
of
reviews
in each class
for
both balanced and
unbalanced cases, in TripAdvisor and Amazon datasets
Balanced
Unbalanced
Negative
Positive
Negative
Positive
MRD
97078
97078
97078 (24%)
316762 (76%)
HRD
3214
3214
3214 (15.7%)
17277 (84.3%)
Hybrid attribute based sentiment classiﬁcation of online reviews for consumer intelligence
Fig. 1
Snippet of different
topics in sample reviews from
MRD: positive, and negative
topics are shown in green and
red respectively. yellow shows a
topic with an ambiguous word,
in this review cheap depicts a
positive sentiment
word distributions in topics and topic distributions in doc-
uments are controlled by hyper-parameters α and β.
We
proceed with symmetric distribution and report results for
default value of 1/K for hyper parameters α and β, where
K represents
the number
of
topics.
Removing low fre-
quency words improves the quality of topics. We remove all
those words that appear in less than 5 documents.
Hyper-
parameter
values for
word2vec and doc2vec models are
shown in Table 4.
Vector size for all the models has been
set to 500,
as no significant improvement in classification
accuracy was seen at
higher dimensions.
Again,
based on
accuracy criteria sub-sampling rate for word2vec models
has been set
to 1e-3 after trying values of 1e-3 and 1e-4,
and for doc2vec the default
value of 1e-5.
Negative sam-
pling was found to give more accuracy than hierarchical
softmax.
Finally,
results have been reported for different
weighting schemes for combined word vectors to represent
a document.
3.3 Optimal number of topics
To find most coherent topics, we use three different methods
to find optimal k.
3.3.1 Perplexity
Perplexity is
widely used evaluation method for
topic
models and for
selection of
best
number
of
topics [10].
A lower perplexity indicates better model performance [7,
10].
It
is usually calculated by splitting the dataset
into
training and testing sets.
We divide data into 80:20 for
training and testing. Given a test dataset D
test
, perplexity is
calculated by:
perplexity(D
test
) = exp

¬

d

n
i=1
log p(w
d,i
)

d
n

(1)
where d is document in test dataset D
test
, w
d,i
is a word in
d, n is the number of words in d and for a topic z, p(w
d, i
)
is equal to

z
p(w|z)p(z|d).
3.3.2 Topic coherence
Topic coherence [38] is defined as
C(z; W
(z)
) =
M

m=2
m−1

l=1
log
d(w
z
m
, w
z
l
) + λ
d(w
z
l
)
(2)
where, z is topic, w
z
i
is the i
th
word in topic z and d(w
z
m
, w
z
l
)
is co-document frequency. Coherence of a topic z is sum of
all log values of conditional probabilities d(w
z
m
, w
z
l
) i.e. co-
occurrence of words. d is the document containing at least
one instance of w
z
m
and simultaneously at least one instance
of w
z
l
.
Larger negative values (log of probabilities) imply
less coherent
topics,
whereas,
values closer to zero imply
coherent
topics.
λ is added to numerator for avoiding log
zero errors.
Table 4
Hyper-parameter
settings for Word2Vec
(continuous bag of words and
skip gram) and Doc2Vec
(paragraph vector-distributed
memory and paragaraph
vector-distributed bag of
words) models
Word2Vec
Doc2Vec
CBOW
SG
PV-DM
PV-DBOW
Vector dimension
500
500
500
500
Window
5
10
10
10
Sub-sampling rate
1e-3
1e-3
1e-5
1e-5
Training algorithm
Negative sampling
Negative sampling
Negative sampling
Negative sampling
min
count
5
5
5
5
Weighting scheme
TFIDF, Sum
TFIDF, Sum
Sum, Concat.
–
B. Bansal, S. Srivastava
Hybrid attribute based sentiment classiﬁcation of online reviews for consumer intelligence
where
–
α is the parameter of Dirichlet prior on topic distribution
of each document
–
β s the parameter of Dirichlet prior on word distribution
of each topic
–
θ
m
is the distribution over topics of m
th
document
–
φ
k
is the posterior distribution over words for k
th
topic
–
z
mn
is the topic for n
th
word in m
th
document
–
w
mn
is nth word in m
th
document
–
K = number of topics
–
N = Number of words in a document and n ∈ {1, .., N
i
}
–
M = number of documents in a corpus
–
m ∈ {1, .., M},
Dir(α) is a Dirichlet
Distribution and
α < 1 (sparse).
–
where k {1, .., K}, Dir(β) is a Dirichlet
Distribution
and β is sparse.
–
φ
tw
represents vector containing conditional probability
P(word = w | topic = t)
–
s
w
represents vector containing sentiment score of each
word; w = word
–
s
t
= sentiment
score of the topic t
representing both
magnitude and polarity
–
S
t
= topic sentiment vector: s
1
s
2
...s
t
–
S
d
= sentiment
vector
consisting of
all
document
sentiments
3.3.3 Silhouette analysis
Often perplexity and human judgment
are not
correlated
[21].
Therefore,
one way to calculate optimal
number of
topics is by treating topics as clusters and then applying
Silhouette Coefficient
[16,
30,
36,
51,
53]
to measure
the similarity (homogeneity)
within each topic (cluster)
and dissimilarity (heterogeneity)
between all
the topics.
Silhouette coefficient is given as:
S =
y(i) − x(i)
max{x(i), y(i)}
(5)
where x(i) represents the average dissimilarity (distance)
between point i and all other points within a same cluster
and y(i)
represents
the minimum average dissimilarity
(distance) between point i and all other points in any other
cluster. Therefore, value of silhouette coefficient close to 1
represents superior clusters.
3.4 Hybrid attribute based sentiment classiﬁcation
HABSC is graphically shown in Fig. 2. In this framework,
we first
detect
most
frequently co-occurring terms,
i.e.
bigrams and trigrams. Phrase detection (bigram and trigram)
is
done
using gensim library [40].
These
phrases
are
Fig. 2
Diagrammatic representation of hybrid attribute based sentiment classification
B. Bansal, S. Srivastava
combinations of frequently co-occurring words also called
multi-word expressions.
These words,
when used together
have a different
meaning or
a different
sentiment.
For
instance, unigrams fully and brand are neutral but bigrams
fully
functional
and brand new have a positive sentiment
(Table 5).
Then,
POS tagging has been applied to retain
aspect
describing and opinion conveying terms.
Nouns,
verbs,
adverbs and adjectives are retained in the text,
with
their variants such as VBD (verb past tense), NNP (proper
noun),
VBG (verb/
gerund),
VBZ (verb,
third person)
(Table 5) and so on [5,
9].
Nouns generally describe the
aspects whereas adverbs and adjectives express emotions or
opinion about aspects.
As mentioned earlier,
POS tagging
results in large number of explicit aspects (generally nouns).
To combine the semantically related and statistically co-
occurring aspects,
we employ LDA.
For instance,
Internet
and WiFi
are
related aspects
that
can now be
easily
combined to form an attribute. Algorithm details HABSC in
its three phases.
Then,
text
is vectorised using TFIDF scheme for more
accurate results as compared to bag-of-words model, which
is followed by topic modelling to find attributes [9,
23].
TFIDF weighting scheme is widely used to assign different
weights
to words
in the
document
according to their
relevance [20]. Weight W
i,j
of a term w
i
in a given document
d
j
is defined as:
W
i,j
= T F
i,j
∗ log
N
DF
i
(6)
where T F is frequency of
term i
in document
j ,
N is
total
documents in the corpus,
DF is total
frequency of
documents containing term i. Then, we employ LDA using
gensim library [41].
List
of weighted words (adverbs and
adjectives)
corresponding to each attribute is matched to
pre existing sentiment lexicon AFINN-111. The words not
matched to lexicon are separated and annotated with the
help of domain experts. To add attribute specific sentiment
bearing terms, similar scale is used as AFINN. These sets of
sentiment bearing words are added to pre existing lexicon,
to customise new enriched attribute specific lexicon. Table 6
illustrate the list
of
attribute specific sentiment
bearing
words extracted in MRD,
that
were not
found in existing
lexicons
and/
or
exhibit
ambiguity.
Then,
we
employ
AFINN and enriched AFINN to find two different sentiment
scores for each attribute. All the attributes are then labelled
with an individual sentiment score based on the equation in
Phase 3 of Algorithm.
Sentiment analysis is done using lexicon package in R,
that
allows sentiment
analysis with customised lexicons.
Now, each document is a mixture of attributes (topics) with
corresponding probabilities (θ ) representing the percentage
of their presence in the document. Therefore, sentiment of
each review is found using the probability of association
with the attribute and the sentiment score of the respective
attribute.
Hence,
sentiment
of
each review is
the
sum
of
weighted attribute-sentiment
scores.
Finally,
accuracy
of
the model
is calculated using the predicted sentiment
polarity of
each review.
In our
study we find document
Table 5
Examples: Phrases
Detected in MRD followed by
retained POS tags
Review 20000
Second review. The first was about no Sims card, will not work
with Verizon.This one is about the down volume button not work.
Review 20000
second review the first was about no sims
card, will not work
(phrases detected)
with verizon this one is about the down volume
button not work.
Review 20000
( Second , JJ ), ( review , NN ), ( first , JJ ), ( was , VBD ),
Retained POS Tags
( Sims
card , NNP ), ( not , RB ), ( work , VB ), ( Verizon , NNP ),
( one , NN ), ( is , VBZ ), ( down , JJ ), ( volume
button , NN ),
( not , RB ), ( work , NN )
Review 20155
...fully functional in a few minutes. The camera, buttons, microphone
and speaker are all working like it came out of the box....
phone that works and looks like brand new.....
Review 20155
....fully
functional in a few minutes the camera, buttons, microphone
(phrases detected)
and speaker are all working like it came out of the box....
phone that works and looks like brand
new.....
Review 20155
....( fully
functional , JJ ), ( few , JJ ), ( minutes , NNS )
Retained POS Tags
,( camera , NN ), ( buttons , NNS ), ( microphone , NN ), ( speaker,NN),
( are , VBP ), ( working , VBG ), ( came , VBD ), ( box.This , NN ),....
( phone , NN ), ( works , VBZ ), ( looks , VBZ ), ( brand new , NN )
Hybrid attribute based sentiment classiﬁcation of online reviews for consumer intelligence
Table 6
Attribute specific customized lexicon in MRD
Attribute
Customized words
Look
New, cheap
Return / Not satisfied
Return, send
Screen
Break, excelente, crack, waterproof
Ease of use
Easy, waterproof
Recommend
Absolutely, recommend
Expectations met
Meet, exceed, fast
Customer satisfaction
Thank, fast, excelent, ok
Battery
Long
Service / warranty
Issue
Shipping
Fast
shipping
level
sentiment
under the assumption that
each review is
a mixture of
weighted and sentiment-labelled attribute,
instead of employing LDA for feature extraction followed
by supervised classification.
4 1. Experimental results and discussion
4.1 Optimal number of topics
Topic coherence vs
number
of
topics
for
both datasets
are listed in Table 7.
The perplexity of
different
models
is represented in Fig.
3.
Evaluation results of
perplexity
measure are consistent
to coherence of topics.
Although,
these methods
are widely used for
model
selection,
in
our study we employ one more method for more reliable
results.
We use silhouette analysis to extract optimal topic
number with 500 iterations in both datasets (Fig. 4). After
considering results of
all
the three methods for
finding
optimal
number of topics,
we select
number of topics for
MRD to be 21 and HRD to be 25. Subjective verification of
topics for interpretability and importance also confirms that
topics are most interpretable and meaningful when K is set
to 21 and 25 for MRD and HRD, respectively.
Table 7
Topic coherence in HRD and MRD
Topic Coherence
Number of topics
MRD
HRD
10
− 2.35
− 1.83
20
− 2.81
− 1.97
30
− 3.18
− 2.31
40
− 3.18
− 2.87
50
− 3.39
− 3.89
Fig. 3
Plot of perplexity in two datasets: MRD and HRD. Number of
topics on X axis vs perplexity on Y axis
4.2 Customised lexicon
The customised lexicon (extended with attribute specific
annotated words) results in superior sentiment classification
as
compared to general
lexicon (Table
8).
Customised
lexicon improves classification accuracy by more than 1%
in each dataset and hence results are reported for AFINN+
customized lexicon.
4.3 Model evaluation and comparison to other
models
We
compare
classification
accuracy
of
HABSC with
following different models:
A.
Topic –Sentiment Models:
–
ASUM:
Sentence based model
that
assigns one
sentiment label and one topic to each sentence [17].
–
JMTS:
Window based model
that
detects ratable
aspects with sentiment orientation [4].
B.
Supervised Models: LR, SVM and RF using following
feature representations and 10-fold cross validation.
–
Word2vec
(CBOW/
SG)
+ Sum/
TFIDF:
Dis-
tributed word vector
representations followed by
Fig. 4
Plot of silhouette coefficients in two datasets: MRD and HRD.
Number of cluster on X axis vs silhouette coefficient on Y axis
B. Bansal, S. Srivastava
Table 8
Example of reviews correctly classified by customised lexicon
but not general lexicon
Reviews
HABSC
HABSC
True consumer
(AFINN)
(Customized
rating out of 5
lexicon)
The phone worked very
good the first
month,
it
started freezing up all the
time and wont
come on
half the time
0.21
− 0.027
1 (Negative)
This phone is a generic
piece of garbage
0.008
− 0.04
1 (Negative)
...
return it
...
DON”T
BUY ONE OF THESE
!!!!!!!!!
various combination methods for document repre-
sentation [6].
–
Doc2Vec
(PV-DM)
+Sum/
Concatenation:
Dis-
tributed paragraph vector representations using dis-
tributed memory model.
–
Doc2Vec (PV-DBOW): Distributed paragraph vec-
tor representations using distributed bag of words
model.
–
TFIDF + n grams: Term Frequency-Inverse Docu-
ment Frequency method for non-distributed vector-
ization [46].
The classification results of HRD are reported in Table 9.
Proposed HABSC exceeds the classification accuracy of
topic-sentiment
models
and supervised learning models.
HABSC has
outperformed
JMTS and
ASUM by
an
increase of 10% in classification accuracy.
Also,
it can be
noted that
the classification accuracy of HABSC exceeds
word2vec models only by 1% for HRD. Distributed vector
representations are known to extract
superior
features in
larger
datasets.
Hence,
we check the efficiency of
our
model on MRD, that consists of more than 400,000 reviews.
The results for MRD are reported in Table 10.
In MRD,
HABSC gives 2% more accurate results as compared to all
supervised models.
Finally,
we
employ McNemar’s
significance
test
to
confirm significant
superiority of
our
proposed method
over
other
existing methods
[20].
Null
hypothesis
state
that
there
is
no
significant
difference
in
the
models.
Results
show that
our
proposed method is
significantly
superior
to all
other
methods
at
99% confidence level,
in both datasets
(Tables
9 and 10).
Our
model
is
not
only more accurate but also computationally inexpensive as
compared to supervised classification of distributed vector
representations. The superior accuracy of our model can be
Table 9
Comparison of classification accuracies (in %) of different models for HRD
HABSC
JMTS [4]
ASUM [17]
Word2Vec [6]
Doc2vec
TFIDF+
and n-grams [46]
CBOW
SG
PV-DM
PV-DBOW
93.4
84
84
Sum
TFIDF
Sum
TFIDF
SUM
CONCAT.
LR
91.18±.8
92.09±.8
92.09±.5
92.17±.62
81.05±2
83.56±.55
81.09±1.7
88.54±.48
SVM
89.95±1
89.87±.85
91.19±.7
91.3±.8
82.2±1
84.3±.04
81.91±1.3
92.2±.8
RF
89.74±.5
90.09±.7
91.15±.8
90.53±.65
80.6±.8
80.5±1.2
80.79±1
88.97±.7
p value
< .001
∗∗∗
< .001
∗∗∗
–
< .001
∗∗∗
< .001
∗∗∗
< .001
∗∗∗
< .001
∗∗∗
< .001
∗∗∗
< .001
∗∗∗
< .001
∗∗∗
< .001
∗∗∗
Mean and confidence interval at 95% level for classification accuracies of supervised models are reported.
HABSC is also compared to all other models using McNemar’s significance test.
Significance level is shown as (
∗
:0.05,
∗∗
:0.01,
∗∗∗
:0.001)
Hybrid attribute based sentiment classiﬁcation of online reviews for consumer intelligence
Table 10
Comparison of classification accuracies (in %) of different models for MRD
HABSC
Word2Vec [6]
Doc2vec
TFIDF+ n-grams [46]
CBOW
SG
PV-DM
PV-DBOW
93.8
Sum
TFIDF
Sum
TFIDF
SUM
CONCAT.
LR
90.08 ± .2
89.3 ± .1
90.5 ± .1
88.8 ± .2
84.1 ± .6
87 ± .9
90.4 ± .7
91.18 ± .1
SVM
90.3 ± .2
90.22 ± .1
90.8 ± .1
90 ± .1
84.1 ± .6
89.6 ± .5
90.7 ± 1
91.7± .16
RF
90.5 ± .2
90.7 ± .2
90.8 ± .2
90.3 ± .2
81.2 ± 2.8
87.2 ± 3
90 ± 3.5
90.5 ±0 .17
p value
-
<< .001
∗∗∗
<< .001
∗∗∗
<< .001
∗∗∗
<< .001
∗∗∗
<< .001
∗∗∗
<< .001
∗∗∗
<< .001
∗∗∗
<< .001
∗∗∗
Mean and confidence interval (at 95% level) for classification accuracies of supervised models with 10-fold cross validation are reported.
HABSC is also compared to all other models using
McNemar’s significance test. Significance level is shown as (
∗
:0.05,
∗∗
:0.01,
∗∗∗
:0.001)
due to the following possible reasons.
First,
it
is possible
that
not
all
documents with word-order importance were
efficiently represented as distributed vectors. Spelling errors
can effect the performance of paragraph vectors, and hence
word vector models have outperformed paragraph vectors.
Also, in short sentences/ documents, paragraph vectors may
not
perform well.
Topic sentiment
models:
ASUM and
JMTS rely on small seed sets of positive and negative words
for
sentiment
classification
task.
Expectedly,
proposed
model also outperformed JST (accuracy of JST=73%). Our
model uses domain and attribute based enriched lexicon for
finding sentiment
of an attribute and hence,
sentiment
of
a document,
which can be a possible reason for superior
classification accuracy.
Also,
in past
similar
sentiment-
topic
models,
based on lexicons,
have
shown superior
accuracies [50].
5 Conclusion
Online consumer review (OCR) mining plays an important
role in consumer intelligence,
customer relationship man-
agement
(CRM),
marketing,
to name a few.
Conventional
review mining methods rely on explicitly finding sentiments
of syntactic and lexical features in documents and overlook
the semantic relationships,
word co-occurrences and con-
textual information in text. Recently, word2vec and doc2vec
models have been proposed that are able to represent words
and paragraphs as distributed vectors and hence,
capture
semantic word relations.
Moreover,
topic-sentiment
mod-
els have exhibited their
capability of
fine grained aspect
based sentiment
analysis.
In this paper,
we studied state-
of-the-art
techniques
for
OCR mining and proposed a
new hybrid method for consumer intelligence. We devised
hybrid attribute based sentiment
classification (HABSC)
which detects bigrams and trigrams in the text followed by
extraction of aspect
descriptions and opinion words using
POS tagging. Then, we employ LDA to combine the most
contextually similar aspects together to form an attribute.
Sentiment score of corresponding attribute, was found using
customised lexicon enriched with domain and attribute spe-
cific knowledge.
Finally,
all
the reviews
in the corpus
were labelled based on the sentiment
state of the labelled
attribute. Most frequently occurring ambiguous words were
also handled by the proposed model,
that can not be done
using most
of the existing methods.
We applied HABSC
on two datasets from two different domains: mobile phones
and hotels.
HABSC showed superior
accuracy as
com-
pared to other
state-of-the-art
models that
are known to
capture semantic word relations
and context.
Although
some of the existing topic-sentiment models do not require
domain and attribute specific knowledge, results show that
incorporating domain and attribute specific knowledge can
B. Bansal, S. Srivastava
significantly increase classification accuracy. The proposed
model can be employed to other domains and other sources
of consumer reviews.
Also,
domain-attribute specific lex-
icons
can be developed using our
model.
Overall,
the
proposed text mining framework can be applied to unstruc-
tured textual data collected in the form of reviews,
tweets
(for short texts [52]),
blogs data and similar forms in any
domain. Researchers can also employ this study on text data
in other languages.
6 Limitations and future work
This
study leaves
a potential
area for
further
research:
finding more effective method to detect
more attribute-
exhaustive
topics
with
less
computational
cost.
Sec-
ondly,
this study will
be further
extended for
short
text
classification using models
like
BTM [52]
and hybrid
approaches [55].
We will
also work on eliminating man-
ual
labelling of attribute specific words in the preexisting
lexicons.
Nevertheless,
incorporating domain and attribute
specific knowledge produces superior results as compared
to the other existing models, hence automatic aspect-context
sensitive labelling of words will be done in the future.
Availability of data and materials
The datasets analysed during the
current study, are available in the Zenodo repository: Amazon Mobile
Review Dataset [https://doi.org/10.5281/zenodo.1211639] and TripAd-
visor Hotel Review Dataset [https://doi.org/10.5281/zenodo.1219899].
The datasets were originally derived from [11] and [4] respectively.
Funding This research did not receive any specific grant from funding
agencies in the public, commercial, or not-for-profit sectors.
References
1.
Abburi
H,
Akkireddy ESA,
Gangashetti
S,
Mamidi
R (2016)
Multimodal
sentiment
analysis
of
telugu songs.
In:
SAAIP@
IJCAI, pp 48–52
2.
Abdelwahab O, Elmaghraby A (2016) Uofl at semeval-2016 task
4:
multi
domain word2vec for
twitter
sentiment
classification.
In:
Proceedings of the 10th international
workshop on semantic
evaluation (SemEval-2016), pp 164–170
3.
Al-Amin M,
Islam MS,
Uzzal
SD (2017) Sentiment analysis of
bengali
comments with word2vec and sentiment
information of
words.
In:
International
conference on electrical,
computer and
communication engineering (ECCE). IEEE, pp 186-190
4.
Alam MH,
Ryu WJ,
Lee
S (2016)
Joint
multi-grain topic
sentiment: modeling semantic aspects for online reviews. Inform
Sci 339:206–223
5.
Appel
O,
Chiclana F,
Carter J,
Fujita H (2018) Successes and
challenges in developing a hybrid approach to sentiment analysis.
Appl Intell 48(5):1176–1188
6.
Bansal B,
Srivastava S (2018) Sentiment classification of online
consumer
reviews using word vector
representations.
Procedia
Computer Science 132:1147–1153
7.
Blei DM, Ng AY, Jordan MI (2003) Latent dirichlet allocation. J
Mach Learn Res 3(Jan):993–1022
8.
Cer
´
on-Guzm
´
an JA,
Le
´
on-Guzm
´
an E (2016) A sentiment
analy-
sis system of spanish tweets and its application in Colombia 2014
presidential election. In: 2016 IEEE international conferences on
big data and cloud computing (BDCloud),
social computing and
networking (socialcom), sustainable computing and communica-
tions (sustaincom)(BDCloud-socialcom-sustaincom), pp 250–257
9.
Chen R,
Xu W (2017)
The determinants
of
online customer
ratings:
a combined domain ontology and topic text
analytics
approach. Electron Commer Res 17(1):31–50
10.
Chen R, Zheng Y, Xu W, Liu M, Wang J (2018) Secondhand seller
reputation in online markets:
a text
analytics framework.
Decis
Support Syst 108:96–106
11.
Dataset (2016) Amazon mobile review dataset. https://www.kaggle.
com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones/
data. Online; Accessed Nov 2017
12.
Giatsoglou M,
Vozalis MG,
Diamantaras K,
Vakali A,
Sarigian-
nidis G,
Chatzisavvas KC (2017) Sentiment
analysis leveraging
emotions and word embeddings. Expert Syst Appl 69:214–224
13.
Hogenboom A, Heerschop B, Frasincar F, Kaymak U, de Jong F
(2014) Multi-lingual support for lexicon-based sentiment analysis
guided by semantics. Decis Support Syst 62:43–53
14.
Hu M, Liu B (2004) Mining opinion features in customer reviews.
In: AAAI, vol 4. pp 755-760
15.
Jiang S,
Lewris J,
Voltmer M,
Wang H (2016) Integrating rich
document
representations for text
classification.
In:
2016 IEEE
systems and information engineering design symposium (SIEDS).
IEEE, 303-308
16.
Jiang Y,
Song X,
Harrison J,
Quegan S,
Maynard D (2017)
Comparing
attitudes
to
climate
change
in
the
media
using
sentiment
analysis
based
on
latent
dirichlet
allocation.
In:
Proceedings
of
the 2017 EMNLP workshop natural
language
processing meets journalism, pp 25–30
17.
Jo Y,
Oh AH (2011)
Aspect
and sentiment
unification model
for online review analysis.
In:
Proceedings of the fourth ACM
international conference on Web search and data mining.
ACM,
pp 815–824
18.
Karami A,
Gangopadhyay A,
Zhou B,
Kharrazi H (2017) Fuzzy
approach topic discovery in health and medical
corpora.
Int
J
Fuzzy Syst 20(4):1334–1345
19.
Karami
A,
Dahl
AA,
Turner-McGrievy G,
Kharrazi
H,
Shaw
G (2018)
Characterizing diabetes,
diet,
exercise,
and obesity
comments on twitter. Int J Inf Manage 38(1):1–6
20.
Kim HK, Kim M (2016) Model-induced term-weighting schemes
for text classification. Appl Intell 45(1):30–43
21.
Koltcov S,
Koltsova O,
Nikolenko S (2014)
Latent
dirichlet
allocation: stability and applications to studies of user-generated
content.
In:
Proceedings of the 2014 ACM conference on web
science. ACM, pp 161-165
22.
Le Q, Mikolov T (2014) Distributed representations of sentences
and documents. In: International conference on machine learning,
pp 1188–1196
23.
Li
G,
Liu F (2014)
Sentiment
analysis
based on clustering:
a
framework in improving accuracy and recognizing neutral
opinions. Appl Intell 40(3):441–452
24.
Lin C,
He Y (2009) Joint
sentiment/topic model
for sentiment
analysis.
In:
Proceedings
of
the
18th
ACM conference
on
Information and knowledge management. ACM, pp 375–384
25.
Liu B (2012) Sentiment analysis and opinion mining.
Synthesis
Lectures On Human Language Technologies 5(1):1–167
26.
Liu B,
Hu M,
Cheng J (2005) Opinion observer:
analyzing and
comparing opinions
on the web.
In:
Proceedings
of
the 14th
international conference on world wide web. ACM, pp 342–351
27.
Liu H (2017)
Sentiment
analysis of
citations using word2vec.
arXiv:170400177
28.
Liu X, Burns AC, Hou Y (2017) An investigation of brand-related
user-generated content on twitter. J Advert 46(2):236–247
Hybrid attribute based sentiment classiﬁcation of online reviews for consumer intelligence
29.
Liu Y,
Jiang C,
Zhao H (2018)
Using contextual
features and
multi-view ensemble learning in product
defect
identification
from online discussion forums. Decis Support Syst 105:1–12
30.
Ma S, Zhang C, He D (2016) Document representation methods
for clustering bilingual documents. Proceedings of the Association
for Information Science and Technology 53(1):1–10
31.
Mei Q, Ling X, Wondra M, Su H, Zhai C (2007) Topic sentiment
mixture: modeling facets and opinions in weblogs. In: Proceedings
of the 16th international conference on world wide web. ACM, pp
171–180
32.
Mikolov T, Chen K, Corrado G, Dean J (2013) Efficient estima-
tion of word representations in vector space. arXiv:13013781
33.
Mikolov T,
Sutskever
I,
Chen K,
Corrado GS,
Dean J (2013)
Distributed
representations
of
words
and
phrases
and
their
compositionality.
In: Advances in neural information processing
systems, pp 3111–3119
34.
Mikolov T,
Wt
Yih,
Zweig G (2013) Linguistic regularities in
continuous space word representations.
In:
Proceedings of
the
2013 conference of the north american chapter of the association
for computational
linguistics:
human language technologies,
pp
746-751
35.
Nielsen F
˚
A (2011) A new anew:
evaluation of a word list
for
sentiment analysis in microblogs. arXiv:11032903
36.
Panichella A,
Dit B,
Oliveto R,
Di Penta M,
Poshynanyk D,
De
Lucia A (2013) How to effectively use topic models for software
engineering tasks? an approach based on genetic algorithms.
In:
2013 35th international
conference on software engineering
(ICSE). IEEE, pp 522–531
37.
Pham DH, Le AC (2017) Learning multiple layers of knowledge
representation for aspect
based sentiment
analysis.
Data Knowl
Eng
38.
Qiang J, Li Y, Yuan Y, Liu W (2018) Snapshot ensembles of non-
negative matrix factorization for stability of topic modeling. Appl
Intell:1–13
39.
Qiao Z, Zhang X, Zhou M, Wang GA, Fan W (2017) A domain
oriented lda
model
for
mining product
defects
from online
customer reviews
40.
Rehurek R.
Gensim.
https://radimrehurek.com/gensim/models/
phrases.html. Last accessed Nov 2017
41.
Rehurek
R,
Sojka
P (2010)
Software
framework
for
topic
modelling with large corpora. In: Proceedings of the LREC 2010
workshop on new challenges for NLP frameworks. Citeseer
42.
Salehan M, Kim DJ (2016) Predicting the performance of online
consumer
reviews:
a sentiment
mining approach to big data
analytics. Decis Support Syst 81:30–40
43.
Sanguansat P (2016) Paragraph2vec-based sentiment analysis on
social
media for business in thailand.
In:
2016 8th international
conference on knowledge and smart technology (KST). IEEE, pp
175–178
44.
Schwenk H (2007) Continuous space language models.
Comput
Speech Lang 21(3):492–518
45.
Spacy https://spacy.io. Last accessed Nov 2017
46.
Tripathy
A,
Agrawal
A,
Rath
SK (2016)
Classification
of
sentiment
reviews
using n-gram machine
learning approach.
Expert Syst Appl 57:117–126
47.
Wang T, Cai Y, Hf Leung, Lau RY, Li Q, Min H (2014) Product
aspect
extraction supervised with online
domain knowledge.
Knowl-Based Syst 71:86–100
48.
Wang W,
Wang H,
Song Y (2017)
Ranking product
aspects
through sentiment analysis of online reviews.
J Exp Theor Artif
Intell 29(2):227–246
49.
Wang Z,
Gu S,
Xu X (2018) Gslda: lda-based group spamming
detection in product reviews. Appl Intell 48(9):3094–3107
50.
Xianghua F, Guo L, Yanyan G, Zhiqiang W (2013) Multi-aspect
sentiment analysis for chinese online social reviews based on topic
modeling and hownet lexicon. Knowl-Based Syst 37:186–195
51.
Xin Y, Yang J, Xie ZQ, Zhang JP (2015) An overlapping semantic
community detection algorithm base on the arts multiple sampling
models. Expert Syst Appl 42(7):3420–3432
52.
Yan X, Guo J, Lan Y, Cheng X (2013) A biterm topic model for
short texts.
In: Proceedings of the 22nd international conference
on world wide web. ACM, pp 1445–1456
53.
Yao Y,
Li
X,
Liu X,
Liu P,
Liang Z,
Zhang J,
Mai
K (2017)
Sensing spatial
distribution of
urban land use by integrating
points-of-interest and google word2vec model. Int J Geogr Inf Sci
31(4):825–848
54.
Yu D,
Mu Y,
Jin Y (2017) Rating prediction using review texts
with underlying sentiments. Inf Process Lett 117:10–18
55.
Zainuddin N,
Selamat
A,
Ibrahim R (2017)
Hybrid sentiment
classification on twitter
aspect-based sentiment
analysis.
Appl
Intell 48(5):1–15
56.
Zhang D, Xu H, Su Z, Xu Y (2015) Chinese comments sentiment
classification based on word2vec and svmperf. Expert Syst Appl
42(4):1857–1863
57.
Zirn C, Stuckenschmidt H (2014) Multidimensional topic analysis
in political texts. Data Knowl Eng 90:38–53
Barkha Bansal
is a doctoral
student
in
Department
of
Applied Sciences at NorthCap
University,
Gurugram.
She
obtained
Master
of
Science
in Applied Mathematics from
NorthCap
University.
She
currently works
in the
area
of
text
mining,
sentiment
analysis
and
their
applica-
tions.
She may be reached at
barkha.bansal20@yahoo.com.
Dr
Sangeet
Srivastava
is
assistant
professor in Depart-
ment
of
Applied
Sciences
at
NorthCap
University.
He
received
his
doctorate
from
IIT,
Delhi
and Post-doctorate
from NTU,
Taiwan.
He held
visiting scientist
position
at
Meteo-France,
Toulouse from
2005-2006. His area of exper-
tise
includes
Data
Mining,
Predictive
Analytics,
Image
Forensics
&
Mathematical
Modelling.
He
is
co-author
of
1 Book,
4 Book chapters,
more than 10 research publi-
cations and currently working on other projects. He can be reached at
sangeetsrivastava@ncuindia.edu.
