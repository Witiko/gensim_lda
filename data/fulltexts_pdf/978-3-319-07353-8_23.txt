M.K. Kundu et al. (eds.), Advanced Computing, Networking and Informatics - Volume 1, 
Smart Innovation, Systems and Technologies 27, 
195 
DOI: 10.1007/978-3-319-07353-8_23, © Springer International Publishing Switzerland 2014 
Efficient Approach for Near Duplicate Document 
Detection Using Textual and Conceptual 
Based Techniques 
Rajendra Kumar Roul, Sahil Mittal, and Pravin Joshi 
BITS Pilani K. K. Birla Goa Campus, Zuarinagar, Goa-403726, India 
rkroul@goa.bits-pilani.ac.in, 
{sahilindia33,pravinjoshi95}@gmail.com 
Abstract. With the rapid development and usage of World Wide Web, there are 
a huge number of duplicate web pages. To help the search engine for providing 
results free from duplicates, detection and elimination of duplicates is required. 
The proposed approach combines the strength of some "state of the art" 
duplicate detection algorithms like Shingling and Simhash to efficiently detect 
and eliminate near duplicate web pages while considering some important 
factors like word order. In addition, it employs Latent Semantic Indexing (LSI) 
to detect conceptually similar documents which are often not detected by 
textual based duplicate detection techniques like Shingling and Simhash. The 
approach utilizes hamming distance and cosine similarity (for textual and 
conceptual duplicate detection respectively) between two documents as their 
similarity 
measure. 
For 
performance 
measurement, 
the 
F-measure 
of 
the 
proposed 
approach 
is 
compared 
with 
the 
traditional 
Simhash 
technique. 
Experimental results show that our approach can outperform the traditional 
Simhash. 
Keywords: F-measure, LSI, Shingling, Simhash, TF-IDF. 
1
Introduction 
The near duplicates not only appear in web search but also in other contexts, such as 
news 
articles. 
The 
presence 
of 
near 
duplicates 
has 
a 
negative 
impact 
on 
both 
efficiency 
and 
effectiveness 
of 
search 
engines. 
Efficiency 
is 
adversely 
affected 
because they increase the space needed to store indexes, ultimately slowing down the 
access time. Effectiveness is hindered due to the retrieval of redundant documents. 
For designing robust and efficient information retrieval system, it is necessary to 
identify and eliminate duplicates. 
Two documents are said to be near duplicates if they are highly similar to each 
other [3]. Here the notion of syntactic similarity and semantic similarity between two 
documents has to be carefully considered. A set of syntactically similar documents 
may not necessarily give positive result when tested for semantic similarity and vice 
versa. So two independent strategies have to be employed to detect and eliminate 
syntactically and semantically similar documents. Most of the traditional duplicate 
196 
R.K. Roul, S. Mittal, and P. Joshi 
detection algorithms [1],[2] have considered only the aspect of syntactic similarity. 
Here, apart from performing textual near duplicate detection, an approach to detect 
and eliminate semantically (conceptually) similar documents has been proposed. 
Another aspect of duplicate detection algorithms is the precision-recall trade off. 
Often an algorithm based on mere presence or absence of tokens in documents to be 
compared 
performs 
low 
on 
precision 
but 
yields 
a 
high 
recall 
considering 
only 
textually similar documents. For example if shingling [1] is implemented with one as 
the shingle size, it yields high recall but low precision. On the other hand, when 
techniques based on co-occurrence of words in document are used, taking into 
account even the order of words, its precision increases by a reasonable amount but its 
recall decreases. Thus, it is essential to give a right amount of importance to co-
occurrence of words in documents but at the same time taking care of the recall. In the 
proposed approach, F-measure [9] has been used as a performance measure to give 
optimum precision-recall combination. 
The outline of the paper is as follows: Section 2reviews previous research work on 
detection of duplicate documents.Section 3 proposesthe approach to effectively detect 
near duplicate documents. Section 4demonstrates the experimental results which are 
followed by conclusion and future work covered in Section 5. 
2
Related Work 
One 
of 
the 
major 
difficulties 
for 
developing 
approach 
to 
detect 
near 
duplicate 
documents was the representation of documents. Broder [1] proposed an elegant 
solution to this by representing a document as a set of Shingles. The notion of 
similarity between two documents as the ratio of number of unique Shingles common 
to both the documents to the total number of unique Shingles in both the documents 
was defined as resemblance. The approach was brute force in nature and thus not 
practical. Charikar [2] proposed another approach which required very low storage as 
compared to the Shingling. The documentswere represented as a short string of 
binaries (usually 64 bits) which is called fingerprint.Documents are then compared 
using this fingerprint which is calculated using hash values. Both of the above 
approaches became the "state of the art” approaches for detection of near duplicate 
documents. Henzinger [3] found that none of these approaches worked well for 
detecting near duplicates on the same site. Thus, they proposed a combined approach 
which had a 
high precision and recall and also overcame the shortcomings of 
Shingling and Simhash. Bingfeng Pi et al. [6] worked on impact of short size on 
duplicate detection and devised an approach using Simhash to detect near duplicates 
in a corpus of over 500 thousand short messages. Sun, Qin et al. [5] proposed a model 
for near duplicate detection which took query time into consideration. They proposed 
a 
document 
signature 
selection 
algorithm 
and 
claimed 
that 
it 
outperformed 
Winnowing – which is one of the widely used signature selection algorithm.Zhang et 
al. [7] proposed a novel approach to detect near duplicate web pages which was to 
work with a web crawler. It used semi-structured contents of the web pages to crawl 
and detect near duplicates. Figuerola et al. [8] suggested the use of fuzzy hashing to 
generate the fingerprints of the web documents. These fingerprints were then used to 
estimate the similarity between two documents. Manku et al. [4] demonstrated 
practicality of using Simhash to identify near duplicates in a corpus containing multi-
billion 
documents. 
They 
also 
showed 
that 
fingerprints 
with 
length 
64 
bits 
are 
