Chapter 11
Does Topic Modelling Reflect Semantic
Prototypes?
Michał Korzycki and Wojciech Korczy´
nski
Abstract.
The chapter introduces a representation of
a textual
event
as a mix-
ture of semantic stereotypes and factual information. We also present a method to
distinguish semantic prototypes that are specific for a given event from generic ele-
ments that might provide cause and result information. Moreover, this chapter dis-
cusses the results of experiments of unsupervised topic extraction performed on
documents from a large-scale corpus with an additional temporal structure. These
experiments were realized as a comparison of the nature of information provided
by Latent
Dirichlet
Allocation based on Log-Entropy weights and Vector Space
modelling. The impact of different corpus time windows on this information is dis-
cussed.
Finally,
we try to answer if the unsupervised topic modelling may reflect
deeper semantic information, such as elements describing given event or its causes
and results, and discern it from factual data.
11.1
Introduction
Unsupervised probabilistic topic modelling is widely used in the information re-
trieval techniques and is a widely applied tool for analysis of large-scale corpora. It
is based on the assumption that text documents are mixtures of topics. This mixture
may be treated as a multinomial probability distribution over the words. A couple
of methods have been developed that allow to create such distributions with various
properties [15, 6, 2].
Although useful for information retrieval purposes, those topic extraction meth-
ods do not deal well with many linguistic issues such as polysemy or synonymy,
since recent researches tend to show that deeper semantical relations found in texts
cannot be directly retrieved using those unsupervised methods [5, 17].
Michał Korzycki
·
Wojciech Korczy´
nski
AGH University of Science and Technology
Al. Mickiewicza 30, 30-962, Krakow, Poland
e-mail:
{
korzycki,wojciech.korczynski
}
@agh.edu.pl
c

Springer International Publishing Switzerland 2015
113
A. Zgrzywa et al. (eds.), New Research in Multimedia and Internet Systems,
Advances in Intelligent Systems and Computing 314, DOI: 10.1007/978-3-319-10383-9
_
11
114
M. Korzycki and W. Korczy´
nski
In this chapter we present the results of a preliminary work that allowed us to
extract deeper semantic information from the text through postprocessing the re-
sults of unsupervised methods. We took advantage of the fact that we operated on
a large-scale corpus (of the order of millions of documents) that possesses an ad-
ditional temporal structure. The suggested approach permits to find in an analyzed
text elements related to the specific semantic prototypes (e.g. mental models) of the
events described within and to discern them from pure factual information, given
that a sufficiently large corpus is available.
The framework of the experiments presented in this chapter is an integral part of
a large scale project related to security and intelligence analysis.
11.2
Related Work
Latent Semantic Analysis (LSA) is an original word/document matrix rank reduc-
tion algorithm,
which extracts word co-occurrences in the frame of a text.
As a
result, each word in the corpus is related to all co-occurring words and to all texts
in which it occurs. This makes a base for an associative text comparison. The appli-
cability of the LSA algorithm is a subject of various types of research – from a text
content comparison [3] to an analysis of human association norm [12]. But there is
still little interest in studying the linguistic significance of LSA-made associations.
Latent Dirichlet Allocation (LDA) is one of the most known generative models
used for topic extraction.
It
was first
presented by David Blei,
Andrew Ng,
and
Michael Jordan in 2003. As the other generative models, it assumes that a collection
of documents may be represented by a mixture of latent
topics,
however words
creating each topic are chosen according to the per-document multinomial Dirichlet
distribution of fixed dimensionality. LDA is a technique based on the ,,bag of words”
paradigm and it can infer distributions of topics e.g. with the use of variational Bayes
approximation [1, 16], Gibbs sampling [6] or expectation propagation [11].
Some recent research was focusing on finding if the relationships coming from
the unsupervised topic extraction methods reflect
semantic relationship reflected
in human association norms. A comparison of human association norm and LSA-
made association lists can be found in [5] and it should be the base of the study.
Results of the other preliminary studies based on such a comparison: [17, 19, 18],
show that the problem needs further investigation. It is worth noticing that all the
types of research referred to, used a stimulus-response association strength to make
a comparison.
The results of the afformentioned research have shown that
using
unsupervised topic extraction methods one is able to create associations between
words that are strongly divergent from the ones obtained by analysing the human
generated associations.
Fully aware of the issues related to the automatic text classification mentioned
above, by using different approaches and postprocessing methods, we show that it
is possible to obtain results more in line with what could be called a semantic text
classification.
