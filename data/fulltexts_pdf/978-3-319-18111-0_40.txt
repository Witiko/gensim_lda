© Springer International Publishing Switzerland 2015 
A. Gelbukh (Ed.): CICLing 2015, Part I, LNCS 9041, pp. 534–544, 2015. 
DOI: 10.1007/978-3-319-18111-0_40 
Mining Parallel Resources for Machine Translation 
from Comparable Corpora 
Santanu Pal
1
, Partha Pakray
2
, Alexander Gelbukh
3
,
and Josef van Genabith
1
1 
Universität Des Saarlandes, Saarbrucken, Germany 
2 
National Institute of Technology, Mizoram, India 
3 
Centro de Investigación en Computación, Instituto Politécnico Nacional, Mexico 
{santanu.pal,josef.vangenabith}@uni-saarland.de, 
www.parthapakray.com,
www.gelbukh.com
Abstract. 
Good 
performance 
of 
Statistical 
Machine 
Translation 
(SMT) 
is 
usually achieved with huge parallel bilingual training corpora, because the 
translations 
of 
words 
or 
phrases 
are 
computed 
basing 
on 
bilingual 
data. 
However, in case of low-resource language pairs such as English-Bengali, the 
performance 
is 
affected 
by 
insufficient 
amount 
of 
bilingual training 
data. 
Recently, comparable corpora became widely considered as valuable resources 
for 
machine 
translation. 
Though 
very 
few 
cases 
of 
sub-sentential 
level 
parallelism 
are 
found 
between 
two 
comparable 
documents, 
there 
are 
still 
potential parallel phrases in comparable corpora. Mining parallel data from 
comparable corpora is a promising approach to collect more parallel training 
data for SMT. In this paper, we propose an automatic alignment of English-
Bengali comparable sentences from comparable documents. We use a novel 
textual 
entailment 
method 
and 
distributional 
semantics 
for 
text 
similarity. 
Subsequently, we apply template-based phrase extraction technique to aligned 
parallel phrases from comparable sentence pairs. The effectiveness of our 
approach 
is 
demonstrated 
by 
using 
parallel 
phrases 
as 
additional 
training 
examples 
for 
an 
English-Bengali 
phrase-based 
SMT 
system. 
Our 
system 
achieves 
significant 
improvement 
in 
terms 
of 
translation 
quality 
over 
the 
baseline system. 
1
Introduction 
Many natural language processing tasks such as corpus-based machine translation 
(MT) heavily rely on bilingual parallel corpora. Statistical Machine Translation 
(SMT) is a kind of corpus-based MT based on probabilistic translation models. The 
model is learned from sentence-aligned parallel corpora. However, a major problem 
of SMT is scarcity of available parallel data. Many language pairs, such as English to 
Indian languages, suffer from the scarcity of parallel data. 
Comparable corpora provide a possible solution to this data scarceness problem to 
some extent. Comparable documents are not strictly parallel: the corpus consists of 
bilingual documents but they are not sentence-aligned; more precisely, they are rough 
translations 
of 
each 
other. 
The 
sentences 
of 
comparable 
corpora 
are 
not 
really 
Mining Parallel Resources for Machine Translation from Comparable Corpora 
535 
translations, but they convey the same information and hence there must exist some 
sentential or sub-sentential level of parallelism. 
Recently, comparable corpora are considered as a valuable resource for acquiring 
parallel data, which can play an important role in improving the quality of machine 
translation (MT) (Smith et al. 2010). The extracted parallel texts from comparable 
corpora are typically added with the training corpus as additional training material 
that is expected to improve performance of SMT systems, specifically for low-density 
language pairs. 
In this paper, we describe a methodology for extracting English-Bengali parallel 
resource from comparable corpora. We did it in three steps. At the first step, we 
clustered the both side of bilingual comparable corpus into several groups using a 
textual entailment (TE) method. At the second step, we established cross-lingual link 
between the groups using n-best list of probabilistic bilingual lexicon. The bilingual 
lexicons are extracted from bilingual training data of the same domain by using a 
statistical word alignment tool GIZA++. At the final step, we used a template-based 
phrase extraction technique between each aligned groups. The extracted phrases were 
aligned using a baseline PB-SMT system, which was trained on the same-domain 
English-Bengali parallel corpus. 
We collected document-aligned comparable corpus of English-Bengali document 
pairs from Wikipedia, which provides a huge collection of documents in many 
different languages. 
Typically, there are two approaches can be applied for grouping corpus according 
to their text similarity: textual entailment and semantic textual similarity. Textual 
Entailment is defined by Dagan et al. (2004) as follows: text T is said to entail 
hypothesis H if H can be inferred from T. The task of textual entailment is to decide 
whether the meaning of H can be inferred from the meaning of T. For example, the 
text T = “John’s assassin is in jail” entails the hypothesis H = “John is dead”; indeed, 
if there exists one’s assassin, then this person is dead. However, T = “Mary lives in 
Europe” does not entail H = “Mary lives in US”. 
On the other hand, Semantic Textual Similarity (STS)
1
task measures the degree of 
semantic equivalence between a pair of texts, e.g. sentences. Four STS evaluation 
tasks were organized in 2012, 2013, 2014, and 2015 at SemEval workshops. STS is 
related to both Textual Entailment (TE) and paraphrasing, but differs in a number of 
ways and it is more directly applicable to a number of NLP tasks. STS is different 
from TE inasmuch as it assumes bidirectional graded equivalence between the pair of 
textual snippets. In case of TE the equivalence is directional, e.g. a car is a vehicle, 
but a vehicle is not necessarily a car. STS also differs from both TE and Paraphrase in 
that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), STS is a 
graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a 
car). This graded bidirectional nature of STS is useful for NLP tasks such as MT 
evaluation, 
information 
extraction, 
question 
answering, 
and 
summarization. 
The 
Textual Entailment system is unidirectional but Semantic Textual Similarity is mainly 
bidirectional. Therefore, we will also use Sematic Textual Similarity technique also. 
1
http://ixa2.si.ehu.es/stswiki/index.php/Main_Page 
