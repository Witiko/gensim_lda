Comparing Semantic Models for Evaluating
Automatic Document Summarization
Michal Campr
(
B
)
and Karel Jeˇzek
Department of Computer Science and Engineering, FAS,
University of West Bohemia, Univerzitn 8,
306 14 Pilsen, Czech Republic
{mcampr,jezek ka}@kiv.zcu.cz
Abstract.
The main focus of this paper is the examination of semantic
modelling in the context of automatic document summarization and its
evaluation.
The main area of our research is extractive summarization,
more speciﬁcally, contrastive opinion summarization. And as it is with all
summarization tasks,
the evaluation of their performance is a challeng-
ing problem on its own. Nowadays, the most commonly used evaluation
technique is ROUGE (Recall-Oriented Understudy for Gisting Evalua-
tion).
It includes measures (such as the count of
overlapping n-grams
or word sequences) for automatically determining the quality of
sum-
maries by comparing them to ideal
human-made summaries.
However,
these measures do not take into account the semantics of words and thus,
for example,
synonyms are not treated as equal.
We explore this issue
by experimenting with various language models, examining their perfor-
mance in the task of computing document similarity.
In particular,
we
chose four semantic models (LSA,
LDA,
Word2Vec and Doc2Vec) and
one frequency-based model
(TfIdf),
for extracting document features.
The experiments were then performed on our custom dataset and the
results of each model are then compared to the similarity values assessed
by human annotators.
We also compare these values with the ROUGE
scores and observe the correlations between them. The aim of our exper-
iments is to ﬁnd a model,
which can best imitate a human estimate of
document similarity.
Keywords:
Contrastive
opinion
summarization
·
Summarization
evaluation
·
ROUGE
·
Semantic
models
·
TfIdf
·
LSA
·
LDA
·
Word2Vec
·
Doc2Vec
·
Document similarity
1
Introduction
In recent years, with rapid growth of information available online, the research
area of automatic summarization has been attracting very much attention. Auto-
matic document summarization aims to transform an input text into a condensed
form, in order to present the most important information to the user. Summa-
rization is a very challenging problem, because the algorithm needs to understand
the text and this requires some form of semantic analysis and grouping of the
c
 Springer International Publishing Switzerland 2015
P. Kr´al and V. Matouˇsek (Eds.): TSD 2015, LNAI 9302, pp. 252–260, 2015.
DOI: 10.1007/978-3-319-24033-6 29
Semantic Models for Summarization Evaluation
253
content using world knowledge. Therefore, attempts at performing true abstrac-
tion (generating the summary from scratch) have not been very successful
so
far. Fortunately, an approximation called extraction exists and is more feasible
for the vast majority of current summarization systems,
which simply need to
identify the most important passages of
the text to produce an extract.
The
output text is often not coherent but the reader can still form an opinion of the
original content.
A very challenging problem, which arises, is the evaluation of summarization
quality.
There are dozens of possible ways for the evaluation of summarization
systems,
and these methods can be classiﬁed basically into two categories [1].
Extrinsic
techniques judge the summary quality on the basis of
how helpful
summaries are for a given task, such as classiﬁcation or searching. On the other
hand,
intrinsic
evaluation is directly based on analysis of the summary,
which
can involve a comparison with the source document, measuring how many main
ideas from it are covered by the summary,
or with an abstract written by a
human.
Recently,
one particular method has become very popular for the evalua-
tion of automatic summarization. ROUGE [2] (Recall-Oriented Understudy for
Gisting Evaluation) includes measures for automatically determining the qual-
ity of system summaries by comparing them to ideal
human-made summaries.
These measures count the number of overlapping units such as n-grams,
word
sequences, or word pairs between the system summary and the ideal summaries
created by humans.
Since the evaluation of automatic summarization is based on a comparison
between the system summary and a human-made one, we wondered if it is possi-
ble to utilize other NLP (Natural Language Processing) methods for evaluating
the system summaries. In this paper, we examine some of the most popular NLP
models and their performances in the task of assessing document similarity.
This paper ﬁrstly describes, what data we used for evaluating these models
and how we annotated them.
Then,
we describe the models which we used in
our experiments,
each in its own section.
Lastly,
we provide the results and
performances of
chosen models in computing document similarities,
and their
comparison to human annotators.
2
Dataset
Our current research is focused on a speciﬁc variation of automatic summariza-
tion: contrastive opinion summarization. The main goal is to analyze the input
documents,
in our case restaurant reviews,
and construct two summaries,
one
depicting the most important positive information and the other providing nega-
tive information. For this task, we constructed a collection from czech restaurant
reviews downloaded from www.fajnsmekr.cz,
in total
of
6008 reviews for 1242
restaurants.
For human annotation,
however,
this is too much,
so we manually
selected 50 restaurants, each with several reviews, so that their combined length
is at least 1000 words.
Three annotators then independently created two sum-
maries for each restaurant, each with approximately 100 words.
