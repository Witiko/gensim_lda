Named Entity Recognition Through
Learning from Experts
Martin Andrews
Abstract
Named Entity Recognition (NER) is a foundational technology for sys-
tems designed to process Natural
Language documents.
However,
many existing
state-of-the-art systems are difficult to integrate into commercial settings (due their
monolithic construction, licensing constraints, or need for corpuses, for example).
In this work, a new NER system is described that uses the output of existing systems
over large corpuses as its training set,
ultimately enabling labelling with (i)better
F1 scores; (ii)higher labelling speeds; and (iii)no further dependence on the external
software.
Keywords
Named Entity Recognition
·
NER
·
Natural
Language Processing
·
NLP
·
Recurrent Neural Network
·
RNN
·
Unstructured data
1
Introduction
One key capability required of natural language processing (NLP) systems is to be
able to identify the people, organisations and locations mentioned in a given text.
These labels (plus further categories that include times, dates, and numeric quantities,
for instance) are essential for understanding the facts described, yet they do not per
se add much to the linguistic structure of the text. Therefore, building systems that
can reliably perform this Named Entity Recognition (NER) has been a focus of
NLP research, since it is an essential stepping-stone to exploring the other linguistic
content in unstructured text.
M. Andrews
(
B
)
Red Cat Labs, Singapore, Singapore
e-mail: Martin.Andrews@RedCatLabs.com
http://www.RedCatLabs.com
© Springer International Publishing Switzerland 2016
K. Lavangnananda et al. (eds.), Intelligent and Evolutionary Systems,
Proceedings in Adaptation, Learning and Optimization 5,
DOI: 10.1007/978-3-319-27000-5_23
281
282
M. Andrews
Unfortunately, while the NER task might be considered largely conquered from
a linguistic research viewpoint, building an effective system is still a challenge in a
commercial setting:
1.
Licenses for many existing academic systems are not conducive to being em-
bedded within commercial systems
2.
Often, existing codebases focus on ‘tweaks’ rather than solid engineering
3.
Commercial
systems may have particular task-specific requirements that
are
difficult to implement on a pre-built system
4.
Training corpuses can be a limiting factor, since commercial uses focus on spe-
cific domains of interest, rather than domains that have well understood corpuses
already available
This work describes an NER system that can be trained from the output of ‘known
good’ systems.
Since the system developed here only requires large volumes of
(machine) annotated text, it essentially sidesteps several of the problems that these
existing systems have in commercial settings.
Moreover, the experiments show that the new system can learn to be better than
its teachers - both in the test scores obtained and labelling speed.
Importantly, the results obtained during training and testing are described here in
full - the models have not been cherry-picked and tweaked for publication - which
illustrates the robustness of this type of model and training process.
2
Model
2.1
Vocabulary Building
As described below, the CoNLL-2003 [1] NER datasets were chosen as the test-bed
for this work, and the unlabelled ‘Large Corpus’ was used to build the vocabulary
and word-embedding features.
A vocabulary was built from the contents of the whole Large Corpus (there were
484k distinct tokens in the 1.0Gb corpus) with the following additional tokenization
steps taken prior to insertion into the dictionary:
1.
Convert to lower case
2.
Replace each string of digits within the token with
NUMBER
(so that, for instance,
‘12.3456’ becomes ‘
NUMBER
.
NUMBER
’)
2.2
Word Embedding Layer
Skip-gram embeddings of size 100 were pre-trained over the whole large corpus and
vocabulary using
word2vec
[2] as provided by the Python package
gensim
[3]
(this required only 15 minutes of wall-clock time).
The token embedding was filtered so that only tokens with 10 mentions or more
were included, yielding an effective vocabulary size of 118,695 distinct tokens. To
