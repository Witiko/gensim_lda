Application of LSTM Neural Networks in Language
Modelling
Daniel Soutner and Ludˇ
ek M¨
uller
University of West Bohemia, Faculty of Applied Sciences, Department of Cybernetics,
Univerzitn´ı 22, Plzeˇ
n, Czech Rep.
{dsoutner,muller}@kky.zcu.cz
www.kky.zcu.cz
Abstract.
Artificial neural networks have become state-of-the-art in the task of
language modelling on a small corpora. While feed-forward networks are able to
take into account only a fixed context length to predict the next word, recurrent
neural networks (RNN) can take advantage of all previous words. Due the diffi-
culties in training of RNN, the way could be in using Long Short Term Memory
(LSTM) neural network architecture.
In this work, we show an application of LSTM network with extensions on a
language modelling task with Czech spontaneous phone calls. Experiments show
considerable improvements in perplexity and WER on recognition system over
n
-gram baseline.
Keywords:
language modelling,
recurrent
neural
networks,
LSTM neural
net-
works.
1
Introduction
Statistical language models (LM) play an important role in the state-of-art large vocab-
ulary continuous speech recognition (LVCSR) systems. Statistically computed
n
-gram
models and class-based LMs are the main models used in LVCSR systems,
however,
subsequent models are becoming more important supplement to existing techniques.
In recent years feed forward neural networks (FFNN) [12] attracted attention due
their ability to overcome biggest disadvantage of
n
-gram models: even when the
n
-
gram is not observed in training, FFNN estimates probabilities of the word based on the
full history [15]. That is in contrast to
n
-gram, where back-off model estimates unseen
n
-grams with
(
n
−
1)
-gram.
To avoid handling with the parameter
n
(number of words in
n
-gram and in FNN
LM) we can use the recurrent neural
network (RNN) architecture [2].
The RNN is
going further in model generalization: instead of considering only the several previous
words (parameter
n
) the recursive weights are assumed to represent short term memory.
More in general we could say that RNN sees text as a signal consisting of words.
Long Short-Term Memory (LSTM) neural
network [8] is different
type of RNN
structure. As was shown, this structure allows to discover both long and short patterns
in data and eliminates the problem of vanishing gradient by training RNN. LSTM ap-
proved themselves in various applications [8][1] and it
seems to be very promising
course also for the field of language modelling [3].
I. Habernal and V. Matousek (Eds.): TSD 2013, LNAI 8082, pp. 105–112, 2013.
c

Springer-Verlag Berlin Heidelberg 2013
106
D. Soutner and L. M¨
uller
In this work we present an application of LSTM language model as an extension
to the basic
n
-gram model and the influence of this modification to the perplexity and
word error rate analysed on English and Czech corpora.
2
LSTM Neural Networks
The vanishing gradient seems to be problematic during the training of RNN as shown
in [8]. This led authors to re-design of the network unit, in LSTM called as a cell. Fig. 1
shows that every LSTM cell contains gates that determine when the input is significant
enough to remember,
when it
should continue to remember or forget the value,
and
when it should output the value. So designed cells may be interpreted as a differentiable
memory.

 
 











 
Fig. 1.
LSTM memory cell with gates
2.1
LSTM Topology
Typical NN unit consists of the input activation which is transformed to output activa-
tion with activation function (usually sigmoidal).
The LSTM cell
provides this more comprehensively: The three cell
inputs called
gates determine when values are allowed to flow into or out of the block’s memory.
Firstly, the activation function is applied to all gates. When input gate outputs a value
close to zero, it zeros out the value from the net input, effectively blocking that value
from entering into the next layer. When forget gate outputs a value close to zero, the
block will effectively forget whatever value it was remembering. The output gate deter-
mines when the unit should output the value in its memory.
Depends on type of LSTM,
consecutions may slightly differ (some modifications
and enhancements were introduced), but the main principals are the same. The training
algorithm and complete equations of LSTM neural network could be found e.g. in [8]
[9].
