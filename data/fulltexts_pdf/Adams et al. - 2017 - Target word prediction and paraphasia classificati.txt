Proceedings of the BioNLP 2017 workshop, pages 1–8,
Vancouver, Canada, August 4, 2017.
c
2017 Association for Computational Linguistics
Target word prediction and paraphasia classification in spoken discourse
Joel Adams
1
,
Steven Bedrick
1
,
Gerasimos Fergadiotis
2
,
Kyle Gorman
3
and
Jan van Santen
1
1
Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
2
Speech & Hearing Sciences Department, Portland State University, Portland, OR
3
Google, Inc., New York, NY
Abstract
We present
a
system for
automatically
detecting and classifying phonologically
anomalous productions in the speech of
individuals with aphasia.
Working from
transcribed discourse samples, our system
identifies neologisms, and uses a combina-
tion of string alignment and language mod-
els to produce a lattice of plausible words
that the speaker may have intended to pro-
duce.
We then score this lattice accord-
ing to various features, and attempt to de-
termine whether the anomalous production
represented a phonemic error or a genuine
neologism. This approach has the potential
to be expanded to consider other types of
paraphasic errors, and could be applied to
a wide variety of screening and therapeutic
applications.
1
Introduction
Aphasia is an acquired neurogenic language dis-
order in which an individual’s ability to produce
or comprehend language is compromised.
It can
be caused by a number
of
different
underlying
pathologies,
but
can generally be traced back to
physical damage to the individual’s brain:
tissue
damage following ischemic or hemorrhagic stroke,
lesions caused by a traumatic brain injury or infec-
tion, etc. It can also be associated with various neu-
rodegenerative diseases, as in the case of Primary
Progressive Aphasia.
According to the National
Institute of Neurological Disorders and Stroke, ap-
proximately 1,000,000 people in the United States
suffer from aphasia, and aphasia is a common con-
sequence of strokes (prevalence estimates for apha-
sia among stroke patients vary, but are typically in
the neighborhood of 30% (Engelter et al., 2006)).
Anomia
is
a the inability to access
and re-
trieve words during language production, and is a
common manifestation of aphasia (Goodglass and
Wingfield,
1997).
An anomic individual will ex-
perience difficulty producing words and naming
items,
which can cause substantial difficulties in
day-to-day communication.
The process of screening for,
diagnosing,
and
assessing anomia is typically manual
in nature,
and requires substantial
time,
labor,
and exper-
tise.
Compared to other neuropsychological
as-
sessment instruments, aphasia-related assessments
are particularly difficult
to computerize,
as they
typically depend on subtle and complex linguis-
tic judgments about the phonological and semantic
similarity of words, and also require the examiner
to interpret phonologically disordered speech. Fur-
thermore, the most commonly used assessments fo-
cus for practical reasons on relatively constrained
tasks such as picture naming, which may lack eco-
logical validity (Mayer and Murray, 2003).
In this work,
we describe an approach to au-
tomatically detecting and analyzing certain cate-
gories of word production errors characteristic of
anomia in connected speech.
Our approach is a
first step towards an automated anomia assessment
tool
that
could be used cost
effectively in both
clinical and research settings,
1
and could also be
applied to other
disorders of
speech production.
The method we propose uses statistical language
models to identify possible errors, and employs a
phonologically-informed edit distance model to de-
termine phonological similarity between the sub-
ject’s utterance and a set
of plausible “intended
words.”
We then apply machine learning tech-
niques to determine which of several
categories
a given erroneous production may fall
into.
We
1
As in the computer-administered (but manually-scored)
assessments developed by Fergadiotis and colleagues (Ferga-
diotis et al., 2015; Hula et al., 2015).
1
show results on intrinsic evaluations comparable
to state-of-the-art sentence completion, as well as
an extrinsic measure of classification well above a
“most-frequent” baseline strategy.
1.1
Anomia and Paraphasias
Anomia can take several different forms, but in this
work we are concerned with
paraphasias
,
which
are unintended errors in word production.
2
There are several categories of paraphasic error.
Semantic errors
arise when an individual uninten-
tionally produces a semantically-related word to
their original, intended word (their “target word”).
A classic semantic error
would be saying “cat”
when one intended to say “dog.”
Phonemic
(sometimes called “formal”)
errors
occur
when the speaker
produces
an unrelated
word that
is
phonemically related
to their target:
“mat” for “cat”, for example.
It is also possible for
an erroneous production to be
mixed
, that is both
semantically and phonemically related to the tar-
get word: “rat” for “cat.” Individuals with anomia
also produce
unrelated
errors,
which are words
that
are neither semantically or phonemically re-
lated to their intended target
word:
for example,
producing “skis” instead of “zipper.”
Each of these categories shares the commonal-
ity that the word produced by the individual is a
“real” word.
There is another family of anomic er-
rors,
neologisms
, in which the individual produces
non-word
productions.
A neologistic production
may be phonemically related to the target, but con-
taining phonological errors: “[dɑɪnoʊsɔɹ]” for “di-
nosaur.” These are often referred to as
phonologi-
cal
paraphasias.
Alternatively, the individual may
produce
abstruse neologisms
,
in which the pro-
duced phonemes bear no discernable similarity to
any “real” lexical item (“[æpməl]” for “comb”
3
).
The present work focuses exclusively on neol-
ogisms,
both of the phonological variety as well
as the abstruse variety.
However, our fundamental
approach can be extended to include other forms,
2
Note that
individuals
without
any sort
of language dis-
order
do occasionally produce errors in their
speech;
this
fact has led to a truly shocking amount of study by linguists.
Frisch & Wright (2002) provide a reasonable overview of the
background and phonology of the phenomenon.
3
This example was taken from a corpus of responses to a
confrontation naming test (Mirman et al., 2010), in which the
subject is shown a picture and required to name its contents.
As such,
in the case of this specific error,
we have
a priori
knowledge of what the target word “should” have been.
Ob-
viously,
in a more naturalistic task or setting,
we would not
have this advantage.
as described in section 6.
Typical methods of diagnosing, staging, and oth-
erwise characterizing anomia involve determining
the number and kinds of paraphasias produced by
an individual
while undergoing some structured
language elicitation process,
for example a con-
frontation naming test (see (Kendall et al.,
2013)
and (Brookshire et al., 2014) for examples of such
a study). As alluded to previously, producing these
counts and classifications is a complex and labori-
ous process.
Furthermore,
it
is also often an in-
herently subjective process:
are “carrot” and “ba-
nana” semantically related?
What
about
“hose”
and “rope”?
Reliability estimates of
expert
human perfor-
mance at
paraphasia classification in confronta-
tion naming scenarios reflect the difficulty in this
task. One recent study reported a kappa-equivalent
score of 0.76 — a score that that is certainly ac-
ceptable, but that leaves much room for disagree-
ment
on the status of specific erroneous produc-
tions (Minkina et al., 2015).
Other reported scores
fall in a similar range (Kristensson et al., 2015), in-
cluding when the productions are from neurotyp-
ical individuals (Nicholas et al., 1989).
Automat-
ing this aspect of the task would not only improve
efficiency,
but
would also decrease scoring vari-
ability.
Having a reliable,
automated method to
analyze paraphasic errors would also expand the
scope of what is currently possible in terms of as-
sessment methodologies.
Notably, the approach we outline in this paper is
explicitly designed to work on samples of natural,
connected speech. It builds upon previous work by
Fergadiotis et al. (2016) on automated analysis of
errors produced in confrontation naming tests, and
extends it into the realm of naturalistic discourse.
It is our hope that,
by enabling automated calcu-
lation of error frequencies and types on narrative
speech, we might make using such material far eas-
ier in practice than it is today.
2
Data
For this work, we use the data set provided by the
AphasiaBank project (MacWhinney et al.,
2011),
which has
assembled a large database of
tran-
scribed interactions between examiners and people
with aphasia,
nearly all of whom have suffered a
stroke.
Notably,
AphasiaBank also includes tran-
scribed sessions with neurotypical controls.
Each
interaction follows a common protocol and script,
2
and is transcribed in great detail using a standard-
ized set of annotation guidelines.
The transcripts
include word-level error codes, according to a de-
tailed taxonomy of errors and associated annota-
tions.
In the case of semantic, formal, and phone-
mic errors,
the word-level
annotations include a
“best guess” on the part of the transcriber as to what
the speaker’s intended production may have been.
Each transcribed session consists of a prescribed
sequence of language elicitation activities, includ-
ing a set of personal narratives (e.g.,“Do you re-
member when you had your stroke? Please tell me
about it.”), standardized picture description tasks,
a story retelling task (involving the story of
Cin-
derella
), and a procedural discourse task.
We noted that the distribution of errors within
sentences seems to obey the power law , with the
majority of
error-containing sentences containg-
ing a single error, followed somewhat distantly by
sentences containing two errors,
with a relatively
steep dropoff thereafter.
For the present study, we
restricted our analysis to sentences that contained
a single error.
Our reasoning for this restriction
was that we do not presently have a theoretically-
informed model of what, if any, relationship there
may be between multiple errors within a sentence.
However,
it seems quite likely that the errors oc-
curring in a sentence containing (for instance) five
paraphasic errors might be somehow related to one
another. We anticipate exploring this phenomenon
in the future (see section 6).
We chose to restrict our data to the story retelling
task due to the constrained and focused vocabulary
of the Cinderella story.
This resulted in
≈
1000
sentences from 385 individuals.
We then identi-
fied sentences containing instances of our errors of
interest:
phonological
paraphasia (AphasiaBank
codes “p:n”, “p:m”) or abstruse neologism (“n:uk”
and “n:k”).
3
Methods
We first tokenized the AphasiaBank data using a
modified version of the Penn Treebank tokenizer
which left contractions as a single word and simply
removed the connecting apostrophe, as these occa-
sionally appear as target words and thus we needed
to treat them as a single token.
We left stopwords
intact, and case-folded all sentences to upper-case.
Cardinal numbers were collapsed into a category
token,
as were ordinal
numbers and dates (each
category was given its own token).
The Aphasia-
Bank transcripts include detailed IPA-encoded rep-
resentations of
neologistic productions,
but
any
“real-world” usage scenario for our algorithm is
unlikely to benefit
from such high-quality tran-
scription.
We therefore translated the non-lexical
productions into a simulated “best-guess” ortho-
graphic representation of the subject’s non-lexical
productions.
We next turned to the question of identifying ne-
ologisms in our sentences.
Simply using a stan-
dard dictionary to determine lexicality could re-
sult in numerous “false positives,” driven largely
by proper
names
of
people,
brands,
etc.
To
avoid this,
we used the SUBTLEX-US corpus
(Brysbaert and New, 2009) to identify neologisms.
SUBTLEX-US was
build using subtitles
from
English-language television shows
and movies,
and Brysbaert and New have demonstrated that it
correlates with a number of psycholinguistic be-
havior measures (most notably, naming latencies)
better than better-known frequency norms such as
those derived from the Brown corpus or CELEX-
2.
Upon identifying a possible non-word produc-
tion,
recall
that
our
next
goal
is
to determine
whether
it
represents
a
phonemic
error
(substi-
tuting “[dɑɪnoʊsɔɹ]” for
“dinosaur”)
or
an
ab-
struse neologism
(a completely novel sequence of
phonemes that
does not
correspond to an actual
word).
To help accomplish this,
we train a lan-
guage model to identify plausible words that
could
fit
in the slot
occupied by the erroneous produc-
tion, and produce a lattice of these candidate target
words (i.e., words that the subject may have been
intending to produce,
given what we know about
the context in which they were speaking).
Our language models for this study were built us-
ing the New York Times section of the Gigaword
newswire corpus (Parker et al., 2011).
After suc-
cess in preliminary experiments,
we filtered this
corpus by first training a Latent Dirichlet Alloca-
tion (LDA) topic model on the corpus using Gen-
sim (Řehůřek and Sojka, 2010) over 20 topics. We
then projected the text of each of the Cinderella nar-
rative samples into the resulting topic space,
and
calculated the centroids for the narrative task. This
yielded a subset of the larger corpus of New York
Times articles that was “most similar” to the Cin-
derella retellings,
and we used these to build our
language models.
We investigated two different language model-
3
ing approaches:
a traditional FST-encoded ngram
language
model,
and a
neural-net
based log-
bilinear (LBL) language model.
For the FST rep-
resentation,
we used the the OpenGrm-NGram
language modeling toolkit
(Roark et
al.,
2012)
and used an n-gram order of 4,
with Kneser-Ney
smoothing (Kneser and Ney, 1995).
For the LBL
approach,
we used a Python implementation
4
of
the language model
described by Mnih and Teh
(Mnih and Teh, 2012).
We used word embeddings
of dimension 100,
and a 5-gram context window.
In both cases we trained two language models: one
trained on the “task-specific” subset of Gigaword,
and another trained on the AphasiaBank control
data.
We combined these with a simple mixing co-
efficient,
λ
as shown in Equation 1 where
P
GW
(w)
is the language model probability of word
w
com-
puted against the Gigaword corpus and the
P
AB
(w)
is the language model
probability trained on the
AphasiaBank controls.
P(w) = λ · P
AB
(w) + (
1
− λ) · P
GW
(w)
(1)
We
evaluate
non-lexical
productions
as
fol-
lows.
First,
we use the Phonetisaurus grapheme-
to-phoneme toolkit (Novak et al.,
2012) to trans-
late our orthographic representation into an esti-
mated phoneme sequence.
We then calculate a
phonologically-aware edit distance between each
non-word production and every word in our lexi-
con up to some maximum edit distance (in our case
4.0).
Phonemes from a related class (e.g.
vowels)
are considered lower cost replacements than those
from another class (e.g.
unvoiced fricatives).
This
gives us a set of candidates which are phonologi-
cally similar to the production.
We next used our language models to produce
lattices representing a set of possible sentences that
the subject could plausibly have been intending to
produce.
At
the point
in the produced sentence
where our error detection system indicated that a
non-word production occurred,
we represent
the
anomaly by the union of all possible words in our
edit-distance constrained lexicon (see figure 3 for
an example sentence lattice).
Finally,
we use the
language models to score the resulting sentence lat-
tice so as to be able to rank the candidate words,
and use the estimated sentence-level
probability
for each candidate word (i.e., the hypothesized in-
tended utterance featuring that word).
Put simply,
4
https://github.com/ddahlmeier/neural_lm
�
�
�
�
�����
�
����
�
��
�
�����
���
�����
����
�����
�����
���
�
����
Figure 1:
An example candidate word lattice for
the production “I can’t move my [vɑɪ] hand.”
for each candidate intended word,
we produce a
version of the subject’s utterance with that hypoth-
esized word in place of the anomalous utterance,
and score this hypothesized utterance with the lan-
guage model.
At this point in the process, we have the follow-
ing information about each erroneous production:
a best-guess orthographic transcription of what the
individual actually produced, and a ranked list of
plausible words that
they could potentially have
been attempting to produce,
together with proba-
bility estimates for each hypothesized production.
To
determine
the
category
of
our
error
productions— again,
between productions repre-
senting phonological errors such as “[dɑɪnoʊsɔɹ]”
for “dinosaur”,
and productions representing ab-
struse neologisms— we trained a binary classifier
using
features
representing
the
characteristics
of
the
candidate
word space
surrounding the
erroneous production.
Our intuition is that phone-
mic errors were much more likely than abstruse
neologisms to have highly-ranked candidate target
words that were
also
phonologically similar to the
subject’s actual production.
To capture this, we performed the following pro-
cedure for
each error-containing utterance.
We
first
divide our list
of candidate intended words
into buckets by edit distance (0.5,
1.0,
1.5,
etc.
5
).
Each bucket
can now be thought
of as a ranked
list
of probabilities,
each representing a possible
hypothesized intended utterance featuring a word
within that
bucket’s edit
distance of
the actual
(anomalous) utterance.
We next
represent
each bucket
with a feature
vector
consisting of
the count
of
words in that
5
Recall that our phonological edit distance metric allows
for partial costs for related phonological substitutions.
4
bucket,
as
well
as
descriptive statistics
regard-
ing the distribution of language model
probabil-
ities in that
bucket
(min,
max,
etc.).
We then
concatenate each bucket’s features together into a
master feature vector for the utterance.
Our ex-
pectation is that
these features will
be relatively
evenly distributed across buckets in the case of ut-
terances containing abstruse neologisms, whereas
utterances featuring phonological paraphasias will
vary according to phonological edit distance.
Once we have computed feature vectors for each
utterance,
we used the Scikit-learn Python ma-
chine learning library (Pedregosa et al.,
2011) to
train a Support
Vector Machine classifier to dis-
tinguish between utterances phonological and ab-
struse neologisms.
We evaluate its performance
using leave-one-out cross-validation.
4
Results
We perform two evaluations of our model:
an in-
trinsic evaluation of how often our system includes
the target word in the top-
n
ranked candidates, and
an extrinsic evaluation where we attempt to clas-
sify a paraphasia between phonological errors and
abstruse neologisms.
Our motivation for evaluating our system’s per-
formance on target word prediction is tied to our
classification assumptions.
In an ideal
case for
a phonological
error,
the target
word should fall
within one of the buckets representing a low edit
distance.
If our language model successfully rates
the target
as likely,
we would see an high maxi-
mum probability within that bucket, which is a fea-
ture in our classifier.
The performance of
our
language models on
the top-
n
ranked evaluation can be seen in Table
1.
The log-bilinear model outperformed the FST
in all
cases.
This finding is similar to state of
the art results for automatic sentence completion
systems–particularly for phonemic errors–as we’ll
discuss in greater detail in Section 5. Both systems
did a better job of predicting the target word for
phonemic errors than they did for abstruse neolo-
gisms.
It’s not immediately clear what the reason
for this is. However, anecdotally, sentences includ-
ing abstruse neologisms are also often agrammati-
cal.
For the evaluation of our classification, we cre-
ated a simple majority class baseline classifier that
always chooses the largest class of errors (phone-
mic errors in this case). This baseline classifier has
Error
n
FST
LBL
Phonemic
1
.43
.52
Phonemic
5
.54
.66
Phonemic
10
.59
.69
Phonemic
20
.67
.77
Phonemic
50
.72
.81
Abstruse Neo.
1
.29
.35
Abstruse Neo.
5
.41
.49
Abstruse Neo.
10
.44
.51
Abstruse Neo.
20
.51
.59
Abstruse Neo.
50
.54
.60
Table 1:
Accuracy of language model predicting
the correct target word within the first
n
results.
Features
FST
LBL
count, mean
.612
.661
count, mean, max
.621
.680
count, mean, max, min
.610
.659
count, mean, max, dist.
.610
.659
Table 2:
Classification accuracy by model.
Base-
line accuracy of choosing the most common error
type is .510.
a classification accuracy of .51. The results of clas-
sification can be seen in Table 2.
Both of our sys-
tems handily outperformed baseline:
the FST by
a relative 20% improvement, and the LBL nearly
33%. As we expected from the top-
n
results, classi-
fication based on the LBL outperformed that based
on the FST.
The “dist” feature listed in table 2 is the edit
distance of a given bucket normalized by the num-
ber of phonemes in the actual error production.
It
was not found to be productive as a feature,
nor
was the minimum language model probability of
words in a given bucket (“min” in the table).
The
best results for both systems were a combination of
count of candidate terms per bucket (“count”) con-
catenated with the maximum and mean language
model
probabilities within a bucket
(“max” and
“min”, respectively).
We varied the mixing-coefficient (
λ
) from Equa-
tion 1 in both the FST and LBL approaches.
As
long as the resulting model includes a non-trivial
weighting of the Cinderella corpus (typically any-
thing better than
λ =
3),
the actual value of the
mixing coefficient was irrelevant to either of our
evaluations. In this, it appears to work as designed,
with the Gigaword corpus providing background
probabilities, and the AphasiaBank Cinderella con-
5
trol retellings increasing the weight of topically im-
portant
words that
are otherwise rare (like “Cin-
derella” and “carriage”).
5
Related Work & Discussion
As far
back as Shannon’s word-guessing game
(Shannon, 1951), researchers have sought to lever-
age the statistical regularities in natural language to
predict missing or subsequent words.
In practice,
however, this proves to be a surprisingly challeng-
ing problem.
Language occurs at
levels beyond
simply choosing lexical
items,
and local
statisti-
cal characteristics of language often fail to capture
syntactic and semantic patterns.
Zweig & Burges
(2012) provide an enlightening discussion on the
limitations of relying on n-gram guessing for syn-
tactically complex tasks such as “identify the miss-
ing word in the sentence,” and also describe a very
challenging language model evaluation task built
around this problem.
They tested a variety of lan-
guage modeling approaches using their task,
and
report that well-trained generative n-gram models
achieve correct
predictions
≈
30
%
of the time.
State-of-the-art performance on the their word pre-
diction task using recurrent
neural
network lan-
gage models,
6
report highest scores are in the mid-
50
%
range (Mirowski
and Vlachos,
2015;
Mnih
and Kavukcuoglu, 2013).
In our case,
the nature of our data renders this
task even more challenging.
Our sentences are of-
ten short and agrammatical (often missing or mis-
using determiners, for example), and are produced
by individuals with impaired language ability.
As such, our results are actually quite similar to
those reported in recent literature.
Our average ac-
curacy of our FST n-gram (over both classes of
errors) selecting the appropriate word is
≈
35
%
while our LBL model’s performance of
≈
43
%
is comparable to the 5-gram LBL performance
of 49
.
3 reported by Mnih and Teh on the MSR
Sentence Completion Challenge dataset (Mnih and
Teh, 2012).
6
Conclusion & Future Work
While the system’s performance is quite good on
both intrinsic and extrinsic evaluation,
there re-
mains much interesting work left to do on the prob-
lem.
6
See De Mulder et al.
(2015) for a recent review on this
subject.
We currently only evaluate sentences with a sin-
gle error, and more generally have not investigated
whether sentences with multiple errors are differ-
ent
from mono-error sentences in terms of error
distribution or structure.
However,
our approach
should
be able to generalize to sentences with ad-
ditional errors, and we will be investigating this in
future work.
Additionally,
the AphasiaBank transcripts in-
clude phrasal dependency and part-of-speech tags
which we are currently not using.
In future work
we will investigate including these as features in
language modelling, as there is some evidence that
this improves the conceptually related task of con-
textual spellcheck(Fossati and Di Eugenio, 2008).
There is quite a bit
of work that
can be done
on the language models as well.
A more nuanced
approach to topic adaptation is worth investigat-
ing,
and we plan to experiment
with using non-
newswire corpora.
Despite our attempts to focus
the corpus via LDA, there is a major difference be-
tween the written language of the New York Times,
and the spoken dialogue between the aphasic sub-
jects and their clinicians.
The most exciting area for further research is the
inclusion of semantic information in our classifica-
tion. While our topic-specific language model pro-
vides our model with some implicit semantic infor-
mation, a more principled approach to semantic rel-
evance could potentially improve the classification
of phonemic errors versus abstruse neologisms by
determining whether a given candidate word is se-
mantically relevant in context.
More intriguingly,
it
would give us a way to start
investigating se-
mantic errors, and those errors that include “real”
words (for example, the previously discussed error
of replacing “dog” with “cat”).
One major limitation of our current
system is
its reliance on human-produced transcriptions of
speech samples.
In practice, transcription is rarely
feasible in clinical settings,
and even in research
settings is
often challenging,
which may seem
to limit
the applicability of
our
approach.
No-
tably,
however,
our system does not
require de-
tailed
phonetic
transcription,
and merely requires
“best-guess” orthographic transcription of neolo-
gisms.
As such,
one could in principle use au-
tomatic speech recognition (ASR)
to analyze a
recording of a patient or research subject, and pro-
duce a transcript on which our methods could be
6
run.
7
Fraser et al. (2015) have had some success
at applying ASR to samples of aphasic speech and
performing downstream analysis on the resulting
transcripts, and we anticipate experimenting with
similar techniques in the future.
Acknowledgments
We thank the BioNLP reviewers for their helpful
comments and advice. This material is based upon
work supported in part
by the National
Institute
on Deafness and Other Communication Disorders
of the National Institutes of Health under awards
R01DC012033 and R03DC014556.
The content
is solely the responsibility of the authors and does
not necessarily represent the official views of the
granting agencies or any other individual.
References
C. E. Brookshire, T. Conway, R. H. Pompon, M. Oelke,
and D.
L.
Kendall.
2014.
Effects
of
intensive
phonomotor treatment on reading in eight individu-
als with aphasia and phonological alexia.
American
Journal of Speech-Language Pathology
23(2):S300–
S311.
M.
Brysbaert
and B.
New.
2009.
Moving beyond
Kučera and Francis: A critical evaluation of current
word frequency norms and the introduction of a new
and improved word frequency measure for American
English.
Behavior Research Methods
41(4):977–
990.
Wim De Mulder, Steven Bethard, and Marie-Francine
Moens. 2015.
A survey on the application of recur-
rent neural networks to statistical language modeling.
Computer Speech & Language
30(1):61–98.
S. T. Engelter, M. Gostynski, S. Papa, M. Frei, C. Born,
V.
Ajdacic-Gross,
F.
Gutzwiller,
and P.
A.
Lyrer.
2006.
Epidemiology of aphasia attributable to first
ischemic stroke:
Incidence,
severity,
fluency,
etiol-
ogy, and thrombolysis.
Stroke
37(6):1379–1384.
G.
Fergadiotis,
S.
Kellough,
and W.
D.
Hula.
2015.
Item Response Theory modeling of the Philadelphia
Naming Test.
Journal
of
Speech,
Language,
and
Hearing Research
58(3):865–877.
Gerasimos
Fergadiotis,
Kyle
Gorman,
and Steven
Bedrick.
2016.
Algorithmic Classification of Five
Characteristic Types of Paraphasias.
American Jour-
nal of Speech-Language Pathology
25(4S):S776–12.
Davide Fossati and Barbara Di Eugenio. 2008.
I saw
tree trees in the park:
How to correct
real-word
spelling mistakes.
In
LREC
.
7
Depending on the specifics of the ASR system,
it may
in fact be possible to retain phonological information, which,
while not necessary, certainly could be helpful to our system.
K. C. Fraser, N. Ben-David, G. Hirst, N. Graham, and
E. Rochon. 2015.
Sentence segmentation of aphasic
speech.
In
ACL
. pages 862–871.
S.
A.
Frisch and R.
Wright.
2002.
The phonetics of
phonological speech errors:
An acoustic analysis of
slips of the tongue.
Journal of Phonetics
30(2):139–
162.
H. Goodglass and A. Wingfield. 1997.
Anomia:
Neu-
roanatomical
and cognitive correlates
.
Academic
Press, New York.
W. D. Hula, S. Kellough, and G. Fergadiotis. 2015.
De-
velopment and simulation testing of a computerized
adaptive version of the Philadelphia Naming Test.
Journal of Speech, Language, and Hearing Research
58(3):878–890.
D. L. Kendall, R. H. Pompon, C. E. Brookshire, I. Mink-
ina,
and L.
Bislick.
2013.
An analysis of
apha-
sic naming errors as an indicator of improved lin-
guistic processing following phonomotor treatment.
American Journal
of
Speech-Language Pathology
22(2):S240–S249.
R Kneser and H Ney. 1995.
Improved backing-off for
M-gram language modeling.
In
1995 International
Conference on Acoustics,
Speech,
and Signal
Pro-
cessing
. IEEE, pages 181–184.
J. Kristensson, I. Behrns, and C. Saldert. 2015.
Effects
on communication from intensive treatment with se-
mantic feature analysis in aphasia.
Aphasiology
29(4):466–487.
B.
MacWhinney,
D.
Fromm,
M.
Forbes,
and A.
Hol-
land. 2011.
AphasiaBank: Methods for studying dis-
course.
Aphasiology
25(11):1286–1307.
J.
Mayer and L.
Murray.
2003.
Functional measures
of naming in aphasia:
Word retrieval in confronta-
tion naming versus connected speech.
Aphasiology
17(5):481–497.
I. Minkina,
M. Oelke,
L. P. Bislick,
C. E. Brookshire,
R. Hunting Pompon, J. P. Silkes, and D. L. Kendall.
2015.
An investigation of aphasic naming error evo-
lution following phonomotor treatment.
Aphasiol-
ogy
epub ahead of print.
D.
Mirman,
T.
J.
Strauss,
A.
Brecher,
G.
M.
Walker,
P.
Sobel,
G.
S.
Dell,
and M.
F.
Schwartz.
2010.
A large,
searchable,
web-based database of
apha-
sic performance on picture naming and other tests
of cognitive function.
Cognitive Neuropsychology
27(6):495–504.
Piotr Mirowski
and Andreas Vlachos.
2015.
Depen-
dency Recurrent Neural Language Models for Sen-
tence Completion.
In
Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International
Joint
Confer-
ence on Natural
Language Processing (Volume 2:
Short
Papers)
.
Association for Computational
Lin-
guistics, Beijing, China, pages 511–517.
7
Andriy Mnih and Koray Kavukcuoglu. 2013.
Learning
word embeddings efficiently with noise-contrastive
estimation.
In C J C Burges, L Bottou, M Welling,
Z Ghahramani,
and K Q Weinberger,
editors,
Ad-
vances in Neural Information Processing Systems 26
,
Curran Associates, Inc., pages 2265–2273.
Andriy Mnih and Yee Whye Teh.
2012.
A fast
and
simple algorithm for training neural probabilistic lan-
guage models.
arXiv preprint arXiv:1206.6426
.
L.
E.
Nicholas,
R.
H.and Maclennan D.
L.
Brook-
shire,
J.
G.
Schumacher,
and S.
A.
Porrazzo.
1989.
Revised administration and scoring procedures for
the Boston Naming Test
and norms for non-brain-
damaged adults.
Aphasiology
3(6):569–580.
J.
R.
Novak,
N.
Minematsu,
and K.
Hirose.
2012.
WFST-based
grapheme-to-phoneme
conversion:
Open source tools for
alignment,
model-building
and decoding.
In
International Workshop on Finite
State Methods and Natural
Language Processing
.
pages 45–49.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2011.
English Gigaword 5th Edition.
Linguistic
Data Consortium: LDC2011T07.
F.
Pedregosa,
G.
Varoquaux,
A.
Gramfort,
V.
Michel,
B.
Thirion,
O.
Grisel,
M.
Blondel,
P.
Prettenhofer,
R.
Weiss,
V.
Dubourg,
J.
Vanderplas,
A.
Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay.
2011.
Scikit-learn:
Machine learning in
Python.
Journal
of
Machine Learning Research
12:2825–2830.
B. Roark, R. Sproat, C. Allauzen, M. Riley, J. Sorensen,
and T. Tai. 2012.
The OpenGrm open-source finite-
state grammar software libraries.
In
ACL
. pages 61–
66.
C.
Shannon.
1951.
Prediction and entropy of printed
English.
Bell System Technical Journal
50:50–64.
Geoffrey Zweig and Chris J C Burges. 2012.
A Chal-
lenge Set
for Advancing Language Modeling.
In
Proceedings of
the NAACL-HLT 2012 Workshop:
Will
We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT
. As-
sociation for Computational
Linguistics,
Montréal,
Canada, pages 29–36.
R. Řehůřek and P. Sojka. 2010.
Software framework for
topic modelling with large corpora.
In
LREC
. pages
45–50.
8
