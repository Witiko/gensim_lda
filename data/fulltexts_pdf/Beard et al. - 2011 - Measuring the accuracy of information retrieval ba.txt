Measuring the Accuracy of Information Retrieval 
Based Bug Localization Techniques 
Matthew Beard 
Computer Science Dept 
Univ. of Alabama in 
Huntsville 
Huntsville, AL 35899 
mdb0013@cs.uah.edu 
Nicholas Kraft 
Univ. of Alabama 
Tuscaloosa, AL 
35487-0290 
nkraft@cs.ua.edu 
Letha Etzkorn 
Computer Science Dept 
Univ. of Alabama in 
Huntsville 
Huntsville, AL 35899 
etzkorl@uah.edu 
Stacy Lukins 
SAIC 
Huntsville, AL 
slukins@cs.uah.edu 
Abstract—Bug localization involves using information about a bug 
to locate affected code sections. Several automated bug localization 
techniques based on information retrieval (IR) models have been 
constructed recently. The “gold standard” of measuring an IR 
technique’s accuracy considers the technique’s ability to locate a 
“first relevant method.” However, the question remains – does 
finding this single method enable the location of a complete set of 
affected methods? Previous arguments assume this to be true; 
however, few analyses of this assumption have been performed. 
In this paper, we perform a case study to test the reliability of 
this “gold standard” assumption. To further measure IR accuracy 
in the context of bug localization, we analyze the relevance of the 
IR model’s “first method returned.” We use various structural 
analysis 
techniques 
to 
extend 
relevant 
methods 
located 
by 
IR 
techniques 
and 
determine 
accuracy 
and 
reliability 
of 
these 
assumptions. 
I.
I
NTRODUCTION
In software maintenance, a key task involves using the 
information about a bug to locate the sections of code in 
which the bug occurs. This process, called bug localization, is 
a form of concept location. Once bug related sections are 
located, the developer can fix the coding error(s) that caused 
the bug. While manually examining code and comments was 
once 
an 
adequate 
technique, 
the 
size 
and 
complexity 
of 
software has grown exponentially in recent years, making 
automated 
techniques 
a 
near 
necessity 
for 
efficient 
maintenance. 
One 
of 
the 
more 
recent 
types 
of 
automated 
bug 
localization 
techniques 
involves 
the 
usage 
of 
information 
retrieval (IR) models to retrieve relevant information about 
the bug. Latent semantic indexing (LSI) is one of the primary 
IR techniques used in these analyses [11] [10]. While LSI 
does 
provide 
useful 
results, 
another 
IR 
technique, 
latent 
Dirichlet allocation (LDA) [1], has shown promise as a more 
robust and thorough technique. concept mining [9] and bug 
localization [8]. 
When 
measuring 
any 
bug 
localization 
technique 
to 
determine accuracy and reliability, the commonly recognized 
“gold standard” requires a technique to return a complete set 
of all code sections that are affected by the bug. In previous 
work, the accuracy of IR techniques in bug localization has 
been measured by a distinctly different “gold standard.” That 
standard is the IR technique’s ability to locate a “first relevant 
method,” (FRM) that is, the first method returned that is 
actually relevant to the bug [11] [7]. Though this existing 
gold standard returns one method that is affected by the bug, 
the question remains – does finding this single method allow 
one to find all parts of the code that are relevant to the bug? 
In previous research, the assumption has been made that once 
the first relevant method is located, structural characteristics 
of the source code can be used to find remaining bug affected 
sections. 
However, 
to 
our 
knowledge, 
only 
one 
brief, 
superficial analysis of this assumption has been performed 
[7]. 
A further area of interest involves determining if the “first 
method returned” (FMR) of a similarity query performed 
against an IR model is relevant to the bug. The first method 
returned is the very first method located by the querying the 
IR model. Since the first method returned may not always be 
directly relevant to a bug (i.e., containing faulty code), the 
question arises – can FMR be used to locate methods that are 
directly relevant to the bug? It is possible that, while not 
directly related, the IR technique has determined underlying 
semantic similarities between the first method returned and 
the bug itself. 
In order to determine whether the previously described 
“gold standard” is a reliable assumption, we examine the 
reliability of the FRM and FMR in locating other bug affected 
methods. 
To 
do 
this, 
we 
analyze 
each 
assumption 
by 
examining 
the 
source 
code 
to 
determine 
whether 
the 
assumption and the structural characteristics of the code lead 
to all (or part) of the sections of code that must be changed in 
order to fix the bug. 
II.
B
ACKGROUND
This section contains a description of information retrieval 
techniques applied to source code, existing bug localization 
techniques, and structural techniques. 
A.
IR Models for Concept Location & Source Code 
Retrieval 
Some recent techniques of operating on and retrieving 
relevant information from source code use models of the 
code, as opposed to operating on the source itself. These 
models represent code as a document collection, or corpus. 
Each document consists of a collection of terms, or words. 
2011 18th Working Conference on Reverse Engineering
1095-1350/11 $26.00 © 2011 IEEE
DOI 10.1109/WCRE.2011.23
124
The 
corpus 
is 
constructed 
from 
semantic 
information 
embedded in code, such as comments and identifiers. The 
point of division (file, method, class, etc.) for documents that 
make up the corpus is also determined by the application of 
the IR technique. 
To obtain results from an IR model, queries are performed. 
User queries can be a document that is modeled with the 
corpus, or a document compared using a similarity measure, 
such as cosine similarity, after the IR model created. Results 
of a query are a list of documents ranked by their similarity to 
the query. 
B.
Bug Localization 
Techniques for bug localization may be classified as static 
or dynamic. Implementing IR techniques in automated bug 
localization falls into the category of static techniques, as we 
work with static source code [6]. 
IR-based static techniques aim to locate areas of source 
code affected by a bug. While IR techniques generally cannot 
identify all locations that must be altered to correct a bug, 
meaning that standard schemes of measuring accuracy are not 
useful [4], they can identify relevant methods where more in-
depth 
analysis 
can 
begin. 
What 
in 
the 
past 
has 
been 
considered the best measure or “gold standard” of accuracy is 
to determine which IR-based static technique ranks the first 
relevant source entity the highest [11]. Since one relevant 
entity is all that is needed to begin examination, the ranking 
of 
that 
entity 
determines 
the 
amount 
of 
information 
a 
programmer must examine before reaching an entity that is 
relevant to the bug. 
C.
Structural Analysis Techniques 
Structural techniques for analyzing and navigating code 
involve an entirely different set of methodologies than those 
discussed so far. In this paper, we focus primarily on static 
call graphs, and also include information from inheritance and 
class relationships. 
III.
P
REVIOUS 
W
ORK
As 
stated 
in 
2.1, 
a 
number 
of 
analyses 
have 
been 
performed on source code in order to localize bugs using IR 
techniques. Others have also used a combination of structural 
and 
IR 
techniques 
to 
further 
extend 
the 
feature 
location 
capabilities of IR. For instance, Poshyvanyk et al. use call 
graphs in conjunction with LSI to perform feature location 
[13]. However, only a few, brief studies on our specific topic 
– measuring the accuracy of IR based bug localization using 
structural techniques – have been performed. Specifically, no 
researchers have previously addressed the degree to which the 
commonly accepted “gold standard” of FRM actually enables 
location of the entire bug. A limited look at LDA’s accuracy 
in this context is performed in [7]. As stated in 2.4, [3] 
considers coupling metrics in light of LDA, but structural 
techniques are not used to provide an accuracy measure of 
LDA, or of the completeness of the first relevant method 
assumption. To our knowledge, no other studies examine IR 
techniques in this context. 
IV.
S
TRUCTURAL 
B
ASED 
A
NALYSIS OF 
IR
A
CCURACY
The focus of this research is to provide a clear picture of 
the accuracy of information retrieval based techniques for bug 
localization using two IR techniques, latent semantic indexing 
and latent Dirichlet allocation. The specific goals of this 
analysis are as follows: 
•
Determine whether the “first relevant method” is a 
reliable place to begin IR-based bug localization to 
test the “gold standard” assumption. 
•
Determine whether the “first method returned” is a 
worthwhile place to begin IR-based bug localization. 
To perform this analysis, we build LDA and LSI models 
of the source code. The models are then queried using 
descriptions of selected bugs, returning ranked results of the 
queries. We use these results to derive two different sets of 
potentially relevant methods – the first relevant method and 
the first method returned. These two sets are compared 
against a set of methods mapped to corresponding bugs. The 
comparison is performed using information provided by (1) 
call graphs, (2) local classes, and (3) class inheritance. 
A.
IR Models 
In 
order 
to 
use 
IR 
models, 
semantic 
information 
is 
extracted from the source and the model is constructed. 
Constructing 
the 
document 
collection
: 
This 
process 
involves separating source code by the desired distinction (in 
our case, by methods), selecting individual pieces of semantic 
information for extraction (comments and identifiers), and 
then 
performing 
preprocessing 
on 
this 
information. 
The 
preprocessing 
phase 
involves 
removal 
of 
stop 
words 
(irrelevant, 
semantically 
insignificant 
words) 
and 
Porter 
stemming of words. We use the NetBeans plug-in outlined in 
[8] to accomplish this. 
IR Model Construction and Querying
: To construct and 
query 
the 
IR 
models, 
we 
use 
Gensim, 
a 
Python 
based 
framework 
for 
Vector 
Space 
Modeling 
[12] 
capable 
of 
performing multiple IR analyses, including LSI and LDA. 
LSI 
is 
performed 
by 
Gensim 
using 
singular 
value 
decomposition. To form the LSI model, Gensim requires a 
corpus 
(the 
output 
of 
the 
plug-in) 
and 
a 
dimensionality 
measure for clustering of documents as input. The output of 
this process is a model that can then be queried using cosine 
similarity. Queries return a ranked set of document numbers 
that represent methods. 
LDA is performed by Gensim using an online variational 
Bayes (VB) algorithm [5]. To form the LDA model, Gensim 
requires 
the 
corpus, 
a 
dimensionality 
measure, 
and 
an 
iteration count. Since VB techniques perform probabilistic 
sampling, the model is re-sampled for a user-defined number 
of iterations in order to achieve optimal results. Querying is 
performed using the same similarity technique as described 
for LSI, rendering output formatted similarly to that of LSI. 
B.
Relevant Methods
From the results returned by the IR technique, we derive 
two sets of data: first relevant method returned and first 
method returned. 
125
The first relevant method returned is the first method 
located by the query against the IR model that is affected by 
the bug. This method may be the only method that must be 
fixed. In this case, structural techniques are not needed to 
verify the accuracy of the IR technique. However, it is often 
the case that multiple methods need to be fixed in order to 
completely resolve a bug. In this instance, structural analysis 
techniques can be used to determine whether this relevant 
method 
is 
a 
good 
starting 
point 
to 
locate 
other 
affected 
methods. 
The first method returned is the very first method located 
by the query against the IR model. This method may or may 
not be directly relevant to the bug in question. Our structural 
analysis 
attempts 
to 
determine 
whether 
these 
returned 
methods have any relation to the bug, direct or indirect. 
C.
Structural Analysis 
To perform structural analysis, we use data gathered using 
STI’s Understand for Java [15] and Doxygen [2]. 
Call Graph Analysis
: Call graph results are derived using 
manual comparison of the IR returned method with methods 
that are callers/callees of that method. The call graph tree is 
traversed using caller and callee chains of length four (i.e., we 
look at methods that called or were called by the IR returned 
method four levels up or down the call graph). A pre-study 
analysis 
determined 
that 
a 
chain 
of 
length 
four 
balanced 
manual work and decaying level of relevancy. 
Local Class/Inheritance Analysis
: Local class results are 
derived by looking at the class that contains the IR returned 
method. This class is examined to determine whether it 
contains any other methods that are affected by the bug. 
Inheritance results are derived in a similar way. 
Our structural analysis methodology consists of employing 
all 
three 
techniques, 
first 
call 
graphs, 
followed 
by 
local 
classes, and concluding with inheritance. 
V.
C
ASE 
S
TUDY OF 
IR
A
CCURACY
Our case studies were performed on the Rhino software 
system, an open source implementation of JavaScript written 
in Java [14]. Version 1.5 release 5 (1.5R5) was chosen for 
analysis, as this release has a large number of documented 
bugs (35 total). Each of the bugs used in our case studies 
were mapped by hand to all respective affected methods. 
This means that we know each set of methods that must be 
altered to fix each bug – in essence, the “solution” to each 
bug. 
Therefore, 
we 
can 
compare 
the 
results 
of 
our 
IR 
techniques 
to 
a 
complete 
solution 
set 
of 
bug 
affected 
functions to determine accuracy and reliability. 
All bugs used in this study meet the following criteria: 
•
Bugs 
existed 
in 
source 
files 
of 
Rhino 
1.5R5 
that 
implemented core and compiler functionality. 
•
Bugs required method-level fixes. 
•
Bugs were fixed in the next release versions of Rhino 
(either 1.5R5.1 or 1.6R1) 
•
Bugs were categorized as 
resolved
or 
verified
and as
fixed
, so only bugs that could be verified and repaired 
are used. 
Included bugs span a wide range of severity levels, from 
critical defects to program enhancements. 
For each of these studies, we use the methodology outlined 
in 4.1 to perform our IR analysis. The corpus for both IR 
techniques consisted of 1,780 documents, each representing a 
unique method. To determine the best dimensionality and/or 
iterations for LSI and LDA, we ran both techniques using a 
variety 
of 
settings. 
For 
LSI, 
it 
was 
determined 
that 
a 
dimensionality measure of 25 clusters was the most effective 
at returning the largest number of highly ranked methods. For 
LDA, a dimensionality measure of 75 topics along with a 
sampling count of 200 iterations was best. 
Queries for each of the cases follow the same methodology 
used in [8]. These queries are formulated using information 
extracted from the bug title and description contained within 
bug reports in the Rhino bug repository. The inclusion of 
semantic information from bug reports was limited to these 
sections as they are the most semantically relevant to code. 
Each query was formed using the following process: 
1.
Extract keywords manually from the bug title 
2.
Extract relevant keywords found in the bug report 
summary 
3.
Include 
useful 
variants 
of 
keywords 
(e.g., 
adding 
parse
in addition to 
parser
) 
4.
Include common abbreviations (e.g., 
eol
for 
end-of-
line
) 
5.
Add words that may be related to the bug (e.g., 
day
in 
addition to 
daylight savings time
). 
A.
First Relevant Method 
The goal of this case study is to determine whether the 
“gold 
standard” 
assumption 
regarding 
the 
first 
relevant 
method (see Chapter 1) is reliable. For this study, we ran each 
query against the corpus using LDA and LSI and used the 
ranked results to obtain the first relevant method returned. 
The first 500 returned methods (out of 1,780 total) were 
analyzed to determine whether a relevant method was present. 
Our sample size was limited to 500 because correlations 
between 
the 
query 
and 
methods 
deeper 
than 
this 
point 
generally become too low to be considered reasonable. Out 
of the 35 bug queries that were performed, LDA found a 
relevant 
method 
for 
32, 
while 
LSI 
located 
31 
relevant 
methods. 
Our 
results 
show 
that 
FRMs, 
when 
combined 
with 
structural techniques, are very effective starting points for 
locating further relevant methods. As illustrated in Figure 1, 
the 
combination 
of 
IR 
based 
and 
structural 
analysis 
techniques allows location of at least a partially complete 
group of relevant methods for 95% (60/63) of the bugs. 
Results returned solely by the IR technique could only 
locate the complete bug affected method set 39% (24/63) of 
the time. The addition of combined call graph and local class 
analysis (inheritance did not provide useful additions in this 
case) on the FRM allows added relevant method information 
to be gathered for an additional 56% of the bug set. 
126
When we examined the “gold standard” accuracy measure, 
which assumes that all relevant methods can be located using 
FRM, 
our 
results 
show 
a 
lower 
degree 
of 
success. 
As 
illustrated in Figure 2, the combined FRM, call graph, and 
local 
class 
information 
are 
able 
to 
assist 
in 
locating 
the 
complete set of all methods affected by a bug for 59% (37/63) 
of the bugs in our sample set. These results demonstrate that 
the FRM “gold standard” is effective, but only for slightly 
more than half of the bugs in this project. 
Example of FRM analysis
: Both IR techniques returned 
ScriptRunTime.name
as the FRM for bug #254915. When 
mapped to the code, this bug affects a total of seven functions. 
Using the “gold standard” assumption, we extended the IR 
results 
using 
structural 
techniques. 
The 
addition 
of 
call 
graphs enabled us to find one other function containing part 
of 
the 
bug, 
Interpreter.Interpret
. 
When 
local 
class 
information was included, we were able to locate four more 
functions affected by the bug within the 
ScriptRunTime
class.
However, one affected method, 
BodyCodeGen.visitSetName
could not be located with this technique. Table 2 shows the 
methods found by the addition of each structural technique. 
Lightly shaded cells indicate a method that could not be 
located with a technique, while dark cells indicate a method 
already located by a previous technique. 
The results of this analysis show that the FRM is a good 
starting point for partial bug localization when combined with 
structural analysis techniques. However, in the context of our 
“gold standard,” we can only conclude that this standard is 
capable of yielding a complete set of bug affected methods 
for approximately two-thirds of bugs in Rhino. To clarify, 
using FRM and structural analysis techniques will generally 
allow us to locate some, but not all of the places in the code 
that must be changed in order to fix the bug. 
B.
First Method Returned
For the second study, we follow a process similar to that of 
the 
first 
study; 
however, 
instead 
of 
looking 
through 
the 
returned 
results 
to 
find 
the 
FRM, 
we 
instead 
gather 
the 
method information for only the first method returned - the 
method that, according to the IR technique, bears the highest 
similarity to the query. Because we gather the first method 
that is returned in this analysis, each bug is associated with a 
returned method, so our sample size is 35 bugs for both LDA 
and LSI. 
We perform the same structural analysis techniques as 
discussed in Section 5.1 on this set of data. However, since 
this case study focuses on the accuracy of each IR technique’s 
FMR, rather than the accuracy of the FRM “gold standard,” 
we divide our visualizations to show the differences between 
results returned by LDA and LSI. 
 

)/2

*,2

'&2


 
+2

Figure 1: Success rate of locating partial relevant 
method sets using first relevant method + structural 
techniques 
 

).2


-2
3


'*2

!


*'2
Figure 2: Success rate for locating complete relevant 
method sets using first relevant method + structural 
techniques 
 "
!
.2


(/2


'*2


)2


"

*,2
Figure 3: Accuracy of first method returned (FMR) by LSI 
+ structural techniques (ST) 
TABLE I. 
E
XAMPLE OF 
L
OCATED 
M
ETHODS 
A
FFECTED BY 
B
UG 
#254915 
Method 
IR 
Call Graph 
Local Class 
Inheritance 
ScriptRunTime.name
Found 
Interpreter.Interpret
Found 
ScriptRunTime.bind
Found 
ScriptRunTime.getBase
Found 
ScriptRunTime.getNameFunctionAndThis
Found 
ScriptRunTime.setName
Found 
BodyCodeGen.visitSetName
127
In order to measure accuracy, we use the data provided by 
structural analysis to determine whether the FMR can be used 
to reach methods related to the bug, regardless of whether the 
FMR 
itself 
is 
a 
directly 
relevant 
method 
(i.e., 
one 
that 
contains part of the bug). If the FMR does not contain the 
bug, but can still be used to reach all or part of the bug, then 
the FMR is indirectly relevant to the bug. 
Our 
results 
show 
that 
the 
combination 
of 
structural 
techniques 
with 
both 
IR 
techniques 
provides 
significant 
extension to the FMR alone. As Figure 3 denotes, LSI’s FMR 
was 
solely 
sufficient 
to 
locate 
relevant 
methods 
that 
correspond 
with 
8% 
(3/35) 
of 
the 
sample 
set 
of 
bugs. 
However, the addition of all three structural techniques (CG, 
CG+LC, 
all) 
increases 
the 
number 
of 
bugs 
for 
which 
a 
relevant method can be located to 54% (19/35). 
Results for LDA are similar. As Figure 4 denotes, LDA’s 
FMR is solely sufficient to locate a relevant method for 3% 
(1/25) of the bug sample set. However, adding all three 
structural techniques increases the number of bugs for which 
a relevant method can be located to 63% (22/35). 
Combining 
IR 
and 
structural 
techniques 
also 
achieves 
some success in locating sets containing all methods affected 
by a bug. LSI plus structural techniques is capable of locating 
all affected methods for 25% (9/35) of the bug set. LDA 
performed similarly in this area, also locating all affected 
methods for 25% of the bug set. 
The results of this analysis show that FMR can be an 
effective starting point for locating methods affected by a 
bug. 
VI.
I
SSUES 
A
FFECTING 
O
UR 
C
ASE 
S
TUDY
There a several issues with our analysis that could affect 
the quality of results returned. As stated in [8], the quality of 
results returned by any semantic-based technique is highly 
dependent on the amount and quality of semantic information 
found in both the source code, as well as queries of the 
source. 
Code 
with 
poorly 
named 
identifiers 
or 
sparse 
commenting 
is 
much 
more
likely 
to 
yield 
poor 
semantic 
results. Bugs that are described incorrectly or poorly are also 
more likely to return methods with little to no correlation to 
the bug. 
VII. C
ONCLUSIONS AND 
F
UTURE 
W
ORK
The studies presented in this paper demonstrate that both 
the first relevant method and the first method returned can be 
used to assist in bug localization. However, we can conclude 
that 
the 
FRM 
“gold 
standard” 
is 
not 
a 
highly 
reliable 
assumption. As we have found, extending the FRM using 
structural techniques is only capable of locating a full set of 
relevant methods 60% of the time. Therefore, the “gold 
standard” technique only produces the assumed results for 
slightly less than two-thirds of the bugs in the Rhino software. 
In future studies, we plan to examine and incorporate the 
metric described in [3], as well as a variety of coupling 
metrics, to derive more information about the IR techniques. 
We also plan to survey techniques of querying LDA corpora 
and examine other techniques of vector space modeling. 
A
CKNOWLEDGMENT
This work was funded in part by the National Science 
Foundation under Grants CCF-0915403 and CCF-0915559 
and by the National Aeronautics and Space Administration 
under Grants NAG5-12725 and NCC8-200.
R
EFERENCES
[1] D.M. Blei, A.Y. Ng, and M.I. Jordan, "Latent Dirichlet Allocation," 
J. 
Machine Learning Research
, vol. 3, pp. 993-1022, 2003. 
[2] Doxygen. [Online]. http://www.doxygen.org/ 
[3] M. Gethers and D. Poshyvanyk, "Using Relational Topic Models to 
Capture Coupling among Classes in Object-Oriented Software 
Systems," in 
Proc. of 26th IEEE International Conference on Software 
Maintenance (ICSM'10)
, Timişoara, Romania, 2010. 
[4] D.A. Grossman and O. Frieder, 
Information Retrieval: Algorithms and 
Heuristics
.: Springer, 2004. 
[5] M. Hoffman, D. Blei, and F. Bach, "Online Learning for Latent 
Dirichlet Allocation," in 
Neural Information Processing Systems (NIPS 
2010) 
, Vancouver, 2010. 
[6] Chao Liu, Xifeng Yan, Long Fei, Jiawei Han, and Samuel P. Midkiff, 
"SOBER: Statistical Model-Based Bug Localization," 
ACM SIGSOFT 
Software Engineering Notes
, vol. 30, no. 5, pp. 289-295, 2005. 
[7] Stacy K. Lukins, Nicholas A. Kraft, and Letha H. Etzkorn, "Bug 
localization using latent Dirichlet allocation," 
Information & Software 
Technology
, vol. 52, no. 9, pp. 972-990, 2010. 
[8] S. K. Lukins, N. A. Kraft, and L. H. Etzkorn, "Source Code Retrieval for 
Bug Localization using Latent Dirichlet Allocation," in 
Proc. 15th 
Working Conf. Reverse Engineering (WCRE 2008)
, 2008, pp. 155-167. 
[9] G. Maskeri, S. Sarkar, and K. Heafield, "Mining Business Topics in 
Source Code using Latent Dirichlet Allocation," in 
Proc. 1st India 
Software Engineering Conference
, 2008, pp. 113-120. 
[10] D. Poshyvanyk, Y.G. Gueheneuc, A. Marcus, G. Antoniol, and V. 
Rajlich, "Combining Probabilistic Ranking and Latent Semantic 
Indexing for Feature Location," in 
IEEE Int'l Conf. Program 
Comprehension (ICPC 2006)
, 2006, pp. 137-148.
[11] D. Poshyvanyk, Y.G. Gueheneuc, A. Marcus, G. Antoniol, and V. 
Rajlich, "Feature Location Using Probabilistic Ranking of Methods 
Based on Execution Scenarios and Information Retrieval," 
IEEE Trans. 
Softw. Eng.
, vol. 33, no. 6, pp. 420-432, June 2007. 
[12] R. Rehurek and P. Sojka, "Software Framework for Topic Modelling 
with Large Corpora," in 
Language Resources and Evaluation (LREC 
2010)
, 2010. 
[13] M. Revelle, B. Dit, and D. Poshyvanyk, "Using Data Fusion and Web 
Mining to Support Feature Location in Software," in 
Proc. of 18th IEEE 
International Conference on Program Comprehension (ICPC'10)
, 
Braga, Portugal, June30 -July 2, 2010, pp. 14-23. 
[14] Rhino. [Online]. http://www.mozilla.org/rhino/ 
[15] Understand for Java. [Online]. http://www.scitools.com/ 
 "
!
)2


(.2


(/2


)2


"

)-2
Figure 4: Accuracy of first method returned (FMR) by 
LDA + structural techniques (ST) 
128
