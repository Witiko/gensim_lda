REAL TIME EVALUATION OF QUALITY OF SEARCH TERMS DURING QUERY 
EXPANSION FOR STREAMING TEXT DATA USING VELOCITY AND RELEVANCE 
by 
NILAYAN BHATTACHARYA 
(Under the Direction of Ismailcem Budak Arpinar) 
ABSTRACT 
The traditional methods of evaluation of retrieved data using precision and recall cannot 
be used to evaluate the quality of twitter data retrieved using the streaming API due to the 
restriction on the access of historic tweets, and the dynamic nature of the microblog posts. For 
this purpose, we propose a novel method to quantify the quality of data retrieved independent of 
the underlying model. Using the change in velocity of tweets due to addition of a search term, 
and the relevance calculated by the underlying model, we evaluate the impact of each search 
term on the quality of data retrieved. 
INDEX WORDS: 
Information Retrieval, Text Analysis, Query Expansion, Data Collection, 
Microblog Analysis 
REAL TIME EVALUATION OF QUALITY OF SEARCH TERMS DURING QUERY 
EXPANSION FOR STREAMING TEXT DATA USING VELOCITY AND RELEVANCE 
by 
NILAYAN BHATTACHARYA 
B Tech, National Institute of Technology, Silchar, India 2009 
A Thesis Submitted to the Graduate Faculty of The University of Georgia in Partial Fulfillment 
of the Requirements for the Degree of 
MASTER OF SCIENCE 
ATHENS, GEORGIA 
2014 
© 2014 
Nilayan Bhattacharya 
All Rights Reserved 
REAL TIME EVALUATION OF QUALITY OF SEARCH TERMS DURING QUERY 
EXPANSION FOR STREAMING TEXT DATA USING VELOCITY AND RELEVANCE 
by 
NILAYAN BHATTACHARYA 
Major Professor: 
Ismailcem Budak Arpinar 
Committee: 
John A. Miller 
Khaled Rasheed 
Electronic Version Approved: 
Julie Coffield 
Interim Dean of the Graduate School 
The University of Georgia 
December 2014
iv 
DEDICATION 
To my mother. 
v 
ACKNOWLEDGEMENTS 
At the outset, I would like to thank my Major Professor, Dr. Ismailcem Budak Arpinar 
for his invaluable suggestions, guidance and motivation during my time here. I would also like to 
thank all the professors of the Dept. of Computer Science, especially Dr. John Miller and Dr. 
Khaled Rasheed for their support. 
Special thanks to all my friends in my project group whose feedback were an important 
part of the conception of this thesis. Finally, my humble appreciation for the wonderful 
experience during my time here at the University of Georgia. 
vi 
TABLE OF CONTENTS 
ACKNOWLEDGEMENTS ............................................................................................................ v
LIST OF TABLES ....................................................................................................................... viii
LIST OF FIGURES ....................................................................................................................... ix
1.
INTRODUCTION AND MOTIVATION .............................................................................. 1
1.1.
Public opinion .................................................................................................................. 1
1.2.
Twitter opinion mining..................................................................................................... 2
1.3.
Social conflicts and role of social media and the internet ................................................ 3
2. INFORMATION RETRIEVAL AND QUERY EXPANSION .............................................. 6
2.1. The theory of Information Retrieval .................................................................................... 6
2.2. Stages of Information Retrieval for text .............................................................................. 7
2.3. Query Expansion with respect to the Twitter API ............................................................. 10
3.
RELATED WORK ............................................................................................................... 15
4. THE METHODOLOGY OF EVALUATION-VELOCITY AND RELEVANCE ............... 18
4.1. The need for a new methodology....................................................................................... 18
4.2. The concept of velocity and relevance ............................................................................... 19
5.
SYSTEM ARCHITECTURE ............................................................................................... 24
6. EXPERIMENTS AND RESULTS ....................................................................................... 28
vii 
6.1. Validation using naïve hashtag expansion ......................................................................... 28
6.2. Validation using Latent Semantic Analysis (LSA):........................................................... 33
6.3 Validation using naïve bayes classification ........................................................................ 37
7.
CONCLUSION AND FUTURE WORK ............................................................................. 42
BIBLIOGRAPHY ......................................................................................................................... 44
viii 
LIST OF TABLES 
Page 
Table 1: Velocity for different periods during the game .............................................................. 29
Table 2: Total tweets retrieved during each stage ........................................................................ 30
Table 3: Velocity for different number of stop words .................................................................. 31
Table 4: Velocity component for different periods ....................................................................... 32
Table 5: No. of tweets and average LSI ........................................................................................ 34
Table 6: Velocity for different terms ............................................................................................ 38
Table 7: Relevance Component for each term .............................................................................. 39
Table 8: Velocity factor for the 3 scenarios .................................................................................. 40
Table 9: Impact Factors for various Velocity factors ................................................................... 40
ix 
LIST OF FIGURES 
Page 
Figure 1: System Architecture for evaluation ............................................................................... 24
Figure 2: Change in velocity during UGA vs FL ......................................................................... 29
Figure 3: Change in Relevance throughout the game ................................................................... 30
Figure 4: Average Relevance across all periods ........................................................................... 31
Figure 5: Impact factor for original and cumulative velocity ....................................................... 32
Figure 6: Average LSI score for each search term ....................................................................... 35
Figure 7: Relevance component for all scenarios ......................................................................... 36
Figure 8: Impact factor for all scenarios ....................................................................................... 36
Figure 9: Comparison of the velocities ......................................................................................... 38
Figure 10: Relevance Component of each search term ................................................................ 39
Figure 11: Comparison of the Velocity Factors for the 3 search terms ........................................ 40
Figure 12: Comparison of the various Impact factors .................................................................. 41
1 
CHAPTER 1 
INTRODUCTION AND MOTIVATION 
1.1. Public opinion 
Public opinion has always been considered as the cornerstone of any popular democratic 
government or a successful business. The importance of public opinion is particularly prominent 
during elections where different news channels run opinion polls to predict the winners of the 
elections even before the counting of the votes starts. Millions of dollars are spent by the 
political parties involved in the elections to foster positive opinion in its favor. Any major 
Government decision that is expected to affect a relatively major group of people is usually 
preceded by the collection of public opinion. Also, before the launch of any business venture, 
one of the first steps that goes into planning is the general opinion of the people towards the 
product that is being launched, the disadvantages of the competitors’ products and the general 
expectations from a new product. For the launch of a totally new product, public opinion also 
helps to gauge the demand that might arise, the supply chain management optimizations that 
would enable close proximity between the production lines, logistics and output and the market. 
The traditional method of collection of people’s opinion is the door to door canvassing and 
questionnaire. Market research consisted of creation of a set of questions, distributing these 
questions among a group of people which pertained to the target customer base; e.g. in a target 
age group, gender, race, economic standing, etc. For a more general opinion, a specially 
identified group which captures certain diversity like age, race, nationality, locality, etc. that can 
be said to represent the general population is identified for the questionnaire. This method is still 
2 
used by some research companies like Pew Research Center and Gallup to gather public 
opinion. The method is known to be reliable, and is an accepted methodology for gathering 
public opinion. However, a commonly agreed distribution of people is still not present in the 
public domain and the above mentioned corporations have their proprietary methodology which 
is not available in the public domain. 
The questionnaire that is collected is analyzed for correlation, outliers and influential data points, 
etc. to infer some relationship among the variables involved. The relationship is used to make the 
different business decisions and strategies. 
The obvious disadvantage of this approach is identifying the group that is targeted for the 
questionnaire. People might not be very receptive towards answering a questionnaire, especially 
if it is too long or tedious unless it is accompanied by some incentive. Also, the answers in the 
questionnaire may not be entirely true, and it totally depends on the user’s options. This is one of 
the primary reasons twitter opinion mining has become so popular. 
1.2. Twitter opinion mining 
Twitter is a microblogging platform where people can post short texts called ‘Tweets’. Twitter 
also has a profile page where the details of a person is present. Twitter is often used by people to 
express their opinions on certain issues or products. They revolutionized the concept of ‘hash-
tags’, making it a common parlance in social-networking. Instead of focusing of an all-
encompassing, total social media experience, twitter consolidated on the brevity of the tweets, 
and the ability of a person to have a ‘follow’ relationship rather than a ‘friendship’. Friendship, 
which is used in Facebook, allows a person to interact with or follow the posts of another person 
3 
only if the person had accepted the friend-request. Twitter allows any person to follow another 
without these restrictions. 
Many of these tweets consist of people voicing their opinions. Thus, many companies have been 
turning to twitter for their analytics instead of the questionnaire method. Since the opinions 
expressed in the tweets are a person’s own, without any incentive or external stimuli, they can be 
assumed to be unbiased. There is a substantial work on sentiment analysis and opinion mining 
going on in Twitter data. Another advantage is the easy availability of the profile of the person, 
from which the different traits which are required like age group, gender, etc. can be filtered out. 
It is also relatively easy to isolate tweets from a particular geographical boundary. 
Twitter allows Computer Scientists to gather data for their study through its API. 
1.3. Social conflicts and role of social media and the internet 
Twitter revolutionized the way political protests were organized and coordinated, which was 
particularly prominent during the Arab Spring revolutions. The first major impact of twitter on 
the politics of a country can be found in the Moldovan elections of 2009. Suspecting 
irregularities in the election process, people organized themselves under the hashtag #pman 
(Piata Marii Adunari Nationale, the main square in Chisinau, the capital of Moldova) [1]. 
Although not very successful, it introduced the use of social media and twitter in particular as a 
medium of communication. 
Soon after, similar protests erupted in Iran after announcement of the results of the presidential 
elections in favor of the current president Mahmoud Ahmadinejad over the opposing candidate 
Mir-Houssein Mosavi. It was also nicknamed the 'Green Revolution' due to the use of Mosavi's 
4 
color-green in avatars and covers. Hash tags like #iranelection and #neda, after the gunshot 
victim Neda Agha-Soltan’s shooting was captured on camera and made it to social networking 
sites like Facebook and YouTube, were made popular. It particularly highlighted the elections to 
the world and also the lack of attention from the world media. 
The 2010-2011 Tunisian revolution can be stated to be the first instance where a dictator was 
taken down and the internet played a part in it. The revolution was the result of a number of 
factors, one of which was 'Wikileaks' exposure of the corruption within the ruling family of 
President Zine El Abidine Ben Ali. Also there was large scale internet censorship from the Govt. 
with Facebook pages and profiles of political protestors being hacked and deleted. The 
revolution finally led to Ben Ali's resignation and subsequent exile from the country. 
#jan25 became the symbol of the Egyptian revolution which served as a rallying cry for the 
January 25, 2011 protests in Egypt against socio-politico-economic corruptions. The Facebook 
page 'We are all Khaled Said', named after the Alexandria protestor who was allegedly beaten to 
death by the police inspired thousands to join in the protests which ultimately lead to widespread 
protests in Alexandria and even spread to the Tahir Square in Cairo. The aftermath was the 
resignation of the President Hosni Mubarak. 
The protests of Ukraine in 2013-14 also had a large role played by Social Networks. The 
'Euromaidan' Facebook page which was one of the principal social media outlet for the protests 
had more than 126,000 likes [2]. Hash-tags like #ukraine, #kiev, #Yanukovych, #euromaidan 
were also trending in twitter. The large scale protests against President Yanukovych for his 
support for closer ties with Russia and away from the EU led to violent protest all over Kiev, 
5 
particularly in Kiev's Maidan Nezalezhnosti, near the European Square. Results were the ouster 
of President Yanukovych, and later border standoff with Russia in Crimea. 
The role of the Social Media in the recent political protests and revolutions have made the 
mining of the data, a very important part of understanding these conflicts. Much recent work 
involve understanding these data and the subsequent inference of the cause and effects of these 
protests from data gathered from social media. 
6 
CHAPTER 2 
INFORMATION RETRIEVAL AND QUERY EXPANSION 
2.1. The theory of Information Retrieval 
Information Retrieval (IR) can be defined, with respect to the context of Computer Science, in its 
broadest sense, as the science of getting relevant information for a particular query from data 
which satisfies certain criteria for the user. Relevance is mostly influenced by user preferences, 
e.g., ‘apple’ may either mean the fruit, or the creator of iPhones, depending on what the user is 
looking for. A query can be defined as a collection of search terms or words which most closely 
align with a user’s needs. Data can be structured (web pages, documents, images, videos) or 
structured (xml, rdf, annotated text, traditional database), in a particular data store or the entire 
World Wide Web. The strategies and algorithms involved in Information Retrieval are many and 
varied, and is strongly influenced by the context and expected results of the search [3]. 
The first and most basic step for an IR system is identifying the linguistic language of the 
documents or corpus. Considering the most basic search algorithm-the linear search which goes 
through each word in the document word by word, we have to ensure that the search term is in 
the language of the documents present. Thus, searching for the word ‘book’ in may yield a result 
while ‘livre’ which is the same word in French may not. Also words like ‘analyze’ and ‘analysis’ 
may not yield the same results although the word root and the underlying meaning of both these 
words is the same and a user might be looking for occurrences of both. 
One of the most significant steps towards any successful IR system is a powerful indexing 
system. Indexing not only significantly improves the speed of the search but also reduces space 
7 
as exemplified through binary trees and hash-tables at the basic level. Quoting Boyd’s law- the 
speed of iteration beats the quality of iteration, indexing becomes imperative for corpus which 
may span millions of documents in this age of big data. 
Measuring relevance is challenging and despite some huge advances in IR and ranking 
algorithms, many researchers still follow the traditional methods of manually annotating the 
retrieved documents. Yet, many new Machine Learning, linguistic and Natural Language 
Processing (NLP) techniques are being developed and used to classify, cluster and rank the 
documents. 
All these stages and the different techniques are discussed in the next section. 
2.2. Stages of Information Retrieval for text 
2.2.1. Document pre-processing: 
To effectively parse through a document, it is broken down to a string of tokens. Tokens can be 
created according to white-spaces or characters. The tokenization mainly depends on the 
language involved and a tokenizer which is effective for English and other Latin based languages 
may not be as efficient for other languages. E.g., considering the CJKV class of languages 
(Chinese, Japanese, Korean, Vietnamese), where each character holds meaning, a white-space 
based tokenization is ineffective. During this process, the stop-words, i.e., the common high 
occurrence words are removed. This not only saves space during indexing, but also prevents 
these words from forming any bias in the results. After that, the tokens can be further processed 
through stemming and lemmatization. 
8 
In stemming, words are reduced to their word-stems. E.g. ‘hunt’, ‘hunter’, ‘hunted’, ‘hunting’ are 
all reduced to ‘hunt’. Porter’s stemmer [10] is an important work in this field. Thus, it does not 
really take into account the actual meaning of the final stem created, rather it tries to narrow 
down all words to a particular root that may be considered to represent all forms of that word. On 
the other hand, lemmatization is a more complex process where the actual morphological root is 
used. E.g. ‘good’, ‘better’, ‘best’ is reduced to ‘good’. It usually involves a more complex 
dictionary, with possible context identification. 
2.2.2. Boolean vs Ranked retrieval 
In Boolean retrieval, the search strings are combined using the binary operators (and, or, not) and 
the results are evaluated accordingly. E.g. the combination of the terms ‘UGA’ and ‘computer 
science’ not ‘bulldawgs’ should return all documents which contain the terms ‘UGA’ and 
‘computer science’ but it should also exclude all documents containing the term 
‘bulldawgs’. The obvious advantage of this approach is that it is simple to implement. The 
simplicity of this approach made this a popular choice for many popular legacy IR systems. 
However, the glaring disadvantage of this approach is that it is concerned with the presence of 
the terms in a particular document, but not really with the context or relevance of the terms. E.g. 
in the above example, a document about the ‘Dept. of Computer Science at UGA’ is more 
important than a documents that describes ‘UGA’ somewhere in the document and the ‘Dept. of 
Computer Science’ of another university in another section. Using the Boolean retrieval, both 
these documents will be equally relevant with respect to the search query. To overcome this 
problem, researchers have proposed a system of ranked retrieval. Starting with Luhn [4], a 
ranking system was evolved according to the placement of the text and the relative distance 
9 
between the search terms. The best known use of ranking in modern times is perhaps the Page-
rank algorithm [5] which was pioneered by Google. 
2.2.3. Term vocabulary and indexing 
Storage and indexing of large volumes of texts is always a challenging task in IR systems. As a 
solution to this problem, the concept of Vectors were introduced [6]. In vectorization of texts, 
each term present in the documents is usually assigned an integer value, and in turn, the entire 
document is converted to a vector of integers, and the entire corpus into a matrix. The 
vectorization not only saves space for storage, but also allows many matrix based algorithms 
which is being used in IR. The advantage of this approach became more prominent with the 
introduction of the cosine similarity metric [7]. Here, the similarity between 2 documents is 
measured by the cosine of the angle between them, where each word present in the entire set of 
documents is considered to be a dimension. Thus, if a set of documents has N unique words, 
each document can be represented as a vector in N dimensional space. 
Luhn’s work on term frequency [8] highlighting the importance of word frequencies further 
strengthened the use of word vectors. For the frequency f(t,d) of a term t in document d, the term 
frequency tf(t,d) is defined by: 
𝑡𝑓
(
𝑡, 𝑑
)
= 𝑓(𝑡, 𝑑)
Inverse document-frequency (idf), proposed by Jones [9], consolidates on the term-frequency 
concept while highlighting the uniqueness of terms that may be used to identify a document. For 
‘N’ number of documents, ‘D’ containing the term ‘t’: 
𝑖𝑑𝑓
(
𝑡, 𝐷
)
= 𝑙𝑜𝑔
𝑁
|
{𝑑 ∈ 𝐷: 𝑡 ∈ 𝑑} 
|
10 
Combining the 2 methods, tf-idf is a very popular technique which is used in document pre-
processing. 
tf − idf = 𝑡𝑓 ∗ 𝑖𝑑𝑓
2.2.4. Evaluating quality 
Precision and recall are the most basic measures for evaluation of quality of retrieved documents. 
Precision is the fraction of retrieved documents which are relevant. 
𝑃 = 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 
#(𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 𝑖𝑡𝑒𝑚𝑠 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑)
#(𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 𝑖𝑡𝑒𝑚𝑠)
Recall is the fraction of relevant documents which are retrieved. 
𝑅 = 𝑅𝑒𝑐𝑎𝑙𝑙 = 
#(𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 𝑖𝑡𝑒𝑚𝑠 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑)
#(𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 𝑖𝑡𝑒𝑚𝑠)
Another important metric which is used is the F-measure which is the weighted harmonic mean, 
i.e., the ratio of the geometic mean to the arithmetic mean, of the precision and recall. 
𝐹 = 
(𝛽
2
+ 1)𝑃𝑅
𝛽
2
𝑃 + 𝑅
Where: Where: 
𝛽
2
𝜖[0, ∞]
2.3. Query Expansion with respect to the Twitter API 
2.3.1. The twitter API 
Twitter is a microblogging platform that enables users to post short text statuses limited to 140 
characters, which can be consolidated with hyperlinks, images and videos. This provides an ideal 
platform for people to voice their opinions on a public forum. Thus, it proves to be an important 
source to get data for opinion mining particularly for corporations and industries trying to gather 
user feedback of their products, governments looking into public opinion [13], etc. For this 
11 
purpose, Twitter provides a streaming API which enables data scientists to collect data based on 
particular tracks (search terms), users or locations. 
The data that is returned is in JSON format which not only contains the text of the tweet but also 
other relevant information like the user information, followers count, location details, etc. [15]. 
Some of these details are often considered as metrics to measure the genuineness of a tweet. E.g. 
if the user picture has ever been changed, if the user has many followers, how old the account is, 
etc. 
The disadvantage of the twitter API is that the number of tweets that can be captured is severely 
rate-limited and throttled. Thus, for a heavily tweeted topic, one might not be able to capture all 
the tweets present at that particular time. Also, people working on geographical and spatial 
aspects of twitter data have to depend on the users that explicitly allow Twitter to share their 
location. Also, the tweets are heavily riddled with noise from @users, #tags and http://urls, 
which require a considerable amount of cleaning, particular for the text section before it is usable 
for mining. 
2.3.2. Relevance feedback and its application in twitter 
Relevance feedback is the process by which the original query is extended by using certain 
features or terms of relevant documents. E.g. if after using the term apple, documents containing 
apple products are marked as relevant, then ‘iPhone’, ‘iPad’, ‘iPod’ etc. might be used with the 
original search term to extend the search query. The quality of the new search query is often 
measured by Roccio’s formula [16] which is given as follows: 
12 
𝑄
𝑖+1
= 𝛼𝑄
𝑖
+ 𝛽 ∑
𝐷
𝑖
| 𝐷
𝑖
|
𝑟𝑒𝑙
− 𝛾
∑
𝐷
𝑖
|𝐷
𝑖
|
𝑛𝑜𝑛−𝑟𝑒𝑙
Where: Q
i+1
= Extended Query vector 
D
i
=Retrieved document vector 
α,β,γ=Weights associated with the vectors
The access of the twitter data through the ‘track’ request parameters is the ideal candidate for the 
use of relevance feedback. The track parameters is a comma separated list of phrases, which is 
used to compare the ‘text’ (actual text of the tweet) , ‘extended-url’, ‘display-url’, (URLs 
embedded within the tweet), ‘screen-name’ (user-name or handle) for presence of the phrases. 
This parameter is an ideal candidate for query expansion which is discussed in the next section. 
2.3.3. Query expansion 
One of the most important application of relevance feedback is its use in query expansion where 
the original query is modified using feedback from the results. In this method, the original query 
is appended with search terms appended with the original search query and the new search query 
is run again against the original corpus. 
The search terms used in query expansion can be generated in a number of ways. One of the 
most popular ways is creating a thesaurus which is created by domain experts and use the terms 
present in that thesaurus to expand the terms. E.g. ‘database’ can mean either the datastores like 
‘relational database’, ‘NoSQL database’ etc., an actual database of data like database for 
‘crime’, ‘sports’, ‘country statistics’ etc, or examples of traditional commercial or open-source 
DBMS systems like ‘MySQL’, “Oracle’, ‘PostgreSQL’, MongoDB’ etc. The different variety of 
13 
data that it can mean can only be understood by someone who understands the concept and can 
effectively map all these terms under one cluster. 
Another popular method is using word co-occurrences. Taking the recent outbreak of ‘Ebola’ as 
an example, news articles will often mention ‘West African’ countries like ‘Guinea’, ‘Sierra 
Leone’, ‘Liberia’ etc. where the worst outbreak has been reported. Recent news on a man called 
‘Eric Duncan’ in ‘Dallas, Texas’ being the first Ebola patient in the US is also very widely 
circulated. Also mentioned is the ‘Ebolavirus’, the pathogen causing the disease. Depending on 
the context of the search, any of these words appended with the original search query ‘ebola’ will 
return more relevant results from any search engine. Thus, people interested in the general state 
of the outbreak may want to keep the name of the African countries in their query, people 
interested in the effect in the US can use the name of the person or the place and people wanting 
to know more about the disease in general and its causal agents may want to append the virus 
name. 
Query logs are also a very popular method of getting feedback for queries. E.g. in the context of 
an online store and other ecommerce sites, the actual terms of a search, and the link of the 
product on which the person clicked or finally bought can help these sites return better results to 
users. 
2.3.4. The challenges of evaluation of quality of twitter data 
The most difficult part of evaluating the quality of data in twitter is the fact that the traditional 
methods of evaluation like precision and recall are not applicable because the data generated 
from the twitter stream is dynamic, and only real time data is retrieved. Twitter does not allow 
access to historic data directly, it can only be obtained from some 3
rd
party providers. However, 
14 
one of the most popular methods of evaluation is manual annotation using crowdsourcing. 
Various platforms like Amazon Mechanical Turk and Crowdflower are used for this purpose. 
Other techniques involve machine learning models and NLP rankings. 
15 
CHAPTER 3 
RELATED WORK 
Ziang et al. [17] proposed a new algorithm called “Iterative Single-Keyword Refinement” where 
the F-measure is measured after iteratively adding a new term to the query vector. The following 
measures were introduced to measure the effectiveness of the new query: 
Δ(k,q) = Set of results retrieved by query ‘q’ but not retrieved by adding term ‘k’ to ‘q’ 
𝑐𝑜𝑠𝑡
(
𝑘, 𝑞
)
= 𝑆(𝑅
(
𝑞
)
∩ 𝑈 ∩ 𝐸
(
𝑘
)
)
𝑏𝑒𝑛𝑒𝑓𝑖𝑡
(
𝑘, 𝑞
)
= 𝑆(𝑅
(
𝑞
)
∩ 𝐶 ∩ 𝐸
(
𝑘
)
)
𝑣𝑎𝑙𝑢𝑒
(
𝑘, 𝑞
)
= 
𝑏𝑒𝑛𝑒𝑓𝑖𝑡(𝑘, 𝑞)
𝑐𝑜𝑠𝑡(𝑘, 𝑞)
Where: R(q) = Set of documents retrieved by query q 
U = The number of documents less retrieved, thus improving precision 
C = The number of documents less retrieved, which decreases recall 
E(k) = Set of results without keyword k 
Note: Value is considered as 0 if both benefit and cost is 0 
Mitra et al. [18] studied the effect of boolean constraints along with additional refinements 
among terms, i.e., it considers terms which are within a particular word limit between each other. 
They also considers fuzzy Boolean operators to construct queries, which rank documents if they 
satisfy a partial constraint even if they does not satisfy all the criteria. Since boolean constraints 
have to be specified by humans, they also suggest an automatic filter using term correlations, i.e. 
how frequently a pair of terms occur. 
16 
Mandala et al. [19] worked on a thesaurus based approach. They used a Wordnet based, co-
occurrence based (proximity of 2 words) approach and a predicate argument (subject-verb, verb-
object and adjective-noun) based approach. Combining the above 3 methods through an average 
method, they also suggest a weighing factor that is used along with the query for its evaluation. 
Gaurav et al. [20] uses geo-tagging and n-gram and hash-tag expansion to study the opinion of 
people during the Venezuelan elections. Using the initial name of the candidates as the base 
query, they expanded the query by aggregating all the n-grams and hashtags, and from the tweets 
retrieved, they calculated the number of unique users tweeting, and the sentiment associated with 
the tweet to predict the winner of the elections. 
Kumar et al. [21] proposed expansion of the query terms using the high frequency unique terms 
which are above a particular threshold. Using the multinomial unigram distribution and 
maximum likelihood model, they expanded the query. They use entropy to analyze the results 
which is defined as: 
𝐸
(
𝑞
)
= ∑ 𝑛
(
𝑑
)
𝑙𝑜𝑔𝑛(𝑑)
𝑛
𝑑=1
Where n(d) = Number of tweets for day d 
Massoudi et al. [22] uses the quality indicators of the number of reposts, the number of 
followers of the account and recency of the account to create a global credibility prior probability 
factor, which is used to calculate the score of each term according to the conditional probability 
of the term occurring in the tweet and taking the top k terms according to the score calculated. 
Bandyopadhyay et al
. 
[23] used the Google Search API and the BBC news, and the first 8 
retrieved results to extend the set of word-level n-grams. From the retrieved queries, they used 
17 
the 5 most frequently used terms. They also reformulated the query by excluding the original 
term. 
Guisado-Gámez et al. [24] use a query phrase and a concept for query expansion. The query 
phrase is expanded using a lexical block that identifies concepts similar to the original 
query. They also use a knowledge base (Wikipedia) to perform topological expansion using both 
the original query and the concept specified. 
The lexical block use Wikipedia as a thesaurus and uses the titles of articles as synonyms. For 
the topological expansion, they use bigrams from the synonyms of the original query, perform 
path analysis using redirects, and finally a community search using the closely linked paths. 
18 
CHAPTER 4 
THE METHODOLOGY OF EVALUATION-VELOCITY AND RELEVANCE 
4.1. The need for a new methodology 
Traditional IR systems use the concept of precision and recall for evaluation. It is very useful in 
the case when the expected results are known, i.e., we can evaluate the number of results 
retrieved from an IR system, how many results exactly fit the model that was used to retrieve the 
results and how many entries are present in the IR system that fit the model. 
The drawback of this method is the inability to measure the quality of results that is retrieved 
using the Twitter Streaming API. The Streaming API uses the track filter, among others, which 
takes a list of search terms as input, and returns all results which contain any of the search terms 
in the text, hash-tag or url of the tweet. Since it is primarily a word based search, not every 
relevant tweet is retrieved. E.g. in our case for retrieving results for the Ukraine conflict, we used 
the search term ‘ukraine’ to start retrieval of tweets. Now, the conflict involves other actors and 
events which were relevant to our context and constantly mentioned in tweets like ‘russia’, 
‘kiev’, ‘crimea’, ‘euromaidan’, etc. These keywords can either be recognized by a domain 
expert, or by an effective key-word expansion algorithm. Some of the tweets pertaining to the 
conflict might not contain the keyword ‘ukraine’, but contain the other keywords, due to which 
we need to expand the query set. 
The Streaming API is rate limited, i.e., a limited number of tweets can be retrieved in a unit of 
time. Thus, we want to ensure that the maximum number of relevant tweets can be retrieved 
during that time period. Also, retrieving using the same keyword can give a stagnation of data, 
19 
i.e., similar types of data related to the context is retrieved. A model to work effectively needs a 
dataset that covers the range of the topic under question. E.g. for a binary classifier, a dataset that 
is heavily inclined towards one class may not be able to correctly classify the other class 
correctly. For this purpose, a balanced dataset is required. 
However, the most fitting measure of retrieval is precision and recall. However, the pre-requisite 
to measure precision and recall is a pre-classified data set. In case of twitter where data retrieval 
is dynamic, precision and recall is not really useful because that will involve first capturing and 
storing the data that is retrieved, then classifying the data, and even that cannot guarantee the 
performance of the model with respect to future data. However, if we know the model to be 
accurate, we can measure the performance change with respect to addition or deletion of 
keywords, and according to that criteria, an evaluation can be made. For this purpose, we 
introduce the concept of velocity and variety that will enable us to evaluate dynamic twitter data. 
4.2. The concept of velocity and relevance 
Velocity: The number of tweets that can be retrieved per second. For a trending topic, the 
velocity increases, while as the interest on the topic goes down, the velocity decreases. This 
directly affects the number of tweets retrieved. 
𝑉𝑒𝑙𝑜𝑐𝑖𝑡𝑦
(
𝑣
)
= 
𝑁𝑜. 𝑜𝑓 𝑡𝑤𝑒𝑒𝑡𝑠 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒𝑑 
𝑇𝑖𝑚𝑒 𝑡𝑎𝑘𝑒𝑛 𝑡𝑜 𝑟𝑒𝑡𝑟𝑖𝑒𝑣𝑒 𝑡ℎ𝑒 𝑡𝑤𝑒𝑒𝑡𝑠
= 
𝑛
𝑡
Query: The number of different search terms which are used to retrieve the tweets. It is 
represented as a vector of strings. For a particular time period, the query can be defined as: 
𝑞
𝑡+1
= 𝑞
𝑡
+ 𝑤 
20 
Where: q
t+1
= Query vector in time period t+1 
q
t 
= Query
vector in time t 
w = Term vector added to the search query 
Acceleration: The rate of change of velocity. It can indicate a spike of interest in a particular 
topic. 
𝑎 = 
𝑣
𝑡+∆𝑡
− 𝑣
𝑡
∆𝑡
Relevance: The fraction of tweets which are useful which were retrieved because of the search 
term. If a total of n tweets were collected, giving n
u
unique tweets and n
u
’ duplicate ones, 
n = n
u
+ n
u
’ 
Out of the n
u
tweets, if a model determines that n
r
are relevant and n
r
’ are irrelevant, 
n
u
= n
r
+n
r
’ 
So, the relevance R is determined by: 
R = n
r
/ n
u
Velocity factor: A constant, which determines the importance of velocity for the evaluation of 
the search term. It is denoted by ‘α’. If we decide that the quality of the search term will be 
determined solely by the number of relevant tweets retrieved by addition of the term, α=0 is 
considered. Although it is up to the discretion of the user to select α, we should be careful to be 
choosing a value such that v
s/
v
max
≯R
. 
Relevance factor: A constant which is used to determine the importance of relevance for 
evaluation of the search term. For a search term ‘q’, the relevance component is denoted by ‘β
q
’. 
21 
For a machine learning model, it can be the accuracy of the model. For a NLP model, it can be 
the cosine similarity from the most relevant tweets. In case all tweets are deemed relevant, 
β
q
= 1 is considered. 
Impact factor: It is the equation that is used to evaluate the quality of the new term to the search 
query. To calculate the impact factor of each term of the search query, we consider 2 variables, 
the ‘Velocity ratio’ (VR) and the ‘Relevance’ (R). 
VR is the ratio of the velocity of tweets after addition of the search term ‘s’ to the maximum 
possible velocity achievable which is determined experimentally. 
𝑉𝑅
𝑠
= 
𝑣
𝑠
𝑣
𝑚𝑎𝑥
Where: v
s
= The velocity of tweets after adding the search term 
v
max 
= The maximum possible velocity achievable 
The Velocity Factor α is associated with the velocity ratio in the equation. Their product forms 
the Velocity Component (VC
s
) for the search term ‘s’ of the equation. 
𝑉𝐶
𝑠
= 𝛼 ∗ 𝑉𝑅
𝑠
The product of the RF 
β
and the relevance forms the Relevance Component (RC
s
) of the 
equation for the search term ‘s’. 
𝑅𝐶
𝑠
= 𝛽 ∗ 𝑅
𝑠
The product of the Velocity Component (VC) and Relevance Component (RC) gives us the 
Impact Factor (IF
s
) for the search term ‘s’. 
𝐼𝐹
𝑠
= 𝑉𝐶
𝑠
∗ 𝑅𝐶
𝑠
=> 𝐼𝐹
𝑠
=
(
𝛼 ∗ 𝑉𝑅
𝑠
)
∗ ( 𝛽
𝑠
∗ 𝑅
𝑠
)
22 
Where: I.F
s
= Impact Factor that determines the quality of the search term ‘s’ 
α = Velocity factor. 
VR
s
= Velocity Ratio for the search term ‘s’ 
β
s 
= Relevance factor. 
R
s
= Relevance 
Thus, substituting the values, we get the impact factor as: 
𝐼𝐹
𝑞
= 𝛼
𝑣
𝑠
𝑣
𝑚𝑎𝑥
∗ 𝛽
𝑠
𝑅
Where: v
t
= Avg. velocity during the period where the query is used 
v
max
= Maximum velocity that can be achieved 
R = n
rq
/n 
𝑛 = 𝑛
𝑟
+ 𝑛
𝑟
′
α = Velocity factor 
β
s
= Relevance factor for the particular search term s 
𝛼
𝑣
𝑠
𝑣
𝑚𝑎𝑥
= Velocity component 
𝛽
𝑞
𝑅
= Relevance component 
The I.F. can lie between [0, α*β]. The significance of the Velocity Factor (VF), α, is that if we 
need a higher velocity of tweets, i.e., more tweets per second, irrespective of the relevance 
associated with the search term, we should keep a higher value of α. The Relevance factor (RF), 
β, signifies the importance of the search term with respect to finding relevant tweets. With 
respect to machine learning algorithms, it can be the accuracy of the model used to classify the 
tweets. This is because, as an example, if a model with 50% accuracy, classifies 50 tweets out of 
23 
100 as the class that we require, then, according to the model, we can say that 25 of those tweets 
actually belong to that class. In that case, β
s
for all the search term [s
1, 
s
2
… s
n
] will be equal to 
the accuracy of the model. Similarly, for a NLP model, we could use the average cosine distance 
between the search term and the set of relevant tweets, or a more comprehensive Roccio’s 
formula to determine the value of each β
s
. 
24 
CHAPTER 5 
SYSTEM ARCHITECTURE 
We have designed a generic architecture that can be used in a system where the concept of 
velocity and relevance can be applied to evaluate the Impact factor of each query term. This was 
designed keeping in mind that the underlying model which is used to evaluate the Relevance 
component is flexible in the sense that it can be a NLP, Machine Learning or simple word-
expansion model. The various components of the system is discussed here below. 
Figure 1: System Architecture for evaluation 
25 

Initial query: It is the initial query that is used with the Streaming API to start the initial 
stream process. The initial query is usually something broadly related to the topic which 
the user is interested in. 

Twitter API: After creating the developers account with Twitter, it provides with the 
OAuth tokens that is used to initialize the streaming process. 

Corpus: The tweets that are returned by the streaming process. It is in the form of JSON 
and each individual tweet has its own key-value pairs. The ‘text’ key is particularly of 
interest as it contains the actual text of the tweet. 

Unique text extractor: Many of the tweets are ‘Retweets’, which is the original tweet 
shared by another person. For our experiment, we are mainly concerned with the 
uniqueness of the tweet, so any duplicate tweet is ignored. 

Index of tweet_id: The ‘id’ key provides a unique integer key to a particular tweet. By 
storing the ids of all the tweets, we ensure the uniqueness of a tweet by querying it 
against the index for duplicity. 

Unique tweets: It is the set of tweets after all the duplicates/ retweets have been removed. 

Text filtering: Since we are only concerned with the text of our tweet for our experiment, 
the JSON is decoded, and the ‘text’ field is extracted. Further the text is analyzed for 
@users and urls which are removed. #tags are converted to words after removing the 
initial ‘#’ symbol. Tokenization also occurs at this stage. 

Filtered text: It contains all the tokenized text of the tweets. Thus the entire corpus is in 
the form of a ‘word matrix’ at this stage. 
26 

Vector transform: Here the document matrix is converted to a vector matrix through 
some statistical NLP technique. 

Vector model: The statistical NLP technique applied to the document matrix. 

Document matrix: The final input matrix where each word has been replaced by a value 
associated with it. 

Model: The Machine Learning or NLP model that is used on the document matrix to 
process it. In our case, we have used the ‘Latent Semantic Indexing (LSI)’ and a ‘Naïve 
hash-tag expansion’ technique to process the matrix, as it is relatively easier to isolate the 
keywords. 

Model fitting: The model is applied to our document matrix. 

Prediction: The output after application of the model, depending on the type of model 
being used. E.g. in our case, since LSI was applied to the corpus, the output will be the 
‘cosine similarity’ between the model, and each tweet. 

Query expansion: The top high-score terms are used to create the new query set that is 
going to be used with the Twitter API. It is used in conjunction with DBPedia Spotlight 
to identify the requisite entities. 

Spotlight: The API that is provided to use DBPedia to identify entities. The terms 
extracted in the previous stage is used to identify the entities which are associated with 
people, places and organization. 

IF evaluation: The Impact Factor is evaluated for each search term at this stage. 
27 

Query set: It’s the final output with a new query set which is used with the Twitter API 
and the process is repeated. 
The system was developed using Python 2.7. The tweets were collected using the tweepy [25] 
package and stored in text files in the file-system. The index of tweets were stored as a python 
dictionary, since the number of indices for the unique tweets came to 4.5 GB of main memory. 
For larger data sets, a dedicated index should be preferred. The Python NLTK [26] library was 
used for Natural Language processing and the Anaconda [27] package was used for the machine 
learning algorithms. The genism [28] package was used to perform the latent semantic analysis. 
The Spotlight [29] API was used for the annotation using DBPedia. 
28 
CHAPTER 6 
EXPERIMENTS AND RESULTS 
6.1. Validation using naïve hashtag expansion 
The first experiment was conducted to highlight the importance of velocity as a component to 
measure the importance of search terms in the query. We chose a dynamic scenario-a football 
game. The expected trend is that the tweets starts flowing once the game starts, goes on 
throughout the game and then eventually fades out after the game ends. This serves as a 
simulation to many real world scenarios like conflicts, business etc. The experiment was 
developed in Python, with the python library ‘tweepy’ used to connect to the streaming API. 
This experiment was conducted was during the Georgia vs Florida football game on 1
st
November, 2014. A naïve hashtag expansion technique was used, where the 1
st
10, most 
frequently used hashtags were used to expand the query vector. Since twitter only allows English 
queries in its track parameters, only the English hashtags were used among the top 10. 
The game was scheduled to start at 15:30 on 1
st
November, 2014. However the process of 
collecting tweets was started at 7:09. During this time, before the start of the game, 2 expansions 
were conducted. The initial query was ‘uga’. The velocity was calculated as the time required to 
collect 100 tweets. Since the velocity was at a low 0.362319, the first expansion was done 
rapidly, which led to addition of the terms ‘Dawgs’, ‘UF’, ‘GAFL14’, ‘FLGA14’, ‘GoDawgs’, 
‘Georgia’, ’UGAvsUF’, ‘bulldogs’. This improved the velocity to 1.11. The second expansion, 
led to addition of the term ‘ItsTime’, which further improved the velocity to 1.899. The 3rd 
expansion, just before the start of the game added the terms ‘UFvsUGA’ which was the official 
29 
hashtag proposed by ESPN before the game, “gators’ , ‘Zelo’ and ‘Florida’. The final expansion 
during half-time were ‘ItsGreatUF’, ‘GatorNation’ and ‘GoGators’. 
The time taken to collect 100 tweets were collected in the log, and the average velocity during 
each expansion, pre-game, half-time, and post-game were calculated using the average of the 
velocities during each period. 
Initial 
1
st
2
nd
3
rd
Start 
Half 
Final 
Velocity 
0.362319 
1.111111111 
1.899098 
3.108734 
15.57337 
22.56493506 
3.749290473 
Table 1: Velocity for different periods during the game 
The chart below clearly shows how the velocity increased during the start of the game and 
slowly faded down after its completion. 
Figure 2: Change in velocity during UGA vs FL 
0
20
40
60
80
100
120
7:9:50
11:5:48
12:25:20
13:47:39
15:21:9
15:58:21
16:29:29
16:53:19
17:17:8
17:33:38
17:45:25
17:57:40
18:8:59
18:17:24
18:24:5
18:40:35
18:54:14
18:59:11
19:6:7
19:11:31
19:19:12
19:32:44
19:57:15
20:28:12
21:9:2
21:51:2
22:40:21
23:30:27
0:40:14
1:57:45
Velocity
Time
Time series plotting of velocity in UGA vs FL
30 
Out of 352086 tweets collected, 57250 or 16.26% of the total tweets were unique. The total 
number of tweets and the number of unique tweets during each expansion and stage of the game 
are mentioned below. 
Initial 
1st 
2nd 
3rd 
Start 
Half 
Final 
Total 
JSON 
106 
200 
10000 
65185 
95282 
55652 
125661 
352086 
Clean 
18 
180 
2068 
9031 
12042 
15518 
18393 
57250 
Table 2: Total tweets retrieved during each stage 
The Relevance Component was calculated using the total number of tweets which have the query 
term mentioned at least once divided by the total number of unique tweets collected. The 
Relevance Component of each term during each period is given below: 
Figure 3: Change in Relevance throughout the game 
0
0.2
0.4
0.6
0.8
1
1.2
Pre-game
1st Exp
2nd Exp
3rd Exp
Start
Half
End
Change in Relevance throughout the game
uga
'Dawgs'
'UF'
'GAFL14'
'FLGA14'
'GoDawgs'
'Georgia'
'UGAvsUF'
'bulldogs'
'ItsTime'
'UFvsUGA'
'Gators'
'Zelo'
'Florida'
'ItsGreatUF'
'GatorNation'
'GoGators'
31 
The cumulative relevance factor across all the periods is given here below. It shows that 
‘Georgia’ and ‘Florida’ have the highest relevance with respect to the number of mentions. 
Considering all tweets collected as relevant, α=1 is considered. 
Figure 4: Average Relevance across all periods 
To calculate the velocity factor, the maximum achievable velocity was measured using stop-
words’ as search terms. 
No. of Stop-words 
1 
2 
5 
10 
20 
No. of tweets collected per 
second 
74.65753 
30.44693 
198 
206.25 
153.0769 
Table 3: Velocity for different number of stop words 
It was found that 10 stop-words give the maximum velocity of 206.25 tweets/second. Thus, v
max
= 206.25 was considered. The velocity of each period was measured and normalized. To 
calculate the velocity factor, 
α was calculated to ensure that the effect of both velocity and 
relevance is equivalent. Thus, it was calculated as: 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Average Relevance 
32 
𝑅𝐹
𝑚𝑎𝑥
𝑅𝐹
𝑚𝑖𝑛
= 𝛼
𝑣
𝑚𝑎𝑥
𝑣
𝑚𝑖𝑛
Thus the value of α
was calculated as 
2.52281005. The final table of the velocity component is 
as follows: 
Initial 
1
st
2nd 
3
rd
Start 
Half 
Final 
Velocity 
(V) 
0.362319 
1.111111111 
1.899098 
3.108734 
15.57337 
22.56494 
3.74929 
V/V
max
0.001757 
0.005387205 
0.009208 
0.015073 
0.075507 
0.109406 
0.018178 
Velocity 
component 
0.004432 
0.013590896 
0.023229 
0.038025 
0.19049 
0.27601 
0.045861 
Table 4: Velocity component for different periods 
Using the velocities corresponding to when the search term was added to the final query, the 
final Impact factor of each term is given below: 
Figure 5: Impact factor for original and cumulative velocity 
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
Impact Factor(IF)
Original
Cumulative
33 
Thus we could see that ‘Florida’ has the highest impact, followed by Georgia. This was because 
these 2 terms encompassed a more general set of tweets that may or may not pertain to the game 
itself. However, considering tweets pertaining to the game, ‘GatorNation’, ‘GoGators’, 
‘ItsGreatUF’ and ‘Gators’ have a higher impact factor which is expected as it was a UF win. 
Even considering the cumulative velocity (v
t-1 
- v
t
), we get similar results. 
6.2. Validation using Latent Semantic Analysis (LSA): 
This experiment was conducted over a long period of time, and done with a NLP model to 
validate the importance of each tweet. It was also conducted ignoring the component of velocity 
to validate the fact that it can be used when the velocity is steady, i.e., there is not much 
variation, thus no real activity regarding the topic in question. This can be useful in particular 
scenarios where spikes are not expected, such as general trends like expressing feelings, opinions 
etc. We have used Latent Semantic Indexing (LSI) to rank each individual tweet and according 
to the LSI score, we rank each query. The python library ‘tweepy’ was used for the twitter API, 
along with ‘gensim’ which was used for the Latent Semantic Analysis (LSA). 
The initial keyword chosen was ‘ukraine’, which was later expanded using LSI to find the top 10 
terms which were found in a particular topic. The total topics selected for LSA was 2. Choosing 
the first topic as the benchmark, we expanded the query vector using the top 10 contributing 
words for each topic. Further, we used DBPedia Spotlight to annotate each term to ensure that 
the contributing word is either a person, place or organization. Finally, we expand the search 
query with the terms ‘russia’, ‘obama’, ‘crimea’ and ‘palin’. 
34 
The tweets were collected for a duration of 6 months from May 2014-October 2014, during 
which a single query expansion was performed using the tweets collected using the initial query 
to build a model. The calculations were conducted using 2 different threshold values of the LSI 
results (0.0 and 0.5), on the 2 data sets (tweets collected from the original and the extended 
queries) and 2 different scenarios (when tweets containing a single search term was considered 
and tweets which contained the search terms irrespective of the other search terms) for a total of 
8 different set of results. 
Total 
Russia 
no 
Russia 
LSI 
Obama 
no 
Obama 
LSI 
Crimea 
no 
Crimea 
LSI 
Palin 
no 
Palin 
LSI 
Ukraine 
no 
Ukraine 
LSI 
5NUO 
598883 
11299 
0.76 
16167 
0.757 
4747 
0.676 
1307 
0.922 
15947 
0.744 
5NUE 
76682 
455 
0.74 
35293 
0.551 
12 
0.595 
14 
0.680 
9 
0.641 
0NUO 
598883 
101922 
0.27 
28430 
0.602 
47615 
0.262 
2314 
0.610 
154711 
0.242 
0NUE 
76682 
10051 
0.316 
35338 
0.550 
294 
0.201 
779 
0.180 
1795 
0.272 
5UO 
598883 
836 
0.603 
2641 
0.611 
398 
0.587 
41 
0.709 
388 
0.629 
5UE 
76682 
114 
0.579 
34956 
0.548 
2 
0.529 
14 
0.680 
0 
0 
0UO 
598883 
11839 
0.348 
2641 
0.611 
6805 
0.251 
608 
0.177 
32073 
0.112 
0UE 
76682 
8186 
0.298 
34956 
0.548 
220 
0.161 
19 
0.617 
237 
0.069 
Table 5: No. of tweets and average LSI 
The table above shows the number of tweets for each scenario, and the average LSI score. Here, 
each row is defined as [0-5][U-NU][O-E]. [0-5] indicates whether the threshold score is 
considered as 0.0 or 0.5. [U-NU] indicates if the tweets considered contained only the particular 
term uniquely or with other terms. [O-E] indicates whether the experiments were conducted on 
the original or the extended dataset. 
It can be seen from the data that ‘Obama’ consistently had a high number of tweets for all 
scenarios, while ‘Ukraine’, being the original query scored high in the original data sets, while its 
score came down when combined with the other terms. ‘Russia’ also had a similar trend like 
35 
Ukraine, indicating a co-relation with Ukraine. However, no tweets were found which had a 
threshold of 0.5 which only contained Ukraine. ‘Crimea’ had a high score in the original data set, 
but drastically reduced in the extended one. Due to the low number of tweets from ‘Palin’, it can 
be virtually ignored. 
Figure 6: Average LSI score for each search term 
The average LSI scores indicate 'Palin' having the highest LSI score across 5 scenarios out of 8. 
‘Obama’ consistently crossed 0.5 across all scenarios. Russia had a high average score when 0.5 
was considered as the benchmark, however it fell down when it is changed to 0. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
5NUO
5NUE
0NUO
0NUE
5UO
5UE
0UO
0UE
LSI Results(
β)
russia_LSI
Obama_LSI
Crimea_LSI
Palin_LSI
Ukraine_LSI
36 
Figure 7: Relevance component for all scenarios 
Calculating the relevance factor, with 
β
q
= the average LSI score, it can be seen that ‘Obama’ 
scores high in all the extended data sets, indicating the addition of ‘Obama’ to the query vector 
drastically increased the number of tweets pertaining to our scenario. 
Figure 8: Impact factor for all scenarios 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
5NUO
5NUE
0NUO
0NUE
5UO
5UE
0UO
0UE
Relevance component
russia_no
Obama_no
Crimea_no
Palin_no
Ukraine_no
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
5NUO
5NUE
0NUO
0NUE
5UO
5UE
0UO
0UE
Impact Factor
Russia
Obama
Crimea
Palin
Ukraine
37 
The Impact factor which was calculated ignoring the velocity component (α = 0), shows how 
Obama scores high, especially in the extended dataset. The original dataset has a more consistent 
Impact factor across the queries, with a bias towards ‘ukraine’, as it was the original query. 
6.3 Validation using naïve bayes classification 
Unlike the previous 2 experiments, where either the VF or the RF was ignored while determining 
the IF of each term, in this experiment, we have considered both the velocity and relevance 
components simultaneously to determine the importance of a particular query. 
The scenario we have considered here is the conflict in the Syria and Iraq region. We had some 
legacy tweets collected from Mar 2014-May 2014, from which 40 relevant and 40 irrelevant 
texts from the tweets were selected and manually annotated as ‘Rel’ or ‘Irr’. These tweets were 
used with a Naïve Bayes classifier and the resulting model gave an accuracy of 65% on a ¾ - ¼ 
split cross-validation. Since we were concerned with only the proof of concept, we did not strive 
for better accuracy. 
Search 
Term 
Ratio 
Value 
amp 
irr : rel 
2.3 : 1.0 
isis 
rel : irr 
2.2 : 1.0 
syria 
rel : irr 
1.8 : 1.0 
iraq 
rel : irr 
1.8 : 1.0 
isis 
irr : rel 
1.2 : 1.0 
iraq 
irr : rel 
1.1 : 1.0 
syria 
irr : rel 
1.1 : 1.0 
amp 
rel : irr 
1.1 : 1.0 
twitter 
irr : rel 
1.1 : 1.0 
palestine 
rel : irr 
1.1 : 1.0 
38 
From the top 10 most informative features [30], the ones contributing to the relevant class were 
chosen. Thus, ‘Syria’ and ‘Iraq’ were selected for the next expansion. However, we also 
manually added the term ‘ISIL’ which is sometimes used in lieu of ISIS. 
Each of these search terms were used in conjunction with our original search term ‘ISIS’ and the 
time taken to collect 100 tweets is recorded for collecting a total of 5000 tweets. The average 
time taken is given below: 
Keyword 
Time for 100 
tweets (seconds) 
Δt 
Velocity 
v/v
max 
ISIS 
146.0869565 
0.68452381 
Syria 
81.98181818 
64.10513834 
1.219782657 
0.005914098 
Iraq 
82.6122449 
63.47471162 
1.210474308 
0.005868966 
ISIL 
51.36363636 
94.72332016 
1.946902655 
0.009439528 
Table 6: Velocity for different terms 
Figure 9: Comparison of the velocities 
It can be seen that ISIL has a higher velocity than Syria and Iraq. 
0
0.5
1
1.5
2
2.5
ISIS
Syria
Iraq
ISIL
Velocity
39 
To calculate the relevance component, we first filter out the 5000 tweets for duplicates. Then we 
use the classifier to classify each tweet. Since the accuracy of the model is 65%, we assume the 
value of the relevance factor, β
q
= 0.65 for each term. The relevance component is then 
calculated. 
Keyword 
Information gain 
Accuracy(β) 
Relevant 
tweets 
β*RC 
Syria 
1.8 
0.65 
1129 
0.14677 
Iraq 
1.8 
0.65 
1004 
0.13052 
ISIL 
1 
0.65 
946 
0.12298 
Table 7: Relevance Component for each term 
Figure 10: Relevance Component of each search term 
To ensure that the Velocity Component has equal weight as the Relevance Component we chose 
α so that α*v/v
max 
and 
β
*n
q
/n are almost equal. We calculated the VC for 3 options, when the 
maximum of both the velocity and relevance, the minimum of the 2 and the average of the 
velocity and relevance for the 3 search terms is considered. 
0.11
0.115
0.12
0.125
0.13
0.135
0.14
0.145
0.15
Syria
Iraq
ISIL
β*RT
40 
Keyword 
v/Vmax 
VF(
α
max
)
VF(
α
min
) 
VF(
α
mean
) 
Syria 
0.005914098 
0.091955034 
0.123925696 
0.111543203 
Iraq 
0.005868966 
0.091253311 
0.12298 
0.110692 
ISIL 
0.009439528 
0.14677 
0.197798571 
0.178034797 
Table 8: Velocity factor for the 3 scenarios 
Figure 11: Comparison of the Velocity Factors for the 3 search terms 
The final impact factor is calculated through the equation for the various values of α. 
Keyword 
IF(Max) 
IF(min) 
IF(mean) 
Syria 
0.01349624 
0.018188574 
0.016371196 
Iraq 
0.011910382 
0.01605135 
0.01444752 
ISIL 
0.018049775 
0.024325268 
0.021894719 
Table 9: Impact Factors for various Velocity factors 
0
0.05
0.1
0.15
0.2
0.25
VF(max)
VF(min)
VF(mean)
Velocity Factor (α)
Syria
Iraq
ISIL
41 
Figure 12: Comparison of the various Impact factors 
It can be seen from the chart that ISIL has the largest IF among the 3 search terms. Although the 
Relevance Component of ISIL is lower than the other 2 terms, the difference in velocity in 
comparison to them compensates for it. Since we adjusted the velocity factor to ensure that the 
velocity component is comparable to the relevance component, the overall IF was affected. 
0
0.005
0.01
0.015
0.02
0.025
0.03
IF(Max)
IF(min)
IF(mean)
Impact Factors
Syria
Iraq
ISIL
42 
CHAPTER 7 
CONCLUSION AND FUTURE WORK 
We have considered 2 scenarios, a naïve approach, and a more specific, vector semantic 
approach. In both scenarios, we tried to evaluate the impact of each search term in the overall 
quality of tweets retrieved. Considering the parameters of velocity and relevance is the 
equivalence of ‘Precision’ and ‘Recall’ that we use in traditional IR systems. Precision is 
measured by the underlying model which is used to evaluate the quality of tweets. We have 
shown that this methodology can be used with any underlying model. 
The problem of measuring Recall in a dynamic streaming system is more challenging because of 
the inability to retrieve historical data for evaluation. For this purpose, we have used the 
parameter of velocity and its changes with additional search terms. With the increase in velocity, 
we can collect more tweets and if the search term is highly relevant, more relevant tweets are 
expected to be fetched. Also, if we don’t have a constraint of space, we can try to achieve 
maximum velocity to retrieve as many tweets as possible so that the probability of getting a 
relevant tweet in a unit of time increases. 
This ranking mechanism is particularly useful when we have to balance between the ability to 
get only relevant tweets, and to get as many relevant tweets according to our model as possible. 
By varying the values of α and β, we can achieve this. This methodology can also be useful when 
an API is accessible by a URL, and the URL length is restricted, so that we can use only a 
limited number of queries. 
43 
Of course, the basis of this methodology is a strong model at the background that has a high 
precision and accuracy. However, using only a single query may lead to highly accurate tweets, 
but fail to identify the other entities that might be involved in the topic under discussion. The use 
of the Impact Factor with different terms that can fully encapsulate the topic under discussion 
will increase the veracity in the data fetched, which is particularly useful for understanding 
topics and trends like social conflicts. However, some research needs to go into this area before it 
can be established. 
Thus, in conclusion, we can infer that choosing the best terms in a search query can drastically 
improve the quality of data retrieved, and the Impact Factor can be the benchmark, especially for 
streaming text data. 
44 
BIBLIOGRAPHY 
[1] http://mashable.com/2009/12/26/twitter-year-review/ 
[2] http://www.washingtonpost.com/blogs/monkey-cage/wp/2013/12/04/strategic-use-of-
facebook-and-twitter-in-ukrainian-protests/ 
[3] Sanderson, Mark, and W. Bruce Croft. "The history of information retrieval 
research." Proceedings of the IEEE 100.Special Centennial Issue (2012): 1444-1451. 
[4] 
Luhn, Hans Peter. "A statistical approach to mechanized encoding and searching of literary 
information." IBM Journal of research and development 1.4 (1957): 309-317. 
[5] Page, Lawrence, et al. "The PageRank citation ranking: Bringing order to the web." (1999). 
[6] Switzer, Paul. "Vector images in document retrieval." Statistical association methods for 
mechanized documentation (1965): 163-171. 
[7] Salton, Gerard. "Automatic information organization and retrieval." (1968). 
[8] Luhn, Hans Peter. "The automatic creation of literature abstracts." IBM Journal of research 
and development 2.2 (1958): 159-165. 
[9] Jones, Karen Sparck. "A statistical interpretation of term specificity and its application in 
retrieval." Journal of documentation 28.1 (1972): 11-21. 
[10] Porter, Martin F. "An algorithm for suffix stripping." Program: electronic library and 
information systems 14.3 (1980): 130-137 
[11] Jones, Karen Sparck. "A statistical interpretation of term specificity and its application in 
retrieval." Journal of documentation 28.1 (1972): 11-21. 
[12] http://nlp.stanford.edu/IR-book/pdf/08eval.pdf 
45 
[13]
Tumasjan, Andranik, et al. "Predicting Elections with Twitter: What 140 Characters Reveal 
about Political Sentiment." ICWSM 10 (2010): 178-185. 
[14] http://oauth.net/ 
[15] https://dev.twitter.com/overview/api/tweets 
[16] Salton, Gerard, and Chris Buckley. "Improving retrieval performance by relevance 
feedback." Readings in information retrieval 24.5 (1997). 
[17] Liu, Ziyang, Sivaramakrishnan Natarajan, and Yi Chen. "Query expansion based on 
clustered results." Proceedings of the VLDB Endowment 4.6 (2011): 350-361. 
[18] Mitra, Mandar, Amit Singhal, and Chris Buckley. "Improving automatic query 
expansion." Proceedings of the 21st annual international ACM SIGIR conference on Research 
and development in information retrieval. ACM, 1998. 
[19] 
Mandala, Rila, Takenobu Tokunaga, and Hozumi Tanaka. "Query expansion using 
heterogeneous thesauri." Information Processing & Management 36.3 (2000): 361-378. 
[20] 
Gaurav, Manish, et al. "Query Expansion to Search Politically Relevant Tweets." (2014). 
[21] 
Kumar, Naveen, and Benjamin Carterette. "Time based feedback and query expansion for 
twitter search." Advances in Information Retrieval. Springer Berlin Heidelberg, 2013. 734-737. 
[22] Massoudi, Kamran, et al. "Incorporating query expansion and quality indicators in searching 
microblog posts." Advances in Information Retrieval. Springer Berlin Heidelberg, 2011. 362-
367. 
[23] Bandyopadhyay, Ayan, et al. "Query expansion for microblog retrieval."International 
Journal of Web Science 1.4 (2012): 368-380. 
46 
[24] Guisado-Gámez, Joan, David Dominguez-Sal, and Josep-LLuis Larriba-Pey. "Massive 
Query Expansion by Exploiting Graph Knowledge Bases." arXiv preprint arXiv:1310.5698 
(2013). 
[25] https://github.com/tweepy/tweepy 
[26] http://www.nltk.org/ 
[27] https://store.continuum.io/cshop/anaconda/ 
[28] http://radimrehurek.com/gensim/ 
[29] https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki/Web-service 
[30]http://www.nltk.org/api/nltk.classify.html#nltk.classify.naivebayes.NaiveBayesClassifier.mo
st_informative_features 
