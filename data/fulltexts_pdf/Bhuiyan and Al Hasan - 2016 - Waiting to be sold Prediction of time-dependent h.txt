Waiting to be Sold: Prediction of Time-Dependent
House Selling Probability
Mansurul Bhuiyan
Department of Computer and Information Science
Indiana University-Purdue University Indianapolis
Indianapolis,
USA
mbhuiyan@iupui.edu
Mohammad Al Hasan
Department of Computer and Information Science
Indiana University-Purdue University Indianapolis
Indianapolis,
USA
alhasan@cs.iupui.edu
I.
A
BSTRACT
Buying or selling a house is one of the important decisions
in a person’s life.
Online listing websites like “zillow.com”,
“trulia.com”,
and “realtor.com” etc.
provide signiﬁcant
and
effective assistance during the buy/sell process. However, they
fail
to supply one important
information of
a house that
is,
approximately how long will
it
take for
a house to be sold
after it ﬁrst appears in the listing? This information is equally
important
for both a potential
buyer and the seller.
With this
information the seller
will
have an understanding of
what
she can do to expedite the sale,
i.e.
reduce the asking price,
renovate/remodel some home features, etc. On the other hand,
a potential
buyer will
have an idea of the available time for
her to react
i.e.
to place an offer.
In this work,
we propose
a supervised regression (Cox regression)
model
inspired by
survival
analysis to predict
the sale probability of
a house
given historical
home sale information within an observation
time window.
We use real-life housing data collected from
“trulia.com”
to validate
the
proposed prediction algorithm
and show its superior performance over traditional regression
methods. We also show how the sale probability of a house is
inﬂuenced by the values of basic house features, such as price,
size,
# of bedrooms,
# of bathrooms,
and school quality.
II.
I
NTRODUCTION
In recent
years,
online resources are frequently being used
as an essential part of a home buyer’s decision-making process.
To aid such a decision making,
potential
buyers visit
various
real
estate listing sites,
subscribe to online forums related to
home market,
and read blogs
for
home buying advice.
In
a recent
study by Google and the National
Association of
Realtors [1],
market
researchers found that
nine out
of
10
home buyers depend on the Internet as their primary research
source. 52% percent of new home buyers start their adventure
of buying a dream home from the Internet.
Among online resources for
home purchase and sale,
the
listing
websites,
such
as,
“trulia.com”,
“zillow.com”,
and
“redﬁn.com”
are
the
most
popular
and resourceful.
They
provide various
services
for
home buyers
so that
the buy-
ers can make an informed decision and effective purchase.
For
example,
for
every home that
is listed,
the listing sites
include information on crime,
schools,
businesses,
commute,
mortgage,
and real
estate agents in the neighborhood of that
home. For the sellers, the listing website’s services include an
effective process for
enlisting home,
inserting textual
home
description,
uploading home images,
and providing answers
to structural ﬁelds,
such as,
price,
size,
number of bedrooms,
number of bathrooms,
etc.
which buyers use in their queries.
One major intelligence that is missing in these sites is how
long does it take for a house to be sold after it ﬁrst appears in
the listing? This intelligence contributes signiﬁcantly towards
the decision-making process of both the sellers and the buyers.
With this
information a seller
will
have an understanding
of
what
she
can do to expedite
the
sale,
i.e.
reduce
the
asking price,
renovate/remodel
some home features,
etc.
On
the other
hand,
a potential
buyer
will
have an idea of
the
available time for
him to react
i.e.
to place an offer.
Also
note that,
for
effective decision making,
the sellers and the
buyers are interested into more detailed analysis beyond the
point estimate of the interval time between the sale and listing
of a house—they want to know how the sale probability of a
house is inﬂuenced with the changes of house features, so that
they can apply the effective intervention as needed to optimize
their objective.
Such services are not
available in any of the
existing home listing websites.
Traditional
supervised regression algorithms
are
not
an
appropriate choice for
predicting the interval
time between
the sale and listing of a house.
This is due to the fact that for
many instances in the training data,
the actual sale event may
occur beyond a ﬁxed observation time window.
If a house is
sold within that
observation time window,
its exact
interval
time is known;
but
if
it
is not
sold,
the exact
interval
time
is not
known except
the knowledge that
the interval
time is
larger than the listing’s age during the end of the observation
time.
The instances of the second group are known as right
censored data in machine learning literature.
Unfortunately,
a
large number of houses fall into the right censored group,
so
ignoring those from the training data wastes signiﬁcant useful
information;
on the other hand,
including those in a training
data yields poor
performance due to an incorrect
response
value of the right censored instances.
In this work, we propose survival analysis [2] inspired mod-
els
for
predicting time-dependent
house selling probability.
Survival
analysis
is
developed to measure the lifespans
of
individuals,
and such a model
effectively accounts
for
the
right censored data instances.
By considering the listing time
2016 IEEE International Conference on Data Science and Advanced Analytics
978-1-5090-5206-6/16 $31.00 © 2016 IEEE
DOI 10.1109/DSAA.2016.58
468
of
a house as the starting of
observation time for
that
data
instance,
and the time until
sale since listing (the interval
time)
as its survival
period,
we model
the above prediction
problem as a survival analysis task. Then we solve this task by
using different variants of survival models, each with a distinct
feature selection strategy.
For a given time interval value,
the
models predict
the probability of a house to be sold by that
time interval,
counted from the time it
has been listed on a
listing site.
For
evaluation,
we collect
real-life housing data
from ”trulia.com” and show the effectiveness of the proposed
models over traditional
regression-based models.
To the best
of our knowledge,
we are the ﬁrst to investigate and propose
a solution for the task of predicting the time-dependent house
selling probability.
The remaining parts of the paper is organized as follows.
In Section III,
we discuss
details
of
the proposed method
including feature selection (Subsection III-A) methodologies
and learning model (Subsection III-B).
In section IV,
we talk
about the data we use in this work.
In Section V,
we discuss
the experimental
setup,
and the empirical
validation of
the
proposed method.
In Section VI,
we discuss
some of
the
related works from real-estate and survival analysis domains.
We end the paper with a conclusion in Section VII.
III.
M
ETHOD
For
a house
H
in a real-estate listing site,
F
is the set
of
features indicating different
aspects of
H
,
such as price,
size, age, neighborhood amenities, school quality, exterior and
interior.
Suppose
H
appears on the site at
time
t
appear
and
gets sold at
time
t
sold
.
The interval
between appearance and
sale events is deﬁned as
Int
=
t
sold
−
t
appear
; using survival
analysis terminologies
Int
is the time period for which a house
is available to be sold i.e. it survives before getting sold. In this
work,
our objective is to predict
the probability that
a house
H
,
represented by the feature vector
F
will
be sold within a
given interval
time.
We use a supervised learning algorithm
from survival analysis domain for solving this prediction task.
In the following,
we discuss the feature selection process and
the learning algorithm.
House Features
Price in thousand
Percentage above or below of median listing price (zipcode wise)
Number of Bedrooms
Number of Bathrooms
Size in square feet
Age of the house
Type of the house (Condo,
Townhome,
Family home etc)
Rating of Elementary school
Rating of Middle school
Rating of High school
Number of words in the text description (exclude stopwords)
Number of Adjectives in the description
TABLE I: Basic House features
A.
Feature Selection
We use two kinds of information for designing the feature
set
F
. The ﬁrst kind is the structured features which consist of
basic house information such as price, sqft, age, type, number
Fig.
1: Word cloud of the house description in our trulia.com
data
of
bedrooms,
number
of
bathrooms,
etc.
These features are
listed in Table I.
The second kind is
the features
that
are
extracted from the text
description of a house.
They include
detailed house features,
such as,
open-concept
design,
center
island included, etc. and also neighborhood amenities, such as
the nearby parks,
close to major roads,
or close to shopping
mall,
etc.
To give the reader
an overall
perspective on the
types of features the house description generally carries,
we
create a word cloud (Figure 1) with top 200 words from the
textual
description of houses in our dataset.
As we can see,
top words
are related to speciﬁc house features
i.e.
open,
basement,
garage,
as well
as the neighborhood features i.e.
park,
community,
and entertainment.
Note that,
these features
need to be
extracted through text
modeling,
which is
an
important step in our data preparation.
House
Exterior
Interior
Neighborhood
House
Rooms
Others
related
Amenities
School
Frontyeard
Backyeard
Bedroom
Bathroom
Kitchen
Layout
Fig.
2: Partial category hierarchy of house features
To extract
features
from the house text
data,
we apply
two well-known unsupervised feature selection algorithms:
topic modeling and deep learning.
The reason for
choosing
these two models over traditional TFIDF based bag-of-words
based model are two folds: ﬁrst,
using TFIDF the size of the
feature vector of a house can be large,
which makes learning
computationally expensive, and second, TFIDF does not model
feature
dependency,
as
it
weighs
a
feature
independently
of
the weights of
other
features.
Topic modeling and deep
learning based feature extraction overcome both of the above
limitations.
Topic
Modeling:
Topic modeling model
each text
as
a
distribution of latent topics (user deﬁned parameter) and each
topic as a distribution of
words.
Latent
Dirichlet
allocation
(LDA)
[3]
is one of
the most
popular
topic modeling algo-
rithms.
LDA uses two Dirichlet
multinomial
distributions to
model the mappings between documents and topics, and topics
and words.
We used LDA to ﬁnd top
d
topics from the text
description data of houses.
In Figure 2,
we present
a partial
category conceptual
hierarchy of house features.
As we can
469
see,
house features can be divided into exterior and interior
features.
Among exterior,
we have the neighborhood,
school,
front-yard,
back-yard.
Among interior,
we have features cat-
egorized into different
rooms
and others
aspects
i.e layout
of
a house.
Execution of
topic modeling on the house text
description data allows us to capture latent
topics related to
these categories.
Since the number of topics is a user-deﬁned
parameter,
LDA also allows
us
to control
the size of
the
feature vector of a house.
We use python package gensim’s
1
LDA implementation to ﬁnd the topics and represent
each
description of a house using
d
(number of topics) dimensional
feature vector.
Deep Learning: In recent years, unsupervised feature learn-
ing using traditional or deep neural networks has become pop-
ular.
These methods help to discover features automatically,
thus
obviate feature engineering using domain knowledge.
Researchers achieve excellent
performance with these deep
learning techniques for
extraction of
features from text
[4],
[5], speech [6] and images [7]. “ParagraphVector” [8] is an un-
supervised feature learning method for text data,
which maps
texts into a continuous vector space of dimension
d
, such that
semantically similar texts appear together i.e.,
form a cluster.
In a nutshell, for a sequence of words
W
=
{
w
0
, w
1
,
· · ·
, w
n
}
,
where
w
i
∈
D
(
D
is the dictionary of
words)
and a text
T
containing the sequence of
words,
the model
maximizes
P r
[
w
i
|
w
0
, w
1
, ..w
i−1
, w
i+1
,
· · ·
w
n
, T
]
over
the text
corpus.
The training of
feature vector
representation of
the text
is
done using stochastic gradient
descent
and the gradient
is
obtained via back-propagation [8]. For a given corpus of texts
i.e. description of houses, we apply an open source python im-
plementation of the “ParagraphVector” named “Doc2Vec” [9]
model
to ﬁnd
d
-dimensional
feature representation of
each
text. Finally, for each house, we assemble the basic house fea-
tures along with the features learned from the text using LDA
or Doc2Vec method for training using supervised learning.
B.
Survival Regression Model
In this section,
we will discuss the primary components of
survival regression and its relation to our problem of predicting
time-dependent selling probability of a house.
Event and Time: For a house
H
appearing on a listing site,
it’s sale event
is our event
of interest.
The observation time
starts with the listing of
H
and survival
time is the interval
between the sale time and listing time.
Right
Censoring:
For
many houses,
sale event
may not
occur
before the end of
the observation,
these houses
are
called right
censored data instances.
Traditional
regression
tasks may ignore these right
censored instances or
replace
the corresponding survival
time by a value higher
than the
time difference between the end of study time and the start
time of observation for that instance. Ignoring data instances is
wasteful and engineering survival time is a crude approxima-
tion. Survival analysis takes consideration of the right censored
data while training and provides a more principal approach of
predicting survival probability.
1
http://radimrehurek.com/gensim/
Survival
Function: Suppose,
p
(
t
)
is a probability density
function of the event
of interest
and
P
(
t
)
is the cumulative
distribution function.
The survival
function
S
(
t
)
denotes the
probability that
the house sale event
i.e.
the event
of interest
has not occurred by time
t
. If
T
is a random variable denoting
the survival time,
the following is true:
P
(
t
)
=
P r
(
T
≤
t
)
,
(1)
S
(
t
)
=
P r
(
T > t
) = 1
−
P
(
t
)
(2)
Hazard Function: The hazards function
h
(
t
)
is deﬁned as
the event
rate at
time
t
,
given that
the event
does not
occur
until
t
or later.
The hazard function can be written as:
h
(
t
) =
p
(
t
)
S
(
t
)
(3)
The equation 3 can be stated as,
the rate of
occurrence of
the event
at
duration
t
is equal
to the density of events at
t
,
divided by the probability of surviving to that duration without
experiencing the event.
C.
Proportional Hazard Model
A widely used and known survival
regression method is
called Proportional Hazard Model. This method is also known
as
Cox regression [10]
(CoxPH).
Assume,
a
set
of
data
instances
X
is given;
X
=
{
(
x
1
, y
1
)
,
· · ·
,
(
x
n
, y
n
)
}
,
where
x
i
is the feature vector
and
y
i
is the time of
the event
of
interest,
for the house
i
.
Suppose,
C
i
is an indicator variable
representing censorship status of data instance
i
:
C
i
=

1
,
The event (selling of a house) occurs
0
,
otherwise (censored data)
According to the Cox Proportional
hazard model,
the haz-
ard (experience the event) at time
t
for a house
i
with feature
vector x
i
can be expressed as,
h
i
(
t
|
x
i
) =
h
0
(
t
)
exp
{
x
T
i
β
}
(4)
here,
h
0
(
t
)
is a baseline hazard function that
indicates the
risk (getting sold) for a house
i
while considering no features
i.e x
i
is unobserved.
The term
exp
{
x
T
i
β
}
describes relative
risk,
a proportionate increase or
decrease in the risk,
with
respect to the features x
i
.
At
some particular
time
t
the probability that
the event
occurs for the house
i
for which
C
i
= 1 and
y
i
=
t
is,
L
i
(
β
) =
h
0
(
t
)
exp
{
x
T
i
β
}

j:y
j
≥y
i
h
0
(
t
)
exp
{
x
T
j
β
}
.
(5)
Note that, the event occurs at time
t
for the data instance (x
i
)
in the numerator of equation 5.
Whereas,
the denominator of
equation 5 includes all the data instances
j
that have observa-
tion time higher or equal to
y
i
, irrespective of censorship status
of
j
,
C
j
. In this way, survival analysis makes use of censored
data instances,
whereas traditional
regression models fail
to
utilize censored instances effectively.
Considering iid,
the
likelihood function (
L
(
β
)
)
can be
written as,
470
L
(
β
) =

i:C
i
=1
exp
{
x
T
i
β
}

j:y
j
≥y
i
exp
{
x
T
j
β
}
(6)
The corresponding log likelihood (
LL
(
β
)
) is,
LL
(
β
) =

i:C
i
=1
⎛
⎝
x
T
i
β
−
log

j:y
j
≥y
i
exp
{
x
T
j
β
}
⎞
⎠
.
(7)
By maximizing the function for β in equation 7, we obtain
maximum likelihood estimation of the model parameters. Once
we learn the model parameters β, survival function
S
(
t
|
x
i
)
is
computed as
S
(
t
|
x
i
) =
S
0
(
t
)
exp{x
T
i
β}
(8)
here,
S
0
(
t
)
is
the baseline survival
function and
S
0
(
t
)
=
exp(
−
h
0
(
t
))
.
We use
1
−
S
(
t
|
x
i
)
to compute the probability
for a house to not survive beyond
t
, in other words, to get sold
by
t
.
Since,
t
is a variable this probability can be computed
for any positive interval time.
IV.
D
ATA
In this section, we discuss the collection and preparation of
the dataset used in this work.
City
Number of House
Carmel,
IN
597
Fisher,
IN
525
Indianapolis,
IN
5,485
Zionsville,
IN
211
Noblesville,
IN
389
TABLE II: Number of house in each city
A.
Data Description
To validate
our
method,
we
collect
housing data
from
“trulia.com”. First, we pick ﬁve major cities: Fishers, Carmel,
Indianapolis, Zionsville and Noblesville in the central Indiana
region and crawl information of all listed houses in those cities.
For
each house,
we crawl
raw text
description,
school,
and
crime information and structured bullet-ed basic house features
i.e price, year of build, number of rooms, type of house, days
in trulia etc.
Another piece of information that
we crawl
in
order to make the dataset suitable for survival analysis is the
current house status i.e. “for sale”, “pending”(offer accepted),
“active contingent”(offer placed), and “public record”(sold). In
this work, we consider “public record”, “pending”, and “active
contingent” status as sold.
To observe the interval period between the listing and sale
date of a house,
we crawl trulia.com in multiple phases,
each
apart by one week.
In the ﬁrst crawl (1st week of November
2015),
we get “days in Trulia” features along with the status.
We ignore houses that are already sold in the ﬁrst crawl.
One
week later,
we crawl the houses again and check their status,
we do this ﬁve more times.
Finally,
we have the data where
each house has the number of days it
is listed in trulia.com
along with a
status
of
sold or
not.
Note
that,
trulia.com
marks the time period of
a listed house to 25+ weeks or
180+ days if
the time period exceeds
25 weeks
or
180
days.
In total,
we crawl
7
,
216
houses.
In Table II,
we show
the breakdown of house count
in ﬁve cities.
To validate the
proposed algorithm, we create two versions of the data. In the
ﬁrst
version,
we set
the end of observation time to 8 weeks,
where we want
to predict
the survival
probability of a house
on 8 weeks mark.
In the second version,
we set this mark to
12 weeks.
B.
Data Cleanup
Among the information that we crawl, basic house features,
school, and crime information are already formatted and clean.
Cleaning is required for the house details text
data.
To clean
text,
we use standardized data cleaning approach,
i.e.
remove
stop words,
stemming and lemmatization of words.
We also
clean some keywords that are written in unstructured abbrevi-
ated form.
For example,
keyword fireplace is written as
frplc,
or firplc in many house details.
C.
Data Statistics
In this section, we discuss statistics of different basic house
features
mentioned in Table I.
Among these features,
six
of
them (Table III)
are real
valued.
We use quantile based
statistics to show the data distribution. Remaining basic house
features are categorical, and we use the histogram to show data
distribution.
In Table III, we present the quantile distributions
of
price,
size in sqft,
description length (word count),
the
number of adjectives used in the description, percentage above
and below from the Zipmedian (Median house price zipcode
wise) value.
As we can see,
the minimum price of a house
is
996
dollar
which is
a very old house built
in
1930
in
Indianapolis area with
$1
/sqf t
asking price.
The maximum
price is
8
.
5
million dollar which is a mansion style
20000
sqft
house in Indianapolis area. The median price of a house from
above mention 5 cities is around
144
K
.
The size in sqft
of
a house shows similar quantile distribution as price.
Through
experiments,
we have observed the impact
of having a nice
description of
a house.
The description is mostly provided
by the homeowner to the listing sites.
50%
of houses has a
description with at
most
40
words (without
stop words i.e
am,
is,
the,
etc) with
6
adjectives.
Finally,
we have statistics
of features explaining how much higher or lower the asking
price of a house is compared to the median price in that
zip
code.
As we can see from last
two rows in Table III,
setting
house price higher than the ZipMedian price is more common.
Next,
we discuss the distributions of six categorical
house
features.
We also show the distribution of actual observations
and right
censored information of
how many houses
are
sold/not-sold within
12
weeks of its appearance on the site. In
Figure 3(a)
and (b)
we show the histogram of
the number
of
bedrooms
and bathrooms.
As
we can see
3
bedrooms
and
2
bathrooms
are the most
popular
choice among the
homeowners.
In Figure 3(c)
and (d)
we plot
age and type
of houses.
The majority of houses are in
0
−
20
years range.
We also have some houses with age more than
100
years.
We have
8
types of house in the data where the majority are
of type Single Family Home (SFH).
In Figure 3(e-g)
we plot
the histogram of the quality of the primary,
middle
and high school.
Each school in trulia.com has three types of
471
Feature
Min
1st Q.
Median
3rd Q.
Max
Price(USD)
996
82,041
144,822
270,476
8,491,100
Size in Sqft
389
1,212
1,736
2,640
20,710
Description Length
2
25
40
50
247
# of Adjective
0
2
4
6
33
Above Zip Median(%)
0
13.6
29.4
72
6057
Below Zip Median(%)
0
11.2
25.1
44.2
98
TABLE III: Quantile Observation of Numeric Features
2
3
4
5
6
7
8
Bedrooms
0
500
1000
1500
2000
2500
3000
3500
Count
Histogram of Bedroom
(a)
1
2
3
4
5
6
7
8
9
Bathrooms
0
200
400
600
800
1000
1200
1400
1600
1800
Count
Histogram of Bathrooms
(b)
0
20
40
60
80
100
120
140
160
180
House A
g
e
0
100
200
300
400
500
600
700
800
900
Count
Histogram of House Age
(c)
Condo
Farm
Invst
Mbl
MFH NewCom SFH
Twnhm
House T
yp
e
0
1000
2000
3000
4000
5000
6000
Count
Histogram of House Type
(d)
Below-Avg
Average
Above-Avg
Primar
y
School
0
1000
2000
3000
4000
5000
Count
Histogram of Primary School Quality
(e)
Below-Avg
Average
Above-Avg
Middle School
0
500
1000
1500
2000
2500
3000
3500
4000
Count
Histogram of Middle School Quality
(f)
Below-Avg
Average
Above-Avg
Hi
g
h School
0
1000
2000
3000
4000
5000
Count
Histogram of High School Quality
(g)
0
2
4
6
8
10
12
Weeks in Trulia
(
Sold
)
0
20
40
60
80
100
120
140
160
180
Count
Histogram of Weeks in Trulia (Sold)
(h)
0
5
10
15
20
25
30
Weeks in Trulia
(
Not Sold
)
0
200
400
600
800
1000
1200
1400
Count
Histogram of Weeks in Trulia (Not Sold)
(i)
Fig.
3:
Histogram of (a) Bedrooms (b) Bathroom (c) Age of the houses (d) Type of the houses (e) Quality of primary (f)
Middle (g) High school (h) Houses Sold within 12 weeks (i) Houses Not Sold
rating Below average, Average and Above Average.
As we can see, primary and high school has similar distribution
compared to middle school.
In Figure
3(h-i),
we
show the histogram of
“weeks
in
trulia.com” for all
houses in the data.
Using selling status of
each house we identify houses that are sold within 12 weeks
of
its appearance in the trulia.com.
Among the rest
of
the
houses,
we identify the houses that
are listed in trulia.com
for more than
12
weeks,
we call these houses right censored.
In Figure 3(h)
we plot
the histogram of
sold houses.
As
we can see the majority of
houses are sold between
4
−
8
weeks.
Figure 3(i) shows the distribution of not
sold houses.
Houses after
12
weeks mark are right
censored and we can
see that the majority of not sold houses are right censored and
such a scenario makes the modeling of time-dependent selling
probability of a house challenging.
472
Model
C-Index (8 weeks)
C-Index (12 Weeks)
# of Observed data (Not Censored)
682
1008
Version 1
No Features (CoxPH)
0.50
0.50
Version 2
Basic House Features (CoxPH)
0.60
0.62
Version 3
Basic House Feature + Topic Modeling (CoxPH + LDA)
0.65
0.69
Version 4
Basic House Feature + Deep Learning (CoxPH + Doc2Vec)
0.61
0.66
Version 5
Basic House Feature + Topic Modeling (FastCox + LDA)
0.63
0.64
Version 6
Basic House Feature + Topic Modeling (SVR + LDA)
0.50
0.52
Version 7
Basic House Feature + Topic Modeling (ElasticNet + LDA)
0.49
0.52
Version 8
Basic House Feature + Topic Modeling (Lasso + LDA)
0.48
0.51
TABLE IV: Performance comparison analysis
V.
E
XPERIMENT
A.
Experimental Setup
To validate the proposed method, we perform two forms of
experiments.
In one (Section V-C),
we measure performance
analysis
of
the method using C-Index metric discussed in
Section V-B. We perform ﬁve-fold cross validation to split the
data into train and test and report C-Index by taking an average
over these ﬁve folds.
For Latent
Dirichlet
Allocation (LDA)
model
discussed in Section III-A,
we use
50
as the number
of
topics after
tuning by grid search.
We run Gensim’s [9]
implementation of
Doc2Vec model
with default
parameter
settings,
except
for
parameter
d
(the dimension of
feature
vector),
which we set to
100
after tuning it using grid search.
We perform this experiment using both 8 and 12 weeks version
of the data. According to our point of view, selling probability
analysis after
2 (8 weeks)
and 3 (12 weeks)
months from
its (house’s)
listed date seemed practical
time frame.
In the
second form of experiment, we want to observe how different
basic house features i.e.
price,
type,
and age inﬂuence the
selling probability of a house.
To measure that,
we leverage
the concept
of
survival
probability associated with the Cox
regression model.
In this experiment,
we also use ﬁve fold
cross validation technique.
After training,
for each test
data,
we change the value of
a feature and compute the survival
probability of
houses
across
weeks
(
1
to
25
).
Finally,
we
take
the
average
of
the
computed survival
probability of
the test
fold and illustrate the results
using a heatmap.
In
this
experiment,
we take the best
performing model
from
experiment
one and use 12 weeks
version of
the data to
train the model.
Note that,
in all
experiments,
we apply data
standardization technique by subtracting the mean from each
feature then divide the values of each feature by its standard
deviation before using it.
B.
Evaluation Metric
Survival models consider that each data instances is associ-
ated with a censoring information.
Evaluating regression on
this
data using regular
metrics
is
not
suitable,
instead,
we
use concordance index (C-Index) [11],
which is widely used
in survival
analysis models.
By deﬁnition,
C-Index has the
same scale as the area under the ROC curve. The concordance
index (C-Index)
[11]
or
concordance probability measures
the effectiveness of
a prediction model
in survival
analysis.
Consider
a pair
of
observations
(
y
i
, y

i
)
and
(
y
j
, y

j
)
,
where
y
i
is the actual
observation (how many days a house was in
market
before getting sold),
and
y

i
is the predicted one.
The
C-Index is deﬁned as the probability of
y

i
> y

j
given
y
i
> y
j
as follows:
C-Index
=
P r
(
y

i
> y

j
|
y
i
> y
j
)
(9)
Similar to AUC metric for classiﬁcation it takes values from
0 to 1,
where 1 is the best
value for
this metric and value
more than 0.5 states improvement
over random performance.
For hazard based survival
regression model,
C-Index can be
computed as,
C-Index
=
1
E

i:C
i
=1

y
j
>y
i
1
[
x
T
j
β
>
x
T
i
β
]
(10)
Here,
E
is the count
for
which
y
j
> y
i
is true,
1[
·
]
is
the indicator function and β is the learned parameter of the
cox hazard proportional
model.
For the traditional
regression
model, C-Index can be computed by replacing indicator func-
tion of the above equation by 1
[
f
(
x
j
)
> f
(
x
i
)]
,
where
f
(
x
i
)
is the predicted target value.
C.
Performance Analysis
In this experiment,
we show the performance of
the de-
veloped algorithm.
To compare and contrast,
we design eight
different
versions of the regression model.
Below we brieﬂy
discuss these versions.
Version 1: No Features ((CoxPH): This is the most straight
forward version of
cox regression model
without
using the
notion of co-variates/features.
Version 2: Basic House Features (CoxPH): This version of
Cox regression model only uses basic house features presented
in Table I.
Version 3:
Basic
House
Feature
+ Topic
Modeling
(CoxPH + LDA):
In this
version,
we extend basic house
features with the features learned from house description data.
This version leverage a technique of
topic modeling known
as
Latent
Dirichlet
Allocation (LDA)
[3]
to ﬁnd the topic
distribution of the documents in the dataset. In this case, each
document is a description of a house.
Version
4:
Basic
House
Feature
+
Deep
Learning
(CoxPH + Doc2Vec): This version is slightly different than the
previous version.
In this version,
we use a deep unsupervised
feature learning algorithm called Doc2Vec [8] to learn feature
from house description.
Version 5:
Basic
House
Feature
+ Topic
Modeling
(FastCox
+ LDA):
In this
version,
we
keep the
setting
of
feature selection as
Version 3 but
we use a elastic net
based regularized version of Cox regression model known as
FastCox [12].
473
Version 6,
Version 7 and Version 8:
In these versions,
we keep the setting of feature selection as version 3 but
we
use various traditional regression algorithms i.e SVR (Support
Vector Regression), Linear regression with elastic net and lasso
regularization criteria.
Note that all these versions are trained
on the non-censored data instances only.
Because of superior
performance of LDA over Deep in this versions we use LDA
based method only.
Observations: We run each version presented above over
the dataset
we prepare from trulia.com.
We prepare two ver-
sions of the data, in one we want to observe the probability of
a house being sold before
8
weeks of its arrival in trulia.com.
In the second version,
we observe the same but for
12
weeks
mark.
In the ﬁrst
row of Table IV,
we show the number of
actual
observations for
8
and
12
weeks,
respectively.
Out
of
total
7216
houses only
682
and
1008
houses are considered
as the observed data points.
As we can see in Table IV, the best performance is achieved
for the Version 3 where we use basic housing features with
text
features extracted using topic modeling algorithm LDA.
The performance of the Version 4 where we use deep learning
is
slightly lower
than the LDA.
The reason behind this
is
that
Doc2Vec model
requires a large amount
of training data
that
we do not
have in our
possession.
Another
important
observation is that
addition of
features extracted for
house
description improves
the
performance
of
the
Cox model.
Finally, the performance of traditional regression models (last
3 rows in Table IV) is poorer than any of the above models.
This is due to the fact
that
these models are not
equipped
to handle right censored data,
where survival analysis models
handle such data more effectively.
D.
Inﬂuence of
Different
Basic Features on Survival
Proba-
bility of a House
In this experiment,
we analyze the inﬂuence of
different
house features over the selling probability of a house within
a time window.
We use the measure of
survival
probability
of Cox models to quantify the inﬂuence of a house feature.
For
example,
if
for
a house,
we measure that
its
survival
probability in week 8 is
40%
,
then we can infer
that
the
possibility of getting sold on and before
8
week is
60%
.
We
use heatmap to illustrate the inﬂuence of different values of a
house feature across overall observation period, in this case,
1
to
25
.
For four features: size,
# of bedrooms,
# of bathrooms
and age of the house, we use 3D heatmap to show the inﬂuence
of these features while pairing with the price. We choose these
four features as they show correlation with the price of a house.
1)
Price of
a House:
First,
we analyze the effects of the
price of a house over its survival
probability.
Our hypothesis
is that
for a house if we increase its price,
in a time
T
the
survival
probability will
also increase,
hence decreasing the
selling probability.
In Figure 4(a),
we plot
time range (1 to
25) in the x-axis,
price range in the y-axis and the computed
survival probability for a speciﬁc time(
T
) and a speciﬁc price
(
P
) in the corresponding block. To decide on the price range,
we use quantile analysis of
the price value in the data.
We
set
10
and
90
percentile value as the lower and upper limit
of the price range,
respectively,
with
50
K
as stepping value.
The transition from red to blue color indicates decreasing of
survival probability hence increasing of selling probability. As
we can see from Figure 4(a),
with the increment of price the
survival
probability increases.
A homeowner can utilize such
observation to decide on the asking price of
a house with
respect to the waiting time he can afford.
2)
Size in Sqft:
The size of
a house is a very important
feature.
According to a report by ﬁndwell.com [13] price per
sqft is one of the inﬂuential factors in the process of valuation
of a house. According to a survey [14] by trulia.com, in 2015
44%
Americans prefer
house between
1400
to
2600
square
feet.
We expect to see an increase in survival probabilities as
we increase the size of a house.
In Figure 4(b),
we show the
heatmap of survival probability with respect to size. Compared
to price, the size of a house has more aggressive effects on its
survival
probability.
As we can see,
the survival
probability
is
considerably high for
houses
with size more than
2600
sqft.
Since the size of a house is directly correlated with its
asking price,
we want to observe the effects of price and size
together
over
selling probability of
a house.
In Figure 5(a),
we make a 3d heatmap with price in the x-axis,
size in the
y-axis and weeks in trulia in the z-axis.
We use colors to
illustrate the survival
probabilities.
To generate this plot,
we
vary the size of a house for a price. As we can see, large size
with higher price (right-bottom,
right-top region) houses has
a lower
probability of
getting sold.
On the other
hand,
low
price,
small
houses (left
region)
has a higher
probability of
getting sold.
3)
Number of Bedroom and Bathroom:
In Figure 4(c) and
(d),
we illustrate the effects of the number of bedrooms and
bathrooms
on the
survival
probability of
a
house.
In the
heatmap,
we use all
possible value of these two features in
the y-axis.
As we can see,
the effect
of these two features is
opposite.
As we increase the number
of
bedrooms,
survival
probability decreases i.e.
sale probability increases.
Whereas,
for the number of bathrooms,
increment
causes an increment
in the survival
probability.
Such observation established the
fact
that
it
is better
to renovate a house by adding a new
bedroom than a new bathroom.
To investigate the reason,
we
decide to see the average cost to renovate/add a new bedroom
compared to the bathroom. According to ﬁxr.com, a cost guide
service,
on average it
takes
approximately
7
,
880
[15]
and
29
,
950
[16] dollar to add a new bedroom and a new bathroom,
respectively. For a home seller, adding a new bedroom is more
adv
antageous than adding a bathroom.
Because a bedroom
contributes less towards the overall
asking price of
a house
than a bathroom.
Since the number
of
bedrooms and bathrooms has direct
inﬂuence over the price of a house,
We follow similar setup
as the size,
to generate 3d heatmap (Figure 5(b)(c)) of price,
bedroom/bathroom and weeks in trulia to show the effect
of
these two features while coupling with price.
As we can see
in Figure 5(b),
for the bedroom selling probabilities are high
for
a low price and higher
number
of
bedrooms (deep-left-
top region).
Whereas for the bathroom (Figure 5(c)),
a higher
number of bathrooms (right
region) make selling probability
474
(a)
(b)
(c)
(d)
0
5
10
15
20
25
Week
Twnhm
SFH
N
ewCom
MFH
Mob
Invst
Farm
Condo
Survival Probability of Different
week on House Type
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
(e)
(f)
(g)
(h)
0
5
10
15
20
25
Week
Blw-Avg
Avg
Abv-Avg
High School
Survival Probability of Different
week on Quality of High School
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
(i)
Fig.
4:
Heatmap of survival
probability on (a) Price (b) Size in sqft
(c) # of Bedrooms (d) # of Bathrooms (e) House type
(f) House age (g) Quality of Primary school
(h) Quality of Middle school
(i) Quality of High school.
Progression from red
color to blue color indicates decreasing survival
probability or increasing sale probability (Please view on color print
out
or
on screen)
lower.
4)
Type of the House:
Type of a house is another important
factor
that
the buyers will
take into consideration.
Single
Family Home, Townhome, and Condo are the most popu-
lar home types among buyers. As it is illustrated in Figure 3(d),
the majority of houses in our data are Single family home. Our
hypothesis over the effects of house types is that
the selling
probability of a house will
be comparatively higher if it
is a
single family or
a town home.
In ﬁgure 4(e),
we show the
observations of survival probabilities with respect to different
house types using the heatmap.
As we can see,
single family
and town homes have higher
selling possibility for
a time
T
than others.
Also according to 2016 Q1 Homeownership
Opportunities and Market
Experience (HOME) Survey [17],
85%
of
owners and
75%
of
renters want
to buy a single-
family home. Such observation helps a homeowner at the ﬁrst
place to decide what type of home is more desirable from the
viewpoint of fast and proﬁtable reselling in future.
5)
Age of a House: The age of the house plays an important
role in the decision-making process of home buyers.
Usually,
when a house was built has effects on the overall asking price
of a house. Generally, an older house has comparatively lower
price than a new one.
In ﬁgure 4(f),
we show the changes
in the survival
probabilities with respect
to different
ages of
the house using heatmap.
As we can see,
older houses have
more selling probability i.e.
lower
survival
probability than
new.
According to a 2014 United States wide survey report
by trulia.com [18],
the median cost of a new home is around
290
,
000
that is approximately
50%
more than the median cost
of
an existing home that
is
198
,
500
.
The report
also says,
2
out
of
5
American will
prefer
a new home if
costs were
the same.
According to our observation,
age of a house will
increase its selling possibility,
since it
brings down the price
signiﬁcantly. For this feature, we plot similar 3d heatmap while
incorporating price. As we can see in Figure 5(d), older houses
with low price has higher
selling probability (deep-top-left)
475
(a)
(b)
(c)
(d)
Fig.
5:
3D Heatmap of survival
probability on (a) Size in sqft
(b) # of Bedrooms (c) # of Bathrooms (d) House age with
respect to Price. Progression from red color to blue color indicates decreasing survival probability or increasing sale probability
(Please view on color print out or on screen)
compared to the new houses.
6)
School
Quality:
Finally,
we observe the effects of
the
quality of nearby schools over selling possibility of a house
for a given time
T
. In this setup, we compute the survival prob-
abilities of houses across the quality of nearby primary, middle
and high schools.
In Figure 4(g-i),
we illustrate the result.
As
we can see, all types of school quality have similar effects on
the survival probability.
Better quality school lower down the
survival probability i.e. increase selling possibility. According
to the “2015 National Association of Realtors Home Buyer and
Seller Generational
Trends” [19] report,
good quality school
factor is numbered sixth important
criteria among the home
buyers in the country. But for the home buyers of age
35
−
49
,
the school district is the fourth important factor. A survey [20]
in trulia.com reports
that
35%
of
American describe their
“dream home” in a good school district.
VI.
R
ELATED
W
ORKS
Data analytics in real
estate domain is an active research
area.
The majority of
the works
contribute in the efﬁcient
modeling of
a real
estate’s price.
To our
knowledge,
this is
the ﬁrst
work to model
time-dependent
selling probability of
a house using survival regression analysis.
In this section,
we
will discuss existing works under two categories.
A.
Real Estate Price Modeling
There exist several works on real-estate price modeling. The
primary objective of these works is real estate appraisal, which
is the process of
estimating the market
value for
an estate
or
property.
Krainer
et
al.
[21]
model
real
estate price by
building metrics which are inspired from ﬁnancial theory, such
as,
price to income ratio,
and price to rent
ratio.
Nagaraja et
al. [22] leverages ﬁnancial time series data to identify different
patterns in estate prices i.e. periodicity, and volatility. Downie
et al.
[23] observes that the low investment-valued estates are
more volatile than the high investment-valued estates. Hedonic
regression [24] technique is one of the popular methods for
estate appraisal in real-life.
In recent
years,
there are multiple works
[25],
[26],
[27]
on real-estate ranking using investment
value index.
Fu et
al.[25] propose a ranking algorithm called ClusRanking that
leverages
the characteristics
of
a neighborhood to estimate
the values of
its nearby estates.
In another
work [27],
the
authors
propose the sparse estate ranking algorithm which
uses opinion about
estates from online reviews and mobility
476
behaviors (e.g., taxi traces, smart card transactions, check-ins).
The most recent work [26] on estate ranking incorporates the
functional
diversity of user communities in a locality.
Thus,
it
leverages different
temporal
urban functions (when most
people in a community going to work or shop,
eat,
etc.),
and
user
mobility information (public transportation traces,
taxi
traces) to rank real
estates.
Recently,
Zhu et
al.
[28] propose
a “Days On Market” (DOM) prediction algorithm for Chinese
real-estate market.
B.
Survival Regression Analysis
Cox proportional
hazards model
[10] is one of the earliest
and most
popular
survival
analysis
approach among data
miners
and statisticians.
To handle large dimensional
data,
researcher design regularization based Cox regression. Notable
among these methods
are L1-regularization based LASSO-
Cox [29],
Elastic Net
penalty term based CoxNet
[30]
and
kernel elastic net penalized Cox regression [31] models.
In recent years,
survival regression analysis based methods
are being applied in various domains ranging from engineering
to statistical to health-care to model time-to-event data. In [32],
[31],
authors
presented a Cox regression based method to
model readmission risk prediction for heart failure and diabetic
patient
respectively.
In [33],
authors estimate the probability
of
success
for
a
project
in a
crowdfunding platform like
“Kickstarter” using survival regression analysis.
VII.
C
ONCLUSION
In this paper, we propose a method for predicting the time-
dependent selling probability of a house. We perform effective
feature selection from the resources
collected from a real
world listing site, “trulia.com”. We map the prediction problem
into survival analysis framework and solve the problem using
the
proportional
hazard model
called Cox regression.
We
empirically validate and show the effectiveness of the proposed
method using real-world housing data.
We also show the
superior performance of the proposed methods over traditional
regression methods.
R
EFERENCES
[1]
A.
J.
S.
from The National
Association of Realtors and Google,
“The
digital house hunt: Consumer and market trends in real estate,” National
Association of Realtors,
2013.
[2]
J.
P.
Klein and M.
L.
Moeschberger,
Survival
analysis: techniques for
censored and truncated data.
Springer
Science & Business Media,
2005.
[3]
D.
M.
Blei,
A.
Y.
Ng,
and M.
I.
Jordan,
“Latent
dirichlet
allocation,”
the Journal of machine Learning research,
vol.
3,
pp.
993–1022,
2003.
[4]
A.
Bordes,
X.
Glorot,
J.
Weston,
and Y.
Bengio,
“Joint
learning of
words and meaning representations for open-text
semantic parsing,” in
International Conference on Artiﬁcial Intelligence and Statistics,
2012,
pp.
127–135.
[5]
T.
Mikolov,
I.
Sutskever,
K.
Chen,
G.
S.
Corrado,
and J.
Dean,
“Distributed representations of
words and phrases and their
composi-
tionality,” in Advances in Neural
Information Processing Systems 26,
2013,
pp.
3111–3119.
[6]
R.
Socher,
E.
H.
Huang,
J.
Pennin,
C.
D.
Manning,
and A.
Y.
Ng,
“Dynamic pooling and unfolding recursive autoencoders for paraphrase
detection,” in Advances in Neural Information Processing Systems, 2011,
pp.
801–809.
[7]
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V.
Vanhoucke,
and A.
Rabinovich,
“Going deeper with convolutions,”
CoRR,
vol.
abs/1409.4842,
2014.
[8]
Q.
V.
Le and T.
Mikolov,
“Distributed representations of sentences and
documents,” arXiv preprint arXiv:1405.4053,
2014.
[9]
R.
ˇ
Reh
˚
u
ˇ
rek and P.
Sojka,
“Software Framework for
Topic Modelling
with Large Corpora,” in Proceedings of
the LREC 2010 Workshop on
New Challenges for NLP Frameworks,
pp.
45–50.
[10]
D.
R.
Cox,
“Regression models and life-tables,” in Breakthroughs in
statistics.
Springer,
1992,
pp.
527–541.
[11]
H.
Steck,
B.
Krishnapuram,
C.
Dehing-oberije,
P.
Lambin,
and V.
C.
Raykar,
“On ranking in survival
analysis:
Bounds on the concordance
index,” in Advances in neural information processing systems, 2008, pp.
1209–1216.
[12]
Y.
Yang and H.
Zou,
“A cocktail
algorithm for
solving the elastic
net
penalized coxs regression in high dimensions,” Statistics and its
Interface,
vol.
6,
no.
2,
pp.
167–173,
2012.
[13]
“How
important
is
price
per
square
foot
when
buying
real
estate?”
http://www.ﬁndwell.com/blog/buying-a-home/
how-important-is-price-per-square-foot-when-buying-real-estate/,
2010.
[14]
“American
dream
home:
Midsized,
suburban
and
modern,”
http://www.trulia.com/blog/trends/american-dream-home/,
2015.
[15]
“Bedroom
remodeling
cost,”
http://www.ﬁxr.com/costs/
bedroom-remodeling,
2016.
[16]
“Bathroom
remodeling
cost,”
http://www.ﬁxr.com/costs/
bathroom-remodeling,
2016.
[17]
“Most
people
want
single-family
homes,”
http://www.realtor.org/reports/2016-q1-housing-opportunities-and-
market-experience-home-survey,
2016.
[18]
“To buy a new or
an existing home? why,
how much,
and where,”
http://www.trulia.com/blog/trends/new-or-existing-home/,
2014.
[19]
“Home buyer
and seller
generational
trends,” http://www.realtor.org/
reports/home-buyer-and-seller-generational-trends,
2016.
[20]
“Where you can buy homes near good schools,” http://www.trulia.com/
blog/trends/near-good-schools/,
2016.
[21]
J. Krainer, C. Wei et al., “House prices and fundamental value,” FRBSF
Economic Letter,
2004.
[22]
C.
H.
Nagaraja,
L.
D.
Brown,
and L.
H.
Zhao,
“An autoregressive
approach to house price modeling,” The Annals of
Applied Statistics,
pp.
124–149,
2011.
[23]
M.-L.
Downie and G.
Robson,
“Automated valuation models: an inter-
national perspective,” 2008.
[24]
P.
A.
Champ,
K.
J.
Boyle,
and T.
C.
Brown,
A primer on nonmarket
valuation.
Springer Science & Business Media,
2012,
vol.
3.
[25]
Y. Fu, H. Xiong, Y. Ge, Z. Yao, Y. Zheng, and Z.-H. Zhou, “Exploiting
geographic dependencies for real estate appraisal: A mutual perspective
of ranking and clustering,” in Proceedings of
the 20th ACM SIGKDD
international
conference on Knowledge discovery and data mining
.
A
CM,
2014,
pp.
1047–1056.
[26]
Y.
Fu,
G.
Liu,
S.
Papadimitriou,
H.
Xiong,
Y.
Ge,
H.
Zhu,
and C.
Zhu,
“Real estate ranking via mixed land-use latent models,” in Proceedings
of
the 21th ACM SIGKDD International
Conference on Knowledge
Discovery and Data Mining.
ACM,
2015,
pp.
299–308.
[27]
Y.
Fu,
Y.
Ge,
Y.
Zheng,
Z.
Yao,
Y.
Liu,
H.
Xiong,
and N.
J.
Yuan,
“Sparse real estate ranking with online user reviews and ofﬂine moving
behaviors,” in Data Mining (ICDM),
2014 IEEE International
Confer-
ence on.
IEEE,
2014,
pp.
120–129.
[28]
H. Zhu, H. Xiong, F. Tang, Y. Ge, Q. Liu, E. Chen, and Y. Fu, “Days on
market: Measuring liquidity in real estate markets,” in Proceedings of the
22th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining.
ACM,
2016.
[29]
R. Tibshirani et al., “The lasso method for variable selection in the cox
model,” Statistics in medicine,
vol.
16,
no.
4,
pp.
385–395,
1997.
[30]
N.
Simon,
J.
Friedman,
T.
Hastie,
R.
Tibshirani
et
al.,
“Regularization
paths
for
coxs
proportional
hazards
model
via coordinate descent,”
Journal of statistical software,
vol.
39,
no.
5,
pp.
1–13,
2011.
[31]
B.
Vinzamuri and C.
K.
Reddy,
“Cox regression with correlation based
regularization for electronic health records,” in Data Mining (ICDM),
2013 IEEE 13th International
Conference on.
IEEE,
2013,
pp.
757–
766.
[32]
H. Neuvirth, M. Ozery-Flato, J. Hu, J. Laserson, M. S. Kohn, S. Ebadol-
lahi,
and M.
Rosen-Zvi,
“Toward personalized care management
of
patients at
risk:
the diabetes case study,” in Proceedings of
the 17th
ACM SIGKDD international
conference on Knowledge discovery and
data mining.
ACM,
2011,
pp.
395–403.
[33]
Y.
Li,
V.
Rakesh,
and C.
K.
Reddy,
“Project
success
prediction in
crowdfunding environments,” in Proceedings of WSDM,
2016.
477
