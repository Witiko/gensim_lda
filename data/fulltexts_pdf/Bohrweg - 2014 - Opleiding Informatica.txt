Universiteit Leiden
Opleiding Informatica
Logic for Soft Component Automata
Name:
Tobias Kapp´e
Date:
15/08/2016
1st supervisor:
Prof.dr.ir. F. Arbab
2nd supervisor:
Dr. M.M. Bonsangue
MASTER’S THESIS
Leiden Institute of Advanced Computer Science (LIACS)
Leiden University
Niels Bohrweg 1
2333 CA Leiden
The Netherlands
Contents
1
Introduction
3
1.1
Acknowledgements
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
2
Related work
4
3
Preliminaries
4
3.1
Common mathematical notation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
3.2
Constraint semirings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
3.3
Büchi-automata .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
4
Component Action Systems
15
5
Soft Component Automata
17
5.1
Composition
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
5.2
Further observations
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
6
Linear Temporal Logic
22
6.1
Operational model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
6.2
SCAs to Büchi-automata
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
6.3
Desired behavior
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
6.4
Towards model checking .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
6.5
Diagnostics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
25
7
Propositional Dynamic Logic
27
7.1
Optimal behavior .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
7.2
Syntax and semantics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
7.3
Partial orders for idling
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
7.4
Towards model checking .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
8
Conclusion
34
9
Further work
34
A Proofs for Subsection 3.2
35
B Proofs for Subsection 7.3
38
B.1
Proofs for the generalized pointwise order
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
38
B.2
Proofs for the generalized lexicographic order
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
C Proofs for Subsection 7.4
42
C.1
Lemma’s for the lexicographic order
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
C.2
Lemma’s for the generalized lexicographic order .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
C.3
Proofs of Theorem 3 and Theorem 4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45
2
1
Introduction
Our surroundings are becoming increasingly driven by programs aimed at automating real-world tasks; self-driving
cars, crop survey robots and supply chain drones are but modest examples of applications growing continuously
more present in our daily lives, in various degrees of visibility.
As these agents make real-life decisions that have
physical consequences for human beings, one looks to formal verification to be able to trust them.
Unfortunately,
while methods aimed at achieving robustness in uncertain environments appear to be fairly well-established
in the context of dynamical systems tasked with steering some continuous signal (see, for example, [
34
]), the
verification of state-based systems seems to lack formalisms that account for fault-tolerance.
What’s more,
as the inherent complexity of
tasks entrusted to autonomous agents grows,
so does the
description of
the agent’s behavior,
in particular in the context of
robustness mechanisms.
To keep this
complexity manageable, a method based on compositional design is appealing.
Such a method should allow a
designer to let robustness of the system emerge from the robustness of its individual components as well as the
methods used for their composition.
Ideally, a compositional system should then be verifiable in a compositional
manner:
by verifying assertions on components, we should be able to verify assertions on compositions thereof.
Moreover, when a system fails to verify a certain property, it is useful to gain some information as to which
subset of components were responsible for the behavior that violated said property.
To make assertions about
the system or its components in particular circumstances, we are also interested in investigating the (optimal)
behavior of a system whose actions are constrained to some set of action sequences allowed by the environment.
In this thesis,
we propose an automata-based formalism for modeling an autonomous system,
which can
be seen as a generalization of (Soft) Constraint Automata [
4
,
1
], where actions are endowed with a (possibly
multidimensional) value that indicates the preference of the component to perform the action.
If the most-
preferred action is unavailable, the agent can opt to execute an action of lesser preference.
It seems intuitively
clear that the presence of alternative (lower-preference) actions makes an agent more robust in coping with
situations that do not match its ideal operating conditions (i.e., those compatible with its best-preference actions).
We present a set of operators aimed at composing these automata, depending on the concerns they represent.
Verification is approached from two angles.
Firstly, we explore an approach based on Linear Temporal Logic
(LTL), which builds heavily upon existing methods; here, behavior is constrained based on a minimum preference
value.
With this LTL-based approach,
we can identify components responsible for behavior that violates a
desired property.
Secondly, we propose a novel method reminiscent of Propositional Dynamic Logic (PDL); here,
we are interested in verifying claims about the optimal behavior (in terms of preference) exhibited by the system
when constrained to some (sequences of) actions.
For the PDL-based approach, we propose two partial orders
for deciding optimality, which incorporate the possibility of the system idling, and explore model checking for a
limited subcase of one such order.
The remainder of this thesis is organized as follows.
In Section 2, we list some existing work related to the
material
on our subject,
and contrast it with our work.
In Section 3,
we discuss the necessary notation.
In
Section 4 we present our model of actions and their composition; in Section 5 we introduce our automata-based
formalism.
We then delve into the two complimentary approaches for verification, with the LTL-based verification
in Section 6 and the PDL-based verification in Section 7.
We present our conclusions in Section 8 and directions
for further work in Section 9.
To retain focus on the material at hand and not get lost in details, some of the
proofs of intermediate lemmas in Section 3 and Section 7 are postponed until the appendices.
1.1
Acknowledgements
The references in this thesis were compiled with the help of the
dblp
computer science bibliography,
1
as provided
by the Computer Science department of the University of Trier and Schloss Dagstuhl.
Over the course of
the last year,
a number of
discussions with Francesco Santini
on the finer points of
c-semirings and Soft Constraint Satisfaction Problems have proven to be very useful; thanks to him, I was able
to develop an intuition of how to use c-semirings to model preferences.
I would also like to thank Marcello Bonsangue (my second supervisor) for the patience he displayed when I
tried to summarize my definitions, propositions and conjectures on the whiteboard and scrap paper in his office;
his technical acumen has been very inspiring.
The members of the Formal Methods group at CWI never failed
to make lunch- and coffee-breaks amusing, and provided some very welcome distraction.
Lastly and perhaps most importantly, I would be remiss not to thank Farhad Arbab (my first supervisor)
and Carolyn Talcott.
Our regular discussions both on Skype and at CWI have been instrumental in steering this
work.
Carolyn has been very helpful in stressing the importance of running examples and toy models, which
gave a firm substrate to discuss our directions.
I also benefited enormously from Farhad’s optimism and can-do
attitude; it was Farhad who always found a positive way to incorporate new lines of reason into the purpose of
this work, and this thesis is much better off for it.
1
http://dblp.uni-trier.de/
3
2
Related work
The work in this thesis uses the theory of c-semirings as proposed by Bistarelli et al. [
6
,
9
].
Also deserving of
mention is [
27
], which presents an algebraic framework similar to c-semirings called valuation structures; these
almost match c-semirings, save for the fact that they require a total order.
We refer to [7] for a comparison.
To be able to model multiple dimensions of preference, we draw upon [
10
,
14
,
8
] to derive techniques for
composing c-semirings in a pointwise and lexicographic manner; this approach contrasts [
17
], which proposes
a slightly more complicated structure called a c-system of monoids.
When composing c-semirings,
we need
a method to transition smoothly between different c-semirings; to this end the notion of a homomorphism is
applied to c-semirings; an additional condition on homomorphisms that preserves maximality is found in [
21
,
15
].
It should be noted that in [
14
],
the notion of a bounded semiring valuation structure corresponds to our
notion of a c-semiring.
Also in [
14
] a generalization of c-semirings called a partially-ordered valuation structure is
proposed; we make no use of this generalization, as it does not require the existence of a unique least upper
bound for each subset, which is something we rely upon to develop material in Section 7.
Preference values used to ensure robustness of autonomous systems can be found in [
31
].
The use of c-semiring
values as labels of transitions can also be found in [
1
].
It should be noted that, in the latter work, preference
values are used as input to preference-based queries to discover similarly-structured automata in a database; in
contrast, this thesis uses preference values to drive the behavior of the agent modeled by an automaton.
Earlier
and slightly less general forms of the work presented in this thesis, particularly those in Section 5, appear in [
18
].
Our notion of preference differs from that of priority in process algebra [
12
,
13
]
in that priority is used
exclusively to ensure that prioritized actions are selected over non-prioritized actions; i.e., a non-prioritized action
is never performed in lieu of a prioritized action.
In contrast, preferences exist in a possibly broad spectrum of
values and may arise compositionally, i.e., from the preferences of constituent actions.
Moreover, prioritized
actions in process algebra do not compose with non-prioritized actions, whereas composability of our actions is
independent of their preference (although some actions may compose more preferably than others).
For the part of this thesis concerned with logic, we make use of Linear Temporal Logic (LTL) [
24
], in particular
the work that relates model checking to automata [
33
].
An application of LTL to Constraint Automata (CA)
appears in the work of Baier et al. [
2
,
3
] as the logic LTL
IO
; since our automata formalisms can be considered a
generalization of CAs, our use of LTL can likewise be considered as a generalization of LTL
IO
.
We also draw
inspiration from Propositional Dynamic Logic [25, 16] to investigate the optimal behavior of our agents.
The use of c-semirings in combination with logic is not new.
In work like [
22
,
23
], however, formulas of a logic
are interpreted over a c-semiring.
This thesis does use c-semirings to establish semantics, but only to compare
the preferences of certain behaviors, not to establish a quantitative satisfaction level of a formula.
The term preference appears in [
32
], in the context of Epistemic Modal Logic.
Here, the authors are concerned
with updating the preferences of an agent based on new knowledge received.
While this aim is certainly not
unrelated to the applications of the work presented in this thesis, we make the simplifying assumption that the
knowledge of our agents is completely described by their state, which in turn completely determines preferences
attached to their actions.
The act of updating preferences is modeled by the transition into a new state.
3
Preliminaries
In an effort to keep this thesis self-contained, we give a brief overview of the notation and terms used.
We start
off with some basic mathematical notions in Subsection 3.1 and continue with definitions from the theory of
c-semirings in Subsection 3.2.
Finally, we review some relevant material on Büchi-automata in Subsection 3.3
3.1
Common mathematical notation
The material in this section will be very familiar and perhaps even uninteresting to the reader with a formal
training in Computer Science or Mathematics,
as most material discussed here is firmly engrained in canon
and can therefore be skipped easily without risking confusion.
An exception is formed by the paragraphs on
operators and streams, where respectively the notion of operator is slightly generalized and some convenient
notation for infinite series called streams is borrowed from [26].
Inference rules
An (inference) rule is a compact way of writing down an implication.
Such a rule is depicted
by writing down the premises above a horizontal line, with the consequence below said line.
As an example of an
inference rule, consider the classic syllogism
All humans are mortal
p
is human
p
is mortal
Assuming the rule above holds, instantiating
p
, one can apply it in all situations where the premises hold; for
example, since all humans are indeed mortal, and Socrates is human, it follows that Socrates is mortal.
4
Sets
A set is understood to be a (possibly infinite) collection of objects.
In the remainder of this section, we
use capital letters
X
,
Y
,
Z
to denote general sets and lower-case symbols
x
,
y
,
z
to denote their elements.
The
unique set that does not contain any elements is called the empty set and denoted by the symbol
∅
.
When
X
is a set and
x
is an object contained in
X
, we write
x ∈ X
and say that
x
is an element of
X
.
Two sets are
presumed equal when all elements of either set are also elements of the other.
When
X
and
Y
are sets such that when
x ∈ X
it holds that
x ∈ Y
, i.e., all elements of
X
are also elements
of
Y
, we call
X
a subset of
Y
and write
X ⊆ Y
.
When there also exists at least one element of
Y
that is not an
element of
X
, we say that
X
is a strict subset of
Y
, written as
X ⊂ Y
; in this case,
X
is said to be smaller than
Y
, while
Y
is larger than
X
.
When
X
and
Y
are sets, we write
X ∪ Y
for their union, i.e., the smallest set
containing all elements of
X
as well as all elements in
Y
.
We furthermore write
X ∩ Y
for their intersection, i.e.,
the largest set containing elements both in
X
and
Y
.
When the intersection of
X
and
Y
is the empty set, we
say that
X
and
Y
are disjoint.
We write
X \ Y
for the largest subset of
X
disjoint with
Y
.
When
x ∈ X
holds
for a finite number of distinct
x
, we refer to
X
as finite; if this is not the case, then
X
is infinite.
We can write down a finite set explicitly using curly braces, e.g.,
{x, y}
for the set whose elements are
x
and
y
exclusively.
Note that an element can occur in a set at most once, and the particular order of elements is of no
importance; consequently, we identify
{x, y}
with
{y, x}
and
{x, x}
with
{x}
.
We can define some infinite sets
such as the set of natural numbers
N
like so:
{
0
,
1
,
2
,
3
, . . . }
; the remaining elements of the set are left implicit.
Sets can also be written down in set-builder notation, i.e.,
X
=
{y ∈ Y
:
φ
(
y
)
}
, where
φ
(
y
) is a description of
the elements
y
of
Y
that qualify for inclusion in
X
.
Another convenient method for defining sets, which we employ often in this thesis, is based on inference rules.
Specifically, we can define a set as the unique smallest set that satisfies one or more inference rules
2
, i.e., the
unique set that satisfies the inference rules such that none of its subsets does, too.
For example, we could have
chosen to define the union of
X
and
Y
(see above) to be the smallest set
X ∪ Y
satisfying
z ∈ X
or
z ∈ Y
z ∈ X ∪ Y
One immediate advantage of the use of inference rules to define a set
X
is that if
x ∈ X
, we immediately know
that the premises of at least one of the inference rules defining
X
must hold for
x
.
We write
X × Y
for the Cartesian product
of
X
and
Y
,
i.e.,
the set containing an element
hx, yi
for all
elements
x ∈ X
and
y ∈ Y
.
Such elements
hx, yi
are called tuples.
In this thesis, we do not distinguish between
X ×
(
Y × Z
) and (
X ×Y
)
×Z
; while, strictly, these are different sets, it should be clear that there exists a unique
correspondence between their elements.
Accordingly, we drop the parentheses and simply write
X × Y × Z
.
Also,
when the correspondence between a set and its elements is clear from the context, we do not distinguish between
X × Y
and
Y × X
.
When we write
hx, yi ∈ X × Y
,
it is implicit that
x ∈ X
and
y ∈ Y
hold;
furthermore,
hx, yi
=
hx
0
, y
0
i
if and only if
x
=
x
0
and
y
=
y
0
.
Existential and universal quantifiers
For brevity, especially within inference rules or set builder notation,
we may replace frequent occurrences of there exists an
x ∈ X
, such that and for all
x ∈ X
, it holds that using
symbols
∃
and
∀
respectively.
The assertion
∃x ∈ X.
∀y ∈ Y.
φ
(
x, y
) is thus read as there exists an element
x ∈ X
, such that for all
elements
y ∈ Y
,
φ
(
x, y
) holds, where
φ
(
x, y
) is an assertion dependent on
x
and
y
.
Decidability
In this thesis, we use the term decidable somewhat loosely to indicate any claim that is verifiable
algorithmically.
For example, the claim 2 + 2 = 4 is decidable, because we can compute the sum on the left-hand
side and verify that it equals the right-hand side.
Relations
A (binary) relation between sets
X
and
Y
is a subset of
X × Y
.
When
R
is such a subset, we write
x R y
as shorthand for
hx, yi ∈ R
; in this case,
x
and
y
are said to be related by
R
.
When
R ⊆ X × X
, we say
that
R
is a relation on
X
.
When
R ⊆ X × Y
and
S ⊆ Y × Z
are relations such that
x R y
and
y S z
, then we
may chain this assertion by writing
x R y S z
instead.
Similarly, we write
x R y, y
0
instead of
x R y
and
x R y
0
.
If
R ⊆ X × Y
is a relation, we write
6
R
for the negation of
R
, i.e., the unique relation
6
R ⊆ X × Y
such that
for all
x ∈ X
and
y ∈ Y
, we have that
x 6
R y
if and only if
x R y
does not hold.
In a slight abuse of notation, we
write
x 6∈ X
when
x
is not an element of
X
.
If
R
is a relation on
X
such that for all
x ∈ X
it holds that
x R x
,
then
R
is said to be reflexive.
If
R
is a relation on
X
such that for all
x, x
0
∈ X
it holds that
x R x
0
if and only if
x
0
R x
,
then
R
is said to be
symmetric; when
x R x
0
and
x
0
R x
imply that
x
=
x
0
,
R
is antisymmetric.
If
R
is a relation on
X
such that for
all
x, x
0
, x
00
∈ X
, if
x R x
0
and
x
0
R x
00
, then
x R x
00
, then
R
is transitive.
If
R
is a relation, we may refer to the
smallest reflexive (symmetric, transitive) relation containing
R
as the reflexive (symmetric, transitive) closure of
R
; such a relation always exists, by the same reasoning we use to define sets based on inference rules.
2
By the Knaster-Tarski theorem, a necessary and sufficient condition for this technique to work (i.e., for a smallest set satisfying
the rules to exist uniquely) is that the inference rules should induce a monotone operator on sets.
The specifics of this are fascinating,
but go beyond the scope of this thesis.
Instead, we summarize them as follows:
membership of
x
in the set should never allow using
the inference rules to disprove membership of
x
in the set.
All inference rules used to define sets in this thesis satisfy this condition.
5
A relation
P
on
X
such that
P
is reflexive, antisymmetric and transitive is a partial order.
When furthermore
for all
x, x
0
∈ X
we have that either
x
P
x
0
or
x
0
P
x
holds,
P
is a total
order.
An example of a total order is
the relation
≤
on
N
.
As a convention, when we use a symbol like
P
to denote a partial order on
X
, we use the
symbol
C
to denote the relation such that
x C x
0
if and only if
x
P
x
0
and
x 6
=
x
0
.
This convention carries over
to other symbols used for partial orders, e.g.

versus
≺
, et cetera.
Functions
A relation
R ⊆ X × Y
is functional
when for all
x ∈ X
there is precisely one
y ∈ Y
such that
x R y
.
We also refer to such a relation as a function and use lower-case letters
f, g, h
to denote functions.
If
f ⊆ X × Y
is a function, we write
f
:
X → Y
and refer to
X
as the domain of
f
and
Y
as the range of
f
.
When
f
:
X → Y
is a function and
x ∈ X
, we also write
f
(
x
) for the unique
y ∈ Y
such that
x f y
and say
that
x
is mapped to
y
by
f
.
Note that we can uniquely define a function
f
:
X → Y
by specifying which
y ∈ Y
is related to each
x ∈ X
.
If
f
:
X × Y → Z
is a function, we abbreviate by writing
f
(
x, y
) instead of
f
(
hx, yi
).
We note that for functions
f, g
:
X → Y
, it holds that
f
=
g
if and only if
f
(
x
) =
g
(
x
) for all
x ∈ X
.
When
f
:
X → Y
and
g
:
Y → Z
are functions, their composition, written
g ◦ f
, is the function
g ◦ f
:
X → Y
defined by
g
(
f
(
x
)) for
x ∈ X
.
When
f
:
X → X
is a function, we define
f
0
as the function that maps every
element of
X
to itself, and for
n ∈ N
we define
f
n
+1
=
f
n
◦ f
.
We write
Y
X
for the set of all functions that
have
X
as domain and
Y
as range, i.e.,
f ∈ Y
X
if and only if
f
is a function
f
:
X → Y
.
As a special case, if we
consider 2 to be the two-element set
{
0
,
1
}
, we identify 2
X
with the set of all subsets of
X
:
if
f ∈
2
X
, then
f
uniquely corresponds to a subset
X
0
of
X
such that
x ∈ X
0
if and only if
f
(
x
) = 1; similarly, we can also obtain
a function
f
:
X →
2 from a subset
X
0
of
X
.
Abusing notation, we write 2
X
ω
for the set of finite subsets of
X
.
When
f
:
X → Y
is a function and
X
0
is a set, we write
f 
X
0
for the restriction of
X
to
X
0
, i.e.,
f ∩
(
X
0
× Y
);
note that
X
0
is not necessarily a subset of
X
and that
f 
X
0
:
X ∩ X
0
→ Y
is again a function.
When
f
:
X → Y
is a function and
X
0
⊆ X
,
we write
f
(
X
0
) for the image of
X
0
under
f
,
i.e.,
the set
{f
(
x
0
)
∈ Y
:
x
0
∈ X
0
}
.
Similarly, when
Y
0
⊆ Y
we write
f
−
1
(
Y
0
) for the inverse image of
Y
0
under
f
, i.e., the set
{x ∈ X
:
f
(
x
)
∈ Y
0
}
.
When
X
1
× X
2
is a set and
i ∈ {
1
,
2
}
,
we write
Pr
i
:
X
1
× X
2
for the projection function,
which maps
hx
1
, x
2
i ∈ X
1
×X
2
to
x
i
.
In accordance with earlier notation, we note that
Pr
i
(
X
1
×X
2
) =
X
i
.
Another function
that we will use often in the sequel is
max
:
N × N → N
, which takes a pair of natural numbers and returns the
largest of the two.
Also, when
P
is a total order, and
S
is a finite set, then
max
P
(
S
) is the unique value
s ∈ S
such that for all
s
0
∈ S
it holds that
s
0
P
s
.
Operators
We refer to a function
~
:
X × X → X
as a (binary) operator on
X
.
We commonly write the
application of such a function in infix notation, i.e., we write
x ~ y
instead of
~
(
x, y
).
When for all
x ∈ X
it
holds that
x ~ x
=
x
,
then
~
is idempotent.
If for all
x, x
0
∈ X
,
it turns out that
x ~ x
0
=
x
0
~ x
,
we call
~
commutative.
Similarly, if for all
x, x
0
, x
00
∈ X
we know that
x ~
(
x
0
~ x
00
) = (
x ~ x
0
)
~ x
00
, then
~
is associative.
When
~
is an associative and commutative operator on
X
, we immediately obtain a generalized operator
~
:
2
X
ω
\ {∅} → X
,
defined for
X
0
=
{x
1
, x
2
, . . . , x
n
} ⊆ X
by
~
X
0
=
x
1
~ x
2
~ . . . ~ x
n
.
Note that the
particular order of the
x
i
does not matter, by commutativity and associativity of
~
.
When applying
~
to some
subset of
X
defined using set-builder notation, we may abbreviate
~
{x
:
φ
(
x
)
}
by writing
~
φ
(
x
)
x
.
We occasionally use the enlarged symbol
~
to denote an operator 2
X
→ X
, i.e., defined on possibly infinite
subsets of
X
.
An example of such an operator would be
S
: 2
2
X
→
2
X
, which takes a set of subsets of
X
and
returns the unique smallest set that contains all such sets as subsets.
Note that, whether the domain includes
infinite subsets or not,
~
can be obtained from
~
by defining
x ~ x
0
=
~
{x, x
0
}
.
When we define
~
thusly, it is
immediate that
~
is idempotent, commutative and associative.
When
R
is a relation on
X
and
~
:
R → X
is a function, we refer to
~
as an operator on
X
up to
R
and
similarly use infix notation.
We call such an
~
idempotent up to
R
if
R
is reflexive and
x ~ x
=
x
for all
x ∈ X
;
~
commutative up to
R
if
R
is symmetric and
x R x
0
implies
x ~ x
0
=
x
0
~ x
.
Lastly,
~
is associative up to
R
if for all
x, x
0
, x
00
∈ X
such that
x R x
0
and
x
0
R x
00
,
then
x R x
00
if and only if (
x ~ x
0
)
R x
00
,
if and only if
x R
(
x
0
~ x
00
); moreover, if
x R x
00
, then (
x ~ x
0
)
~ x
00
=
x ~
(
x
0
~ x
00
).
Words and languages
Let
X
be a set and let
n ∈ N
.
A word
x
over
X
of length
n
is a tuple
hx
1
, x
2
, . . . , x
n
i
such that for all
1
≤ i ≤ n
it holds that
x
i
∈ X
.
We juxtapose the elements of
x
when spelling out a word,
i.e.,
x
=
x
1
x
2
· · · x
n
.
We write
X
n
for the set of words over
X
of length
n
.
Note that the empty tuple
hi
is
a word over
X
of length 0;
this empty word is given the symbol

.
If
x ∈ X
n
and
x
0
∈ X
m
,
then
x · x
0
(the
concatenation of
x
and
x
0
) is the word
x
1
x
2
· · · x
n
x
0
1
x
0
2
· · · x
0
m
∈ X
m
+
n
.
We can lift the concatenation operator
to sets, by defining
X · Y
=
{x · y
:
x ∈ X, y ∈ Y }
.
A set of words over
X
(of arbitrary length) is referred to as a
language over
X
; we commonly use the letter
L
to denote a language.
The set of words over
X
is denoted by
X
∗
(the Kleene closure of
X
) and is alternatively defined as
X
∗
=
S
{X
n
:
n ∈ N}
.
The languages that can be constructed starting with finite languages,
union,
concatenation and Kleene
closure are regular languages.
It is well known that for every regular language
R
, there exists a finite automaton
A
R
that accepts this language, i.e.,
L
(
A
R
) =
R
, and that it is decidable whether or not a regular language is
a subset of another regular language.
The constructions for this are beyond the scope of this thesis; we refer
to [28] for an excellent discussion of the relevant theory.
6
If
x ∈ X
n
⊆ X
∗
,
then
|x|
=
n
is the length of
x
.
Lastly,
if
x, x
0
∈ X
∗
such that
|x|
≤ |x
0
|
and for all
1
≤ i ≤ |x|
it holds that
x
i
=
x
0
i
, then
x
is called a prefix of
x
0
; if moreover
x 6
=
x
0
, then
x
is a strict prefix of
x
0
.
The set of prefixes of
x ∈ X
∗
is denoted by
prefix
(
x
).
Lexicographic order
Given a partial order
P
S
on a set
S
, one can define the induced lexicographic order
P
S
∗
as the smallest relation on
S
∗
that satisfies the rules
w ∈ S
∗

P
S
∗
w
w, x ∈ S
∗
e, f ∈ S
e C
S
f
e · w
P
S
∗
f · x
w, x ∈ S
∗
e ∈ S
w
P
S
x
e · w
P
S
∗
e · x
Induction
Let
X
be a set and let
R
be a relation on
X
.
We call
R
a well-founded relation on
X
if,
for
any subset
X
0
of
X
,
there exists an element
x
0
0
∈ X
0
such that for all
x
0
∈ X
0
\ {x
0
0
}
it holds that
x
0
6
R x
0
0
.
Well-founded relations are useful, because they give rise to a proof principle called induction.
If we want to prove
that a property
φ
(
x
) holds for all
x ∈ X
, it suffices to show that the following inference rule holds:
3
∀x
0
∈ X.
if
x
0
R x
and
x 6
=
x
0
then
φ
(
x
0
)
φ
(
x
)
As a concrete case, we can see that the relation
≤
on
N
is a well-founded relation.
To show that a property
φ
(
n
)
holds for all
n ∈ N
, we should show that
φ
(0) holds, and that if
φ
(
n
) holds, then
φ
(
n
+ 1) holds, too.
Complete lattices
A complete lattice is a tuple
hL, ≤,
W
,
V
i
such that
≤
is a partial
order on
L
and
W
,
V
: 2
L
→ L
are operators such that for all
L
0
⊆ L
,
W
L
0
is the least upper bound of
L
0
, i.e., if
` ∈ L
such that
for all
`
0
∈ L
0
it holds that
`
0
≤ `
, then it also holds that
`
0
≤
W
L
0
≤ `
.
Similarly,
V
L
0
is the greatest lower
bound of
L
0
, i.e., if
` ∈ L
such that for all
`
0
∈ L
0
it holds that
` ≤ `
0
, then
` ≤
V
L
0
≤ `
0
holds, also.
Streams
We write
X
ω
for the set
X
N
.
An element of
X
ω
is referred to as a stream [
26
] over
X
, and commonly
denoted using the Greek letters
µ
,
ν
, et cetera.
We call
µ
(0) the head of the stream and write
µ
0
for the unique
stream defined by
µ
0
(
n
) =
µ
(
n
+ 1) for
n ∈ N
,
referred to as the tail
or derivative [
26
].
More generally,
the
k
-th derivative of
µ ∈ X
ω
is the unique stream
µ
(
k
)
such that
µ
(
k
)
(
n
) =
µ
(
k
+
n
).
We identify
X
ω
× Y
ω
with
(
X × Y
)
ω
.
Accordingly, if
hµ, νi ∈ X
ω
× Y
ω
we may regard
hµ, νi
as a stream; in particular, this allows us to
abbreviate
µ
(
k
)
, ν
(
k
)
by writing
hµ, νi
(
k
)
.
3.2
Constraint semirings
We now continue by giving the basic definitions we use from the theory of Constraint Semirings, or c-semirings
for short.
The name constraint here originates from their earlier use as algebraic valuation structures for Soft
Constraint Satisfaction Problems [
9
]; we use c-semirings as a convenient structure for reasoning about preferences
of actions and their compositions.
The definition below diverges slightly from [
9
,
10
] in the
∨
-operator; this is,
however, only a slight generalization; most c-semirings in the literature can still be written in the form below.
Definition 1
([
6
, Definition 2.1.2])
.
A Constraint Semiring (c-semiring) is a tuple
hE,
W
, ⊗,
0
,
1
i
, such that
E
is
a set with
0
,
1
∈ E
,
W
: 2
E
→ E
is an operator,
⊗
is a commutative and associative operator on
E
and for all
e ∈ E
,
E ⊆ E
and
E ⊆
2
E
, the following hold:
–
W
{e}
=
e
,
W
∅
= 0 and
W
E
= 1.
–
W
E
0
∈E
(
W
E
0
) =
W
(
S
{E
0
:
E
0
∈ E}
) (the flattening property).
– 0
⊗ e
= 0 and 1
⊗ e
=
e
.
–
⊗
distributes over
W
, i.e.,
e ⊗
W
E
=
W
{e ⊗ e
0
:
e
0
∈ E}
.
Every c-semiring
hE,
W
, ⊗,
0
,
1
i
induces a relation
≤
E
defined as the smallest relation that satisfies the rule
e, e
0
∈ E
e ∨ e
0
=
e
0
e ≤
E
e
0
Lemma 1
([
6
, Theorem 2.1.1])
.
Let
hE,
W
, ⊗,
0
,
1
i
be a c-semiring.
Then
≤
E
is a partial
order.
Moreover, for
al l
e ∈ E
, we have that 0
≤
E
e ≤
E
1.
Proof.
For reflexivity, let
e ∈ E
and consider that
e ∨ e
=
e
by idempotency of
∨
, thus
e ≤
E
e
.
For antisymmetry,
let
e, e
0
∈ E
such that
e ≤
E
e
0
≤
E
e
.
Then
e
0
=
e ∨ e
0
=
e
by commutativity of
∨
.
For transitivity, let
e, e
0
, e
00
∈ E
such that
e ≤
E
e
0
≤
E
e
00
.
Then
e ∨ e
00
=
e ∨
(
e
0
∨ e
00
) = (
e ∨ e
0
)
∨ e
00
=
e
0
∨ e
00
=
e
00
, thus
e ≤
E
e
00
.
Let
e ∈ E
; to see that
0
≤
E
e
, consider that
0
∨ e
=
W
∅ ∨
W
{e}
=
W
(
{e} ∪ ∅
) =
W
{e}
=
e
.
Similarly, to see
that
e ≤
E
1, consider that
e ∨
1 =
W
{e} ∨
W
E
=
W
(
{e} ∪ E
) =
W
E
= 1.
3
Again, the details are fascinating, but beyond the scope of this thesis.
7
Let
hE,
W
, ⊗,
0
,
1
i
be a c-semiring.
We call
E
the carrier and often use
E
as the symbol
for the c-semiring.
In this case, the corresponding operators and constants are denoted by
W
E
,
∨
E
,
⊗
E
,
0
E
and
1
E
; we drop the
subscripts when only one c-semiring appears in the context.
The relation
≤
E
is called the induced order of
E
, in
accordance with Lemma 1.
Again, we drop the subscript when no confusion is likely.
The operator
W
is referred
to as the choice operator, for reasons that will become clear in a moment.
The act of applying
⊗
is referred to
as composition.
The constants
0
and
1
are referred to as the bottom and top elements of
E
, respectively.
We
denote c-semirings using double struck capital letters, such as
E
,
F
et cetera.
Intuitively, a c-semiring
E
provides us with a set of preference values in the carrier, which we can attach to
actions.
The operator
∨
models choice among actions; this is reflected in the relation
≤
E
:
if we have two actions
α
and
β
with preferences
e
α
and
e
β
such that
e
α
<
E
e
β
(i.e.,
e
α
∨ e
β
=
e
α
), then
β
is said to be preferred over
α
.
Note, however, that neither
e
α
≤
E
e
β
nor
e
β
≤
E
e
α
needs to hold in general (i.e.,
≤
E
need not be a total order);
if this is the case,
e
α
and
e
β
are said to be incomparable.
The operator
⊗
models composition of preferences:
when
α
and
β
are composable actions,
e
α
⊗ e
β
is the preference attached to their composition.
This intuition is
be made more explicit in Section 5, where we talk about actions and composition in more detail.
It is often convenient to prohibit actions that are assigned the bottom preference; consequently we refer to
such actions as infeasible actions, while any other action is said to be feasible.
Examples
Many different c-semirings exist.
Although a sizable number of c-semirings can be said to have the
same structure, it is often convenient to use a c-semiring that reflects the context of the actions best.
We now
consider three examples of c-semirings with meaningful differences with regard to their structure, and mention
the contexts in which they could be used.
The simplest useful example of a c-semiring is the Boolean semiring
B
, in which
– The carrier is given by the set
{⊥, >}
.
– The choice operator is given for
B ⊆ B
by
W
B
B
=
>
if and only if
> ∈ B
.
– The composition operator is given for
b, b
0
∈ B
by
b ⊗
B
b
0
=
>
if and only if
b
=
>
=
b
0
.
– 0
B
=
⊥
and 1
B
=
>
.
Because the Boolean semiring allows only two levels of preference, it is primarily of interest as a theoretical
device.
Using the Boolean semiring, we can capture formalisms unconcerned with preferences by assigning the
preference
⊥
to actions that are not permitted, and
>
to actions that are.
For an example of a c-semiring with an infinite carrier, consider the weighted semiring
W
, in which
– The carrier is given by the set
N ∪ {∞}
.
– The choice operator is given for
W ⊆ W
by
_
W
W
=
(
∞
W
=
{∞}
min(
W \ {∞}
)
otherwise
In which min(
S
) is understood to be unique smallest number in
S ⊆ N
.
– The composition operator is given for
w, w
0
∈ W
by
w ⊗
W
w
0
=
(
∞
∞ ∈ {w, w
0
}
w
+
w
0
otherwise
– 0
W
=
∞
and 1
W
= 0.
Intuitively, the weighted semiring models that we can assign weights from
N ∪ {∞}
to actions.
Consequently,
actions with a lower weight are always preferred over actions with a higher weight — note that this means that
≤
W
coincides with the familiar relation
≥
on natural numbers.
Actions with unbounded weight
∞
are infeasible.
The weight of a composed action is given by the sum of the weights of the actions.
Up until this point, we have given only examples of c-semirings whose induced order is a total order.
To see
that c-semirings whose order is not total have their merit, too, we consider an example inspired by [
5
].
The
unix
semiring
4
U
is the c-semiring where
– The carrier is given by the subsets of
P
=
{
R
,
W
,
X
}
, i.e,
U
= 2
P
.
– The choice operator is given for
U
=
{u
1
, u
2
, . . . , u
n
} ⊆ U
by the finite intersection operator
T
: 2
U
→ U
.
– The composition operator is given by the union, i.e, when
u, u
0
∈ U
then
u ⊗
U
u
0
=
u ∪ u
0
.
– 0
U
=
P
and 1
U
=
∅
.
4
The
unix
semiring owes its name to the permission bits read, write and execute that appear in
unix
-like operating systems.
8
When using the
unix
semiring, a preference attached to an action can be interpreted as the privileges (read, write
or execute) necessary for executing the action.
By its choice of operators, the
unix
semiring models the principle
of least privilege:
an action
α
is preferred over another action
β
if and only if the privileges required for
α
are a
strict subset of those required for
β
.
If
α
and
β
were to require the privileges
{
R
,
X
}
and
{
W
}
respectively, they
would be incomparable.
This stands to reason, because read- and execute-permissions on a file may or may not
entail a higher level of privilege than writing permissions.
5
Also, the privileges required for a composed action
are given by the union of privileges of the component actions, which seems reasonable to assume.
As an additional example of a c-semiring whose carrier consists of sets, we refer to [10, Section 6.6].
Homomorphisms
To move smoothly between c-semirings, we need the notion of homomorphism.
Intuitively,
a homomorphism [
21
] from c-semirings
E
to
F
is a function that preserves the algebraic structure of its domain.
Definition 2.
Let
E
and
F
be c-semirings.
A homomorphism from
E
to
F
is a function
h
:
E → F
such that:
–
h
(0
E
) = 0
F
and
h
(1
E
) = 1
F
.
– For all
E ⊆ E
,
h
(
W
E
E
) =
W
F
h
(
E
).
– For all
e, e
0
∈ E
,
h
(
e ⊗
E
e
0
) =
h
(
e
)
⊗
F
h
(
e
0
).
As an example of a homomorphism, consider that there exists a unique homomorphism
ι
E
from the Boolean
semiring
B
to any other c-semiring
E
:
6
simply let
ι
E
map
⊥
to
0
E
and
>
to
1
E
; the remainder of the conditions
are be validated by the additional requirements in Definition 1.
For our purposes, however, we need a similar yet
subtly different requirement, which is given by the following definition.
We refer to [
21
] for a more extensive
discussion of homomorphisms.
Definition 3.
Let
E
and
F
be c-semirings,
h
:
E → F
a function and
e ∈ E
.
We call
h e
-reflecting when for all
e
0
∈ E
,
h
(
e
)
≤
F
h
(
e
0
) if and only if
e ≤
E
e
0
.
We call
h
simply reflecting if
h
is
e
-reflecting for all
e ∈ E
.
As an example of an
e
-reflecting homomorphism for
e ∈ E
for an arbitrary c-semiring
E
, consider the threshold
function t
e
:
E → B
given by
t
e
(
e
0
) =
(
> e ≤
E
e
0
⊥
otherwise
When
≤
E
is a total order,
⊗
is idempotent and
e 6
=
0
, one can show that
t
e
is an
e
-reflecting homomorphism
(Lemma 26).
In this case, we refer to
t
e
:
E → B
as the
e
-threshold homomorphism of
E
, or simply the
e
-threshold
map if it is not guaranteed to be a homomorphism.
In general, the
e
-threshold map is not reflecting; consider for
example
t
7
:
W → B
.
Here,
t
7
(9) =
⊥
=
t
7
(10), thus
t
7
(9)
≤
B
t
7
(10), even though 9
≤
W
10 does not hold (recall
that
≤
W
coincides with
≥
).
Threshold maps are primarily useful when reducing preference-oriented formalisms
to those that do not concern themselves with preferences, as we will see in Section 6.
Cancellative elements
In order to properly define composition of c-semirings later on in this section, we
need to talk about cancellative elements [
8
] of a c-semiring, a somewhat technical concept closely related to
residuation [
8
] and collapsing elements [
14
].
7
A cancellative element
e
of a c-semiring
E
is a preference value for
which a simple cancellation rule with regard to
⊗
holds.
Definition 4.
Let
E
be a c-semiring.
An element
e ∈ E
is a cancellative element if for all
e
0
, e
00
∈ E
, when
e ⊗ e
0
=
e ⊗ e
00
we know that
e
0
=
e
00
.
The set of cancellative elements of
E
is written
C
(
E
), while the set of
non-cancellative elements of
E
is written
C
(
E
).
When
C
(
E
) =
E \ {
0
}
, we call
E
a cancellative c-semiring.
Examples of cancellative c-semirings include the Boolean semiring
B
and the weighted semiring
W
.
The
unix
-
semiring
U
is non-cancellative, since
{
R
,
W
} ⊗
U
{
W
}
=
{
R
} ⊗
U
{
W
}
, even though
{
R
,
W
} 6
=
{
W
}
.
In general, a
c-semiring
E
whose composition operator
⊗
is idempotent is not cancellative [14], with the exception of
B
.
It is important to stress, however, that even though the only examples we provide for cancellative c-semirings
have an induced order that is total, there exist cancellative c-semirings for which this is not the case; we will see
an example of this in the remainder of this section, when we define the join composition of c-semirings.
The
converse is true, as well:
there exist totally-ordered c-semirings that are not cancellative (refer to Lemma 28).
Composition
When different concerns of varying importance need to be modeled by preference values,
it
is useful to have the tools to construct a c-semiring that reflects those concerns, using simpler c-semirings as
building blocks.
For example, suppose that we have actions that carry a weight and a
unix
-permission.
The
following scenarios seem plausible:
(i)
We want to choose an action that is Pareto-optimal, in the sense that an action is preferred over another
action if it improves on either weight or preference, while keeping the other preference as least as good.
5
Especially when one considers the finer details of
unix
-permissions, such as the
setuid
bit.
6
In this sense,
B
is the initial object in the category of c-semirings and homomorphisms.
7
As a matter of fact, collapsing elements are exactly the elements that are non-cancellative; refer to Lemma 27.
9
(ii)
We are primarily concerned with choosing the action with the lowest weight, and secondarily with choosing
the action that requires the least privileges.
Conceivably, we should be able to construct a c-semiring for both scenario’s using the weighted semiring
W
and
the
unix
-semiring
U
.
In the following, we define a number of composition operators for c-semirings that allow us
to accomplish this.
To model scenario (i), we can use the product of c-semirings [10]; this is again a c-semiring (Lemma 29).
Definition 5.
Let
E
and
F
be c-semirings.
Their product composition, written
E × F
, is the c-semiring where
– The carrier is given by the Cartesian product of the carriers
E × F
.
– The choice operator
W
E×F
: 2
E×F
→ E × F
is given for
S ⊆ E × F
by
_
E×F
S
=
D
_
E
Pr
1
(
S
)
,
_
F
Pr
2
(
S
)
E
– The composition operator
⊗
E×F
is given for
he, f i , he
0
, f
0
i ∈ E × F
by
he, f i ⊗
E×F
he
0
, f
0
i
=
he ⊗
E
e
0
, f ⊗
F
f
0
i
– 0
E×F
=
h
0
E
,
0
F
i
and 1
E×F
=
h
1
E
,
1
F
i
.
As an instance of
the construction above,
consider the c-semiring
W × U
;
this semiring faithfully models
scenario (i).
As an example, consider an action
α
with preference
e
α
=
h
7
, {
R
}i ∈ W × U
.
This action is preferred
over the actions
β
and
γ
with preferences
e
β
=
h
7
, {
R
,
W
}i ∈ W × U
and
e
γ
=
h
13
, {
R
,
X
}i ∈ W × U
respectively,
since:
e
α
∨
W×U
e
β
=
h
7
, {
R
}i ∨
W×U
h
7
, {
R
,
W
}i
=
h
7
∨
W
7
, {
R
} ∨
U
{
R
,
W
}i
=
h
7
, {
R
}i
=
e
α
e
α
∨
W×U
e
γ
=
h
7
, {
R
}i ∨
W×U
h
13
, {
R
,
X
}i
=
h
7
∨
W
13
, {
R
} ∨
U
{
R
,
X
}i
=
h
7
, {
R
}i
=
e
α
Meanwhile, the preference of
β
is not comparable to
γ
, since:
e
β
∨
W×U
e
γ
=
h
7
, {
R
,
W
}i ∨
W×U
h
13
, {
R
,
X
}i
=
h
7
∨
W
13
, {
R
,
W
} ∨
U
{
R
,
X
}i
=
h
7
, {
R
}i 6
=
e
β
, e
γ
Thus, if
β
and
γ
were the only available actions, both could be considered optimal
in terms of preference.
Indeed,
it is easy to verify that
≤
E×F
is the product order obtained from
≤
E
and
≤
F
[10].
To cope with scenario (ii), the lexicographic composition of c-semirings [
14
] can be defined as below.
We
note that the “most significant” component of a preference in the carrier can be non-cancellative only when the
second component is the bottom preference.
If this is not done, then the resulting object may fail to be a proper
c-semiring, because distributivity of the composition operator over the choice operator fails to hold.
Definition 6.
Let
E
and
F
be two c-semirings.
Their lexicographic composition, written
E . F
, is the c-semiring
such that
– The carrier is the set (
C
(
E
)
× F
)
∪
(
C
(
E
)
× {
0
F
}
).
– The choice operator is given, for
S ⊆ E × F
, by
_
E.F
S
=
D
_
E
Pr
1
(
S
)
,
_
F
m
(
S
)
E
in which
m
(
S
) contains all
and only those elements
f
of
Pr
2
(
S
) such that
h
W
E
Pr
1
(
S
)
, f i ∈ S
.
– The composition operator as well
as the bottom and top elements are defined as in Definition 5.
Intuitively,
we can state that the choice operator chooses the value
e
for the first position between the
preferences that occur in the leftmost components of elements of
S
, after which the second position is chosen
amongst
m
(
S
), i.e., the preference values that co-occur with
e
in
S
.
This makes sense, since only the values in
S
that are not dominated on their first position should contribute towards the second position of
W
E.F
.
It is
important to note that when
m
(
S
) is empty (which may occur when
≤
E
is not a total order),
W
F
m
(
S
) = 0
F
.
As a point of order, we note that Definition 6 above appears to contradict [
14
, Theorem 1] in that
≤
E
is not
required to be total.
However, we believe that such a restriction is not necessary.
To the best of our knowledge,
a proof of this claim was first provided in [19, Appendix A]; we have reproduced this proof in Theorem 5.
Consider the c-semiring
W.U
; we can use this c-semiring to model scenario (ii).
An action
α
, with preference
e
α
=
h
7
, {
R
}i
, is preferred over the actions
β
and
γ
with preferences
h
7
, {
R
,
W
}i
and
h
10
, ∅i
respectively, since
e
α
∨
W.U
e
β
=
h
7
, {
R
}i ∨
W.U
h
7
, {
R
,
W
}i
=
h
7
∨
W
10
, {
R
} ∨
U
{
R
,
W
}i
=
h
7
, {
R
}i
=
e
α
10
e
α
∨
W.U
e
γ
=
h
7
, {
R
}i ∨
W.U
h
10
, ∅i
=
D
7
∨
W
10
,
_
U
{{
R
}}
E
=
h
7
, {
R
}i
=
e
α
We note that,
in the latter case,
m
(
{e
α
, e
γ
}
) =
{{
R
}}
,
because 7
∨
W
10 = 7,
and
{
R
}
is the unique element
u ∈ U
such that
h
7
, ui ∈ S
.
We thus see that
e
α
≤
W.U
e
β
, e
γ
matches scenario (ii), since for
e
γ
the lower weight
in the first position takes precedence over the smaller privilege set in the second position, and for
e
β
the first
positions match, but the preferences in the second position of
e
α
are a strict subset of the preferences in the
second position of
e
β
.
Indeed, the order
≤
E.F
can easily be shown to be the lexicographic order of
≤
E
and
≤
F
.
One technical
objection to the use of the product c-semiring operator of Definition 5 is that it does not
preserve cancellativity.
Even worse,
E × F
is not cancellative, even if
E
and
F
are, for the simple reason that
h
0
E
,
1
F
i ⊗
E×F
h
1
E
,
0
F
i
=
h
0
E
,
0
F
i
=
h
0
E
,
0
F
i ⊗
E×F
h
1
E
,
0
F
i
As seen in the construction above, non-cancellative elements can occur only in the most significant position
of the lexicographic product when the least significant position is the bottom element.
Because we need to
construct mappings from component c-semirings to composed c-semirings in the sequel (e.g., from
E
to
E . F
), it
is useful to be able to preserve cancellativity of c-semirings when using a product construction.
This is done by
effectively prohibiting non-cancellative elements to appear in the carrier of the result, as follows.
Definition 7.
Let
E
and
F
be two cancellative c-semirings.
Their join composition,
written
E 
F
,
is the
c-semiring where
– The carrier is the set (
C
(
E
)
× C
(
F
))
∪ {h
0
E
,
0
F
i}
.
–
The choice and composition operators, as well as the bottom and top elements, are defined as in Definition 5.
It is now easy to see that
E
F
is indeed a c-semiring (Lemma 32) and is cancellative (Lemma 33).
8
Similar to the
product of c-semirings, the order of the join
≤
E
F
is the product order, but on the restricted carrier.
In a sense,
E
F
can be regarded as the largest cancellative c-semiring contained in
E×F
; indeed,
C
(
E×F
)
∪{h
0
E
,
0
F
i}
=
E
F
.
As hinted in the above, we need a method to embed component c-semirings into a composed c-semiring.
To
this end, we define the canonical
injections
κ
E×F
L
:
E → E × F
and
κ
E×F
R
:
F → E × F
as follows:
κ
E×F
L
(
e
) =
(
h
0
E
,
0
F
i
e
= 0
E
he,
1
E
i
otherwise
and
κ
E×F
R
(
f
) =
(
h
0
E
,
0
F
i
f
= 0
F
h
1
E
, f i
otherwise
It can easily be seen that
κ
E×F
L
and
κ
E×F
R
are reflecting homomorphisms from
E
and
F
respectively to
E × F
.
Moreover,
κ
E×F
R
can be seen as a reflecting homomorphism from
F
to
E . F
, and if
E
is cancellative, then
κ
E×F
L
can be seen as a reflecting homomorphism from
E
to
E . F
.
Lastly, if
E
(respectively
F
) is cancellative, then
κ
E×F
L
(respectively
κ
E×F
R
) can be seen as a reflecting homomorphism from
E
(respectively
F
) to
E 
F
.
We adjust the
superscript to denote the range of the canonical injection we are talking about (e.g.,
κ
E.F
L
for the homomorphism
from
E
to
E . F
), but note that their effective mappings remain the same.
Further observations
We conclude this section by observing a number of properties of c-semirings to be
used later on.
The first property we consider is monotonicity of the choice and composition operators.
Lemma 2
([
6
,
Theorem 2.1.2])
.
Let
E
be a c-semiring with
e, e
0
, e
00
such that
e ≤ e
0
.
Then the choice and
composition operators are monotonic, i.e.,
e ∨ e
00
≤ e
0
∨ e
00
and
e ⊗ e
00
≤ e
0
⊗ e
00
.
Proof.
If
e ≤ e
0
,
then
W
{e, e
0
}
=
e
0
,
thus
e ∨ e
00
∨ e
0
=
W
{e, e
00
, e
0
}
=
W
{e, e
0
} ∨ e
00
=
e
0
∨ e
00
,
therefore
e ∨ e
00
≤ e
0
∨ e
0
.
For the second claim, we can see that
e
0
⊗ e
00
= (
e ∨ e
0
)
⊗ e
00
= (
e ⊗ e
00
)
∨
(
e
0
⊗ e
00
), thus we
know that
e ⊗ e
00
≤ e
0
⊗ e
00
.
One immediate application of Lemma 2 is in showing intensivity:
Lemma 3 ([6, Theorem 2.1.3]).
Let
E
be a c-semiring, with
e, e
0
∈ E
.
Then
⊗
is intensive, i.e.,
e
0
⊗ e ≤ e
.
Proof.
By Lemma 1, we know that
e
0
≤
1, thus by Lemma 2 we can conclude that
e ⊗ e
0
≤ e ⊗
1 =
e
.
Lemma 3 turns out to be a useful tool in proving a number of properties; we rely on this lemma in Section 7,
for example.
In literature concerned with Soft Constraint Satisfaction Problems (SCSPs), such as [
6
], intensivity means
that an additional
soft constraint on an SCSP never changes the problem such that a better solution than
the best solution to the original
problem appears.
We exploit intensivity along these lines in Section 6.
It
should be noted, however, that intensivity does not strictly mean that performing a composition of actions is
never preferred over performing a single action;
the way we set up our formalisms in Section 5 is such that
8
We point out that
W 
W
is an example of a cancellative c-semiring whose induced order is not total.
11
actions performed by compositions need to be compositions of exactly one action per component, even if some
of those actions are effectively non-operations.
As such, we trust components to attach a preference value to
non-operations in states where other components should be allowed to progress.
Another property that we use in Section 7 is that a c-semiring can be viewed as a complete lattice.
More
specifically, it holds that the choice operator of a c-semiring can be seen as a lowest upper bound operator, which
in turn induces a greatest lower bound operator.
The proof of this claim appears in Appendix A.
Lemma 4
([
6
, Theorem 2.1.4])
.
Let
E
be a c-semiring.
Then there exists a unique operator
V
: 2
E
→ E
such
that
hE, ≤,
W
,
V
i
is a complete lattice.
In accordance with Lemma 4,
we use the operator
V
E
to denote the greatest lower bound operator of a
c-semiring
E
.
As before, we drop the subscript if no confusion is likely.
3.3
Büchi-automata
We now discuss some material from formal language theory used in this thesis.
In particular, we use Büchi-
automata [
11
], which allow us to give concise descriptions of sets of streams.
Moreover, Büchi-automata have a
number of pleasant computational and compositional properties, which allow us later on to establish a decision
procedure for our logic.
Definition 8.
A Büchi-automaton
9
is a tuple
A
=
Q,
∆
, →, q
0
, F
such that
–
Q
is a finite set of
states, with
q
0
∈ Q
the initial state and
F ⊆ Q
the accepting states
– ∆ is a finite set of
symbols, also called the alphabet of
A
–
→ ⊆ Q ×
∆
× Q
is a relation called the transition relation
If
hq, δ, q
0
i ∈→
, we write
q
δ
−→
q
0
.
Given a stream
ν ∈
∆
ω
and a Büchi-automaton
A
=
Q,
∆
, →, q
0
, F
,
we can follow the transitions in
A
according to the items that appear in
ν
(provided the transition structure of
A
admits this), starting in
q
0
:
q
0
ν
(0)
−−→
q
1
ν
(1)
−−→
q
2
ν
(2)
−−→
. . .
In doing so, we obtain a stream of visited states
µ
, with
µ
(
n
) =
q
n
for
n ∈ N
, called a trace (note that there may
be more than one such trace obtained from
ν
).
A Büchi-automaton induces a set of streams referred to as its
language, defined as all streams
ν ∈
∆
ω
that have a trace which visits an accepting state infinitely often.
Definition 9.
Let
A
=
Q,
∆
, →, q
0
, F
be a Büchi-automaton.
The trace relation of
A
, written
A
, and its
infinitary lifting, written
ω
A
, are the smallest relations between
Q
ω
and ∆
ω
satisfying the rules
hµ, νi ∈
(
Q ×
∆)
ω
µ
(0)
ν
(0)
−−→
µ
(1)
µ 
A
ν
∀n ∈ N. µ
(
n
)
A
ν
(
n
)
µ 
ω
A
ν
The language accepted by
A
, written
L
(
A
), is the smallest set satisfying the rule
µ 
ω
A
ν
µ
(0) =
q
0
µ
−
1
(
F
) is infinite
ν ∈ L
(
A
)
Deciding emptiness
Interestingly, it is algorithmically decidable whether, given a Büchi-automaton
A
,
L
(
A
)
is empty.
This comes down to finding an accepting state that can be reached from the initial state with a chain
of transitions, and from which a chain of transitions can be taken back to that state.
This state can be found
using, for example, a depth-first search starting in the initial state for accepting states, followed by a depth-first
search starting in every reachable accepting state for a path back to that same state.
Lemma 5
([
33
, Proposition 11])
.
Let
A
=
Q,
∆
, →, q
0
, F
be a Büchi-automaton.
Then
L
(
A
)
6
=
∅
if and only
if there exist
w ∈ Q
∗
with
w
=
w
1
w
2
· · · w
n
and
x ∈
∆
∗
with
x
=
x
1
x
2
· · · x
n−
1
such that
–
w
1
=
q
0
.
– There exists a
n
0
∈ N
with
n
0
< n
and
w
n
0
=
w
n
∈ F
.
–
w
1
x
1
−→
w
2
x
2
−→
. . .
x
n−
1
−−−→
w
n
.
9
Strictly, the definition here is of a non-deterministic Büchi-automaton.
We do not use deterministic Büchi-automata in this
thesis, as they are strictly less powerful [33].
12
Proof.
Suppose
ν ∈ L
(
A
).
Then there exists a
µ ∈ Q
ω
such that
µ 
ω
A
ν
,
µ
(0) =
q
0
and
µ
−
1
(
F
) is infinite.
Because
F
is finite and
N
is infinite, there exists a
q ∈ F
and
n, n
0
∈ N
such that
q ∈ F
,
n
0
< n
and
µ
(
n
0
) =
q
=
µ
(
n
).
We then construct
w
=
µ
(0)
µ
(1)
· · · µ
(
n
)
∈ Q
∗
and
x
=
ν
(0)
ν
(1)
· · · ν
(
n −
1).
We already know that
µ
(0) =
q
0
and
w
n
0
=
µ
(
n
0
) =
µ
(
n
) =
w
n
.
From
µ 
ω
A
ν
, we also know that
µ
(0)
ν
(0)
−−→
µ
(1)
ν
(1)
−−→
· · ·
ν
(
n−
1)
−−−−−→
µ
(
n
),
or, equivalently, that
w
0
x
0
−→
w
1
x
1
−→
· · ·
x
n−
1
−−−→
w
n
.
The proof in the other direction is similar.
Note that Lemma 5 provides us with a
ν ∈ L
(
A
) in case
L
(
A
) is non-empty, which can serve as a counterexample
to the assertion that
L
(
A
) is empty.
We use this later on in Section 6.
Closure properties
Languages accepted by Büchi-automata are also surprisingly robust under set operations,
such as intersection, although the constructions involved tend to be somewhat non-trivial.
We highlight two
useful instances.
Lemma 6
([
33
, Proposition 6])
.
Languages accepted by Büchi-automata are closed under intersection.
More
precisely,
let
A
i
=
Q
i
,
∆
, →
i
, q
0
i
, F
i
be a Büchi-automaton for
i ∈ {
0
,
1
}
.
Then we can construct a Büchi-
automaton
A
such that
L
(
A
) =
L
(
A
0
)
∩ L
(
A
1
).
Proof.
We construct the Büchi-automaton
A
=
2
× Q
0
× Q
1
,
∆
, →,
0
, q
0
0
, q
0
1
, F
,
in which
hi, q
0
, q
1
i ∈ F
if
and only if
q
0
∈ F
0
or
q
1
∈ F
1
, and the transition relation
→
is the smallest relation satisfying the rules
q
0
δ
−→
0
q
0
0
q
1
δ
−→
1
q
0
1
hi, q
0
, q
1
i 6
∈ F
hi, q
0
, q
1
i
δ
−→
hi, q
0
0
, q
0
1
i
q
0
δ
−→
0
q
0
0
q
1
δ
−→
1
q
0
1
hi, q
0
, q
1
i ∈ F
hi, q
0
, q
1
i
δ
−→
h
1
− i, q
0
0
, q
0
1
i
Now, let
ν ∈ L
(
A
).
Then there exists a
µ ∈ Q
ω
such that
µ 
ω
A
ν
.
We construct the stream
µ
i
∈ Q
ω
i
as follows:
if
µ
(
n
) =
hi, q
0
, q
1
i
, then
µ
i
(
n
) =
q
i
.
Let
n ∈ N
, then
µ
(
n
)
δ
−→
µ
(
n
+ 1), thus consequently
µ
i
(
n
)
δ
−→
i
µ
(
n
+ 1),
and therefore
µ
i
ω
A
i
ν
.
Also,
µ
i
(0) =
q
0
i
.
To see that
ν ∈ L
(
A
i
), it remains to be shown that
µ
−
1
i
(
F
i
) is infinite.
If
µ
(
n
)
∈ F
, there must exist a
n
0
> n
with
µ
(
n
0
)
∈ F
(otherwise
µ
−
1
(
F
) would be finite).
Let
n
0
be the
smallest such
n
0
, and write
µ
(
m
) =
hi
m
, q
m
0
, q
m
1
i
for
m ∈ N
, then
hi
n
, q
n
0
, q
n
1
i
ν
(
n
)
−−−→
i
n
+1
, q
n
+1
0
, q
n
+1
1
ν
(
n
+1)
−−−−→
· · ·
ν
(
n
0
−
1)
−−−−−→
D
i
n
0
, q
n
0
0
, q
n
0
1
E
Since
µ
(
n
) =
hi
n
, q
n
0
, q
n
1
i ∈ F
, we know that
q
n
i
∈ F
i
and
i
n
+1
= 1
− i
n
.
Moreover, since
µ
(
k
) =
i
k
, q
k
0
, q
k
1
6∈ F
for
n < k < n
0
,
we know that
i
k
=
i
k
+1
.
From this,
it follows that
i
n
0
= 1
− i
n
.
This implies that for every
n ∈ N
with
µ
(
n
) =
hi, q
0
, q
1
i ∈ F
there exists a
n
0
∈ N
with
n
0
> n
and
µ
(
n
0
) =
h
1
− i, q
0
0
, q
0
1
i ∈ F
, and therefore
that,
for
i ∈ {
0
,
1
}
there exist infinitely many
n ∈ N
such that
µ
(
n
) =
hi, q
0
, q
1
i
with
q
i
∈ F
i
,
entailing that
for
i ∈ {
0
,
1
}
there exist infinitely many
n ∈ N
such that
µ
i
(
n
)
∈ F
i
;
it follows that
µ
−
1
i
(
F
i
) is infinite,
thus
ν ∈ L
(
A
i
), and therefore
ν ∈ L
(
A
0
)
∩ L
(
A
1
).
For the other direction, let
ν ∈ L
(
A
0
)
∩ L
(
A
1
), then there exists for
i ∈ {
0
,
1
}
a
µ
i
∈ Q
ω
i
such that
µ
i
(0) =
q
0
i
and
µ
i
ω
A
ν
.
We construct the stream
µ ∈ Q
ω
according to the rules
µ
(0) =
0
, q
0
0
, q
0
1
µ
(
n
) =
hi, q
n
0
, q
n
1
i 6∈ F
µ
(
n
+ 1) =
hi, µ
0
(
n
+ 1)
, µ
1
(
n
+ 1)
i
µ
(
n
) =
hi, q
n
0
, q
n
1
i ∈ F
µ
(
n
+ 1) =
h
1
− i, µ
0
(
n
+ 1)
, µ
1
(
n
+ 1)
i
We can see that
µ
(0) is the initial state of
A
.
To see that
µ 
ω
A
ν
, let
n ∈ N
.
If
µ
(
n
) =
i
n
, q
k
0
, q
k
1
6∈ F
, then
since
µ
i
(
n
)
ν
(
n
)
−−−→
1
µ
i
(
n
+ 1) for
i ∈ {
0
,
1
}
, by construction of
→
it follows that
µ
(
n
) =
hi, µ
0
(
n
)
, µ
1
(
n
)
i
ν
(
n
)
−−−→
hi, µ
0
(
n
+ 1)
, µ
1
(
n
+ 1)
i
=
µ
(
n
+ 1)
Similarly, we can find that
µ
(
n
)
ν
(
n
)
−−−→
µ
(
n
+ 1) when
µ
(
n
)
∈ F
; therefore
µ 
ω
A
ν
.
To see that
µ
−
1
(
F
) is infinite,
assume towards a contradiction that
µ
(
n
)
∈ F
for finitely many
n ∈ N
.
Then it follows that for
i ∈ {
0
,
1
}
we
have
µ
i
(
n
)
∈ F
i
for finitely many
n ∈ N
,
which contradicts that
µ
−
1
i
(
F
i
) is infinite.
We thus conclude that
µ
−
1
(
F
) is infinite and therefore that
ν ∈ L
(
A
).
Note that the construction in Lemma 6 gives us an algorithm to decide whether two Büchi-automata share a
common stream, and if so, provides us with an example of such a stream:
simply construct the automaton that
accepts the intersection of the languages and use Lemma 5 to find out whether this intersection is empty — if
not, we obtain a stream accepted by both.
Another closure property enjoyed by Büchi-automata is complementation, i.e., given a Büchi-automaton
A
we can construct a Büchi-automaton that accepts precisely the streams that are not accepted by
A
.
The proof
for this is somewhat involved and omitted from this thesis; we refer to [30] for an example.
Lemma 7
([
30
,
Theorem 2.6])
.
Languages accepted by Büchi-automata are closed under complement:
if
A
=
Q,
∆
, →, q
0
, F
is a Büchi-automaton, then we can construct a Büchi-automaton
A
C
such that
ν ∈ L
(
A
)
if and only if
ν 6
∈ L
(
A
C
).
13
We conclude this section by noting a closure property of Büchi-automata that will be useful in Section 6.
Lemma 8.
Let
A
i
=
Q
i
,
∆
, →
i
, q
0
i
, F
i
be a Büchi-automaton for
i
∈ {
0
,
1
}
.
Then there exists a Büchi-
automaton
A
such that
ν ∈ L
(
A
) if
and only if
there exists an
n
such that for all
0
≤ k < n
it holds that
ν
(
k
)
∈ L
(
A
0
) and
ν
(
n
)
∈ L
(
A
1
).
The proof of Lemma 8 is omitted, for it is beyond the scope of this thesis.
To get a general idea, one could prove
Lemma 8 by constructing an alternating Büchi-automaton [
33
] accepting the described language.
From this
alternating Büchi-automaton one can then construct a Büchi-automaton that accepts exactly the same language.
We refer to [33] for the translation of alternating Büchi-automata to Büchi-automata.
14
4
Component Action Systems
To define our formalism for agents, we first need to define precisely what we mean by actions and compositions
thereof.
This is captured by the definition below.
Definition 10.
A Component Action System (CAS) is a tuple
h
Σ
,
#
,

i
such that
– Σ is a set of actions.
– The composability relation
#
is a reflexive and symmetric relation on Σ.
– The composition operator

:
#
→
Σ is idempotent, commutative and associative on Σ up to
#
.
Given a CAS
h
Σ
,
#
,

i
, we refer to Σ as the carrier of the CAS;
we use Σ as the symbol for an abstract
CAS.
If Σ is a CAS, the corresponding relation and operator are denoted by
#
Σ
and

Σ
.
As with c-semirings,
we drop the subscript when only one CAS appears in the context.
We can justify Definition 10 on an intuitive level.
Suppose one component of our agent wants to perform the
action
σ
, while another component wants to perform the action
τ
.
If
σ
=
τ
, the components agree on the action
and thus that same action should be performed (hence reflexivity of
#
and idempotency of

).
If
σ 6
=
τ
, the
actions might still make sense to be executed in composition; they could be wholly unrelated (e.g.,
σ
could be
pick up payload and
τ
could be send heartbeat signal) or complementary (e.g.
σ
could be pick up payload and
τ
could be release payload).
If this is the case, we need a method to derive the composed operation; in the former
case, one can probably compose the actions concurrently (e.g.
pick up payload while sending heartbeat signal),
while in the latter case the composed action may constitute a logical consequence of the component actions (e.g.
exchange payloads).
We require commutativity of the composition operator so as not to distinguish composing action
σ
with
action
τ
from composing the actions in reverse order.
This seems reasonable, as there is no order (temporal or
otherwise) between the actions under composition that should make their composition non-commutative — such
an ordering would have to arise from an ordering in the components, which is absent from our model.
The condition that the composition operator is associative up to composability guarantees that,
when
composing a number of mutually composable actions, the resulting action does not depend on the particular
order of composition.
However,
it does not preclude the possibility that for actions
σ, τ, ρ ∈
Σ,
action
σ
is
composable with both
τ
and
ρ
, but
σ

τ
is not composable with
ρ
(and
σ

ρ
is not composable with
τ
).
Such
a situation may occur precisely when
τ 6
#
ρ
.
This, too, seems reasonable:
the action pick up payload may be
composable with both release payload and burn payload, but exchange payloads should be incomposable with
burn payload, because the latter is incomposable with the component action release payload.
A simple CAS
We now consider a simple and very concrete CAS called the movement CAS
M
.
Suppose we
want to model movements on a sphere with a north and south pole.
Our set of actions
M
could consist of:
– movement generally in the cardinal directions:
north
,
east
,
south
and
west
– movement purely in the cardinal directions:
north
?
,
east
?
,
south
?
and
west
?
– movement in the intercardinal directions:
northeast
,
northwest
,
southeast
and
southwest
– staying on the same latitude or longitude, or both:
stay
lat
,
stay
lon
and
stay
It seems reasonable that movement that does not make assertions about longitude should be composable with
longitudinal movement; for example, the action
east
should be composable with the actions
north
and
stay
lat
.
Also, movement purely in one cardinal direction should be composable with actions that require movement in that
same cardinal direction.
Lastly, actions are inherently composable with themselves.
All of these requirements
are modeled by defining the composability relation
:
to be the reflexive and symmetric closure of the smallest
relation satisfying the respective rules
m ∈ {
north
,
south
,
stay
lat
}
m
0
∈ {
east
,
west
,
stay
lon
}
m
:
m
0
m ∈ {
north
,
south
,
east
,
west
}
m
:
m
?
The composition operator

can now be defined to match our intuition; let
m, m
0
∈ M
, then:
– If
m
=
m
0
, then
m

m
0
=
m
.
–
If
m ∈ {
north
,
south
}
and
m
0
∈ {
east
,
west
}
,
then
m

m
0
=
m
0

m
is the corresponding intercardinal
direction (e.g.,
northeast
when
m
=
north
and
m
0
=
east
).
– If
m ∈ {
north
,
south
}
and
m
0
=
stay
lat
then
m

m
0
=
m
0

m
=
m
?
.
– If
m ∈ {
east
,
west
}
and
m
0
=
stay
lon
then
m

m
0
=
m
0

m
=
m
?
.
– If
m
=
stay
lon
and
m
0
=
stay
lat
, then
m

m
0
=
m
0

m
=
stay
.
The proof that
hM,
:
,

i
is a CAS is left as an exercise to the reader.
15
A general CAS
While specifying a CAS explicitly gives the designer a lot of control, we can see that even
for a (conceptually) simple CAS such as the movement CAS above some care needs to be taken to keep the
structure from violating the requirements of a CAS.
Most of the time, it is easier to impose a certain structure
on the actions and let the composability relation and composition operator arise from this structure.
We now
present such a CAS, based on the action model of Constraint Automata [
4
,
1
], where an action consists of a set
of ports that fire, along with the data values that flow at those ports.
10
In the most general sense, we can imagine that the ports represent sensors and actuators of the agent, and
that the datum that flows at a firing port represents a sensor reading or the particular mode of actuation; for
example, a port
rotate
could be connected to a rotor, with the datum

(respectively
) representing clockwise
(respectively counterclockwise) actuation.
On the level of components, we may suppose that a port represents a
(possibly shared) channel on which a component may interact with other components, and the datum at the port
represents the information that flows through the channel the instant the action is performed.
For an extensive
case study that uses this paradigm to represent the behavior of an autonomous agent, we refer to [18].
Fix a set of ports
P
and a data domain
D
.
For our actions,
we choose
A
=
D
P
,
i.e.,
A
consists of the
functions from
P
to
D
; every action thus represents an assignment of data to ports.
We isolate a special datum
∗
∈ D
; when
α ∈ A
and
α
(
p
) =
∗
for some
p ∈ P
, we say that
α
does not fire
p
; contrarily, when
α
(
p
)
6
=
∗
,
α
does fire
p
.
When
α ∈ A
, we write
fire
(
α
) for firing set of
α
, i.e.,
fire
(
α
) =
{p ∈ P
:
α
(
p
)
6
=
∗
}
.
Recall that in Constraint Automata, actions are composable if and only if they agree on common ports [
4
].
This
means that actions
α, β ∈ A
are composable when ports that they both fire (i.e., the elements
p ∈
fire
(
α
)
∩
fire
(
β
))
carry the same datum (i.e.,
α
(
p
) =
β
(
p
)).
As a result, we can choose our composability relation
♦
to be the
smallest relation satisfying the following rule:
α, β ∈ A
α
fire
(
β
)
=
β
fire
(
α
)
α ♦ β
Lastly, we need to define the composition operator.
Recall that for Constraint Automata, the composition of
two composable actions is defined to fire the union of their ports, with the data flow at common ports given by
either action, and the data flow at a firing port unique to that action given by the data flow at that port in that
action.
Formally, this means that our composition operator

can be defined for
α ♦ β
as follows:
(
α

β
)(
p
) =





α
(
p
)
p ∈
fire
(
α
)
β
(
p
)
p ∈
fire
(
β
)
∗
otherwise
We conclude this section by showing that the action model used by Constraint Automata is indeed a CAS.
Lemma 9.
hA, ♦,

i
, as defined above, is a CAS.
Proof.
It is easy to see that
♦
is symmetric and reflexive and that

is idempotent by inspecting their definitions.
For the following, let
α, β, γ ∈ A
.
Before we proceed, we observe that from the definition of

it follows that
if
α ♦ β
,
then
fire
(
α

β
) =
fire
(
α
)
∪
fire
(
β
).
To see that

is commutative up to
♦
,
let
α ♦ β
and
p ∈ P
.
If
p ∈
fire
(
α
) and
p ∈
fire
(
β
), then (
α

β
)(
p
) =
α
(
p
) and (
β

α
)(
p
) =
β
(
p
).
But
α
(
p
) =
β
(
p
) since
α ♦ β
.
Thus
(
α

β
)(
p
) = (
β

α
)(
p
).
If
p ∈
fire
(
α
) and
p 6
∈
fire
(
β
),
then (
α

β
)(
p
) =
α
(
p
) = (
β

α
)(
p
);
similarly,
we find
that
α

β
and
β

α
agree when
p 6
∈
fire
(
α
) and
p ∈
fire
(
β
).
The case where
p 6
∈
fire
(
α
) and
p 6∈
fire
(
β
) remains,
but then (
α

β
)(
p
) =
∗
= (
β

α
)(
p
).
We conclude that
α

β
=
β

α
and thus that

is commutative.
We must also show that

is associative up to
♦
, i.e., that when
α ♦ β
and
β ♦ γ
, then it follows that
γ ♦ α
if and only if (
α

β
)
♦ γ
, which in turn holds if and only if
α ♦
(
β

γ
).
Assume that
α ♦ γ
and let
p ∈ P
.
If
p ∈
fire
(
α

β
)
∩
fire
(
γ
), then either
p ∈
fire
(
α
) or
p ∈
fire
(
β
).
If
p ∈
fire
(
α
), then (
α

β
)(
p
) =
α
(
p
) by definition
of

and
α
(
p
) =
γ
(
p
) since
α ♦ γ
, thus (
α

β
)(
p
) =
γ
(
p
).
If on the other hand
p ∈
fire
(
β
), we can derive that
(
α

β
)(
p
) =
γ
(
p
) by a similar reasoning.
We conclude that (
α

β
)
♦ γ
.
If on the other hand we assume that
α ♦
(
β

γ
),
let
p ∈
fire
(
α
)
∩
fire
(
γ
).
Then
p ∈
fire
(
α
)
∩
(
fire
(
β
)
∪
fire
(
γ
)) =
fire
(
α
)
∩
fire
(
β

γ
) immediately,
and thus
α
(
p
) = (
β

γ
)(
p
) =
γ
(
p
);
consequently,
α ♦ γ
.
We have thus established that
α ♦ γ
if and only if
α ♦
(
β

γ
); we can prove that
α ♦ γ
if and only if (
α

β
)
♦ γ
in an analogous fashion.
Lastly, we need to show that if
α
,
β
and
γ
are all related by
♦
, then (
α

β
)

γ
=
α

(
β

γ
).
First, observe
that these expressions are well-defined, because (
α

β
)
♦ γ
and
α ♦
(
β

γ
) are true, by the reasoning above.
If
p ∈
fire
(
α

β
),
then
p ∈
fire
(
α
) or
p ∈
fire
(
β
).
In the former case,
((
α

β
)

γ
)(
p
) = (
α

β
)(
p
) =
α
(
p
) =
(
α

(
β

γ
))(
p
) by definition of

.
In the latter case,
p ∈
fire
(
β

γ
) and thus (
α

(
β

γ
))(
p
) = (
β

γ
)(
p
) =
β
(
p
) = (
α

β
)(
p
) = ((
α

β
)

γ
)(
p
), also by definition of

.
If
p 6∈
fire
(
α

β
), then ((
α

β
)

γ
)(
p
) =
γ
(
p
) =
((
α

β
)

γ
)(
p
), regardless of whether
p ∈
fire
(
γ
).
10
This is a slight misrepresentation; formally, transitions in Constraint Automata are labelled with a firing set of ports and a data
constraint that represents a constraint satisfaction problem
P
on the firing set.
The data that flows at the ports are required to
constitute a solution to
P
.
We argue that this distinction does not matter when we limit ourselves to finite data domains, as any
Constraint Automaton with a finite data domain can be represented to match the description above by splitting
P
into a set of
Constraint Satisfaction Problems such that each problem matches exactly one solution to
P
.
16
5
Soft Component Automata
We now turn our attention to the formalism used to model components.
We propose a framework that can
be seen as a generalization of (Soft) Constraint Automata [
4
,
1
],
called Soft Component Automata in which
automata can be composed,
and the preferences of their composed actions are the composed preferences of
their actions.
The term soft has been used to describe generalizations that involve adding preferences (c.f.
Soft
Constraint Satisfaction versus Constraint Satisfaction [
9
,
10
] and Soft Constraint Automata versus Constraint
Automata [
1
]).
The term component emphasizes that these automata are compositional, i.e., we can construct
more complex automata from a basic set of simple automata.
Definition 11.
A Soft Component Automaton (SCA) is a tuple
Q,
Σ
, E, →, q
0
, t
such that
–
Q
is a finite set of states with
q
0
∈ Q
the initial state.
– Σ is a Component Action System called the underlying CAS.
–
E
is a c-semiring called the underlying c-semiring with
t ∈ E
called the threshold value.
–
→ ⊆ Q ×
Σ
× E × Q
is a finite relation called the transition relation.
When
hq, σ, e, q
0
i ∈ →
we write
q
σ, e
−−→
q
0
.
We interpret the threshold value in one of two ways;
in Section 6 it serves as a minimum value for feasible
actions,
whereas in Section 7,
we use it as a value that determines the preference of the agent to perform a
special non-operation called idling.
We ignore the threshold value for the remainder of this section.
We can depict an SCA graphically as a labelled transition system:
we draw a graph with a vertex for every
state, and edges labeled with elements from
E ×
Σ between states to represent transitions.
Consider, for example,
a component responsible for letting an agent patrolling on a sphere, along a path parallel to the equator.
We
could model this component using the movement CAS
M
and the weighted semiring
W
, as in Figure 1.
11
q
W
q
E
east
,
5
west
,
5
stay
lon
,
5
west
,
0
stay
lon
,
5
east
,
0
Figure 1:
A component modeling a patrolling movement.
The component depicted in Figure 1 starts in state
q
W
, where it is moving towards the westmost waypoint.
As long as it can move west (i.e., as long as the waypoint has not been reached), it will do so, since the action
west
has the lowest weight and therefore the highest preference.
When this action is not available, the component
tries to stay on the same longitude, by performing the action
stay
lon
(hoping that it will be able to progress at
some point); it may also try to turn by performing the action
east
.
The state
q
E
is the dual to
q
W
:
here, the
component attempts to move towards the eastmost waypoint.
5.1
Composition
The composition operator we propose for SCAs is defined below.
Note that only transitions whose actions are
composable give rise to transitions with composed actions; the composed action of such a transition is obtained
from the CAS, while the composed preference is provided by the c-semiring.
Definition 12.
Let
A
i
=
Q
i
,
Σ
, E, →
i
, q
0
i
, t
i
be an SCA for
i ∈ {
0
,
1
}
.
The composition of
A
0
and
A
1
, written
A
0
./ A
1
, is the SCA
A
=
Q,
Σ
, E, →, q
0
, t
such that
– The set of states
Q
is
Q
0
× Q
1
, with the initial
state
q
0
=
q
0
0
, q
0
1
.
– The threshold value
t
is
t
0
⊗ t
1
∈ E
.
– The transition relation
→
is the smallest relation that satisfies the rule
q
0
σ
1
, e
0
−−−→
0
q
0
0
q
1
σ
1
, e
1
−−−→
2
q
0
1
σ
0
#
σ
1
hq
0
, q
1
i
σ
0
 σ
1
, e
0
⊗e
1
−−−−−−−−−→
hq
0
0
, q
0
1
i
By unwinding the definitions of the action system
hA, ♦,

i
in combination with Definition 12, one can see that
Soft Component Automata properly generalize (Soft) Constraint Automata:
transitions compose if and only if
they agree on common ports, and the composed action consists of all ports involved firing in concert.
11
When depicting SCAs graphically, the CAS and c-semiring used should be obvious from or specified in the context.
17
Let us first consider an example of the composition operator.
Let
A
move
be the SCA in Figure 1.
Suppose
our agent can also diverge from the path (by moving north or south) but we prefer the agent to stay on track.
A possible component tasked with this responsibility,
A
diverge
, is depicted in Figure 2; this SCA, too, uses the
movement CAS and the weighted semiring.
The initial state of this component
q
M
signifies that the agent is
centered on the path.
As long as the agent can remain centered, it will do so by executing the action
stay
lat
; if
this is not possible, it tries to diverge from the path to either the north or south.
In a state where the agent has
diverged (
q
N
or
q
S
for northward and southward diversion respectively), the agent always prefers to converge
back to the path by moving in the opposite direction; failing this, the agent attempts to stay on that latitude
(again, executing
stay
lat
).
q
S
q
M
q
N
north
,
0
south
,
5
north
,
5
south
,
0
stay
lat
,
2
stay
lat
,
0
stay
lat
,
2
Figure 2:
A component modeling divergence from the path in two directions.
Since
A
move
and
A
diverge
share the same CAS and c-semiring, we can compose them to obtain
A
move
./ A
diverge
,
a component that is aware of actions involved in patrolling and divergence, as well as their composed preferences.
The resulting SCA is sketched in Figure 3; here, we see that the agent can be in one of six states, depending on
its current direction and divergence.
We now discuss the states whose outgoing transitions are drawn.
–
When the agent is in state
q
W,M
, it is on the path and moving towards the western waypoint.
We can see
that the most preferred action is the self-transition labeled with the action
west
?
(weight 0), which moves
directly towards the western waypoint.
Alternatively, the agent may diverge to the north- or southwest
(weight 5), stay put (also weight 5) or turn around to the east (also weight 5).
Failing that, the agent can
opt to move due north or south (weight 10), and as a last resort the agent can opt to turn around to the
southeast (weight 15).
– When in state
q
E,S
, the agent has diverged from the path to the south and is attempting to move to the
eastern waypoint.
Here, the most preferred action is
northeast
, which brings the agent back on the path
and moves towards the waypoint.
Failing that, the agent can move due east (with weight 2), or return to
the path while preserving its direction or turning around (weight 5).
If those fail, the agent can stay in its
current position (weight 7), or turn around while remaining diverged from the path (weight 7).
q
W,N
q
W,M
q
W,S
q
E,N
q
E,M
q
E,S
north
?
,
10
northwest
,
5
south
?
,
10
southwest
,
5
stay
,
5
west
?
,
0
north
?
,
5
northeast
,
0
east
?
,
2
stay
,
7
east
?
,
5
west
?
,
7
southeast
,
15
northwest
,
5
Figure 3:
A sketch of
A
move
./ A
diverge
.
For readability, we write
q
x,y
instead of
hq
x
, q
y
i
.
To prevent clutter, only
the transitions exiting
q
W,M
and
q
E,S
are drawn; all other transitions are similar.
We can conclude from the discussion above that our composition operator serves the purpose of letting the
description of our final system remain concise; rather than manually specify the 6 states and 42 transitions of
A
move
./ A
diverge
, we specify the 5 states and 13 transitions of
A
move
and
A
diverge
.
Moreover, since our CAS took
care of which actions can compose meaningfully, and our c-semiring provided the composed preferences, there
was no need to work these out manually.
18
One objection, however, is that some actions are insufficiently differentiated in terms of preference.
Take, for
example, the state
q
W,M
, where the actions
northwest
,
southwest
and
east
?
have the same weight; conceivably, we
would prefer that the agent proceeds beside the path, rather than having it turn around, i.e., the agent should
try to move around obstacles rather than switch target waypoints prematurely.
Arguably, this problem is due
to the simple reason that we really have two concerns at play here:
A
move
models the concern that the agent
should patrol, while
A
diverge
models that the agent should stay on the path.
By using the composition operator
to compose
A
move
and
A
diverge
, we have lumped their preferences in the same c-semiring:
–
The preference of 5 for
northwest
and
southwest
in
q
W,M
is the result of the weight of 0 for
west
in
q
W
of
A
move
, as well as the weight 5 for
north
and
south
in
q
M
of
A
diverge
.
–
The preference for 5 for
east
?
in
q
W,M
is the result of the weight 5 for
east
in
q
W
of
A
move
and 0 for
stay
lat
in
q
M
of
A
diverge
.
To remedy this,
we need to keep separation of concerns when composing SCAs.
We first define how to
move SCAs between c-semirings.
Using functions between c-semirings,
we can then define new composition
operators.
These operators first use a canonical injection to change the underlying c-semiring of the operands
into a c-semiring composed of the underlying c-semirings of both operands.
The results are then composed using
the composition operator seen before.
An added advantage is that we immediately obtain composition operators
for SCAs with different underlying c-semirings.
Definition 13.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA and let
f
:
E → F
be a function.
The application of
f
to
A
, written
f
(
A
), is the SCA
Q,
Σ
, F, →
h
, q
0
, f
(
t
)
, where
→
h
is the smallest relation satisfying the rule
q
σ, e
−−→
q
0
q
σ, f
(
e
)
−−−−→
h
q
0
Definition 14.
Let
A
i
=
Q,
Σ
, E
i
, →
i
, q
0
i
, t
i
be an SCA for
i ∈ {
0
,
1
}
.
– The product composition of
A
0
and
A
1
, written
A
0
× A
1
, is given by
κ
E
0
×E
1
L
(
A
0
)
./ κ
E
0
×E
1
R
(
A
1
)
– If
E
0
is cancellative, then the lexicographic composition of
A
0
and
A
1
, written
A
0
. A
1
, is given by
κ
E
0
.E
1
L
(
A
0
)
./ κ
E
0
.E
1
R
(
A
1
)
– If
E
0
and
E
1
are cancellative, then the join composition of
A
0
and
A
1
, written
A
0
A
1
, is given by
κ
E
0
E
1
L
(
A
0
)
./ κ
E
0
E
1
R
(
A
1
)
Using our new composition operators, we can can remedy the objection raised to the example of the patrolling
agent.
Specifically, the SCA
A
move
. A
diverge
has transitions where the preference originating from
A
move
is present
on the most significant component; accordingly, the transitions labeled with
northwest
and
southwest
in
q
W,M
now have the weight
h
0
,
5
i ∈ W . W
, which makes them preferred over the transition labeled with
east
?
with
weight
h
5
,
0
i ∈ W . W
.
Compromise and harmonization
One important observation about SCAs is that,
in a state,
the most
preferred actions need not be composable.
As a result, the most preferred action in the composed state need not
be the composition of the most preferred actions of the components.
This property of SCAs represents that
they are capable of compromise:
if components disagree on the most-preferred action, their composition can
have a most-preferred action which forms a middle ground between the preferences of both components.
In
general, compromise is not observed in compositions where all actions are pairwise composable; this is the case
for the example with
A
move
and
A
diverge
above.
However, if we adjust
A
diverge
to prefer converging straight back
to the path over converging in its general direction (see Figure 4), then the most-preferred action
west
in
q
W
is
incomposable with the most-preferred action
north
?
in
q
S
.
However,
west
in
q
W
is composable with
north
in
q
S
,
the latter having preference 1.
Consequently, the most-preferred action in
q
W,S
of
A
move
./ A
0
diverge
is
northwest
with weight 1.
A concept closely related to compromise is harmonization:
where compromise is the result of incompos-
ability between actions, harmonization can be regarded as the result of incomposability between preferences.
Harmonization occurs when the most-preferred actions are composable, but their preferences do not compose as
preferably as the preferences of other actions.
For instance, consider a component
A
L
with the
unix
-semiring as
underlying c-semiring; this component may perform an action
σ
that requires reading privileges or an action
τ
that requires writing privileges (c.f.
Figure 5a).
We can see that in
A
L
,
both
σ
and
τ
are most preferred
actions,
on account of their preferences being incomparable.
Another component
A
R
has a single action
ρ
,
requiring reading privileges, too (c.f.
Figure 5b).
Suppose both
σ
and
τ
are composable with
ρ
.
The composition
A
L
./ A
R
has a single most-preferred action
σ

ρ
; this action is preferred over the composed action
τ

ρ
, since
the latter has preference
{
R
,
W
}
, while the former has preference
{
R
}
.
19
q
S
q
M
q
N
north
?
,
0
north
,
1
south
,
5
north
,
5
south
?
,
0
south
,
1
stay
lat
,
2
stay
lat
,
0
stay
lat
,
2
Figure 4:
The adjusted divergence component
A
0
diverge
.
q
1
q
0
1
q
00
1
σ, {
R
}
τ, {
W
}
(a) The component
A
L
.
q
2
q
0
2
ρ, {
R
}
(b) The component
A
R
.
Figure 5:
Two components with the
unix
-semiring as underlying c-semiring.
5.2
Further observations
We conclude this section with some observations relevant to composition and homomorphisms.
First, we note
that homomorphisms are compatible with composition.
Lemma 10.
Let
A
0
and
A
1
be SCAs with underlying c-semiring
E
,
for
i
∈ {
0
,
1
}
.
Let
h
:
E → F
be a
homomorphism.
Then
h
(
A
0
./ A
1
) =
h
(
A
0
)
./ h
(
A
1
).
Proof.
The states of
h
(
A
0
./ A
1
) and
h
(
A
0
)
./ h
(
A
1
) are the same,
as are the initial
states,
the underlying
c-semiring and underlying CAS.
Let
t
i
be the threshold value of
A
i
for
i ∈ {
0
,
1
}
.
The threshold value of
h
(
A
0
./ A
1
) is
h
(
t
0
⊗
E
t
2
), which is equal to
h
(
t
1
)
⊗
F
h
(
t
1
) by the definition of homomorphism; the latter value
is precisely the threshold value of
h
(
A
0
)
./ h
(
A
1
).
It remains to show that the transition relations of
h
(
A
0
./ A
1
) and
h
(
A
0
)
./ h
(
A
1
) are the same.
If
hq
0
, q
1
i
σ, f
−−→
hq
0
0
, q
0
1
i
is a transition of
h
(
A
0
./ A
1
), then
hq
0
, q
1
i
σ
0
 σ
1
, e
0
⊗
E
e
1
−−−−−−−−−−→
hq
0
0
, q
0
1
i
is a transition of
A
0
./ A
1
such that
q
i
σ
i
, e
i
−−−→
q
0
i
is a transition of
A
i
for
i ∈ {
0
,
1
}
with
h
(
e
0
⊗
E
e
1
) =
f
and
σ
0

σ
1
=
σ
.
But then
q
i
σ
i
, h
(
e
i
)
−−−−−→
q
0
i
is a transition of
h
(
A
i
) for
i ∈ {
0
,
1
}
,
making
hq
0
, q
1
i
σ
0
 σ
1
, h
(
e
0
)
⊗
F
h
(
e
1
)
−−−−−−−−−−−−−−→
hq
0
0
, q
0
1
i
a transition of
h
(
A
0
)
./ h
(
A
1
).
Since
f
=
h
(
e
0
⊗
E
e
1
) =
h
(
e
0
)
⊗
F
h
(
e
1
), this transition matches the transition we started with,
making the transition relation of
h
(
A
0
./ A
1
) a subset of the transition relation of
h
(
A
0
)
./ h
(
A
1
).
The proof of
the other inclusion is similar.
Next, we observe that
./
is commutative and associative up to composability of SCAs.
In accordance with
the lemma below, we can drop parentheses when writing down compositions of SCAs.
Lemma 11.
Let
R
be a relation that relates SCAs with the same CAS and c-semiring.
Then
(a) The operator
./
is commutative up to
R
.
(b) The operator
./
is associative up to
R
.
Proof.
Throughout this proof, let
A
i
=
Q
i
,
Σ
, E, →
i
, q
0
i
, t
i
be an SCA for
i ∈ {
0
,
1
,
2
}
.
It suffices to prove that
A
0
./ A
1
=
A
1
./ A
0
and
A
0
./
(
A
1
./ A
2
) = (
A
0
./ A
1
)
./ A
2
, since
./
preserves the CAS and c-semiring of its
operands.
To see Lemma 11a, first observe that the underlying c-semiring and CAS of
A
0
./ A
1
and
A
1
./ A
0
are
E
and Σ respectively.
The sets of states of
A
0
./ A
1
and
A
1
./ A
0
are
Q
0
× Q
1
and
Q
1
× Q
0
, which we do not
differentiate between (c.f.
Section 3).
Likewise, the initial states
q
0
0
, q
0
1
and
q
0
1
, q
0
0
are easily identified.
Also,
the threshold value of
A
0
./ A
1
is
t
0
⊗ t
1
, which is equal to the threshold value
t
1
⊗ t
0
of
A
1
./ A
0
.
It remains to show that
hq
0
, q
1
i
σ, e
−−→
hq
0
0
, q
0
1
i
is a transition of
A
0
./ A
1
if and only if
hq
1
, q
0
i
σ, e
−−→
hq
0
1
, q
0
0
i
is
a transition of
A
1
./ A
0
.
Assuming the former, we know that there exist transitions
q
i
σ
i
, e
i
−−−→
i
q
0
i
for
i ∈ {
0
,
1
}
such that
σ
0
#
σ
1
,
σ
0

σ
1
=
σ
and
e
0
⊗ e
1
=
e
.
But then
A
1
./ A
0
has a transition
hq
1
, q
0
i
σ
1
 σ
0
, e
1
⊗e
0
−−−−−−−−−→
hq
0
1
, q
0
0
i
.
Since

and
⊗
are commutative, this transition is exactly the transition we set out to find.
The proof in the
other direction is analogous.
For Lemma 11b, first observe that the underlying c-semiring and CAS, as well as the set of states, initial state
and threshold values of (
A
0
./ A
1
)
./ A
2
and
A
0
./
(
A
1
./ A
2
) are identical by reasoning similar to the above.
It
remains to show that
hq
0
, q
1
, q
2
i
σ, e
−−→
hq
0
0
, q
0
1
, q
0
2
i
is a transition of (
A
0
./ A
1
)
./ A
2
if and only if it is a transition
of
A
0
./
(
A
1
./ A
2
).
Assuming the former, we know that there exist transitions
hq
0
, q
1
i
σ
0
, e
0
−−−→
hq
0
0
, q
0
1
i
of
A
0
./ A
1
and
q
2
σ
2
, e
2
−−−→
2
q
0
2
of
A
2
such that
σ
0
#
σ
2
,
σ
0

σ
2
=
σ
and
e
0
⊗ e
2
= 3; by the same argument, we derive that
20
there exist transitions
q
i
σ
i
, e
i
−−−→
i
q
0
i
of
A
i
for
i ∈ {
0
,
1
,
2
}
such that
σ
0
#
σ
1
, (
σ
0

σ
1
)
#
σ
2
, (
σ
0

σ
1
)

σ
2
=
σ
and
(
e
0
⊗ e
1
)
⊗ e
2
=
e
.
By associativity of

up to
#
and associativity of
⊗
, we know that
σ
1
#
σ
2
,
σ
0
#
(
σ
1

σ
2
),
(
σ
0

σ
1
)

σ
2
=
σ
0

(
σ
1

σ
2
) and (
e
0
⊗ e
1
)
⊗ e
2
=
e
0
⊗
(
e
1
⊗ e
2
).
Accordingly,
hq
1
, q
2
i
σ
1
 σ
2
, e
1
⊗e
2
−−−−−−−−−→
hq
0
1
, q
0
2
i
is
a transition of
A
1
./ A
2
and therefore
hq
0
, q
1
, q
2
i
σ, e
−−→
hq
0
0
, q
0
1
, q
0
2
i
is a transition of
A
0
./
(
A
1
./ A
2
).
The proof
in the other direction is analogous.
Finally, we note that similar associativity and commutativity rules hold for the operators
×
and
, except
that for commutativity, we need a (reflecting) homomorphism to move between orders of composition.
Since
this homomorphism does not change the order of preferences, we disregard it in further notation and treat
×
and
as though they were commutative and associative, dropping parentheses whenever convenient.
Lastly,
.
is associative (as long as the most significant components have a cancellative underlying c-semiring), but not
commutative, as one would expect from its definition.
Lemma 12.
Let
A
i
=
Q
i
,
Σ
, E
i
, →
i
, q
0
i
, t
i
be an SCA for
i ∈ {
0
,
1
,
2
}
.
Then
(a) There exists a reflecting homomorphism
h
such that
A
0
× A
1
=
h
(
A
1
× A
0
).
(b) Also, (
A
0
× A
1
)
× A
2
=
A
0
×
(
A
1
× A
2
).
Proof.
For Lemma 12a,
let
h
:
E
1
× E
0
→ E
0
× E
1
be the function given for
e
1
∈ E
1
and
e
0
∈ E
0
by
h
(
e
1
, e
0
) =
he
0
, e
1
i
.
It is easy to see that
h
is indeed a reflecting homomorphism.
To see that
A
0
×A
1
=
h
(
A
1
×A
1
),
first observe that
h ◦ κ
E
1
×E
0
L
=
κ
E
0
×E
1
R
and
h ◦ κ
E
1
×E
0
R
=
κ
E
0
×E
1
L
(
∗
).
We can now derive as follows:
h
(
A
1
× A
0
) =
h

κ
E
1
×E
0
L
(
A
1
)
./ κ
E
1
×E
0
R
(
A
0
)

(Definition 14)
=
h

κ
E
1
×E
0
L
(
A
1
))
./ h
(
κ
E
1
×E
0
R
(
A
0
)

(Lemma 10)
=
h

κ
E
1
×E
0
R
(
A
0
))
./ h
(
κ
E
1
×E
0
L
(
A
1
)

(Lemma 11a)
=
κ
E
0
×E
1
L
(
A
0
)
./ κ
E
0
×E
1
R
(
A
1
)
(By (
∗
))
=
A
0
× A
1
(Definition 14)
For Lemma 12b, first observe that
κ
(
E
0
×E
1
)
×E
2
L
◦ κ
E
0
×E
1
L
=
κ
E
0
×
(
E
1
×E
2
)
L
(i)
κ
E
0
×
(
E
1
×E
2
)
R
◦ κ
E
1
×E
2
L
=
κ
(
E
0
×E
1
)
×E
2
L
◦ κ
E
0
×E
1
R
(ii)
κ
E
0
×
(
E
1
×E
2
)
R
◦ κ
E
1
×E
2
R
=
κ
(
E
0
×E
1
)
×E
2
R
(iii)
Now, we can derive
(
A
0
× A
1
)
× A
2
=
κ
(
E
0
×E
1
)
×E
2
L

κ
E
0
×E
1
L
(
A
0
)
./ κ
E
0
×E
1
R
(
A
1
)

./ κ
(
E
0
×E
1
)
×E
2
R
(
A
2
)
(Definition 14)
=

κ
(
E
0
×E
1
)
×E
2
L
◦ κ
E
0
×E
1
L
(
A
0
)
./ κ
(
E
0
×E
1
)
×E
2
L
◦ κ
E
0
×E
1
R
(
A
1
)

./ κ
(
E
0
×E
1
)
×E
2
R
(
A
2
)
(Lemma 10)
=

κ
E
0
×
(
E
1
×E
2
)
L
(
A
0
)
./ κ
(
E
0
×E
1
)
×E
2
L
◦ κ
E
0
×E
1
R
(
A
1
)

./ κ
(
E
0
×E
1
)
×E
2
R
(
A
2
)
(By (i))
=
κ
E
0
×
(
E
1
×E
2
)
L
(
A
0
)
./

κ
(
E
0
×E
1
)
×E
2
L
◦ κ
E
0
×E
1
R
(
A
1
)
./ κ
(
E
0
×E
1
)
×E
2
R
(
A
2
)

(Lemma 11b)
=
κ
E
0
×
(
E
1
×E
2
)
L
(
A
0
)
./

κ
E
0
×
(
E
1
×E
2
)
R
◦ κ
E
1
×E
2
L
(
A
1
)
./ κ
(
E
0
×E
1
)
×E
2
R
(
A
2
)

(By (ii))
=
κ
E
0
×
(
E
1
×E
2
)
L
(
A
0
)
./

κ
E
0
×
(
E
1
×E
2
)
R
◦ κ
E
1
×E
2
L
(
A
1
)
./ κ
E
0
×
(
E
1
×E
2
)
R
◦ κ
E
1
×E
2
R
(
A
2
)

(By (iii))
=
κ
E
0
×
(
E
1
×E
2
)
L
(
A
0
)
./ κ
E
0
×
(
E
1
×E
2
)
R

κ
E
1
×E
2
L
(
A
1
)
./ κ
E
1
×E
2
R
(
A
2
)

(Lemma 10)
=
A
0
×
(
A
1
× A
2
)
(Definition 14)
We thus conclude that (
A
0
× A
1
)
× A
2
=
A
0
×
(
A
1
× A
2
).
Lemma 13.
Let
R
be a relation on SCAs that relates two SCAs if and only if both have a cancellative c-semiring.
(a)
is commutative up to
R
, modulo a reflecting homomorphism.
(b)
is associative up to
R
.
Proof.
The proof is similar to that of Lemma 12, with the same homomorphism
h
.
Lemma 14.
Let
A
i
=
Q
i
,
Σ
, E
i
, →
i
, q
0
i
, t
i
be an SCA for
i ∈ {
0
,
1
,
2
}
such that
E
0
and
E
1
are cancellative.
Then (
A
0
. A
1
)
. A
2
=
A
0
.
(
A
1
. A
2
)
Proof.
The proof is similar to that of Lemma 12b.
21
6
Linear Temporal Logic
In this section, we develop an operational model of SCAs, so as to characterise their exhibited behavior in terms
of the (infinite) sequences of actions they allow, based on their threshold value.
We show that this behavior can
be captured by a Büchi-automaton.
We then propose an instance of Linear Temporal Logic (LTL), called LTL
Σ
,
in which atomic propositions are the actions of a CAS Σ.
We use LTL
Σ
to describe the desired behavior of
SCAs.
We then show that a formula
φ
of LTL
Σ
can be translated into a Büchi-automaton that accepts precisely
the streams that validate
φ
.
With this construction,
we propose a decision procedure that uses the classic
observations for Büchi-automata highlighted in Subsection 3.3, in the same fashion as [
33
].
If the behavior of an
SCA
A
=
A
1
./ A
1
./ . . . ./ A
n
does not satisfy a formula
φ
, the supposed decision procedure provides us with a
specific instance of the behavior of
A
that falsifies
φ
.
The novelty of SCAs is that we can, in some circumstances,
analyze the behavior to get an indication as to which components in
{A
1
, A
2
, . . . , A
n
}
were responsible for letting
this behavior exist in
A
.
6.1
Operational model
Our description of the behavior of an SCA is based on the assumption that SCAs are allowed to execute only
actions whose preference is bounded from below by the threshold value of the SCA.
As such, the threshold value
is treated as a minimum satisfaction level
for the component.
By adjusting the threshold value, the SCA can
tune its standards:
with a high threshold value, the SCA expects to be able to execute high-preference actions,
while a lower threshold value indicates that the SCA is content with lower-preference actions as well.
Definition 15.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
The trace relation of
A
, written
A
, and its infinitary
lifting, written
ω
A
, are the smallest relations on (
Q ×
Σ)
ω
satisfying the rule
hµ, νi ∈ Q
ω
×
Σ
ω
e ∈ E
t ≤ e
µ
(0)
ν
(0)
, e
−−−−→
µ
(1)
µ 
A
ν
∀n ∈ N. µ
(
n
)
A
ν
(
n
)
µ 
ω
A
ν
The language of
A
, written
L
(
A
), is the smallest set satisfying the rule
µ 
ω
A
ν
µ
(0) =
q
0
ν ∈ L
(
A
)
Intuitively, the language of an SCA
A
includes precisely the streams of actions
ν
, such that we can find a stream
of states
µ
in
A
, starting in the initial state, where the actions on the transitions between the states are labeled
with the actions from
ν
, and with preference values bound from below by the threshold value.
When
A
is an
SCA and
ν ∈ L
(
A
), we refer to
ν
as a behavior of
A
.
6.2
SCAs to Büchi-automata
The language of an SCA is preserved by some c-semiring maps, as shown below.
Lemma 15.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA, and let
h
:
E → F
be a function.
If
h
is
t
-reflecting, then
A
and
h
(
A
) share their behaviors, i.e.,
L
(
A
) =
L
(
h
(
A
)).
Proof.
Assume that
h
is
t
-reflecting.
We first show that
A
coincides with
h
(
A
)
,
and
ω
A
with
ω
h
(
A
)
.
Assume that
hµ, νi ∈
(
Q ×
Σ)
ω
is such that
µ 
A
ν
holds.
This is true precisely when there exists a transition
µ
(0)
ν
(0)
, e
−−−−→
µ
(1) in
A
such that
t ≤ e
, which in turn exists if and only if there is a transition
µ
(0)
ν
(0)
, h
(
e
)
−−−−−−→
µ
(1)
in
h
(
A
), and
h
(
t
)
≤ h
(
e
).
Since
h
(
t
) is the threshold value of
A
, the latter holds if and only if
µ 
h
(
A
)
ν
.
Now,
µ 
ω
A
ν
if and only if
µ
(
n
)
A
ν
(
n
)
for all
n ∈ N
,
if and only if
µ
(
n
)
h
(
A
)
ν
(
n
)
for all
n ∈ N
,
if and only if
µ 
ω
h
(
A
)
ν
.
Let
ν ∈ L
(
A
); by definition this is true if there exists a
µ ∈ Q
ω
such that
ν
(0) =
q
0
and
µ 
ω
A
ν
.
But due to the above, the latter holds if and only if
µ 
ω
h
(
A
)
ν
.
The preceding is necessary and sufficient to
conclude that
ν ∈ L
(
h
(
A
)).
We can use Lemma 15 to show that languages recognized by SCAs are also recognized by Büchi-automata.
Lemma 16.
Let
A
be an SCA.
Then there exists a Büchi-automaton
A
B
such that
L
(
A
) =
L
(
A
B
).
Proof.
Let
A
=
Q,
Σ
, E, →, q
0
, t
.
First, note that
t
t
is
t
-reflecting (by Lemma 26).
Then
L
(
t
t
(
A
)) =
L
(
A
) by
Lemma 15.
We construct a Büchi-automaton
A
B
as follows:
– The set of states is
Q
, the set of states of t
t
(
A
), and the initial state is
q
0
.
– The alphabet is Σ, the carrier of the CAS of t
t
(
A
).
22
– The set of accepting states is again
Q
, i.e., all states of our Büchi-automaton are accepting.
– The transition relation is the smallest relation
→
B
that satisfies
q
σ, >
−−→
t
t
q
0
q
σ
−→
B
q
0
It now suffices to prove that
L
(
A
B
) =
L
(
t
t
(
A
)).
As in Lemma 15, we first set out to prove that
A
B
coincides
with
t
t
(
A
)
.
Let
hµ, νi ∈
(
Q ×
Σ)
ω
be such that
µ 
A
B
ν
.
This is true if and only if there exists a transition
µ
(0)
ν
(0)
−−→
B
µ
(1), which (by definition of
→
B
) is true if and only if there exists a transition
µ
(0)
ν
(0)
, >
−−−−→
t
t
µ
(1).
Since
t
t
(
t
) =
> ≤
B
>
, the latter holds if and only if
µ 
t
t
(
A
)
ν
.
By a similar argument as in Lemma 15, we find
that
µ 
ω
A
B
ν
if and only if
µ 
ω
t
t
(
A
)
ν
.
Now, let
ν ∈ L
(
A
B
); this is true if and only if there exists a stream
µ ∈ Q
ω
such that
µ
(0) =
q
0
,
µ
(
n
)
∈ Q
for infinitely many
n ∈ N
(this claim holds vacuously) and
µ 
ω
A
B
ν
.
By the reasoning above, the latter holds if
and only if
µ 
ω
t
t
(
A
)
ν
, which is true precisely when
ν ∈ L
(
t
t
(
A
)) =
L
(
A
); we conclude that
L
(
A
B
) =
L
(
A
).
6.3
Desired behavior
To describe the desired behavior of the SCA, we discuss a modest variation of Linear Temporal Logic [
33
,
24
]
that is capable of recognizing composed actions by their component actions.
In this logic, every formula describes
one or more behaviors, as is made precise in the definitions that follow.
Definition 16.
Let Σ be a CAS.
The set of valid formulas of LTL
Σ
, written
L
Σ
, is the smallest set satisfying
>
∈ L
Σ
σ ∈
Σ
σ ∈ L
Σ
φ ∈ L
Σ
¬φ ∈ L
Σ
φ, ψ ∈ L
Σ
φ ∧ ψ ∈ L
Σ
φ
U
ψ ∈ L
Σ
φ ∈ L
Σ
φ

∈ L
Σ
When writing down formulas of LTL
Σ
, the order of precedence is as follows:
first comes the right-side denoted
unary connective

, followed by the left-side denoted unary connective
¬
, and last come the binary connectives
∧
and
U
.
The formula
¬φ

∧ ψ
should thus be read as (
¬
(
φ

))
∧ ψ
.
For some formulas we need to use parentheses
to disambiguate, for example between
φ ∧
(
ψ
U
χ
) and (
φ ∧ ψ
)
U
χ
.
Definition 17.
The semantics of LTL
Σ
, written
|
=, is the smallest relation between
L
Σ
and Σ
ω
satisfying
ν ∈
Σ
ω
ν |
=
>
ν ∈
Σ
ω
ν
(0) =
σ
ν |
=
σ
ν 6|
=
φ
ν |
=
¬φ
φ |
=
ν
and
ψ |
=
ν
ν |
=
φ ∧ ψ
hν, φi ∈
Σ
ω
× L
Σ
∃n ∈ N.
h
ν
(
n
)
|
=
ψ
and
∀k < n.ν
(
k
)
|
=
φ
i
ν |
=
φ
U
ψ
φ ∈ L
Σ
ν |
=
φ
λ, ξ ∈
Σ
ω
∀n ∈ N. ν
(
n
) =
λ
(
n
)

ξ
(
n
)
λ |
=
φ

We extend the use of
|
= to SCAs as follows:
if
A
is an SCA with underlying CAS Σ and
φ ∈ L
Σ
, then
A |
=
φ
if
and only if for all
ν ∈ L
(
A
) it holds that
ν |
=
φ
.
The symbol
>
is thus interpreted as a formula that holds for every behavior, while
σ ∈
Σ holds for all behaviors
whose head is
σ
.
The formula
φ ∧ ψ
holds for all behaviors that satisfy both
φ
and
ψ
, while the formula
¬φ
holds for the behaviors that do not satisfy
φ
.
Lastly, the formula
φ
U
ψ
asserts that there exists an
n ∈ N
such
that all derivatives up to the
n
-th derivative of the behavior satisfy
φ
,
and that the
n
-th derivative satisfies
ψ
; this connective is therefore best read as
φ
holds for a finite number of steps, after which
ψ
holds, or, more
informally,
φ
holds until
ψ
does.
The unary connective

is novel and deserves some further explanation.
Inspecting the definition, we can see
that
φ

is validated by behaviors for which there exists another behavior, such that the composed behavior of
the two validates
φ
.
Note that
ν |
=
φ
implies
ν |
=
φ

, since

is idempotent.
Also,
ν |
=
σ

holds for behaviors
ν
whose head
ν
(0) is an action that has
σ
as a component action.
Expressiveness
Using the rather minimal syntax introduced in Definition 16, one can derive a number of
useful connectives and modalities, which give a nice syntactic sugar over the language and enrich its intuitive
expressiveness.
Following [2], we mention a few:
–
The classic propositional directives of
∨
(disjunction),
→
(implication) and
↔
(equivalence) can be derived
using the connectives
¬
and
∧
; for example,
φ ∨ ψ
can be defined as
¬
(
¬φ ∧ ¬ψ
), by De Morgan’s law.
23
–
The modality
3
φ
, defined as a shorthand for
>
U
φ
.
By the semantics in Definition 17, we can see that
this formula holds for behaviors
ν ∈
Σ
ω
where there exists an
n ∈ N
such that
>
|
=
ν
(
k
)
for all
k < n
,
and
φ |
=
ν
(
n
)
.
Since the former claim is vacuously true, we can interpret
3
φ
to be a formula that holds
for all
ν ∈
Σ
ω
where
φ
holds after a finite number of steps.
Informally, we can thus read this formula as
eventually
φ
holds.
–
The dual modality to the above,
2
φ
, defined as
¬
3
¬φ
.
From this definition, we can see that this formula
holds for a behavior
ν ∈
Σ
ω
such that there is not a finite number of steps
n ∈ N
for which
φ |
=
ν
(
n
)
does
not hold.
This implies that
φ |
=
ν
(
n
)
must hold for all
n ∈ N
.
We can therefore read
2
φ
as
φ
always holds.
As an example of
the expressiveness of
LTL for SCAs,
consider the SCA
A
=
A
move
./ A
diverge
as seen
in Section 5.
If
we want to verify that the agent can keep patrolling indefinitely,
we can check whether
A |
=
2
(
3
east

∧
3
west

) holds; if it does, then any behavior will, at any point, eventually perform an action
composed of going east,
and will
also eventually perform an action composed of going west.
Note that the
connective

here allows us to also capture the behaviors where the agent turns around to the east by,
for
example, performing the action
southeast
.
6.4
Towards model checking
We now show that, given a formula
φ ∈ L
Σ
, we can construct a Büchi-automaton
A
that accepts precisely the
streams that are modeled by
φ
.
In [
33
], Vardi claims that the structure of the proof of the lemma below appears
in [
29
];
to our eyes, the latter paper appears to be concerned with a different type of logic, so we give a full
proof instead of a citation.
The part concerned with the

-operator is novel (if somewhat obvious).
Lemma 17.
Let
φ ∈ L
Σ
.
Then we can construct a Büchi-automaton such that
ν |
=
φ
if and only if
ν ∈ L
(
A
).
Proof.
We construct the desired Büchi-automaton by induction on the structure of
φ
.
For the base case, we have
that either
φ
=
>
or
φ
=
σ
for some
σ ∈
Σ.
In either case, it should be clear to construct a Büchi-automaton
such that
ν |
=
φ
if and only if
ν ∈ L
(
A
).
For the inductive step, assume that the claim holds for all subformulas
of
φ
; we distinguish based on the operator at hand:
–
If
φ
=
¬φ
0
, by the induction hypothesis there exists a Büchi-automaton
A
φ
0
such that
ν |
=
φ
0
if and only
if
ν ∈ L
(
A
φ
0
).
By Lemma 7, we can construct a Büchi-automaton
A
φ
such that
ν ∈ L
(
A
φ
) if and only if
ν 6
∈ L
(
A
φ
0
).
By construction, we now know that
ν ∈ L
(
A
φ
) if and only if
ν 6
|
=
φ
0
, i.e.,
ν |
=
¬φ
0
=
φ
.
–
If
φ
=
φ
0
∧φ
1
, by the induction hypothesis there exist Büchi-automata
A
i
for
i ∈ {
0
,
1
}
such that
ν ∈ L
(
A
i
)
if
and only if
ν |
=
φ
i
for
i ∈ {
0
,
1
}
.
By Lemma 6,
we can construct a Büchi-automaton
A
such that
L
(
A
) =
L
(
A
0
)
∩ L
(
A
1
).
Now
ν ∈ L
(
A
) if and only if
ν ∈ L
(
A
i
) for
i ∈ {
0
,
1
}
if and only if
ν |
=
φ
i
for
i ∈ {
0
,
1
}
if and only if
ν |
=
φ
0
∧ φ
1
=
φ
.
–
If
φ
=
φ
0
U
φ
1
, by the induction hypothesis there exist Büchi-automata
A
i
for
i ∈ {
0
,
1
}
such that
ν ∈ L
(
A
i
)
if
and only if
ν |
=
φ
i
for
i ∈ {
0
,
1
}
.
By Lemma 8,
we can construct a Büchi-automaton
A
such that
ν ∈ L
(
A
) if and only if there exists an
n ∈ N
such that for all 0
≤ k < n
it holds that
ν
(
k
)
∈ L
(
A
0
) and
ν
(
n
)
∈ L
(
A
1
).
But the latter condition holds if and only if there exists an
n ∈ N
such that for all 0
≤ k < n
it holds that
ν
(
k
)
|
=
φ
0
and
ν
(
n
)
∈ φ
1
, i.e., precisely when
ν |
=
φ
0
U
φ
1
=
φ
.
–
If
φ
=
φ

0
,
by the induction hypothesis there exists a Büchi-automaton
A
0
=
Q
0
,
∆
, →
φ
0
, q
0
, F
such
that
ν ∈ L
(
A
0
) if and only if
ν |
=
φ
0
.
Now,
let
A
φ
be the Büchi-automaton
Q,
∆
, →
φ
, q
0
, F
be the
Büchi-automaton where
→
φ
is the smallest relation satisfying the rule
q
σ
−→
ψ
q
0
σ
=
ρ

τ
q
ρ
−→
φ
q
0
If
ν ∈ L
(
A
φ
),
then there exists a
µ ∈ Q
ω
such that
µ 
ω
A
φ
ν
.
Thus,
for every
n ∈ N
,
we have that
µ
(
n
)
ν
(
n
)
−−−→
φ
µ
(
n
+ 1).
But then for every
n ∈ N
there exist
σ
n
, τ
n
such that
σ
n
=
ν
(
n
)

τ
n
and
q
σ
n
−−→
φ
0
q
0
,
by construction of
→
φ
.
Choose
λ
(
n
) =
σ
n
and
ξ
(
n
) =
τ
n
.
We then know that
µ
(
n
)
λ
(
n
)
−−−→
φ
0
µ
(
n
+ 1) for all
n ∈ N
, thus
µ 
ω
A
φ
0
λ
and therefore
λ ∈ L
(
A
φ
0
).
Since
λ |
=
φ
0
and
λ
(
n
) =
ν
(
n
)

ξ
(
n
), also
ν |
=
φ
0

=
φ
.
Conversely, let
ν |
=
φ
=
φ

0
.
Then there exist
λ, ξ ∈
Σ
ω
such that
λ |
=
φ
0
and
λ
(
n
) =
ν
(
n
)

ξ
(
n
).
Then
λ ∈ L
(
A
φ
0
), thus there exists a
µ ∈ Q
ω
such that
µ 
ω
A
φ
0
λ
,
µ
(0) =
q
0
and
µ
−
1
(
F
) is infinite.
Since for
every
n ∈ N
we have that
µ
(
n
)
λ
(
n
)
−−−→
φ
0
µ
(
n
+ 1), also
µ
(
n
)
ν
(
n
)
−−−→
φ
µ
(
n
+ 1).
As a consequence,
µ 
ω
A
φ
ν
,
thus
ν ∈ L
(
A
φ
).
Unfortunately,
the steps used for the negation and
U
-operator in the proof above can cause an exponential
blowup in the number of states of the automaton being constructed.
As a result, the worst-case complexity of
24
the construction is non-elementary [
33
], i.e., given by a nested series of exponentials, whose depth depends on
the depth of
φ
.
For an approach that is computationally feasible, we suspect that it is possible to extend the construction
found in [
33
] to work with the

-operator.
The problem here is that it is not immediately obvious how to obtain
the negated dual of a formula
φ

, since
¬φ

is generally not equivalent to
(
¬φ
)

.
Consider for example the case
where Σ =
M
, and set
φ
=
east
; if
ν ∈ M
ω
such that
ν
(0) =
east
, we find that
ν 6|
=
¬
east

, while
ν |
=
(
¬
east
)

.
Decision procedure
We now combine the material in preceding paragraphs to sketch a procedure that decides
whether
A |
=
φ
.
This procedure follows the general setup of [33].
– Given an SCA
A
, construct the Büchi-automaton
L
(
A
B
) such that
L
(
A
) =
L
(
A
B
) (c.f.
Lemma 16).
– Given a formula
φ ∈ L
Σ
, obtain the Büchi-automaton
L
(
A
φ
) such that
ν |
=
φ
if and only if
ν ∈ L
(
A
φ
).
– Construct the Büchi-automaton
A
φ
such that
ν ∈ L
(
A
φ
) if and only if
ν 6∈ L
(
A
φ
) (c.f.
Lemma 7).
– Construct the Büchi-automaton
A
∩
such that
L
(
A
∩
) =
L
(
A
B
)
∩ L
(
A
φ
) (c.f.
Lemma 6).
–
Check whether
L
(
A
∩
) is empty (using Lemma 5); if so, output that
A |
=
φ
, otherwise output that
A 6
|
=
φ
.
For correctness of the above, suppose that
L
(
A
∩
) is non-empty.
This is true if and only if there exists a
ν ∈
Σ
ω
such that
ν ∈ L
(
A
B
) and
ν ∈ L
(
A
φ
).
But this holds if and only if
ν ∈ L
(
A
) and
ν 6∈ L
(
A
φ
), which in turn is
true if and only if
ν ∈ L
(
A
) and
ν 6
|
=
φ
, i.e., when
A 6
|
=
φ
.
6.5
Diagnostics
If
A
is an SCA with
µ 
ω
A
ν
, we know that
µ
induces an infinite path through
Q
; furthermore, this path is not
only labeled with actions, but also with preferences.
The definition below provides us with a summary of the
preferences that appear along
µ
, which we call the diagnostic value.
Note that, since there may be more than
one preference for the action
ν
(
n
) from state
µ
(
n
) to state
µ
(
n
+ 1), we need to take some care in condensing
the values.
Definition 18.
Let
A
=
Q,
Σ
, E, →, q
0
be an SCA.
If
µ 
ω
A
ν
, we call
η ∈ E
ω
a diagnostic stream of
hµ, νi
,
when we have that
µ
(
n
)
ν
(
n
)
, η
(
n
)
−−−−−−→
µ
(
n
+ 1) for all
n ∈ N
.
We write
H
µ,ν
for the set of diagnostic streams of
hµ, νi
.
The diagnostic value of
hµ, νi
, written
e
µ,ν
, is then defined as
e
µ,ν
=
^
n
_
{η
0
(
k
) :
η
0
∈ H
µ,ν
}
:
k ∈ N
o
The diagnostic value gives us a necessary condition for
µ 
ω
A
ν
to hold.
Lemma 18.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
If
µ 
ω
A
ν
, then
t ≤ e
µ,ν
.
Proof.
We know that
µ 
ω
A
ν
, i.e.,
µ
(
n
)
ν
(
n
)
, e
n
−−−−−→
µ
(
n
+ 1) for all
n ∈ N
.
Define
η ∈ E
ω
as
η
(
n
) =
e
n
.
Then
η
is a diagnostic stream of
hµ, νi
, by construction.
Thus
η ∈ H
µ,ν
, and moreover,
t ≤ η
(
n
) for all
n ∈ N
.
Since
η
(
n
)
≤
W
{η
0
(
n
) :
η
0
∈ H
µ,ν
}
for all
n ∈ N
,
it follows that
t ≤
W
{η
0
(
n
) :
η
0
∈ H
µ,ν
}
for all
n ∈ N
.
Therefore,
t ≤
V
{
W
{η
0
(
n
) :
η
0
∈ H
µ,ν
}}
=
e
µ,ν
.
By Lemma 18, if for an SCA
A
=
Q,
Σ
, E, →, q
0
, t
it holds that
ν ∈ L
(
A
), and we want to adjust
A
such
that
ν 6∈ A
, we can do so by adjusting the threshold value
t
such that
t 6
≤ e
µ,ν
for all
µ
with
µ 
ω
A
ν
.
We note
that, as
→
is finite, there are only finitely many distinct such
e
µ,ν
, and so this transformation can be applied
iteratively.
A caveat to this method is that, by adjusting the threshold value of
A
, we may also eliminate other
(possibly desirable) behavior from
A
; in the worst case, it may turn out that
L
(
A
) =
∅
after adjusting
t
.
Another
caveat is that if
e
µ,ν
=
1
for some
µ
with
µ 
ω
A
ν
, then such a
t
does not exist.
This, however, is to be expected:
if
e
µ,ν
=
1
, then
ν
is, in some intuitive sense, part of the “optimal” behavior of
A
; if the optimal behavior of an
SCA is undesirable, then something must be wrong with its preferences!
A different use of Lemma 18 allows us to find out which components of a composed SCA can be regarded as
“responsible” for allowing some undesired behavior
ν
.
Suppose, for instance, that
A
=
./
i∈I
A
i
, i.e.,
A
is the
composition of SCAs
A
i
=
Q
i
,
Σ
, E, →
i
, q
0
i
for
i ∈ I
.
Then, if
ν ∈ L
(
A
) is some behavior exhibited by
A
, by
Lemma 18, it holds that
t ≤ e
µ,ν
for some
µ ∈ Q
ω
.
In particular, it holds that
N
i∈I
t
i
≤ e
µ,ν
.
We adopt the convention that, if
I
0
⊆ I
such that
N
i∈I
0
t
i
≤ e
µ,ν
, we call
I
0
a suspect subset of
I
.
We can
now see that if
I
0
is a suspect subset of
I
, we need to change at least one
t
i
with
i ∈ I
0
if we want
µ 6 
ω
A
ν
to
hold — if not, then
N
i∈I
0
t
i
≤ e
µ,ν
, thus
N
i∈I
t
i
=
N
i∈I
0
t
i
⊗
N
i∈I\I
0
t
i
≤ e
µ,ν
by intensivity of
⊗
(Lemma 3).
By extension, we can see that we need to adjust at least as many threshold values as there are mutually disjoint
suspect subsets of
I
— i.e., if
I
=
{I
1
, I
2
, . . . , I
k
} ⊆
2
I
such that
I
i
is suspect for 1
≤ i ≤ k
and
I
i
∩ I
j
=
∅
for
1
≤ i < j ≤ k
, then
A
has at least
|I |
components that need adjusting.
Furthermore, if
I
0
is a suspect subset of
25
I
and does not have a strict subset that is suspect, we call
I
0
culpable.
If
I
0
is culpable, then
A
i
with
i ∈ I
0
can
be said to contribute towards allowing the behavior
ν
to exist, or (at least), not prevent it.
Using the above, if
φ ∈ L
Σ
is a formula of LTL
Σ
such that
A 6|
=
φ
, then with the decision procedure outlined
above we obtain (through Lemmas 5 and 16) streams
µ ∈ Q
ω
and
ν ∈
Σ
ω
such that
ν 6|
=
φ
,
ν ∈ L
(
A
) and
µ 
ω
A
ν
.
Moreover,
µ
and
ν
have a simple description (also by Lemma 5), thus we can compute
e
µ,ν
.
Using the
discussion above, we can then identify the culpable components of
A
, i.e., the components of
A
responsible for
allowing the behavior
ν
to exist in
A
.
Example
Suppose that our agent is modeled by the automaton
A
move
. A
diverge
, and suppose that the threshold
value of both automata is 5
∈ W
.
If we want to check that the agent never strays from the path to the north,
we can do so by verifying whether
A |
=
¬
3
north

holds.
This is decidable, and if the outcome is negative, we
obtain a behavior
ν
for which
ν |
=
¬¬
3
north

holds, i.e.,
ν |
=
3
north

holds.
It turns out that such a behavior exists; consider, for instance, the behavior
ν
, with
ν
(0) =
northwest
and
ν
(
n
) =
west
for
n ≥
1.
The accompanying stream of
states for this behavior is
µ
,
with
µ
(0) =
q
W,M
and
µ
(
n
) =
q
W,N
for
n ≥
1.
The unique diagnostic stream of
hµ, νi
is
η
,
with
η
(0) =
h
0
,
5
i
and
η
(
n
) =
h
0
,
2
i
for
n ≥
1; hence, the diagnostic value is
e
µ,ν
=
h
0
,
5
i
.
Now,
A
move
. A
diverge
=
κ
W.W
L
(
A
move
)
./ κ
W.W
R
(
A
diverge
).
Also,
the threshold value of
κ
W.W
L
(
A
move
) is
h
5
,
0
i
while the threshold value of
κ
W.W
R
(
A
diverge
) is
h
0
,
5
i
.
Since
h
5
,
0
i 6
≤
W.W
h
0
,
5
i
, while
h
0
,
5
i ≤
W.W
h
0
,
5
i
, we find
that
{A
diverge
}
is the only culpable subset of components.
To eliminate
ν
from
A
move
. A
diverge
, we thus need to
choose a threshold value
t
for
A
diverge
such that
t 6≤
W.W
e
µ,ν
; for example,
t
=
h
0
,
2
i
would do.
26
7
Propositional Dynamic Logic
In this section, we propose an alternative logic for SCAs inspired by Propositional Dynamic Logic (PDL), called
PDL
Σ
.
This logic contrasts LTL
Σ
in the previous section in that for LTL
Σ
our formulas pertained to actions
with preferences above the threshold value, whereas for PDL
Σ
our formulas are only concerned with the optimal
behavior (for a certain notion of
optimality).
Furthermore,
whereas LTL
Σ
makes assertions about infinite
sequences (i.e., streams) of actions, PDL
Σ
is concerned with finite sequences (i.e., words) of actions.
First, we define the notion of behavior used for this logic, specifically the optimal
behavior, parameterised
in terms of a partial
order.
We then carry on by defining the syntax and semantics of PDL
Σ
,
along with a
discussion on the intuition behind the connectives and a small example of its use.
Subsequently, two non-trivial
partial orders are proposed, both of which include the possibility of the agent idling, so as to compare the merit
of doing something versus not doing something.
We conclude this section with some further investigation into the
structure of preferences of optimal behavior, with the partial order instantiated to one of the proposed orders.
For the purpose of the discussion ahead, it is useful to make a distinction between two types of words.
We
refer to a finite sequence of actions from a CAS as an action word, whereas a finite sequence of preferences from
a c-semiring is a preference word.
7.1
Optimal behavior
We first need to formally establish what we mean by (optimal) behavior.
Suppose our agent is modeled by an
SCA
A
, and is currently in a state
q ∈ Q
.
The behavior exhibited in this state is determined by all series of
transitions that start in
q
, where each preference should be feasible (i.e., should not be the bottom preference).
Definition 19.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
The behavior relation of
A
, written
⇒
A
, is the smallest
relation in
Q ×
Σ
∗
× E
∗
× Q
satisfying the rules
q
,

==
⇒
A
q
q
σ, e
−−→
q
0
e 6
= 0
q
0
w,
x
==
⇒
A
q
00
q
σ·w,
e·x
=====
⇒
A
q
00
in which we write
q
w,
x
==
⇒
A
q
0
for
hq, w, x, q
0
i ∈⇒
A
.
If
q
w,
x
==
⇒
A
q
0
for some
q
0
∈ Q
, we call
hw, xi
a behavior of
q
.
As an example of the behavior relation, consider the SCA
A
move
in Figure 1.
Then the following is true:
12
q
W
east
·
west
·
west
,
5
·
5
·
0
============
⇒
A
move
q
W
q
E
west
·
stay
lon
,
5
·
5
=========
⇒
A
move
q
W
Thus
east
·
west
·
west
and
west
·
stay
lon
are behaviors of
q
W
and
q
E
respectively.
However,
q
W
west
·
west
,
0
·
5
========
⇒
A
move
q
W
does not hold, as
q
W
west
,
5
−−−−→
q
W
is not a transition of
A
move
.
In essence, the behavior relation allows us to pair
the sequences of actions that are allowed in a state with the preferences attached to those actions.
Given a set of behaviors, it is useful to determine which of them can be regarded as optimal.
But to speak of
optimality, we need to be able to compare sequences of preferences from the underlying c-semiring of the SCA,
specifically sequences of preferences that appear on transitions of the SCA.
To this end, it is useful to have a
handle on the preferences used by an SCA.
Definition 20.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
The preference alphabet of
A
, written
P
(
A
), is defined
by
P
(
A
) =
{e ∈ E
:
∃q, q
0
∈ Q, σ ∈
Σ
. q
σ, e
−−→
q
0
}
.
We say that
A
is threshold-free if
t 6
∈ P
(
A
).
For technical reasons that become clear later, we often work with threshold-free SCAs.
Because a state may have infinitely many behaviors, there may not be a single behavior among them that is
not dominated by another, according to the partial order.
For this reason, we offer the option to restrict the set
of behaviors under consideration, so as to guarantee the existence of at least one optimal behavior.
Definition 21.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA and let
P
be a partial
order on
P
(
A
)
∗
.
Let
L ⊆
Σ
∗
and
q ∈ Q
.
Then the
P
-optimal behavior within
L
of
q
in
A
, written
opt
A
P
(
q, L
), is the smallest set satisfying the
rule
q
w,
x
==
⇒
q
0
w ∈ L
∀w
0
∈ L.
h
if
q
w
0
,
x
0
===
⇒
q
00
,
then
x 6
C x
0
i
hw, xi ∈
opt
A
P
(
q, L
)
Note that, since
P
is a partial order in the above,
opt
A
P
(
q, L
) may contain more than one behavior.
Suppose
P
is a partial
order,
A
=
Q,
Σ
, E, →, q
0
, t
an SCA,
q
a state of
A
and
L ⊆
Σ.
Inspecting
Definition 21, we can see that
opt
A
P
(
q, L
) contains the behaviors of
q
in
L
that have a preference word that is not
dominated by the preference word of any other behavior of
q
that is also in
L
.
12
Recall that the numeral 0 is not the same as 0
W
=
∞ ∈ W
.
27
As an example, suppose
P
is the lexicographic order on
W
∗
induced by
≤
W
, e.g., 6
·
6
P
6
·
5
P
5
·
5
P
5
·
5
·
5.
If
L
=
{
west
,
north
·
stay
lon
,
north
·
south
}
and we consider the SCA
A
diverge
in Figure 2, we can see that
opt
A
diverge
P
(
q
S
, L
) =
{h
north
·
stay
lon
,
0
·
2
i}
(1)
This is because
west
is not an action allowed in
q
S
(i.e.,
q
S
6
west
,
e
====
⇒
q
for any
e ∈ W
and
q
a state of
A
diverge
), and
the action word
north
·
south
is assigned preference word 0
·
5, which is dominated by the preference word 0
·
2 of
north
·
stay
lon
.
We remark that the lexicographic order, by its nature, orders any prefix of
w
before
w
itself; as a
result, we find that for
L
=
{
north
·
south
,
north
}
, it holds that
opt
A
diverge
P
(
q
S
, L
) =
{h
north
·
south
,
0
·
5
i}
However,
A
diverge
may conceivably prefer going north to
q
M
and staying there over going north, and then south
again.
We investigate partial orders more suitable for this possibility further on in this section.
7.2
Syntax and semantics
We are now ready to formally define the syntax and semantics of PDL
Σ
.
For brevity,
we limit ourselves to
formulas in which negation can appear only above the atoms, and the only two modalities are reminiscent of the
universal modality in PDL [
16
].
Moreover, the analog of “programs” in our logic is limited to finite sets of action
words, i.e., we do not allow the richness of program expressions as in PDL.
Definition 22.
Let Σ be a CAS, and let
At
be a finite set of atoms.
The set of
valid formulas for PDL
Σ
that
uses atoms from
At
, written
L
At
Σ
, is the smallest set satisfying the rules
a ∈
At
a ∈ L
At
Σ
¬a ∈ L
At
Σ
φ, ψ ∈ L
At
Σ
φ ∨ ψ ∈ L
At
Σ
φ ∧ ψ ∈ L
At
Σ
L ⊆
Σ is finite
φ ∈ L
At
Σ
[
L
]
φ ∈ L
At
Σ
[
L
]
∗
φ ∈ L
At
Σ
We abbreviate [
{w
1
, w
2
, . . . , w
n
}
]
φ
with [
w
1
, w
2
, . . . , w
n
]
φ
, and likewise for [
·
]
∗
.
The semantics of
a formula of
PDL
Σ
is given in terms of
states of
an automaton
A
and an interpretation
of its atoms.
We also need a partial order on
P
(
A
)
∗
,
which should order action words in some sensible way
(preferably based on the induced order of the c-semiring).
The interpretation of (negated) atoms, disjunction
and conjunction is done in the familiar ways, but the modalities require a bit more care.
Definition 23.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA,
At
a set of atoms,
π
:
At
→
2
Q
a function and
P
a
partial
order on
P
(
A
)
∗
.
The
P
-semantics of PDL
Σ
for
A
, written
|
=
A
P
, is the smallest relation between
Q
and
L
At
Σ
that satisfies the rules
a ∈
At
q ∈ π
(
a
)
q |
=
A
P
a
a ∈
At
q 6∈ π
(
a
)
q |
=
A
P
¬a
q |
=
A
P
φ
and
q |
=
A
P
ψ
q |
=
A
P
φ ∧ ψ
q |
=
A
P
φ
or
q |
=
A
ψ
q |
=
A
P
φ ∨ ψ
L ⊆
Σ is finite
hq, φi ∈ Q × L
At
Σ
∀ hw, xi ∈
opt
A
P
(
q, L
)
.

if
q
w,
x
==
⇒
A
q
0
then
q
0
|
=
A
P
φ

q |
=
A
P
[
L
]
φ
L ⊆
Σ is finite
hq, φi ∈ Q × L
At
Σ
∀n ∈ N. q |
=
A
P
[
L
n
]
φ
q |
=
A
P
[
L
]
∗
φ
In the sequel, we always specify the function
π
when it is important for the content of
|
=
A
P
.
Consider the semantics for the modalities
[
·
]
and
[
·
]
∗
as found in Definition 23.
Suppose that
A
is an SCA, with
q
a state of
A
and
P
a partial order on
P
(
A
)
∗
.
If
φ ∈ L
At
Σ
and
L ⊆
Σ is finite, we can see that
q |
=
A
P
[
L
]
φ
holds
for states
q
in which all
P
-optimal behaviors of
q
within
L
lead to states where
φ
holds.
For example, consider
the SCA
A
diverge
from Figure 2.
As we have seen in Equation 1,
north
·
stay
lon
is the unique optimal behavior
of
q
S
within
L
=
{
west
,
north
·
stay
lon
,
north
·
south
}
, which leads to
q
M
.
Thus, if
center
is an atom such that
π
(
center
) =
{q
M
}
, then
q
S
|
=
A
P
[
L
]
center
holds, since all
P
-optimal behaviors of
q
S
in
L
lead to states where
center
holds.
The semantics of the modality
[
·
]
∗
is slightly more complex.
From Definition 23, we can see that
q |
=
A
P
[
L
]
∗
φ
holds if
q |
=
A
P
[
L
n
]
φ
holds for all
n ∈ N
.
We can thus surmise that, if we consider any
P
-optimal behavior
q
within
L
n
, then this behavior leads to a state where
φ
holds.
More informally,
q |
=
A
P
[
L
]
∗
φ
holds when, given
that we need to execute
n
action words from
L
, we can only end up in a state where
φ
holds if we choose an
P
-optimal behavior within
L
n
.
We note that, as the preference of an action may differ based on the state of the
28
SCA, it is not the case that we can just concatenate
n
optimal behaviors from
L
to obtain an optimal behavior
in
L
n
.
As an example of the use of
[
·
]
∗
, consider the SCA
A
move
. A
diverge
as sketched in Figure 3.
If we want to
assert that the optimal behavior of the agent in
q
W,M
, when restricted to move longitudinally after every two
moves, is such that the agent remains centered after every six steps, we can set
L
=
M
2
· {
south
,
southeast
,
north
,
northeast
}
and assert that
q
W,M
|
=
A
P

L
2

∗
center
holds.
We note that for the fragment of PDL
Σ
that does not include the modality
[
·
]
∗
, one can compute
opt
A
P
(
q, L
)
for any SCA
A
=
Q,
Σ
, E, →, q
0
, t
with
q ∈ Q
and
L ⊆
Σ finite,
and a given (decidable) partial
order
P
;
obviously, we can then decide
q |
=
A
P
φ
by means of a simple recursive procedure.
To decide full PDL
Σ
, we need
to use the particulars of
P
.
7.3
Partial orders for idling
We now propose two partial orders that can be used as the partial order in the
P
-semantics as outlined above.
Both of these partial orders are based on the idea that, for an agent, it is sometimes better to not perform any
action, i.e., to idle.
Recall that, in the case for
A
diverge
discussed in the above,
north
·
south
was preferred over
north
in
q
S
by the lexicographic ordering on
W
∗
.
An ordering that incorporates idling can help, by ordering
north
over
north
·
south
in
q
S
, based on the knowledge that it is better to “go north to the center of the track
and then do nothing”, than “go north and to the center and then go south again”.
This is modeled by assuming
the existence of an idling action that is available in all states and does not affect the state of the agent in any
meaningful way.
Furthermore, we assume that the preference for this action is also fixed.
The orders we discuss can be derived from any threshold-free SCA
A
.
To be precise, given a threshold-free
SCA
A
with threshold value
t
, we derive a partial order on
P
(
A
)
∗
such that
t
is the preference of idling (note
that our interpretation of
t
differs here from that in Section 6).
Given a preference word
w ∈ P
(
A
)
∗
, we need to construct the idling set of of
w
, as outlined below.
Definition 24.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
The threshold projection of
A
is the function
p
A
:
(
P
(
A
)
∪ {t}
)
∗
→ P
(
A
)
∗
that
deletes all
occurrences of
t
from a word
w
.
Let
w ∈ P
(
A
)
∗
and
n ∈ N
;
the
n
-augmented idling set of
w
, denoted by
idle
A
(
w, n
), is defined by
idle
A
(
w, n
) =
p
−
1
A
(
p
A
(
w
))
∩ E
n
For instance, let
A
be an SCA with threshold value
t
and
e ∈ P
(
A
) such that
e 6
=
t
; then
p
A
(
etete
) =
eee
.
Also:
idle
A
(
ee,
4) =
{eett, etet, ette, tete, ttee}
Note that, when
w ∈ p
A
(
P
(
A
)
∗
), it is sufficient to show that
|w
0
|
=
n
and
p
A
(
w
0
) =
w
for
w
0
∈
idle
A
(
w, n
) to
hold; this condition is immediately satisfied by action words of a threshold-free SCA.
Generalized pointwise order
We are now ready to define our first proposed partial order that incorporates
idling.
Definition 25.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
The operator
W
E
n
: 2
E
n
→ E
n
is defined
for
n ∈ N
and
E ⊆ E
n
as follows:
_
E
n
E
=
_
E
Pr
1
(
E
)
·
_
E
Pr
2
(
E
)
· · · · ·
_
E
Pr
n
(
E
)
The operator
W
E
n
induces a partial
order
≤
E
n
on
E
n
in the same manner as seen before:
if
w, x ∈ E
n
such that
W
E
n
{w, x}
=
x
, then
w ≤
E
n
x
.
The generalized pointwise order of
A
, denoted

A
, is the smallest relation on
P
(
A
)
∗
that satisfies the rule
w, x ∈ P
(
A
)
∗
p
= max(
|w|, |x|
)
_
E
p
idle
(
w, p
)
≤
E
p
^
E
p
idle
(
x, p
)
w 
A
x
To get a feeling for the relation

A
, we reconsider
A
move
(Figure 1).
In state
q
W
, the action word
west
·
east
has
preference word 5
·
5, while the action word
west
has preference word 5.
If we suppose that the threshold value
t
is 1 (note that this makes
A
move
threshold-free), then
idle
A
move
(5
·
5
,
2) =
{
5
·
5
}
and
idle
A
move
(5
,
2) =
{
5
·
1
,
1
·
5
}
.
Accordingly, we find that
_
W
2
{
5
·
5
}
= 5
·
5
≤
W
2
5
·
5 =
^
W
2
{
5
·
1
,
1
·
5
}
_
W
2
{
5
·
1
,
1
·
5
}
= 1
·
1
6
≤
W
2
5
·
5 =
^
W
2
{
5
·
5
}
29
and therefore that 5
6

A
5
·
5, but 5
·
5

A
5, i.e., the preference word of the action word
west
is considered better
than that of the action word
west
·
east
, signaling that we prefer the agent not to keep switching directions.
To see that

A
is indeed a partial order, we need an alternative characterization of the application of
V
E
n
and
W
E
n
to idling sets, based on the following functions.
Definition 26.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
We define the functions
]
A
:
E
∗
→ E
∗
and
[
A
:
E
∗
→ E
∗
inductively, as follows:
]
A
(
w
) =
(
t
w
=

]
A
(
w
0
)
· e ∨
E
|w|
+1
w
0
· e · t
w
=
w
0
· e
[
A
(
w
) =
(
t
w
=

[
A
(
w
0
)
· e ∧
E
|w|
+1
w
0
· e · t
w
=
w
0
· e
Using these functions, we can then establish the following (proofs appear in Appendix B).
Lemma 19.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
If
w, x ∈ P
(
A
)
∗
such that
|w|
=
n
=
|x|
and
w ≤
E
n
x
, then
we have that
]
A
(
w
)
≤
E
n
+1
]
A
(
x
)
[
A
(
x
)
≤
E
n
+1
[
A
(
w
)
Lemma 20.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
Then for
w ∈ E
∗
and
n ∈ N
with
|w| ≤ n
, the
following equalities hold:
_
idle
A
(
w, n
) =
]
n−|w|
A
(
w
)
^
idle
A
(
w, n
) =
[
n−|w|
A
(
w
)
It is useful to note that for an SCA
A
with
w ∈ P
(
A
)
∗
, we can compute
]
A
(
w
) (respectively
[
A
(
w
)) reasonably
easy:
by inspecting Definition 26, we find that we need approximately
|w|
2
applications of
∨
(respectively
∧
)
to compute
]
A
(
w
) (respectively
[
A
(
w
)).
Lemma 20 then provides us with a computationally feasible way to
establish whether for an SCA
A
with
w, x ∈ P
(
A
)
∗
it holds that
w 
A
x
:
simply apply
]
A
or
[
A
as necessary to
obtain action words of equal length, and compare the results.
To decide whether
w 
A
x
, we thus need on the
order of max(
|w|, |x|
)
3
applications of
∨
or
∧
in the worst case.
Using these lemmas, one can now establish that

A
is indeed a partial order, as long as
A
is threshold-free.
Theorem 1.
Let
A
be an SCA.
If
A
is threshold-free, then

A
is a partial
order on
P
(
A
)
∗
.
Proof.
Throughout the following, let
w, x ∈ P
(
A
)
∗
and
p
= max(
|w|, |x|
).
If
w ∈ P
(
A
)
∗
, then
p
A
(
w
) =
w
(as
A
is threshold-free, so
w
does not contain any occurrence of
t
); therefore,
w ∈
idle
A
(
w, |w|
).
Moreover, if
w
0
∈
idle
A
(
w, |w|
), then
|w|
=
|w
0
|
and
p
A
(
w
0
) =
w
; since
|p
A
(
w
0
)
|
=
|w|
, we know
that
w
0
does not contain any occurrence of
t
, and therefore necessarily
w
0
=
w
.
Consequently,
idle
A
(
w, |w|
) =
{w}
.
For reflexivity, assume that
w
=
x
and observe that
w 
A
x
holds, since we can derive
_
E
p
idle
A
(
w, p
) =
_
E
p
idle
A
(
w, |w|
)
=
w ≤
E
p
x
=
^
E
p
idle
A
(
x, |x|
)
=
^
E
p
idle
A
(
x, p
)
For antisymmetry, consider that when
w 
A
x
and
x 
A
w
, it holds that
_
E
p
idle
A
(
w, p
)
≤
E
p
^
E
p
idle
A
(
x, p
)
≤
E
p
_
E
p
idle
A
(
x, p
)
≤
E
p
^
E
p
idle
A
(
w, p
)
Assuming (without loss of generality) that
p
=
|w|
, we can conclude that
w ≤
E
p
^
E
p
idle
A
(
x, |w|
)
≤
E
p
_
E
p
idle
A
(
x, |w|
)
≤
E
p
w
Consequently, for all elements
x
0
∈
idle
A
(
x, |w|
) it holds that
w ≤
E
p
x
0
≤
E
p
w
, and therefore by antisymmetry of
≤
E
p
we have that
w
=
x
0
.
Thus,
idle
A
(
x, |w|
) =
{w}
(note that
idle
A
(
x, |w|
) is not empty, because
|x| ≤ |w|
).
From this we know that
p
A
(
w
) =
x
.
But since
w ∈ P
(
A
)
∗
and
A
is threshold-free, it follows that
w
=
p
A
(
w
) =
x
.
For transitivity, let
w 
A
x
and
x 
A
y
.
We now need to show that
w 
A
y
; this is done by a case distinction
on the order of the lengths.
Let
q
=
max
(
|x|, |y|
) and
r
=
max
(
|w|, |x|, |y|
).
From
w 
A
x
and
x 
A
y
, we have
_
E
p
idle
A
(
w, p
)
≤
E
p
^
E
p
idle
A
(
x, p
)
_
E
q
idle
A
(
x, q
)
≤
E
q
^
E
q
idle
A
(
y, q
)
30
(i) For the first case, let
|w|, |x| ≤ |y|
.
Then
r
=
q
and therefore we can derive
_
E
r
idle
A
(
w, r
) =
]
r−|w|
A
(
w
)
(Lemma 20)
=
]
r−p
A
(
]
p−|w|
A
(
w
))
=
]
r−p
A

_
E
p
idle
A
(
w, p
)

(Lemma 20)
≤
E
n
]
r−p
A

^
E
p
idle
A
(
x, p
)

(Lemma 19)
≤
E
n
]
r−p
A

_
E
p
idle
A
(
x, p
)

(Lemma 19)
=
]
r−p
A
(
]
p−|x|
A
(
x
))
(Lemma 20)
=
]
r−|x|
A
(
x
)
=
_
E
r
idle
A
(
x, r
)
(Lemma 20)
≤
E
n
^
E
r
idle
A
(
y, r
)
Thus
w 
A
y
.
(ii) For the second case, assume that
|w|, |y| ≤ |x|
.
Then
r
=
p
=
q
and we can derive
_
E
r
idle
A
(
w, r
) =
_
E
p
idle
A
(
w, p
)
≤
E
p
^
E
p
idle
A
(
x, p
)
≤
E
p
_
E
p
idle
A
(
x, p
)
=
_
E
q
idle
A
(
x, q
)
≤
E
q
^
E
q
idle
A
(
y, q
)
=
^
E
r
idle
A
(
y, r
)
Thus
w 
A
y
.
(iii)
The third case, where
|x|, |y| ≤ |w|
is completely analogous to the first case, except that the proof uses the
second parts of Lemma 20 and Lemma 19.
We thus derive that
w 
A
x
in this case, too.
We can see that the proof of Theorem 1 depends on the SCA being threshold free for reflexivity, antisymmetry
and transitivity:
for reflexivity,
we use the threshold-freeness of
A
to show that
idle
(
w, |w|
)
=
{w}
,
for
antisymmetry we use that
p
A
(
w
) =
w
and for transitivity we use Lemma 20,
which also requires
A
to be
threshold-free.
Generalized lexicographic order
The generalized pointwise order defined above makes no distinction
between the positions of a preference word.
As a result, it can easily be seen that it is not a total order for most
SCAs; for example, for
A
diverge
, where the underlying c-semiring is
W
, we see that 0
·
5
∈ W
∗
is not related to
5
·
4
·
0
∈ W
∗
by

A
diverge
if we assume that the threshold value is 3.
We now propose a relation on preference
words that does include such a distinction between positions.
Before we talk about the generalized lexicographic order, we recall from Section 3 that the partial order
≤
E
on
E
induces a lexicographic partial order
≤
E
∗
on
E
∗
.
Using the lexicographic order on
E
∗
, we can then define
the generalized lexicographic order on the preference alphabet of an SCA, as follows.
Definition 27.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA, and let
n ∈ N
.
We write
v
n
A
and
v
A
for the smallest
relations on
{w ∈ P
(
A
)
∗
:
|w| ≤ n}
, and
P
(
A
)
∗
respectively satisfying the rules
∀w
0
∈
idle
A
(
w, n
)
. ∃x
0
∈
idle
A
(
x, n
)
. w
0
≤
E
∗
x
0
w v
n
A
x
w, x ∈ P
(
A
)
∗
n
= max(
|w|, |x|
)
w v
n
A
x
w v
A
x
To get a feeling for the generalized lexicographic order, consider the action words
west
·
stay
lon
and
east
·
east
·
east
,
which represent sequences of actions available in state
q
W
of
A
move
(in Figure 1).
The preference words attached
to these are 0
·
5 and 5
·
0
·
0 respectively.
If
we set the threshold value of
A
move
to
t
= 3,
we find that
5
·
0
·
0
≤
E
∗
0
·
5
·
3, thus 5
·
0
·
0
v
3
A
move
0
·
5 and consequently 5
·
0
·
0
v
A
move
0
·
5.
Therefore, the action word
west
·
stay
lon
is preferred over the action word
east
·
east
·
east
.
31
Informally, the generalized lexicographic order encodes that preferences for actions closer to the present are
more important than preferences for actions further in the future.
Also, a shorter action word may be preferred
over a longer action word in cases where, by inserting the idling preference, it can achieve a better preference.
Vice versa, a longer action word is preferred over a shorter action word if the shorter action word cannot possibly
achieve the preference of the longer action word by inserting idling.
Similar to the generalized pointwise order, we can show that
v
A
is a partial order if
A
is a threshold-free
SCA.
We first make the following observations about
v
n
A
; the proofs are fairly easy and appear in Appendix B.
Lemma 21.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA and let
n ∈ N
.
The relation
v
n
A
is reflexive.
Lemma 22.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA, and let
n ∈ N
.
If
w, x ∈ P
(
A
)
∗
are such that
n
= max(
|w|, |x|
) and
w v
n
A
x v
n
A
w
, then
w
=
x
.
Lemma 23.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA, and let
n ∈ N
.
The relation
v
n
A
is transitive.
The following lemma is slightly harder to prove, but very important to show that
v
A
is a partial order:
it
allows us to vary
n
in
v
n
A
, as long as we keep comparing words of at most length
n
.
Again, a proof is provided
in Appendix B.
Lemma 24.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
If
w, x ∈ P
(
A
)
∗
and
n ∈ N
such that
|w|, |x| ≤ n
, then
w v
n
A
x
if and only if
w v
n
+1
A
x
.
With these lemma’s in place, we are now ready to prove that
v
A
is indeed a partial order for threshold-free
SCAs.
Theorem 2.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
The relation
v
A
is a partial
order.
Proof.
Let
w ∈ P
(
A
)
∗
.
Then
w v
A
w
since
w v
|w|
A
w
, the latter being a consequence of Lemma 21.
It follows
that
v
is reflexive.
Likewise,
antisymmetry of
v
A
is a consequence of Lemma 22;
let
w, x ∈ P
(
A
)
∗
and set
p
= max(
|w|, |x|
).
Then
w v
A
x v
A
w
is due to
w v
p
A
x v
p
A
w
.
By Lemma 22 we can conclude that
w
=
x
.
A bit more work is required to show transitivity.
Let
w, x, y ∈ h
(
E
∗
) be such that
w v x v y
.
Then choose
the constants
p, q, r, s
as follows:
p
= max(
|w|, |x|
)
q
= max(
|x|, |y|
)
r
= max(
|w|, |y|
)
s
= max(
|w|, |x|, |y|
)
Now we know that
w v
p
A
x
and
x v
q
A
y
.
By Lemma 24, we know that
w v
s
A
x v
s
A
y
and by Lemma 23 we can
derive that
w v
s
A
y
.
Again by Lemma 24, we can then derive that
w v
r
A
y
and therefore conclude
w v
A
y
.
Finally, the following observation is also useful
Lemma 25.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA, and let
n ∈ N
.
If
≤
E
is a total
order, so is
v
A
.
Proof.
First, note that if
≤
E
is a total order, then so is
≤
E
∗
.
In particular, this implies that for
w, x ∈ P
(
A
)
∗
,
whenever
w 6≤
E
∗
x
we have
x ≤
E
∗
w
.
In Theorem 2, we have shown that
v
A
is a partial order.
Totality remains to be shown.
To show that
v
A
is
total, we need to show that for all
w, x ∈ P
(
A
)
∗
with
n
=
max
(
|w|, |x|
) we have either
w v
n
A
x
or
x v
n
A
w
.
We
already know that either
w v
n
A
x
or
w 6v
n
A
x
holds.
In the former case, we are done.
In the latter case we can
derive the following:
w 6
v
n
A
x
then
∃w
0
∈
idle
A
(
w, n
)
. ∀x
0
∈
idle
A
(
x, n
)
. w
0
6
≤
E
∗
x
0
(Definition 27)
then
∃w
0
∈
idle
A
(
w, n
)
. ∀x
0
∈
idle
A
(
x, n
)
. x
0
≤
E
∗
w
0
(totality of
≤
E
∗
)
then
∀x
0
∈
idle
A
(
x, n
)
. ∃w
0
∈
idle
A
(
w, n
)
. x
0
≤
E
∗
w
0
(first-order reasoning)
then
x v
n
A
w
(Definition 27)
We have thus shown that either
w v
n
A
x
or
x v
n
A
w
, proving that
v
A
is total.
A note on compositionality
As seen in the above,

A
and
v
A
are partial orders only when
A
is a threshold-
free SCA.
This is somewhat of a flaw with regard to our objective of compositionality, in that threshold-freeness
of SCAs is not a compositional property.
It is fairly easy to come up with SCAs
A
1
and
A
2
with underlying
c-semiring
E
that are threshold-free,
while
A
1
./ A
2
is not;
essentially all
one needs to do is find distinct
e
1
, e
2
, e
3
, e
4
such that
e
1
⊗ e
2
=
e
3
⊗ e
4
.
32
7.4
Towards model checking
Suppose that
A
is an SCA, we can decide whether
q |
=
A
P
φ
holds, and that the set
R
=
[
{
opt
P
A
(
q, L
n
) :
n ∈ N} ⊆
Σ
∗
× P
(
A
)
∗
has a regular structure.
Then we can sketch the following decision procedure for
q |
=
A
P
[
L
]
∗
φ
:
1.
Reinterpret
A
as a finite automaton
A
F
, with
q
as initial state, the set Σ
× P
(
A
)
13
as alphabet and the
states
q
0
∈ Q
such that
q
0
|
=
A
P
φ
as accepting states.
2.
Construct a finite automaton
A
R
that accepts
R
.
3.
Decide whether
L
(
A
R
)
⊆ L
(
A
F
) holds; if this is the case, then any
hw, xi ∈
opt
P
A
(
q, L
n
) induces a path in
A
that leads to a state where
φ
holds.
It seems not altogether unreasonable to expect that there are cases where
R
is regular.
Consider, for example,
the SCA depicted in Figure 6, which uses the c-semiring
W
and the CAS
M
.
q
1
q
2
q
3
east
,
5
west
,
5
east
,
0
Figure 6:
An SCA with a regular structure on maximally preferred actions.
If we set
L
=
{
west
,
east
}
and assume the threshold value to be
t
= 10 then one can infer from the structure of
A
that the following is true:
[
{
opt
A
v
A
(
q
2
, L
n
) :
n ∈ N}
=
 
5
2
n
·
0
,
(
west
·
east
)
n
·
east
:
n ∈ N
∪
 
5
2
n
,
(
west
·
east
)
n
:
n ∈ N
The set above is regular, but it is not quite clear under which circumstances such a pattern arises, and how to
construct one when it does.
We found that if one concentrates purely on preference words, i.e., when one ignores the indirection added by
pairing of action words to preference words, it is possible to find a regular structure, as outlined in the theorems
below.
First, we need to define what it means for a preference word to be effective; intuitively, effective preference
words are words that consist of preference values that are not dominated by the idling preference of the SCA.
Definition 28.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
We call
a word
w ∈ P
(
A
)
∗
effective when
w
=

, or
when
w
=
e · w
0
with
e <
E
t
and
w
0
is effective.
We call
w
non-effective when it is not effective.
We are now ready to state the main results of this section.
Their proofs are reasonably complicated, and require
a series of lemmas about the generalized lexicographic order; we refer to Appendix C for a full treatment.
Theorem 3.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
Let
L ⊆ P
(
A
)
∗
be non-empty and finite, and
let
≤
E
∗
be total.
Let
w
=
max
v
A
(
L
) and choose the longest
x ∈
prefix
(
w
)
∩ L
such that
x · w
is the
≤
E
∗
-maximum
of (
prefix
(
w
)
∩ L
)
· w
.
14
If
w
is effective or
x
is a proper prefix of
w
, then max
v
A
(
L
n
) =
x
n−
1
· w
for
n ≥
1.
Theorem 4.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
Let
L ⊆ P
(
A
)
∗
be non-empty and finite, and
let
≤
E
∗
be total.
Let
w
=
max
v
A
(
L
).
If
w
is non-effective and
w · w
is the
≤
E
∗
-maximum of (
prefix
(
w
)
∩ L
)
· w
,
then max
v
A
(
L
n
) =
w · z
n−
1
for
n ≥
1, where
z
is the
≤
E
∗
-maximum of the shortest elements in
L
.
Corollary 1.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
Let
L ⊆ P
(
A
)
∗
be non-empty and finite, and
let
≤
E
∗
be total.
Then
{
max
v
A
(
L
n
) :
n ∈ N}
is regular.
Proof.
Follows from Theorem 3 and Theorem 4, when one observes that the conditions posed on
w
cover all
possible cases, and that either case makes the claimed set a regular language.
Our intuition is that the theorems above are a useful
first step towards investigating the conditions for
regularity of the set
R
, and therefore towards model checking for (a fragment of) PDL
Σ
.
Unfortunately, we were
unable to come up with a way to lift the regularity of the set described above to a regular description of
R
.
The
problem here is exactly the indirection:
if
hw, xi
is a behavior of
q
1
in
A
, and
hw, x
0
i
is a behavior of
q
2
in
A
,
then it is very possible that
x 6
=
x
0
, and thus that
hw, xi
is not optimal behavior in
q
1
, while
hw, x
0
i
is optimal
behavior in
q
2
.
13
Here, we idenitfy the word
hσ
1
, e
1
i hσ
2
, e
2
i · · · hσ
n
, e
n
i ∈
(Σ
× P
(
A
))
∗
with
hσ
1
σ
2
· · · σ
n
, e
1
e
2
· · · e
n
i ∈
Σ
∗
× P
(
A
)
∗
.
14
Such an
x
always exists, for
w · w ∈
(
prefix
(
w
)
∩ L
)
· w
and (
prefix
(
w
)
∩ L
)
· w
is finite.
33
8
Conclusion
We have proposed a framework for modeling agents using an automata formalism called Soft Component Automata
(SCAs).
Since actions and their preferences originate from well-defined algebraic structures (Component Action
Systems and Constraint Semirings,
respectively),
we obtain easily definable composition operators.
These
operators compose SCAs such that, in the composition, composed actions meaningfully represent their component
actions, and practically useful preferences for composed actions arise from the preferences of the component
actions.
Using SCAs and composition, one can specify the preferences and actions available in each state of an
agent concisely.
We then considered two approaches to verification of SCAs.
For the first approach, we reduced SCAs to
Büchi-automata and proposed a logic based on LTL that reflected their compositional nature.
We showed that
model checking of SCAs using this paradigm is feasible, and sketched a decision procedure for the logic based on
a well-known decision procedure for LTL.
We furthermore argued in favor of using LTL for SCAs, by showing
that one can trace undesired behavior back to the component (or combination of components) it originated from.
An alternative approach that we consider for verification for SCAs is based on PDL.
Here, our formulas make
assertions about the optimal behavior of the SCA when restricted to a set of allowed actions, for a user-provided
partial order that dictates which behavior is optimal.
We provided two such partial orders based on the idea that
an agent can elect not to perform any action, and that doing so may sometimes be preferable over performing an
action.
We then briefly expanded upon a possible way to obtain a model checking procedure for our logic based
on regularity of optimal behavior, and argued that for at least one instantiation of the partial order, there are
indications that such a regular structure exists under some circumstances.
9
Further work
Further work in the theory of c-semirings may have its application in the theory of SCAs.
For example, given
two c-semirings that model separate concerns, it may be possible to construct a c-semiring that reflects that
we want to satisfy both concerns,
but in which we prefer to keep both concerns at least somewhat satisfied,
i.e., a preference value where one concern has the maximum preference and the other has a very low preference
should not be preferred over a preference value where both concerns are reasonably high.
Such a “utilitarian”
composition can then give rise to a similar composition operator for SCAs.
In [
20
], Koehler and Clarke showed that all Port Automata can be constructed from a small set of atomic
Port Automata.
Since Port Automata can be viewed as an instance of SCAs for a particular CAS, it would be
interesting to see if their techniques can be generalized, to the point where we can obtain a sufficient condition
on a CAS that ensures that a similar decomposition result holds for SCAs.
The desire for a computationally more feasible construction of Büchi-automata from formulas in LTL
Σ
is also
a good starting point for further work.
If such a construction exist, it would mean that we can indeed apply the
decision procedure proposed, as well as the methods for diagnostics of undesired behavior.
Furthermore, in [
2
],
Baier et al. propose an extension of LTL, called LTL
IO
, that incorporates regular expressions that generalize the
U
-operator to the point where it can capture the pathway modalities of
[
·
]
and
h·i
found in PDL; lifting this
extension to LTL
Σ
seems like a useful extension, particularly if we can generalize the regular expressions using
the structure imposed by the CAS.
A serious flaw in our proposal for PDL
Σ
is that it does not incorporate composition.
Further research is
required to see if composition can given a place in PDL
Σ
; perhaps one can look into generalizing the intersection
operator found in some extensions of classic PDL to work on the level of a CAS.
Compositionality is also lacking
in PDL
Σ
where threshold-free automata are concerned.
More investigation of the partial orders is necessary to
see if this problem can be resolved.
Lastly, we leave open the question of whether an efficient model checking procedure for PDL
Σ
can exist for
particular instances of the partial order.
Further investigation of the generalized lexicographic order is necessary
to see if our observations of regularity can be lifted to the set of (restricted) optimal behaviors of a state.
Similar
investigations of the generalized pointwise order can also prove useful in this regard.
34
A
Proofs for Subsection 3.2
Lemma 26.
Let
E
be a c-semiring with
e ∈ E
and
e 6
= 0
E
, and let t
e
:
E → B
be the function defined by
t
e
(
e
0
) =
(
>
e ≤
E
e
0
⊥
otherwise
Then t
e
is
e
-reflecting.
Furthermore, if
≤
E
is total,
⊗
E
is idempotent and
e 6
= 0
E
, then t
e
is a homomorphism.
Proof.
We first show that
t
e
is
t
-reflecting.
Let
e
0
∈ E
.
Observe that
t
e
(
e
) =
>
.
If
e ≤ e
0
, then
t
e
(
e
0
) =
>
, thus
t
e
(
e
)
≤
t
e
(
e
0
); if on the other hand
e 6≤ e
0
, then t
e
(
e
0
) =
⊥
, thus t
e
(
e
)
6
≤
t
e
(
e
0
).
Now assume that
≤
E
is a total order,
⊗
E
is idempotent and
e 6
=
0
E
.
We show that
t
e
is a homomorphism in
this case.
Note that since
e 6
= 0
E
, it holds that
e 6≤
E
0
E
, thus t
e
(0
E
) =
⊥
= 0
B
.
Also, t
e
(1
E
) =
>
= 1
B
.
Let
E ⊆ E
.
To see that
t
e
(
W
E
E
)
=
W
B
t
e
(
E
), consider two cases.
If there exists an
e
0
∈ E
such that
e ≤
E
e
0
,
then since
e
0
≤
W
E
E
we have that
e ≤
W
E
E
.
Consequently,
t
t
(
W
E
E
)
=
>
, and
> ∈
t
e
(
E
), thus
W
B
t
e
(
E
) =
>
.
If
on the other hand,
e 6≤ e
0
for all
e
0
∈ E
,
we know that
e
0
< e
for all
e
0
∈ E
.
But then
W
E
E < e
,
thus
e 6≤
W
E
E
.
Consequently,
t
e
(
W
E
E
) =
⊥
.
Also,
t
e
(
E
) =
{⊥}
, thus
W
E
t
e
(
E
) =
⊥
.
In either case, we find that
t
e
(
W
E
E
) =
W
B
t
e
(
E
).
To see that
t
e
(
e
0
⊗
E
e
00
) =
t
e
(
e
0
)
⊗
B
t
e
(
e
00
),
we also consider two cases.
If
e
0
< e
or
e
00
< e
,
we have that
e
0
⊗
E
e
00
< e
by intensivity (Lemma 3).
Thus
t
e
(
e
0
⊗
E
e
00
) =
⊥
=
⊥ ⊗
B
⊥
=
t
e
(
e
0
)
⊗
B
t
e
(
e
00
).
If on the other hand
e ≤ e
0
, e
00
, we have that
e
=
e ⊗
E
e ≤ e
0
⊗
E
e
00
, by idempotency and monotonicity of
⊗
E
(Lemma 2).
Accordingly,
t
e
(
e
0
⊗
E
e
00
) =
>
=
> ⊗
E
>
= t
e
(
e
0
)
⊗
E
t
e
(
e
00
).
Definition 29
([
14
, Definition 7])
.
Let
E
be a c-semiring.
An element
e
of
E
is collapsing if there exist
e
0
, e
00
∈ E
such that
e
0
< e
00
and
e
0
⊗ e
=
e
00
⊗ e
.
Lemma 27
([
19
, Lemma 1])
.
Let
E
be a c-semiring and
e ∈ E
.
Then
e
is cancellative if and only if it is not
collapsing.
Proof.
Assume that
e
is cancellative.
Then for all
e
1
, e
2
∈ E
with
e
1
6
=
e
2
, in particular those for which
e
1
< e
2
,
it holds that
e
1
⊗ e 6
=
e
2
⊗ e
.
It then follows that
e
cannot be collapsing.
For the other direction, assume that
e
is not cancellative.
Then there exist
e
1
, e
2
for which
e
1
6
=
e
2
and
e ⊗ e
1
=
e ⊗ e
2
.
If
e
1
and
e
2
are ordered by
<
we are done immediately.
Otherwise, choose
e
3
=
e
1
⊕ e
2
.
Immediately, we see that
e
1
≤ e
3
; it can also be seen
that
e
1
6
=
e
3
(for otherwise
e
1
and
e
2
would be ordered by
<
); thus we have that
e
1
< e
3
.
Moreover:
e
3
⊗ e
= (
e
1
⊕ e
2
)
⊗ e
= (
e
1
⊗ e
)
⊕
(
e
2
⊗ e
) =
e
1
⊗ e
which establishes that
e
is collapsing.
Lemma 28.
There exists a c-semiring that is totally ordered yet not cancellative.
Proof.
Consider the c-semiring
E
=
h{∅, {a}, {a, b}},
S
, ∩, ∅, {a, b}i
.
This c-semiring is totally ordered,
since
∅ ≤
E
{a} ≤
E
{a, b}
.
Also,
{a} ∩ {a, b}
=
{a}
=
{a} ∩ {a}
,
and therefore
{a}
is not cancellative,
making
E
a
totally ordered yet non-cancellative c-semiring.
Lemma 29
([
10
, Theorem 57])
.
Let
E
and
F
be c-semirings.
The product c-semiring
E×F
is indeed a c-semiring.
Proof.
We verify the axioms of Definition 1 one-by-one.
– Let
he, f i ∈ E × F
.
Then we can compute
_
E×F
{he, f i}
=
D
_
E
{e},
_
F
{f }
E
=
he, f i
_
E×F
∅
=
D
_
E
Pr
1
(
∅
)
,
_
F
Pr
2
(
∅
)
E
=
D
_
E
∅,
_
F
∅
E
=
h
0
E
,
0
F
i
= 0
E×F
_
E×F
E × F
=
D
_
E
Pr
1
(
E × F
)
,
_
F
Pr
2
(
E × F
)
E
=
D
_
E
E,
_
F
F
E
=
h
1
E
,
1
F
i
= 1
E×F
– Let
S ⊆
2
E×F
.
Then
_
E×F
[
{S
:
S ∈ S}
=
D
_
E
Pr
1

[
{S
:
S ∈ S}

,
_
F
Pr
2

[
{S
:
S ∈ S}
E
(def.
W
E×F
)
=
D
_
E
[
{
Pr
1
(
S
) :
S ∈ S},
_
F
[
{
Pr
2
(
S
) :
S ∈ S}
E
(elementary)
=
D
_
E
n
_
E
Pr
1
(
S
) :
S ∈ S
o
,
_
F
n
_
F
Pr
2
(
S
) :
S ∈ S
oE
(flattening)
=
_
E×F
nD
_
E
Pr
1
(
S
)
,
_
F
Pr
2
(
S
)
E
:
S ∈ S
o
(def.
W
E×F
)
=
_
E×F
n
_
E×F
S
:
S ∈ S
o
(def.
W
E×F
)
35
– Let
he, f i ∈ E × F
.
Then
h
0
E
,
0
F
i ⊗
E×F
he, f i
=
h
0
E
⊗
E
e,
0
F
⊗
F
f i
=
h
0
E
,
0
F
i
Similarly
h
1
E
,
1
F
i ⊗
E×F
he, f i
=
he, f i
.
– Let
S ⊆ E × F
and
he, f i ∈ E
.
Then
he, f i ⊗
E×F
_
E×F
S
=
he, f i ⊗
E×F
D
_
E
Pr
1
(
S
)
,
_
F
Pr
2
(
S
)
E
(def.
W
E×F
)
=
D
e ⊗
E
_
E
Pr
1
(
S
)
, f ⊗
F
_
F
Pr
2
(
S
)
E
(def.
⊗
E×F
)
=
D
_
E
{e ⊗
E
e
0
:
e
0
∈
Pr
1
(
S
)
},
_
F
{f ⊗
F
f
0
:
f
0
∈
Pr
2
(
S
)
}
E
(distributivity)
=
D
_
E
{
Pr
1
(
he, f i ⊗
E×F
s
) :
s ∈ S},
_
F
{
Pr
2
(
he, f i ⊗
E×Fs
) :
s ∈ S}
E
(elementary)
=
_
E×F
{he, f i ⊗
E×F
s
:
s ∈ S
(def.
W
E×F
)
Lastly, it is easy to verify that
⊗
E×F
is commutative and associative.
Lemma 30 ([19, Lemma 2]).
Let
E
and
F
be c-semirings.
If
S ⊆
2
E×F
, then
_
E.F

[
{S
:
S ∈ S}

=
_
E.F
n
_
E.F
S
:
S ∈ S
o
Proof.
By unrolling the definitions, we find that we essentially have to establish the following equalities:
_
E
Pr
1

[
{S
:
S ∈ S}

=
_
E
Pr
1
n
_
E.F
S
:
S ∈ S
o
(1)
_
F
m

[
{S
:
S ∈ S}

=
_
F
m
n
_
E.F
S
:
S ∈ S
o
(2)
To prove Equation 1, we use that
Pr
1
n
_
E.F
S
:
S ∈ S
o
=
n
_
E
Pr
1
(
S
) :
S ∈ S
o
thus we can derive
_
E
Pr
1

[
{S
:
S ∈ S}

=
_
E

[
{
Pr
1
(
S
) :
S ∈ S}

(def.
Pr
1
)
=
_
E
n
_
E
Pr
1
(
S
) :
S ∈ S
o
(flattening
W
E
)
=
_
E
Pr
1
n
_
E.F
S
:
S ∈ S
o
(def.
Pr
1
and
W
E.F
)
One can easily prove that for a c-semiring
E
with
E
0
⊆ E ⊆ E
, if
W
E
E ∈ E
0
then
W
E
E
0
=
W
E
E
(
∗
).
We can
then prove Equation 2 as follows:
_
F
m
n
_
E.F
S
:
S ∈ S
o
=
_
F
n
e
2
:
D
_
E
Pr
1
n
_
E.F
S
:
S ∈ S
o
, e
2
E
∈
n
_
E.F
S
:
S ∈ S
oo
(def.
m
)
=
_
F
n
_
F
m
(
S
0
) :
S
0
∈ S,
_
E
Pr
1
n
_
E.F
S
:
S ∈ S
o
=
_
E
Pr
1
(
S
0
)
o
(def.
W
E.F
)
=
_
F
n
_
F
m
(
S
0
) :
S
0
∈ S,
_
E
Pr
1

[
{S
:
S ∈ S}

=
_
E
Pr
1
(
S
0
)
o
(Equation 1)
=
_
F
n
e
2
:
S
0
∈ S,
_
E
Pr
1

[
{S
:
S ∈ S}

=
_
E
Pr
1
(
S
0
)
,
e
2
∈ m
(
S
0
)
o
(flattening
W
F
)
=
_
F
n
e
2
:
S
0
∈ S,
D
_
E
Pr
1

[
{S
:
S ∈ S}

, e
2
E
∈ S
0
o
(def.
m
and (
∗
))
=
_
F
n
e
2
:
D
_
E
Pr
1

[
{S
:
S ∈ S}

, e
2
E
∈
[
{S
:
S ∈ S}
o
(def.
S
)
=
_
F
m

[
{S
:
S ∈ S}

(def.
m
)
Thus establishing the desired equalities.
Lemma 31 ([19, Lemma 3]).
Let
E
and
F
be c-semirings such that
S ⊆ E . F
and
s ∈ E . F
.
Then
s ⊗
E.F
_
E.F
S
=
_
E.F
{s ⊗
E.F
s
0
:
s
0
∈ S}
36
Proof.
Let
s
=
he, f i
and
S
0
=
{s ⊗
E.F
s
0
:
s
0
∈ S}
.
First, note that the following equality holds immediately by
distributivity of
⊗
E
:
e
1
⊗
_
E
Pr
1
(
S
) =
_
E
Pr
1
(
S
0
)
(1)
Unwinding the definitions and using Equation 1, we find the following for the left-hand side of the claim:
s ⊗
E.F
_
E.F
S
=
D
_
E
Pr
1
(
S
0
)
, e
2
⊗
F
_
F
m
(
S
)
E
(2)
Similarly, for the right-hand side we find that
_
E.F
{s ⊗
E.F
s
0
:
s
0
∈ S}
=
_
E.F
S
0
=
D
_
E
Pr
1
(
S
0
)
,
_
F
m
(
S
0
)
E
Having established that the first components are equal,
it remains to prove that this holds for the second
component as well.
If
e ∈ C
(
E
) then
f
=
0
F
, thus
f ⊗
F
W
F
m
(
S
) =
0
F
.
Moreover, for any element
he
0
, f
0
i ∈ S
0
it
must be that
f
0
= 0
F
, thus
W
F
m
(
S
0
) = 0
F
.
The case where
e ∈ C
(
E
) remains.
Let
m
0
(
S
) =
{f ⊗
F
f
0
:
f
0
∈ m
(
S
)
}
.
We can immediately rephrase the
second component of Equation 2 as
W
F
m
0
(
S
).
It now suffices to prove that
m
0
(
S
) =
m
(
S
0
).
Let
f ⊗
F
f
0
∈ m
0
(
S
).
Then
h
W
E
Pr
1
(
S
)
, f
0
i ∈ S
, thus
s ⊗
E.F
D
_
E
Pr
1
(
S
)
, f
0
E
=
D
_
E
Pr
1
(
S
0
)
, f ⊗
F
f
0
E
∈ S
0
and therefore
f ⊗
F
f
0
∈ m
(
S
0
).
For the other direction,
let
f ⊗
F
f
0
∈ m
(
S
0
).
Then
he ⊗
E
W
E
Pr
1
(
S
)
, f ⊗
F
f
0
i ∈ S
0
,
thus there exists a
he
00
, f
00
i ∈ S
such that
he ⊗
E
f
00
, f ⊗
F
f
00
i
=
D
e ⊗
E
_
E
Pr
1
(
S
)
, f ⊗
F
f
0
E
Because
e ∈ C
(
E
),
we can derive that
e
00
=
W
E
Pr
1
(
S
0
) and thus
f
00
∈ m
(
S
0
),
which implies that
f ⊗
F
f
00
=
f ⊗
F
f
0
∈ m
0
(
S
0
).
Theorem 5.
Let
E
and
F
be c-semirings.
Then
E . F
is indeed a c-semiring.
Proof.
The proofs that for
he, f i ∈ E . F
we have that
W
E.F
{he, f i}
=
he, f i
,
as well
as
W
E.F
∅
=
0
E.F
and
W
E.F
E . F
=
1
E.F
are similar to Lemma 29, as are the proofs that
⊗
E.F
is commutative and associative, and
that for
s ∈ E . F
it holds that
s ⊗
E.F
0
E.F
=
0
E.F
and
s ⊗
E.F
1
E.F
=
s
.
For the flattening principle, we refer to
Lemma 30, and for distributivity of
⊗
E.F
over
W
E.F
, we refer to Lemma 31.
Lemma 4
([
6
, Theorem 2.1.4])
.
Let
E
be a c-semiring.
Then there exists a unique operator
V
: 2
E
→ E
such
that
hE, ≤,
W
,
V
i
is a complete lattice.
Proof.
By Lemma 1, we already know that
≤
is a partial order.
It remains to be shown that
W
is indeed the
least upper bound operators, and that we can find a suitable greatest lower bound operator
V
.
We begin with
W
.
Let
E ⊆ E
, and let
e ∈ E
such that for all
e
0
∈ E
it holds that
e
0
≤ e
(i.e.,
e
is an upper bound of
E
).
Then
for all
e
0
∈ E
,
e
0
≤
W
E ≤ e
, because we can derive:
e
0
∨
_
E
=
_
(
{e
0
} ∪ E
) =
_
E
e ∨
_
E
=
_
(
{e} ∪ E
)
=
_

[
e
00
∈E
{e, e
00
}

=
_
{e ∨ e
00
:
e
00
∈ E
0
}
=
_
{e}
=
e
We now choose
V
to be the operator that takes the least upper bound of all lower bounds of the input, i.e.,
^
E
=
_
{b ∈ E
:
∀e
0
∈ E.b ≤ e
0
}
To show that
V
is the greatest lower bound operator, let
e ∈ E
and
E ⊆ E
such that for all
e
0
∈ E
we have that
e ≤ e
0
.
Then
e ≤
V
E
, because we can derive as follows
e ∨
^
E
=
_
(
{e} ∪ {b ∈ E,
∀e
0
∈ E.b ≤ e
0
}
)
=
_
{b ∈ E
:
∀e
0
∈ E.b ≤ e
0
}
=
^
E
37
Moreover, for all
e
0
∈ E
it holds that
V
E ≤ e
0
, since
e
0
∨
^
E
=
_
{e
0
∨ b
:
∀e
00
∈ E.b ≤ e
00
}
=
_
{e
0
}
=
e
0
Thus for all
e
0
∈ E
,
e ≤
V
E ≤ e
0
.
We can therefore conclude that
hE, ≤,
W
,
V
i
is a complete lattice.
To see that
V
is unique, let
~
: 2
E
→ E
also be a greatest lower bound operator.
Then for all
E ⊆ E
we can
derive that
~
E ≤
V
E ≤
~
E
, thus
~
E
=
V
E
by antisymmetry of
≤
, and therefore
~
=
V
.
Lemma 32.
Let
E
and
F
be cancellative c-semirings.
Then
E 
F
is indeed a c-semiring.
Proof.
We prove that
E 
F
is closed under the operators of Definition 7; all other properties can be verified
similar to Lemma 29.
Let
S ⊆ E 
F
.
Then we know that
Pr
1
(
S
)
⊆ C
(
E
)
∪ {
0
E
}
=
E
,
thus
W
E
Pr
1
(
S
)
∈ E
.
If,
however,
W
E
Pr
1
(
S
) =
0
E
, then
Pr
1
(
S
) is empty or
Pr
1
(
S
) =
{
0
E
}
.
In the former case, we know that
Pr
2
(
S
) is empty and
in the latter case, we know that
Pr
2
(
S
) =
{
0
F
}
(otherwise
S
would not be a subset of
E 
F
) — thus, in both
cases we know that
W
F
Pr
2
(
S
) =
0
F
.
We can therefore conclude that,
in the case where
W
E
Pr
1
(
S
) =
0
E
(or,
by symmetry,
W
F
Pr
2
(
S
) =
0
F
), it holds that
W
E
F
S ∈ E 
F
.
The case remains where
W
E
Pr
1
(
S
)
∈ C
(
E
) and
W
F
Pr
2
(
S
)
∈ C
(
F
).
But then
W
E
F
S ∈ E 
F
by definition of
W
E
F
.
For closure of
E 
F
under
⊗
E
F
,
we observe that if
e
1
, e
2
∈ C
(
E
),
then
e ⊗
E
e
0
∈ C
(
E
),
too.
To see
this,
take
e
3
, e
4
∈ C
(
E
) such that (
e
1
⊗
E
e
2
)
⊗
E
e
3
= (
e
1
⊗
E
e
2
)
⊗
E
e
4
;
by associativity of
⊗
E
,
we know that
e
1
⊗
E
(
e
2
⊗
E
e
3
) =
e
1
⊗
E
(
e
2
⊗
E
e
4
); by cancellativity of
e
1
it follows that
e
2
⊗
E
e
3
=
e
2
⊗
E
e
4
, and by cancellativity
of
e
2
we have that
e
3
=
e
4
.
By symmetry, a similar argument holds for
F
.
Thus, if
he, f i , he
0
, f
0
i ∈ E 
F
, then
if
he, f i
=
0
E
F
or if
he
0
, f
0
i
=
0
E
F
, we have that
he, f i ⊗
E
F
he
0
, f
0
i
=
0
E
F
∈ E 
F
.
If this is not the case,
then
e, e
0
∈ C
(
E
) and
f, f
0
∈ C
(
F
), thus
he, f i ⊗
E
F
he, f i
=
he ⊗
E
e
0
, f ⊗
F
f
0
i ∈ C
(
E
)
× C
(
F
)
⊆ E 
F
.
Lemma 33.
Let
E
and
F
be cancellative c-semirings.
Then
E 
F
is cancellative.
Proof.
Let
he
1
, f
1
i , he
2
, f
2
i , he
3
, f
3
i ∈ E 
F
be such that
he
1
, f
1
i ⊗
E
F
he
2
, f
2
i
=
he
1
, f
1
i ⊗
E
F
he
3
, f
3
i
,
with
he
1
, f
1
i 6
=
0
E
F
.
Then we know that
e
1
∈ C
(
E
) and
f
1
∈ C
(
F
).
Also,
by definition of
⊗
E
F
,
we have that
e
1
⊗
E
e
2
=
e
1
⊗
E
e
3
and
f
1
⊗
F
f
2
=
f
1
⊗
F
f
3
.
Since
e
1
∈ C
(
E
) and
f
1
∈ C
(
F
), we have that
e
2
=
e
3
and
f
2
=
f
3
,
thus
he
2
, f
2
i
=
he
3
, f
3
i
.
As a consequence,
he
1
, f
1
i
is cancellative for all
he
1
, f
1
i ∈ E 
F
with
he
1
, f
1
i 6
=
0
E
F
,
thus
E 
F
is cancellative.
B
Proofs for Subsection 7.3
B.1
Proofs for the generalized pointwise order
Lemma 34.
Let
E
be a c-semiring.
If
w, x, y, z ∈ E
∗
with
|w|
=
n
=
|y|
and
|x|
=
m
=
|z|
, then
w·x∨
E
n
+
m
y ·z
=
(
w ∨
E
n
y
)
·
(
x ∨
E
m
z
) and
w · x ∧
E
n
y · z
= (
w ∧
E
m
y
)
·
(
x ∧
E
m
z
).
Proof.
For the first equality, observe that the
p
-th position of
w · x ∨
E
n
+
m
y · z
is equal to the
p
-th position of
w ∨ x
when
p ≤ n
and to the (
p − n
)-th position of
y ∨ z
otherwise.
At any rate, it is equal to the
p
-th position
of (
w ∨
E
n
x
)
·
(
y ∨
E
m
z
).
The proof of the second equality is similar.
Lemma 35.
Let
E
be a c-semiring.
If
w, x, y, z ∈ E
∗
with
|w|
=
n
=
|y|
and
|x|
=
m
=
|z|
,
then when
w ≤
E
n
y
and
x ≤
E
m
z
it follows that
w · x ≤
E
n
+
m
y · z
.
Moreover, when
y ≤
E
n
w
and
z ≤
E
m
y
, it follows that
w · x ≤
E
n
+
m
y · z
.
Proof.
By application of
Lemma 34,
we have that
w · x ∨
E
n
+
m
y · z
= (
w ∨
E
n
y
)
·
(
x ∨
E
m
z
) =
y · z
,
thus
w · x ≤
E
n
+
m
y · z
.
The proof of the second claim is similar.
Lemma 19.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
If
w, x ∈ P
(
A
)
∗
such that
|w|
=
n
=
|x|
and
w ≤
E
n
x
, then
we have that
]
A
(
w
)
≤
E
n
+1
]
A
(
x
)
[
A
(
x
)
≤
E
n
+1
[
A
(
w
)
Proof.
We prove the first claim by induction on
|w|
.
If
|w|
=

the claim holds immediately.
Now let
w
=
y · e
and
x
=
z · f
with
w ≤ x
.
Then
y ≤
E
n−
1
z
and
e ≤ f
.
Assuming that the claim holds for
y
and
z
we derive
]
A
(
y · e
) =
]
A
(
y
)
· e ∨
E
n
+1
w · e · t
(Definition 26)
≤
E
n
+1
]
A
(
z
)
· f ∨
E
n
+1
z · f · t
(Induction hypothesis, Lemma 35)
38
=
]
A
(
z · f
)
(Definition 26)
The proof of the second claim is similar.
Lemma 36.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA and let
n ∈ N
.
If
n ∈ N
, then
]
A
(
t
n
) =
t
n
+1
=
[
A
(
t
n
).
Proof.
We prove the first equality by induction on
n
.
If
n
= 0, then the claim holds immediately by Definition 26.
Assume now that the claim holds for
n
and derive
]
A
(
t
n
+1
) =
]
A
(
t
n
)
· t ∨
E
n
t
n
+1
· t
=
t
n
+1
· t ∨
E
n
t
n
+1
· t
=
t
n
+2
The proof of the second equality is similar.
Lemma 37.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA and let
m, n ∈ N
.
Then
]
m
A
(
t
n
) =
t
m
+
n
=
[
m
A
(
t
n
).
Proof.
We prove the first equality by induction on
m
.
If
m
= 0, the equality holds immediately.
Assume now
the equality holds for
m
and derive
]
m
+1
A
(
t
n
) =
]
A
(
]
m
A
(
t
n
)) =
]
A
(
t
m
+
n
) =
t
m
+
n
+1
In which the second step follows from Lemma 36.
The proof of the second claim is similar.
Lemma 38.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
If
w, x ∈ P
(
A
)
∗
with
|w|
=
n
=
|x|
, then
]
A
(
w ∨
E
n
x
) =
]
A
(
w
)
∨
E
n
+1
]
A
(
x
)
[
A
(
w ∧
E
n
x
) =
[
A
(
w
)
∧
E
n
+1
[
A
(
x
)
Proof.
We prove the first claim by induction on
|w|
.
If
w
=

, then
]
A
(
 ∨
E
n

) =
]
A
(

) =
t
=
t ∨
E
n
+1
t
=
]
A
(

)
∨
E
n
+1
]
A
(

)
Now let
w
=
y · e
and
x
=
z · f
and assume that the claim holds for
x
and
z
.
Then
]
A
(
y · e ∨
E
n
z · f
) =
]
A
((
y ∨
E
n−
1
z
)
·
(
e ∨ f
))
(Lemma 34)
=
]
A
(
y ∨
E
n−
1
z
)
·
(
e ∨ f
)
∨
(
y ∨
E
n−
1
z
)
·
(
e ∨ f
)
· t
(Definition 26)
= (
]
A
(
y
)
∨
E
n−
1
]
A
(
z
))
·
(
e ∨ f
)
∨
E
n
+1
(
y ∨
E
n−
1
z
)
·
(
e ∨ f
)
· t
(Induction hypothesis)
=
]
A
(
y
)
· e ∨
E
n
+1
y · e · t ∨
E
n
+1
]
A
(
z
)
· f ∨
E
n
+1
y · f · t
(Lemma 34)
=
]
A
(
y · e
)
∨
E
n
+1
]
A
(
z · f
) =
]
A
(
w
)
∨
E
n
+1
]
A
(
x
)
(Definition 26)
The proof of the second claim is similar.
Lemma 39.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
If
w · e ∈ P
(
A
)
∗
, then for all
n ∈ N
it holds that
]
n
A
(
w
)
· e ≤
E
|w|
+
n
]
n
A
(
w · e
)
[
n
A
(
w · e
)
≤
E
|w|
+
n
[
n
A
(
w
)
· e
Proof.
We prove the first claim by induction on
n
.
If
n
= 0 then the claim holds vacuously.
For
n
= 1 we derive
]
A
(
w
)
· e ≤
E
|w|
+
n
]
A
(
w
)
· e ∨
E
|w|
+
n
w · e · t
=
]
A
(
w · e
)
Now assume that the claim holds for all
n
0
≤ n
and derive:
]
n
+1
A
(
w
)
· e
=
]
n
A
(
]
A
(
w
))
· e
≤
E
|w|
+
n
+1
]
n
A
(
]
A
(
w
)
· e
)
(Induction hypothesis)
≤
E
|w|
+
n
+1
]
n
+1
A
(
w · e
)
(Induction hypothesis, Lemma 19)
The proof of the second claim is similar.
Lemma 40.
Let
A
=
Q,
Σ
, E, →, q
0
, t
.
If
w ∈ P
(
A
)
∗
, then for all
n ∈ N
it holds that
]
n
A
(
w
)
· t ≤
E
n
+
|w|
+1
]
n
+1
A
(
w
)
[
n
+1
A
(
w
)
≤
E
n
+
|w|
+1
[
n
A
(
w
)
· t
39
Proof.
We prove the first claim by induction on
n
.
For the basis,
where
n
= 0,
we distinguish two cases.
If
w
=

, we find that
]
0
A
(

)
· t
=
 · t
=
t
=
]
A
(

).
If
w
=
x · e
, we simply derive
]
0
A
(
x · e
)
· t
=
x · e · t ≤
E
n
+
|w|
+1
]
A
(
x
)
· e ∨
E
n
+
|w|
+1
x · e · t
=
]
1
A
(
x · e
)
Now assume that the claim holds for
n
and all
w
and observe that
]
n
+1
A
(
w
)
· t
=
]
n
A
(
]
A
(
w
))
· t ≤
E
n
+
|w|
+1
]
n
+1
A
(
]
A
(
w
)) =
]
n
+2
A
(
w
)
In which we use the induction hypothesis and Lemma 19 in the second step.
The proof of the second claim is
similar.
Lemma 41.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be an SCA.
If
n ∈ N
such that
n ≥
1,
w ∈ P
(
A
)
∗
and
e ∈ P
(
A
), then
]
n
A
(
w · e
) =
]
n
A
(
w
)
· e ∨
E
n
+
|w|
]
n−
1
A
(
w · e
)
· t
[
n
A
(
w · e
) =
[
n
A
(
w
)
· e ∧
E
n
+
|w|
[
n−
1
A
(
w · e
)
· t
Proof.
Let
w · e ∈ P
(
A
)
∗
.
We prove first the claim by induction on
n
.
For the base case, where
n
= 1, the claim
is true by definition of
]
A
.
Now assume the claim is true for
n
and derive
]
n
+1
A
(
w · e
) =
]
A
(
]
n
A
(
w · e
))
=
]
A
(
]
n
A
(
w
)
· e ∨
E
n
+
|w|
]
n−
1
A
(
w · e
)
· t
)
(Induction hypothesis)
=
]
A
(
]
n
A
(
w
)
· e
)
∨
E
n
+
|w|
]
A
(
]
n−
1
A
(
w · e
)
· t
)
(Lemma 38)
=
]
A
(
]
n
A
(
w
))
· e ∨
E
n
+
|w|
+1
]
n
A
(
w
)
· e · t ∨
E
n
+
|w|
+1
]
A
(
]
n−
1
A
(
w · e
))
· t ∨
E
n
+
|w|
+1
]
n−
1
A
(
w · e
)
· t · t
(Definition 26)
=
]
n
+1
A
(
w
)
· e ∨
E
n
+
|w|
+1
]
n
A
(
w
)
· e · t ∨
E
n
+
|w|
+1
]
n
A
(
w · e
)
· t ∨
E
n
+
|w|
+1
]
n−
1
A
(
w · e
)
· t · t
=
]
n
+1
A
(
w
)
· e ∨
E
n
+
|w|
+1
]
n
A
(
w · e
)
· t ∨
E
n
+
|w|
+1
]
n−
1
A
(
w · e
)
· t · t
(Lemma 39)
=
]
n
+1
A
(
w
)
· e ∨
E
n
+
|w|
+1
]
n
A
(
w · e
)
· t
(Lemma 40)
The proof of the second claim is similar.
Lemma 42.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
If
w ∈ P
(
A
)
∗
and
n > |w|
, then
idle
A
(
w, n
) =
{y · e
:
w
=
x · e, y ∈
idle
A
(
x, n −
1)
} ∪ {x · t
:
x ∈
idle
A
(
w, n −
1)
}
Proof.
First, observe that since
w ∈ P
(
A
)
∗
and
A
is threshold-free, we know that
w
=
p
A
(
w
).
To show that
w
0
∈
idle
A
(
w, n
) it therefore suffices to show that
p
A
(
w
0
) =
w
and
|w
0
|
=
n
.
We begin by proving the inclusion left-to-right.
Let
w
0
∈
idle
A
(
w, n
).
If
w
0
=
x · t
, then
p
A
(
x
) =
p
A
(
w
0
) =
w
,
and since
|x|
=
n−
1, we have that
x ∈
idle
A
(
w, n−
1).
Therefore
w
0
∈ {x·t
:
x ∈
idle
A
(
w, n−
1)
}
.
The case remains
where
w
0
=
y · e
for
e ∈ P
(
A
).
Let
w
=
x · e
0
for
e
0
∈ p
A
(
E
), then
y · e
=
w
0
=
p
A
(
w
) =
p
A
(
x · e
0
) =
p
A
(
x
)
· p
A
(
e
0
),
and since
e
0
6
=
t
, it follows that
p
A
(
x
) =
y
; together with
|y|
=
n −
1 we conclude that
y ∈
idle
A
(
x, n −
1) and
thus
w
0
∈ {y · e
:
w
=
x · e, y ∈
idle
A
(
x, n −
1)
}
.
For the right-to-left inclusion, let
w
0
∈ {y ·e
:
w
=
x·e, y ∈
idle
A
(
x, n−
1)
}∪{x·t
:
x ∈
idle
A
(
w, n−
1)
}
.
If
w
0
=
x · t
for
x ∈
idle
A
(
w, n −
1), then
p
A
(
w
0
) =
p
A
(
x · t
) =
p
A
(
x
) =
w
.
With
|w
0
|
=
n
it follows that
w
0
∈
idle
A
(
w, n
).
If on the other hand
w
0
=
y · e
for
w
=
x · e
and
y ∈
idle
A
(
x, n −
1), then
p
A
(
w
0
) =
p
A
(
y
)
· p
A
(
e
) =
x · e
=
w
;
together with
|w
0
|
=
n
it follows that
w
0
∈
idle
A
(
w, n
).
Lemma 20.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
Then for
w ∈ E
∗
and
n ∈ N
with
|w| ≤ n
, the
following equalities hold:
_
idle
A
(
w, n
) =
]
n−|w|
A
(
w
)
^
idle
A
(
w, n
) =
[
n−|w|
A
(
w
)
Proof.
We prove the first equality by induction on (
|w|, n
), ordered lexicographically.
For this, our base cases
include those where
n
=
|w|
and where
|w|
= 0.
For the case where
|w|
= 0, observe that
idle
A
(
, n
) =
{t
n
}
, and
conclude that
idle
A
(
, n
) =
]
n
A
(

) by Lemma 37.
For the cases where
n
=
|w|
, first observe that
idle
A
(
w, |w|
) =
{w}
.
Now we can derive that
_
idle
A
(
w, |w|
) =
w
=
]
0
A
(
w
)
Assume now that the claim holds for
n
0
and
w
0
with (
|w
0
|, n
0
) lexicographically smaller than (
|w|, n
), and derive:
_
E
n
idle
A
(
z · e, n
) =
_
E
n
(
{y · e
:
y ∈
idle
A
(
z, n −
1)
} ∪ {x · t
:
x ∈
idle
A
(
z · e, n −
1)
}
)
(Lemma 42)
40
=
_
E
n
{y · e
:
y ∈
idle
A
(
z, n −
1)
} ∨
E
n
_
E
n
{x · t
:
x ∈
idle
A
(
z · e, n −
1)
}
(Flattening)
=

_
E
n−
1
idle
A
(
z, n −
1)
· e

∨
E
n

_
E
n−
1
idle
A
(
z · e, n −
1)
· t

(Lemma 34)
=
]
n−|w|
A
(
z
)
· e ∨
E
n
]
n−|w|−
1
A
(
z · e
)
· t
(Induction hypothesis)
=
]
n−|w|
A
(
z · e
)
(Lemma 41)
=
]
n−|w|
A
(
w
)
The proof of the second claim is similar.
B.2
Proofs for the generalized lexicographic order
Lemma 21.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA and let
n ∈ N
.
The relation
v
n
A
is reflexive.
Proof.
This follows immediately from the definition.
Let
w ∈ P
(
A
)
∗
such that
|w|
≤
E
∗
n
.
Since for any
w
0
∈
idle
A
(
w, n
), we immediately have that
w
0
≤
E
∗
w
0
, and thus
w
0
v
n
A
w
0
.
Lemma 22.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA, and let
n ∈ N
.
If
w, x ∈ P
(
A
)
∗
are such that
n
= max(
|w|, |x|
) and
w v
n
A
x v
n
A
w
, then
w
=
x
.
Proof.
If
n
=
|x|
, then
idle
A
(
x, n
) =
{x}
.
Thus we know that for every
w
0
∈
idle
A
(
w, n
) it holds that
w
0
≤
E
∗
x
.
Moreover, we know that there exists a
w
00
∈
idle
A
(
w, n
) such that
x ≤
E
∗
w
00
.
Since the former assertion also
holds for
w
00
, we know that
w
00
≤
E
∗
x ≤
E
∗
w
00
and therefore
w
00
=
x
by antisymmetry of
≤
E
∗
.
But then
w
00
does
not contain any occurrence of
t
(since
x
does not), so
|w|
=
n
and thus
w
00
=
w
, allowing us to conclude that
w
=
x
.
For the case where
n
=
|w|
a similar argument holds.
Here, we know that
idle
A
(
w, n
) =
{w}
, and there exists
a
x
0
∈
idle
A
(
x, n
) such that
w ≤
E
∗
x
0
.
Moreover, for all
x
00
∈
idle
A
(
x, n
) it holds that
x
00
≤
E
∗
w
.
Since the latter
assertion holds for
x
0
as well, we have that
w ≤
E
∗
x
0
≤
E
∗
w
.
Therefore,
w
=
x
0
and it follows that
x
0
does not
contain any occurrence of
i
, thus
x
0
=
x
, leading to a conclusion that
w
=
x
.
Lemma 23.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA, and let
n ∈ N
.
The relation
v
n
A
is transitive.
Proof.
Let
w, x, y ∈ P
(
A
)
∗
such that
|w|, |x|, |y| ≤
E
∗
n
and
w v
n
A
x v
n
A
y
.
We need to show that
w v
n
A
y
.
Let
w
0
∈
idle
A
(
w, n
).
Then by
w v
n
A
x
there exists an
x
0
∈
idle
A
(
x, n
) such that
w
0
≤
E
∗
x
0
.
For this particular
x
0
, by
x v
n
A
y
we can find a
y
0
∈
idle
A
(
y, n
) such that
x
0
≤
E
∗
y
0
.
Therefore,
w
0
≤
E
∗
y
0
by transitivity of
≤
E
∗
.
Since
such a
y
0
can be found for any
x
0
∈
idle
A
(
w, n
), we have established that
w v
n
A
y
.
Lemma 24.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
If
w, x ∈ P
(
A
)
∗
and
n ∈ N
such that
|w|, |x| ≤ n
, then
w v
n
A
x
if and only if
w v
n
+1
A
x
.
Proof.
For the claim from left to right, let
w
2
∈
idle
A
(
w, n
+ 1).
We need to find an
x
2
∈
idle
A
(
x, n
+ 1) such
that
w
2
≤
E
∗
x
2
.
Choose a
k
such that the
k
-th position of
w
2
is
t
.
Such a
k
always exists, since
n
+ 1
> |w|
.
Remove this position from
k
to obtain
w
1
∈
idle
A
(
w, n
).
By the premise, there exists an
x
1
∈
idle
A
(
x, n
) such
that
w
1
≤
E
∗
x
1
.
Insert a
t
at the
k
-th position of
x
1
to obtain
x
2
∈
idle
A
(
x, n
+ 1).
Then
w
2
≤
E
∗
x
2
, because we
inserted the same character into the same position of
w
1
and
x
1
to obtain
w
2
respectively
x
2
.
We conclude that
w v
n
+1
A
x
.
For the claim from right to left, let
w
1
∈
idle
A
(
w, n
).
We need to find an
x
1
∈
idle
A
(
x, n
) such that
w
1
≤
E
∗
x
1
.
Choose
w
2
=
t · w
1
.
Then
w
2
∈
idle
A
(
w, n
+ 1), thus by the premise there exists a
x
2
∈
idle
A
(
x, n
+ 1) such that
w
2
≤
E
∗
x
2
.
The remainder of the proof is a case analysis.
For the first and easiest case, assume that
x
2
starts with
t
.
Write
x
2
=
t · x
1
.
Since
t · w
1
≤
E
∗
t · x
1
, it follows
that
w
1
≤
E
∗
x
1
.
Moreover,
x
1
∈
idle
A
(
x, n
), since
p
A
(
x
1
) =
h
(
t · x
1
) =
p
A
(
x
2
) =
x
.
If,
on the other hand,
x
2
does not start with
t
,
then we immediately know that
x 6
=

.
Write
e
for the
first character of
x
and choose
x
1
=
x · t
n−|x|
.
First, observe that
x
1
∈
idle
A
(
x, n
).
We also know that
t < e
,
since
t · w
1
≤
E
∗
x
2
and
x
2
does not start with
t
(and must therefore start with
e 6
=
t
).
It remains to show that
w
1
≤
E
∗
x
1
.
We can ignore the case where
w
1
starts with
t
, for then have
w
1
≤
E
∗
x
1
and are done immediately.
Let us restate our premises in the final remaining case.
We know that neither
w
1
nor
x
1
starts with
t
(and
therefore both
w
and
x
are non-empty), and that
t ≤
E
e
, where
e
is the first character of
x
(and therefore of
x
1
).
Consider
w
3
=
w
1
· t
.
Also by our premise, there exists an
x
3
∈
idle
A
(
x, n
+ 1) such that
w
3
≤
E
∗
x
3
.
If
x
3
does
not start with
t
, then the first character of
w
1
precedes
e
and we have
w
1
≤
E
∗
x
1
.
If
x
3
does start with
t
, then
the first character of
w
1
precedes
t
in
≤
E
, which in turn precedes
e
and we have
w
1
≤
E
∗
x
1
again.
41
C
Proofs for Subsection 7.4
In this appendix,
we work towards a proof of Theorem 3 and Theorem 4.
The proof for these theorems is
somewhat involved,
and requires that we develop some auxiliary lemma’s on
≤
E
∗
and
v
A
for c-semirings
E
and threshold-free automata
A
.
To abbreviate notation, we fix the symbol
E
for any c-semiring, and
A
for any
threshold-free SCA with underlying c-semiring
E
.
C.1
Lemma’s for the lexicographic order
We begin by proving some helpful lemma’s on the lexicographic order
Lemma 43.
Let
w, x ∈ E
∗
such that
w
is a prefix of
x
.
Then
w ≤
E
∗
x
.
Proof.
Write
x
=
w · y
.
Then
 ≤
E
∗
y
, and consequently
w ≤
E
∗
w · y
=
x
.
Lemma 44.
Let
w, x ∈ E
∗
.
If
w <
E
∗
x
and
w
is not a prefix of
x
, then
x
is not a prefix of
w
Proof.
Suppose towards a contradiction that
x
is a prefix of
w
.
Then
x ≤
E
∗
w
by Lemma 43.
If
x
=
w
then
w <
E
∗
w
; if
x <
E
∗
w
then
x <
E
∗
w <
E
∗
x
; both are contradictions.
Lemma 45.
Let
w, x, y ∈ E
∗
.
If
w <
E
∗
x
and
w
is not a prefix of
x
, then
w · y <
E
∗
x
and
w <
E
∗
x · y
.
Proof.
If
w
is not a prefix of
x
, then
w · y 6
=
x
.
By Lemma 44, we have that
x
is not a prefix of
w
, and thus that
w 6
=
x · y
.
It remains to be shown that
w · y ≤
E
∗
x
and
w ≤
E
∗
x · y
.
Let
u
be the largest common prefix of
w
and
x
.
Write
w
=
u · w
0
and
x
=
u · x
0
; we now know that
w
0
≤
E
∗
x
0
.
Because
w
is not a prefix of
x
,
w
0
cannot be empty.
Likewise,
x
0
cannot be empty because
x
is not a prefix of
w
.
We now show that
w · y ≤
E
∗
x
.
Write
w
0
=
e · w
00
and
x
0
=
f · x
00
.
Since
u
is the largest common prefix
of
w
and
x
,
it follows that
e 6
=
f
and thus we know that
e <
E
f
.
But then
e · w
00
· y ≤
E
∗
f · x
00
and thus
w · y
=
u · w
0
· y
=
u · e · w
00
· y ≤
E
∗
u · f · x
00
=
u · x
0
=
x
.
The proof that
w ≤
E
∗
x · y
is similar.
Lemma 46.
Let
w, x, y, z ∈ E
∗
such that
w <
E
∗
x
and
w
is not a prefix of
x
.
Then
w · y <
E
∗
x · z
.
Proof.
If
w <
E
∗
x
and
w
is not a prefix of
x
, then
w · y <
E
∗
x
by Lemma 45.
But then
w · y
is also not a prefix
of
x
(otherwise
w
would be too); again by Lemma 45 we conclude that
w · y <
E
∗
x · z
.
Lemma 47.
Let
w, x, y ∈ E
∗
such that
w ≤
E
∗
x
and
|w|
=
|x|
, then
w · y ≤
E
∗
x · y
.
Proof.
If
w
=
x
, then
w · y ≤
E
∗
x · y
by reflexivity of
≤
E
∗
.
Otherwise we know that
w <
E
∗
x
.
Since
|w|
=
|x|
but
w 6
=
x
, we know that
w
is not a prefix of
x
; then
w · y ≤
E
∗
x · y
follows by Lemma 46.
Lemma 48.
Let
w, x, y, z ∈ E
∗
such that
|w|
=
|x|
.
If
w · y ≤
E
∗
x · z
,
then either
w
=
x
and
y ≤
E
∗
z
,
or
w <
E
∗
x
.
Proof.
We prove the claim by induction on
|w|
.
If
|w|
= 0, then
w
=

=
x
and
y ≤
E
∗
z
immediately.
If
|w| >
0,
let
w
=
e · w
0
and
x
=
f · x
0
and assume that
w
0
· y ≤
E
∗
x
0
· z
implies
w
0
=
x
0
and
y ≤
E
∗
z
or
w
0
<
E
∗
x
0
.
If
e
=
f
,
then
w
0
· y ≤
E
∗
x
0
· z
by definition of
≤
E
∗
,
and thus (making use of the induction hypothesis) either
w
=
e · w
0
=
f · x
0
=
x
and
y ≤
E
∗
z
,
or
w
=
e · w
0
<
E
∗
f · x
0
=
x
.
If
on the other hand
e <
E
f
,
then
w
=
e · w
0
≤
E
∗
f · x
0
=
x
by definition of
≤
E
∗
, and
w 6
=
x
, thus
w <
E
∗
x
.
Lemma 49.
Let
w, x, y, z ∈ E
∗
.
If
x ≤
E
∗
y
and
|x|
=
|y|
, then
w · x · z ≤
E
∗
w · y · z
.
Proof.
If
x
=
y
, then
w · x · z
=
w · y · z
and thus the claim holds immediately.
If
x <
E
∗
y
, then
x
is not a prefix
of
y
(since
x 6
=
y
and
|x|
=
|y|
), thus
x · z <
E
∗
y · z
by Lemma 46.
By definition of
≤
E
∗
we can conclude that
w · x · z ≤
E
∗
w · y · z
.
C.2
Lemma’s for the generalized lexicographic order
Lemma 50.
Let
w, x ∈ P
(
A
)
∗
such that
|w|
+
|x| ≤ p
.
Then
idle
A
(
w · x, p
) =
[
|w|≤k≤p−|x|
idle
A
(
w, k
)
·
idle
A
(
x, p − k
)
Proof.
We start with the inclusion from left to right.
Let
t ∈
idle
A
(
w · x, p
).
Since
p
A
(
t
) =
w · x
,
we can
write
t
=
u · v
such that
p
A
(
u
) =
w
and
p
A
(
v
) =
x
.
Therefore
u ∈
idle
A
(
w, |u|
) and
v ∈
idle
A
(
x, p − |u|
)
and thus
t ∈
idle
A
(
w, |u|
)
·
idle
A
(
x, p − |u|
).
Since
|w|
=
|p
A
(
u
)
|
≤ |u|
=
p − |v|
≤ p − |x|
,
we know that
t ∈
S
|w|≤k≤p−|x|
idle
A
(
w, k
)
·
idle
A
(
x, p − k
).
Now,
for the inclusion from right to left,
let
t ∈
idle
A
(
w, k
)
·
idle
A
(
x, p − k
) for some
|w|
≤ k ≤ p − |x|
.
Then
t
=
u · v
with
u ∈
idle
A
(
w, k
) and
v ∈
idle
A
(
x, p − k
).
Thus
p
A
(
t
)
=
p
A
(
u
)
· p
A
(
v
)
=
w · x
and
|t|
=
|u|
+
|v|
=
k
+
p − k
=
p
; we conclude that
t ∈
idle
A
(
w · x, p
).
42
Lemma 51.
Let
w, x, y ∈ P
(
A
)
∗
such that
w
is effective.
Then
w · x v
A
w · y
if and only if
x v y
.
Proof.
We start by proving the claim from left to right.
If
|x| ≤ |y|
, we know that for all
u ∈
idle
A
(
w · x, |w · y|
) we
have
u ≤ w · y
.
Let
x
0
∈
idle
A
(
x, |y|
).
Then we know that
w · x
0
∈
idle
A
(
w · x, |x|
+
|y|
) by Lemma 50.
Therefore
w · x
0
≤ w · y
, and thus
x
0
≤ y
by Lemma 48.
We conclude that
x v
A
y
.
We argue the case where
|x| > |y|
by induction on
w
.
If
w
=

, then the claim holds trivially.
Assume now
that
w
=
e · w
0
and that the claim holds for
w
0
.
By our premise, we know that
w · x
=
e · w
0
· x ≤ u
for some
u ∈
idle
A
(
w · y, |w|
+
|y|
).
Write
u
=
f · u
0
.
Observe that, since
p
A
(
u
) =
p
A
(
f · u
0
) =
x · z
, we know that
f
must
be either
e
or
i
.
Because we know that
e > t
, it follows that
f
=
e
(otherwise
u
=
i · u
0
< e · w
0
· x
=
w · x
).
By
Lemma 48, we know that
w
0
· x ≤ u
0
.
Since
u
0
∈
idle
A
(
w
0
· y, |w
0
|
+
|x|
), we have that
w
0
· x v
A
w
0
· y
from which
x v y
follows by the induction hypothesis.
We now prove the claim from right to left.
Let
p
=
max
(
|x|, |y|
) and
t ∈
idle
A
(
w · x, |w|
+
p
).
If
w
is a prefix
of
t
, then write
t
=
w · x
0
.
We know that
x
0
∈
idle
A
(
x, p
), and thus that there exists a
y
0
∈
idle
A
(
y, p
) such that
x
0
≤ y
0
.
But then
t
=
w · x
0
≤ w · y
0
∈
idle
A
(
w · y, |w|
+
p
).
If
w
is not a prefix of
t
, then let
z
be the longest
common prefix of
w
and
t
.
Write
w
=
z · w
0
.
By construction,
w
0
6
=

and
z · t
is a prefix of
t
; write
t
=
z · t · u
to
reflect this.
Since
w
is effective, so is
w
0
, from which we know that
i < w
0
, thus
z · t < z · w
0
=
w
by definition of
≤
.
Choose any
y
0
∈
idle
A
(
y, p
).
By Lemma 46, it follows that
t
=
z · t · u ≤ w · y
0
∈
idle
A
(
w · y, |w|
+
p
).
We can
conclude that
w · x v
A
w · y
.
Lemma 52.
Let
w ∈ P
(
A
)
∗
be effective.
Then
 v
A
w
.
Proof.
If
w
=

we are done immediately by reflexivity of
v
A
.
Assume now that
w
=
e · w
0
.
Let
u ∈
idle
A
(
, |w|
),
then
u
=
i
|w|
.
Since
i <
E
e
, it follows immediately that
u
=
i · t
|w
0
|
≤ e · w
0
=
w
.
Thus
 v w
.
Lemma 53.
Let
w ∈ P
(
A
)
∗
be effective.
If
x
is a prefix of
w
, then
x v
A
w
.
Proof.
Let
w
=
x · w
0
.
By Lemma 52, we know that
 v
A
w
0
, and by Lemma 51, we conclude
x v x · w
0
=
w
.
Lemma 54.
Let
w, x, y ∈ P
(
A
)
∗
such that
w v
A
x
.
If
|w| ≥ |x|
and
w 6
=
x
, then
w · y v x
.
Proof.
We know that there exists some
x
0
∈
idle
A
(
x, |w|
) such that
w < x
0
(equality is ruled out by the premises).
Then
w · y < x
0
· t
|y|
by Lemma 46.
Since
x
0
· t
|y|
∈
idle
A
(
x, |w · y|
), we then know that
w · y v
A
x
.
Lemma 55.
Let
w, x, y ∈ P
(
A
)
∗
such that
w v
A
x
.
If
|w| > |x|
, then
w · y v x · y
.
Proof.
We prove the claim by induction on
y
.
If
y
=

then the claim holds immediately.
Assume now that
y
=
e · y
0
and that the claim holds for
y
0
.
We know that there exists an
x
0
∈
idle
A
(
x, |w|
) such that
w ≤ x
0
.
Choose
x
00
=
x
0
· e
.
Then
x
00
∈
idle
A
(
x · e, |w|
+ 1), and by Lemma 47, we know that
w · e ≤ x
0
· e
.
Therefore
w · e v
A
x · e
.
Since
|x|
+ 1
< |w|
+ 1,
we immediately have that
w · y
=
w · e · y
0
v x · e · y
0
=
x · y
by our
induction hypothesis.
Lemma 56.
Let
w, x, y ∈ P
(
A
)
∗
such that
w v
A
x
,
|w| ≤ |x|
, but
w
is not a prefix of
x
.
Then
w · y v x · y
.
Proof.
Let
t ∈
idle
A
(
w · y, |x · y|
).
We need to show that
t ≤ x · y
.
By Lemma 50, we know that
t
=
u · v
for
u ∈
idle
A
(
w, k
) and
v ∈
idle
A
(
y, |x·y|−k
), with
|w| ≤ k ≤ |x|
.
Then choose
w
0
=
u·t
|x|−k
.
Since
w
0
∈
idle
A
(
w, |x|
)
and
w v
A
x
, we know that
w
0
≤ x
.
Since
w
is not a prefix of
x
, neither is
u
.
Write
x
=
r · s
such that
|r|
=
|u|
.
Since
u · t
|x|−k
≤ r · s
and
u 6
=
r
,
it follows by Lemma 48 that
u < r
.
Consequently,
by Lemma 46,
we have
t
=
u · v ≤ r · s · y
=
x · y
and thus
w · y v x · y
.
Lemma 57.
Let
w, x, y ∈ P
(
A
)
∗
such that
w v
A
x
.
If
w
is not a prefix of
x
, then
w · y v x · y
.
Proof.
Either
|w| > |x|
or
|w| ≤ |x|
.
In the former case the claim follows from Lemma 55, while in the latter
case the claim follows from Lemma 56
Lemma 58.
Let
w, x ∈ P
(
A
)
∗
such that
w
and
x
are effective.
Then
w ≤ x
if and only if
w v
A
x
.
Proof.
We isolate a few special
cases first.
If
w
is a prefix of
x
,
then
w ≤ x
by Lemma 43 and
w v
A
x
by
Lemma 53, and so the claim holds immediately.
If on the other hand
x
is a strict prefix of
w
, then write
w
=
x · z
.
Assume towards a contradiction that
w ≤ x
.
Then
x · z ≤ x
implies that
z ≤ 
by Lemma 48.
But then
z
=

by antisymmetry,
which is a contradiction
because
x
is a strict prefix of
w
.
We thus know that
w 6
≤ x
.
Similarly, assume that
x · z
=
w v
A
x
.
Then, by
Lemma 51, we know that
z v 
and (since
 v z
by Lemma 52) that
z
=

, again contradicting that
x
is a strict
prefix of
w
.
We therefore know that
w 6
v x
.
Since both
w 6
≤ x
and
w 6v x
when
x
is a strict prefix of
w
, the
claim holds in this case too.
Assume for the remainder that
w
is not a prefix of
x
and
x
is not a prefix of
w
.
Let
z
be the longest common
prefix of
w
and
x
.
Write
w
=
z · w
0
and
x
=
z · x
0
.
Observe that neither
w
0
nor
x
0
is empty, for otherwise
w
43
(respectively
x
) would be a prefix of
x
(respectively
w
).
Write
e
for the first position of
w
0
and
f
for the first
position of
x
0
and note that
e 6
=
f
.
Choose
p
= max(
|w
0
|, |x
0
|
).
For the direction from left to right, assume that
w ≤ x
.
Then
w
0
≤ x
0
by Lemma 48; since
e 6
=
f
we can
also derive that
e < f
by Lemma 48.
Let
w
00
∈
idle
A
(
w
0
, p
).
If
w
00
starts with
i
, then
p
=
|x
0
|
and we know that
w
00
< x
0
∈
idle
A
(
x
0
, p
).
If
w
00
does not start with
i
, it starts with
e
.
But then
w
00
< x
0
· t
p−|x|
∈
idle
A
(
x
0
, p
).
At
any rate,
w
0
v
A
x
0
.
But then, by Lemma 51, we can derive that
w
=
z · w
0
v z · x
0
=
x
.
For the direction from right to left, let
w v
A
x
.
Then
z · w
0
v z · x
0
, so by Lemma 51 we see that
w
0
v x
0
.
If
p
=
|x
0
|
, then
w
0
· t
|w
0
|−|x
0
|
≤ x
0
.
If
p
=
|w
0
|
, then
w
0
≤ x
00
for some
x
00
∈
idle
A
(
x
0
, p
).
Note that
x
00
cannot start
with
i
, because then
w
0
> x
00
since
w
0
is effective; thus
x
00
starts with
f
.
In either case, we can derive from
e 6
=
f
and Lemma 48 that
e < f
, and thus
w
0
< x
0
, from which we have that
w
=
z · w
0
≤ z · x
0
=
x
.
Lemma 59.
Let
x, y, z ∈ P
(
A
)
∗
such that
x ≤ y · x
and
y
is effective.
Then
x v
A
y · x
Proof.
First, observe that the claim holds for
y
=

.
We prove the claim for
y
=
f · y
0
by induction on
x
.
For the base case, where
x
=

, the claim holds by virtue of Lemma 52.
Now write
x
=
e · x
0
and assume the
claim holds for
x
0
.
If
e · x
0
≤ f · y
0
· e · x
0
, then either
e < f
, or
e
=
f
and
x
0
≤ y
0
· e · x
0
by Lemma 48.
If
e < f
,
then let
z ∈
idle
A
(
x, |y · x|
) and write
z
=
g · z
0
.
Whether
g
=
i
or
g
=
e
,
we can derive that
z
=
g · z < f · y
0
· x
, allowing us to derive that
x v
A
y · x
in this case.
If
e
=
f
, then
y
0
· e
is effective, and so by
the induction hypothesis we know that
x
0
v y
0
· e · x
0
.
But then by Lemma 51,
x
=
e · x
0
v f · y
0
· e · x
0
=
y · x
.
Lemma 60.
Let
w, x, y ∈ P
(
A
)
∗
such that
w
and
x
are effective and
w · y ≤ x · y
.
Then
w · y v
A
x · y
.
Proof.
If
|w|
=
|x|
then the claim holds immediately.
If
|w| > |x|
, then
w · y
is not a prefix of
x · y
.
Then by
Lemma 45 we know that
w · y ≤ x · y · t
|w|−|x|
.
The latter sequence is contained in
idle
A
(
x · y, |w · y|
), and thus
we have that
w · y v
A
x · y
.
If
|w| < |x|
, write
x
=
t · u
such that
|t|
=
|w|
.
Now, since
w · y ≤ t · u · y
, by Lemma 48 we have either
w
=
t
and
y ≤ u · y
, or
w < t
.
In the former case,
u
is effective and therefore by Lemma 59 it follows that
y v
A
u · y
,
from which we have
w · y v t · u · y
=
x · y
by Lemma 51, since
w
=
t
is effective.
In the case where
w < t
, we know that
w
is not a prefix of
t
and thus that
w < t · u
=
x
by Lemma 45.
Note
that since
|w|
=
|t|
,
w
is also not a prefix of
x
.
Since
w
and
x
are effective, we know by Lemma 58 that
w v
A
x
.
Then, by Lemma 57 we have that
w · y v x · y
.
Lemma 61.
Let
w ∈ P
(
A
)
∗
;
w
is non-effective if and only if
w
=
e · w
0
such that
i 6
≤ e
or
w
0
is non-effective.
Proof.
If
w
is non-effective, then
w 6
=

.
Write
w
=
e · w
0
.
If
i < e
and
w
0
is effective, then
w
is effective, too.
Thus either
i 6
≤ e
or
w
0
is non-effective.
Lemma 62.
Let
w
be non-effective and let
x
be a non-effective proper prefix of
w
.
Then
x 6v
A
w
Proof.
Write
w
=
x · t
.
Let
z
be the longest effective prefix of
x
;
write
x
=
z · u
.
Observe that
u
must be
non-empty (otherwise
x
=
z
would be effective) and that if the first position of
u
is
e
, then
i 6≤ e
(otherwise
z
could be longer).
Now choose
x
0
=
z · t
|w|−|x|
· u ∈
idle
A
(
x, |w|
).
Because
|w| > |x|
, we know that
i
|w|−|x|
· u 6
≤ u · t
and thus we can derive
x
0
=
z · t
|w|−|x|
· u 6≤ z · u · t
=
w
.
It follows that
x 6v
A
w
.
Lemma 63.
Let
w, x, y ∈ P
(
A
)
∗
such that
w
is non-effective and
|x| < |y|
.
Then
w · x 6
v
A
w · y
.
Proof.
Note that we can disregard the case for
w
=

, for

is effective.
For the remainder, it suffices to prove
that
z 6≤ w · y
for some
z ∈
idle
A
(
w · x, |w|
+
|y|
).
Let
w
=
z · w
0
such that
z
is the longest effective prefix of
w
.
Since
w
is non-effective,
w
0
is non-effective.
Then
w
0
=
e · w
00
with
i 6
≤ e
(otherwise
z
could be longer).
Assume towards a contradiction that
w
0
· x 6v
A
w
0
· y
.
Then in particular
i
|y
0
|−|x
0
|
· w
0
· x ≤ w
0
· y
.
By Lemma 48 and the fact that
|y
0
|
> |x
0
|
,
this would imply
that
i ≤ e
,
which is a contradiction.
We thus conclude that
w
0
· x 6
v w
0
· y
.
Consequently,
we know that
w · x
=
z · w
0
· x 6
v z · w
0
· y
=
w · y
by Lemma 51.
Lemma 64.
Let
w, x, y ∈ P
(
A
)
∗
such that
w·x ≤ x·w
,
y v
A
x
and
|y| ≤ |x|
.
Furthermore, let
w
be non-effective
and let
x
be an effective prefix of of
w
.
Then
w · y v x · w
.
Proof.
As a special case, note that when
x
is empty,
y
must also be empty and then the claim holds immediately.
We assume that
x 6
=

for the remainder of this proof.
Let
z ∈
idle
A
(
w · y, |x · w|
).
Since
|w · y| ≤ |x · w|
, our
objective is to show that
z ≤ x · w
.
If
w
is a prefix of
z
, then by Lemma 50
z
=
w · y
0
for some
y
0
∈
idle
A
(
y, |x|
).
Since
y v
A
x
we know that
y
0
≤ x
and therefore
z
=
w · y
0
≤ w · x ≤ x · w
.
If
w
is not a prefix of
z
, then we can write
w
=
t · t · u
such that
t
is a proper prefix of
w
; consequently, write
w
=
t · v
.
We also factor
w
=
p · e · q
such that
p
is the longest effective prefix of
w
— this is possible because
w
44
is non-effective.
Note that
e
is non-effective and, since
|x| ≥
1 and
x
is effective, we know that the first
|p|
+ 1
positions of
x · w
are effective.
Now that
t
and
p
are both prefixes of
w
, one must be a prefix of the other.
In cases where
t
is a proper prefix of
p
, we know that
t < p
by Lemma 43.
Since
p
is effective, we can derive
that
t · t < p
.
Now
t · t
cannot be a prefix of
p
, and so we can derive by Lemma 46 that
z
=
t · t · u < p · e · q · x
=
w ·x
.
When
p
is a prefix of
t
, write
x · w
=
r · s
such that
|r|
=
|p|
+ 1; note that
r
is effective and therefore not a
prefix of
p · e
.
Assume towards a contradiction that
r < p · e
.
Then by Lemma 46,
x · w
=
r · s < p · e · q · x
=
w · x
,
contradicting our assumption — thus
p · e < r
.
If
t
=
p
, then
p · t < r
since
r
is effective; it follows by Lemma 46
that
z
=
t · t · u
=
p · t · u < r · s
=
x · w
.
If
p
is a proper prefix of
t
, then
p · e
is a prefix of
t
(since both are prefixes
of
w
), and so
t < r
by Lemma 45, from which we derive (again by Lemma 46) that
z
=
t · t · u < r · s
=
x · w
.
Lemma 65.
Let
w, x ∈ P
(
A
)
∗
such that
w
is non-empty and effective.
If
w · x
=
x · w
, then
x
is effective.
Proof.
If
|w| ≥ |x|
, then
w · x
=
x · w
implies that the first
|w|
positions of
x · w
are effective; this includes the
positions of
x
and therefore
x
is effective.
We prove the case for
|w| < |x|
by induction on
x
.
For the base case, we have
x
=
e · f
.
Since
|w| < |x|
and
w
is non-empty, it follows that
w
=
g
for
g > t
.
Since
g · e · f
=
w · x
=
x · w
=
e · f · g
, we know that
g
=
f
and
g
=
e
, making
e · f
=
x
effective.
For the inductive step, write
x
=
y · z
such that
|y|
=
|z|
and assume the claim
holds for
z
.
Then
w · y · z
=
w · x
=
x · w
=
y · z · w
.
From this, we learn that
y · z
=
z · w
and
w
=
y
; therefore
w · z
=
z · w
.
If
|w| ≥ |z|
then we know that
z
is effective by the reasoning above, otherwise it follows from the
induction hypothesis.
Since
w
=
y
,
y
is also effective, making
x
=
y · z
effective as well.
C.3
Proofs of Theorem 3 and Theorem 4
Theorem 3.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
Let
L ⊆ P
(
A
)
∗
be non-empty and finite, and
let
≤
E
∗
be total.
Let
w
=
max
v
A
(
L
) and choose the longest
x ∈
prefix
(
w
)
∩ L
such that
x · w
is the
≤
E
∗
-maximum
of (
prefix
(
w
)
∩ L
)
· w
.
15
If
w
is effective or
x
is a proper prefix of
w
, then max
v
A
(
L
n
) =
x
n−
1
· w
for
n ≥
1.
Proof.
Since
x
is a prefix of
w
, we can write
w
=
x · y
.
It will now suffice to prove that
max
v
A
(
L
n
) =
x
n
· y
.
We
prove the claim by induction on
n
.
For the base case, where
n
= 1, observe that max
v
A
(
L
1
) =
w
=
x
1
· y
.
Assume that the claim holds for
n
; we need to prove it for
n
+ 1.
Write
max
v
A
(
L
n
+1
) =
t · u
such that
t ∈ L
and
u ∈ L
n
.
The inductive step is divided into three parts.
First, we show that
t
must be an effective prefix of
w
.
Then, we prove that
u
=
x
n
· y
.
Finally, we argue that
t
=
x
.
Now assume towards a contradiction that
t
is not a prefix of
w
.
First, note that
t v
A
w
since
t ∈ L
.
Then
from
t v
A
w
and Lemma 57 we have that
t · u v
A
w · u
.
But then, since
w · u v
A
max
(
L
n
+1
) =
t · u
, we have
that
t · u
=
w · u
, which is a contradiction because it implies that
t
=
w
, making
t
a prefix of
w
.
If
w
is effective, then
t
is effective, too.
If
w
is non-effective, we need to rule out only the possibility that
t
=
w
; if
t
is a proper prefix of
w
, then
t
is effective by Lemma 62.
Note that, in this case, also by Lemma 62,
we know that
x
must be effective (otherwise
x 6v
A
w
).
Assume for the remainder of this paragraph that
t
=
w
.
Write
u
=
r · s
such that
r ∈ L
and
s ∈ L
n−
1
— this is possible since
n ≥
1.
Because
w · x · s v
A
w · r · s
, by
Lemma 63 it follows that
|r| ≤
E
∗
|x|
.
From this, we can also derive that
r v
A
x
:
if
r
is a prefix of
x
then
r
is
effective and so
r v
A
x
immediately by Lemma 53; if
r
is not a prefix of
x
, then
r v
A
x
(for otherwise
x
<
r
would imply that
w
<
r
by Lemma 54, contradicting
r v
A
w
).
Lastly, note that since
w · w ≤
E
∗
x · w
, it follows
by Lemma 48 that
w · x ≤
E
∗
x · w
.
We now have all premises in place to invoke Lemma 64 and conclude that
w · r v
A
x · w
, thus
w · r · s v
A
x · w · s
by Lemma 57.
Since
x · w · s v
A
w · r · s
, we know that
w · r · s
=
x · w · s
; it
follows that we can choose
t ∈ L
and
u ∈ L
n
such that
max
v
A
(
L
n
+1
) =
t · u
and
t
is effective.
For the remainder
of this proof we can therefore safely assume that
t
is effective.
To show that
u
=
x
n
· y
, first observe that, since
t · u
=
max
v
A
(
L
n
+1
) and
t ·
max
v
A
(
L
n
)
∈ L
n
+1
, we know
that
t ·
max
v
A
(
L
n
)
v
A
t · u
.
But then, because
t
is effective, by Lemma 51 we know that
max
v
A
(
L
n
)
v
A
u
and
therefore
u
= max
v
A
(
L
n
) since
u ∈ L
n
.
By the induction hypothesis, we then know that
u
=
x
n
· y
.
It remains to be shown that
t
=
x
.
Since both
t
and
x
are prefixes of
w
, either
t
is a prefix of
x
, or vice versa.
If
t
is a prefix of
x
,
then write
x
=
t · x
0
.
Then,
since
w · x ≤
E
∗
x · w
,
we can derive that
t · t · x
0
· y
=
t · x · y ≤
E
∗
x · x · y
=
t · x
0
· t · x
0
· y
, and thus that
t · x
0
· y ≤
E
∗
x
0
· t · x
0
· y
by Lemma 48.
But then, again by
Lemma 48, we know that
t · x
0
≤
E
∗
x
0
· t
.
Consequently, we know that
(
t · x
0
)
n
· y ≤
E
∗
(
x
0
· t
)
n
· x
0
· y
, and thus
that
t ·
(
t · x
0
)
n
· y ≤
E
∗
t ·
(
x
0
· t
)
n
· x
0
· y
.
From this, we have that
t · x
n
· y ≤
E
∗
(
t · x
0
)
n
+1
· y
=
x
n
+1
· y
.
If
x
is a prefix of
t
, write
w
=
t · w
0
.
Then
t · w ≤
E
∗
x · w
, and so
t · x · y ≤
E
∗
x · t · w
0
.
But then
t · x ≤
E
∗
x · t
by Lemma 48.
From this, we can derive that
t · x
n
· y ≤
E
∗
x
n−
1
· t · x · y
by repeated application of Lemma 49.
Since
t · w ≤
E
∗
x · x · y
, we also have that
x
n−
1
· t · x · y ≤
E
∗
x
n−
1
· x · x · y
=
x
n
+1
· y
, and so
t · x
n
· y ≤
E
∗
x
n
+1
· y
.
In either case
t · x
n
· y ≤
E
∗
x
n
+1
· y
.
Because
t
and
x
are effective,
we can derive by Lemma 60 that
t · x
n
· y v
A
x
n
+1
· y
.
Since
x
n
+1
· y v
A
t · u
=
t · x
n
· y
, it follows that
t · x
n
· y
=
x
n
+1
· y
, and therefore
t
=
x
.
In conclusion, we have that
max
(
L
n
+1
) =
t · u
=
x · x
n
· y
=
x
n
+1
· y
=
x
n
· w
, thus establishing the theorem.
15
Such an
x
always exists, for
w · w ∈
(
prefix
(
w
)
∩ L
)
· w
and (
prefix
(
w
)
∩ L
)
· w
is finite.
45
Theorem 4.
Let
A
=
Q,
Σ
, E, →, q
0
, t
be a threshold-free SCA.
Let
L ⊆ P
(
A
)
∗
be non-empty and finite, and
let
≤
E
∗
be total.
Let
w
=
max
v
A
(
L
).
If
w
is non-effective and
w · w
is the
≤
E
∗
-maximum of (
prefix
(
w
)
∩ L
)
· w
,
then max
v
A
(
L
n
) =
w · z
n−
1
for
n ≥
1, where
z
is the
≤
E
∗
-maximum of the shortest elements in
L
.
Proof.
We prove the claim by induction on
n
.
For our base case, where
n
= 1, the claim holds immediately.
For
the inductive step, assume that the claim holds for
n
.
Write
max
v
A
(
L
n
+1
) =
t · u
such that
t ∈ L
n
and
u ∈ L
;
we now need to show that
t · u
=
w · z
n
.
First, assume towards a contradiction that
t · u
=

.
If this is the case, then
 ∈ L
.
However,
 v
A
w
since
max
v
A
(
L
) =
w
.
Since
w
is non-effective,
we moreover know that
w 6
=

.
But then
t · u v
A
w · 
n
∈ L
n
+1
,
contradicting that
max
v
A
(
L
n
+1
) =
t · u
.
We thus know that
t · u 6
=

.
We can therefore assume without loss of
generality that
t 6
=

, choosing
u
=

if necessary.
Assume towards a contradiction that
t 6
=
w · z
n−
1
.
Then
t
<
w · z
n−
1
by the induction hypothesis.
If
t
is not
a prefix of
w · z
n−
1
, then
t · u
<
w · z
n−
1
· u
by Lemma 57, contradicting that
t · u
is the
v
A
-maximum of
L
n
+1
.
If
t
is a proper prefix of
w · z
n−
1
, then
t
is either effective or non-effective.
If, on the one hand,
t
is non-effective,
then
t 6
v
A
w · z
n−
1
by Lemma 62, which is a contradiction.
If, on the other hand,
t
is effective, then
t
is a proper
prefix of
w
.
But then
t · w <
E
∗
w · w
and so
t · w ≤
E
∗
w · t
.
If
t · w
=
w · t
, then
w
is effective by Lemma 65,
since
t
is non-empty and effective.
Therefore
t · w <
E
∗
w · t
and thus
t · w
<
w · t
.
Because
u v
A
w
, we moreover
know that
t · u v
A
t · w
by Lemma 51.
By transitivity, we then know that
t · u
<
w · t ∈ L
n
+1
.
With the latter
observation, we have again reached a contradiction.
We thus surmise that
t
=
w · z
n−
1
.
Observe that
t
is non-effective.
If
|u|
> |z|
,
then
t · u
<
t · z
by Lemma 63,
again contradicting that
max
v
A
(
L
n
+1
) =
t · u
.
We therefore know that
|u| ≤
E
∗
|z|
.
Since no element of
L
is shorter than
z
, it follows that
|u|
=
|z|
.
If
u <
E
∗
z
, then
t · u <
E
∗
t · z
and again
t · u
<
t · z
; thus
u ≥ z
.
Since
u ≤
E
∗
z
, it follows that
u
=
z
.
In conclusion, we know that
t
=
w · z
n−
1
and
u
=
z
, and therefore
max
v
A
(
L
n
+1
) =
t · u
=
w · z
n−
1
=
w · z
n
,
thus establishing the claim.
46
References
[1]
Farhad Arbab and Francesco Santini.
Preference and Similarity-Based Behavioral Discovery of Services.
In
Proc. Web Services and Formal
Methods (WS-FM), pages 118–133, 2012.
URL:
http://dx.doi.org/10.
1007/978-3-642-38230-7_8
.
[2]
Christel Baier, Tobias Blechmann, Joachim Klein, and Sascha Klüppelholz.
Formal verification for compo-
nents and connectors.
In Proc. Formal
Methods for Components and Objects, pages 82–101, 2008.
URL:
http://dx.doi.org/10.1007/978-3-642-04167-9_5
.
[3]
Christel
Baier,
Tobias Blechmann,
Joachim Klein,
Sascha Klüppelholz,
and Wolfgang Leister.
Design
and verification of
systems with exogenous coordination using Vereofy.
In Proc.
International
Sympo-
sium on Leveraging Applications, ISoLA 2010, pages 97–111, 2010.
URL:
http://dx.doi.org/10.1007/
978-3-642-16561-0_15
.
[4]
Christel Baier, Marjan Sirjani, Farhad Arbab, and Jan Rutten.
Modeling component connectors in Reo by
constraint automata.
Science of Computer Programming, 61:75–113, 2006.
URL:
http://dx.doi.org/10.
1016/j.scico.2005.10.008
.
[5]
Vijay G.
Bharadwaj and John S.
Baras.
Towards automated negotiation of access control
policies.
In
Proc. International
Workshop on Policies for Distributed Systems (POLICY), pages 111–119, 2003.
URL:
http://dx.doi.org/10.1109/POLICY.2003.1206965
.
[6]
Stefano Bistarelli.
Semirings for Soft Constraint Solving and Programming, volume 2962 of Lecture Notes
in Computer Science.
Springer, 2004.
URL:
http://dx.doi.org/10.1007/b95712
.
[7]
Stefano Bistarelli, Hélène Fargier, Ugo Montanari, Francesca Rossi, Thomas Schiex, and Gérard Verfaillie.
Semiring-based CSPs and valued CSPs:
Basic properties and comparison.
In Over-Constrained Systems,
pages 111–150, 1995.
URL:
http://dx.doi.org/10.1007/3-540-61479-6_19
.
[8]
Stefano Bistarelli and Fabio Gadducci.
Enhancing constraints manipulation in semiring-based formalisms.
In Proc. European Conference on Artificial
Intelligence (ECAI), pages 63–67, 2006.
[9]
Stefano Bistarelli,
Ugo Montanari,
and Francesca Rossi.
Constraint solving over semirings.
In Proc.
International
Joint Conference on Artificial
Intelligence, IJCAI 95, pages 624–630, 1995.
[10]
Stefano Bistarelli,
Ugo Montanari,
and Francesca Rossi.
Semiring-based constraint satisfaction and
optimization.
J. ACM, 44(2):201–236, 1997.
URL:
http://dx.doi.org/10.1145/256303.256306
.
[11]
Julius Richard Büchi. On a decision method in restricted second order arithmetic. In Proc. Logic, Methodology
and Philosophy of Science, pages 1–11, Stanford, Calif., 1962. Stanford Univ. Press.
[12]
Rance Cleaveland and Matthew Hennessy.
Priorities in process algebras.
Inf. Comput., 87(1/2):58–77, 1990.
URL:
http://dx.doi.org/10.1016/0890-5401(90)90059-Q
.
[13]
Rance Cleaveland, Gerald Lüttgen, and V. Natarajan.
Priority and abstraction in process algebra.
Inf.
Comput., 205(9):1426–1458, 2007.
URL:
http://dx.doi.org/10.1016/j.ic.2007.05.001
.
[14]
Fabio Gadducci, Matthias M. Hölzl, Giacoma Valentina Monreale, and Martin Wirsing.
Soft constraints
for lexicographic orders.
In Advances in Artificial
Intelligence and Its Applications, Mexican International
Conference on Artificial
Intelligence,
MICAI,
pages 68–79,
2013.
URL:
http://dx.doi.org/10.1007/
978-3-642-45114-0_6
.
[15]
Xuechong Guan and Yongming Li.
On conditions for mappings to preserve optimal solutions of semiring-
induced valuation algebras.
Theor. Comput. Sci., 563:86–98, 2015.
URL:
http://dx.doi.org/10.1016/j.
tcs.2014.10.016
.
[16]
David Harel, Dexter Kozen, and Jerzy Tiuryn.
Dynamic logic.
MIT press, 2000.
[17]
Matthias M. Hölzl, Max Meier, and Martin Wirsing.
Which soft constraints do you prefer? Electr. Notes
Theor. Comput. Sci., 238(3):189–205, 2009.
URL:
http://dx.doi.org/10.1016/j.entcs.2009.05.020
.
[18]
Tobias Kappé, Farhad Arbab, and Carolyn Talcott.
A Compositional Framework For Preference-Aware
Agents.
In Proc. Verification and Validation of Cyber-Physical
Systems, 2016.
To appear.
[19]
Tobias Kappé, Farhad Arbab, and Carolyn Talcott.
A Compositional Framework For Preference-Aware
Agents.
CWI Technical Report FM-1603, May 2016.
URL:
https://repository.cwi.nl/noauth/search/
fullrecord.php?publnr=24625
.
47
[20]
Christian Koehler and Dave Clarke.
Decomposing port automata.
In Proc. ACM Symposium on Applied
Computing (SAC), pages 1369–1373, 2009.
URL:
http://doi.acm.org/10.1145/1529282.1529587
.
[21]
Sanjiang Li and Mingsheng Ying.
Soft constraint abstraction based on semiring homomorphism.
Theor.
Comput. Sci., 403(2-3):192–201, 2008.
URL:
http://dx.doi.org/10.1016/j.tcs.2008.03.029
.
[22]
Alberto Lluch-Lafuente and Ugo Montanari.
Quantitative
µ
-calculus and CTL defined over constraint
semirings.
Theor. Comput. Sci., 346(1):135–160, 2005.
URL:
http://dx.doi.org/10.1016/j.tcs.2005.
08.006
.
[23]
Fabio Martinelli,
Ilaria Matteucci,
and Francesco Santini.
Semiring-based specification approaches for
quantitative security.
In Proc. Quantitative Aspects of Programming Languages and Systems, QAPL, pages
95–109, 2015.
URL:
http://dx.doi.org/10.4204/EPTCS.194.7
.
[24]
Amir Pnueli.
The temporal logic of programs.
In Proc. Symposium on Foundations of Computer Science
(SFCS), pages 46–57, 1977.
URL:
http://dx.doi.org/10.1109/SFCS.1977.32
.
[25]
Vaughan R.
Pratt.
Semantical
considerations on Floyd-Hoare logic.
In Proc.
Annual
Symposium on
Foundations of Computer Science, pages 109–121, 1976.
URL:
http://dx.doi.org/10.1109/SFCS.1976.
27
.
[26]
Jan J. M. M. Rutten.
A coinductive calculus of streams.
Mathematical
Structures in Computer Science,
15(1):93–147, 2005.
URL:
http://dx.doi.org/10.1017/S0960129504004517
.
[27]
Thomas Schiex, Hélène Fargier, and Gérard Verfaillie.
Valued constraint satisfaction problems:
Hard and
easy problems.
In Proc. International
Joint Conference on Artificial
Intelligence, IJCAI 95, pages 631–639,
1995.
[28]
Jeffrey O. Shallit. A Second Course in Formal Languages and Automata Theory. Cambridge University Press,
2008.
URL:
http://www.cambridge.org/gb/knowledge/isbn/item1173872/?site_locale=en_GB
.
[29]
Rivi
Sherman,
Amir Pnueli,
and David Harel.
Is the interesting part of process logic uninteresting? A
translation from PL to PDL.
SIAM J. Comput., 13(4):825–839, 1984.
URL:
http://dx.doi.org/10.1137/
0213051
.
[30]
A. Prasad Sistla, Moshe Y. Vardi, and Pierre Wolper.
The complementation problem for büchi automata
with applications to temporal logic (extended abstract).
In Proc. Automata, Languages and Programming,
pages 465–474, 1985.
URL:
http://dx.doi.org/10.1007/BFb0015772
.
[31]
Carolyn L. Talcott, Farhad Arbab, and Maneesh Yadav.
Soft agents:
Exploring soft constraints to model
robust adaptive distributed cyber-physical agent systems.
In Software, Services, and Systems — Essays
Dedicated to Martin Wirsing on the Occasion of
His Retirement
from the Chair of
Programming and
Software Engineering, pages 273–290, 2015.
URL:
http://dx.doi.org/10.1007/978-3-319-15545-6_18
.
[32]
Johan van Benthem and Fenrong Liu. Dynamic logic of preference upgrade. Journal of Applied Non-Classical
Logics, 17(2):157–182, 2007.
URL:
http://dx.doi.org/10.3166/jancl.17.157-182
.
[33]
Moshe Y. Vardi.
An automata-theoretic approach to linear temporal logic.
In Proc. Logics for Concurrency
-
Structure versus Automata (8th Banff Higher Order Workshop),
pages 238–266,
1995.
URL:
http:
//dx.doi.org/10.1007/3-540-60915-6_6
.
[34]
Soo Yeong Yi and Myung Jin Chung.
Robustness of fuzzy logic control for an uncertain dynamic system.
IEEE Trans. Fuzzy Systems, 6(2):216–225, 1998.
URL:
http://dx.doi.org/10.1109/91.669018
.
48
