Detektovanje plagijata u visokoškolskim ustanovama 
primenom tehnika i metoda istraživanja podataka.
Nenad Bulatović 
Departman za Industrijsko Inženjerstvo i Menadžment 
Fakultet tehničkih nauka 
Novi Sad, Srbija 
nenad.bulatovic@gmail.com 
Sadržaj— Savremene metode razmene informacija kao što su 
Internet 
i 
knjige 
u 
elektronskom 
formatu, 
omogućavaju 
efikasnu 
razmenu 
znanja, 
olakšavaju 
i 
ubrzavaju 
proces 
učenja, ali takođe i olakšavaju plagijarizam koji je sve češća 
pojava u studentskim radovima. U radu je prikazana tehnika 
za 
detektovanje 
plagijarizma 
upotrebom 
Kosinusine 
mere 
sličnosti i Modelovanja vektorskog prostora, implementiranih 
unutar 
radnog 
okvira 
Gensim. 
Automatizacija 
Gensima 
je 
ostvarena 
putem 
Simservera, 
nadograđenim 
sa 
korisničkim 
okruženjem pisanim u PHP-u. 
Ključne riječi— Cosine Similiarity; Tf-idf; LSI; Gensim; detekcija 
plagijata; 
I.
U
VOD
Nove generacije studenata, u odnosu na prethodne, nisu 
samo promenile njihov način izražavanja, odeću, 
muziku 
koju slušaju, način na koji komuniciraju, kako je to tokom 
decenija bivalo uobičajeno od jedne do naredne generacije. 
Umesto 
postepene 
promene, 
došlo 
je 
do 
velikog 
diskontinuiteta, jaza među generacijama. Ovaj jaz je izazvan 
naglim razvojem i rasprostiranjem digitalne tehnologije u 21. 
veku [1]. Zaista, mnogi autori 21. vek nazivaju "Digitalnom 
erom" ili "Informatičkim vekom". 
Savremene generacije studenata predstavljaju generacije 
koje u potpunosti odrastaju uz nove tehnologije kao što su 
personalni 
računari, 
mobilni 
telefoni, 
internet, 
tableti, 
socijalne 
mreže, digitalni 
mediji, elektronske 
knjige i 
sl. 
Usled toga, promenio se ne samo način na koji uče gradivo 
tokom studija, već i njihov pristup prema učenju uopšte. Sve 
veći 
broj 
studenata 
će 
željenu 
informaciju 
tražiti 
putem 
pretrage 
po 
internetu, 
čitanjem 
veb 
stranica, 
blogova, 
Youtube tutorijala, a sve manje putem tradicionalnih izvora 
kao što su knjige ili naučni radovi dostupni široj javnosti, bez 
obzira na to da li su isti u elektronskoj ili papirnoj formi. 
Navedeni pristup ima svoje prednosti i mane. Prednosti 
su svakako to da se brže dospe do traženih informacija, često 
već u obrađenoj, pogodnoj, suštinskoj formi, koja drastično 
skraćuje vreme potrebno za učenje ili istraživanje. Kao mana 
ističe 
se 
necelovitost 
prikaza, 
bez 
potrebne 
širine 
zarad 
postizanje 
boljeg 
uvida 
u 
celokupnu 
materiju, 
često 
neohodnu prilikom naučnih istraživanja, kakvi bi, mada na 
elementarnom nivou, trebalo da budu i studenstki radovi. U 
takvim 
online 
izvorima, 
veoma 
često 
površno 
su 
interpretirana 
originalna 
dela, 
a 
u 
najgorem 
slučaju 
su 
potpune kopije tuđih radova, ali bez adekvatnog navođenja 
pravog autora – originalnih izvora. 
Sledeći problem u novom pristupu traženja informacija, 
njihovog sakupljanja i objedinjavanja, te njihove naknadne 
konverzije u skup novih znanja, koje treba da urodi pisanjem 
originalnih studentskih radova je to što taj savremeni proces 
takođe omogućava i lakše preuzimanje tuđih dela u celini ili 
delovima i proglašavanje istih za sopstvene, u potpunosti ili 
delimično bez referenciranja izvora [2]. 
II.
P
REGLED 
S
TANJA U 
O
BLASTI
Kompanija iParadigms koja je razvila jedno od vodećih 
komercijalnih rešenja za detekciju plagijata [2], Turnitin, je u 
svojoj studiji [3] objavila klasifikovanih 10 opštih tipova 
plagijarizma. 
Oni 
su, 
po 
redu, 
sa 
kratkim 
opisom 
od 
najozbiljnijih do najbezazlenijih, sledeći: 
1.
Klon, potpisivanje tuđeg rada kao sopstvenog. 
2.
Ctrl-C, rad u kojem su značajni delovi kopirani iz 
tuđeg rada. 
3.
Pronađi-zameni, menjanje ključnih reči i fraza, ali 
zadržavanje suštinskog sadržaja izvornog rada. 
4.
Remiks, parafraziranje iz različitih izvora i njihovo 
uklapanje u jednu celinu. 
5.
Reciklaža, korišćenje sopstvenih radova u celosti ili 
delimično, bez referenciranja istih, odnosno plagijat 
sopstvenih radova. 
6.
Hibrid, 
kombinovanje 
dobro 
citiranih 
izvora 
sa 
delovima koji su takođe kopirani ali nisu citirani. 
7.
Mešavina, rad koji predstavlja mešavinu kopiranog 
materijala iz više različitih izvora, bez adekvatnog 
referenciranja. 
8.
Greška 404, rad koji sadrži citate ka nepostojećim 
izvorima ili pogrešne informacije o citatima. 
9.
Agregator, sadrži pravilno citiranje, ali u osnovi ne 
predstavlja originalan rad. 
10.
Preslikavanje, rad sadrži pravilno citiranje, ali se 
suviše oslanja na skup reči i strukturu originalnog 
teksta [2]. 
Iz navedene klasifikacije se uočava da detekcija plagijata 
ne 
može 
biti 
puka 
provera 
pasus-na-pasus, 
rečenica-na-
rečenicu, 
ili 
reč-na-reč, 
između 
dva 
dokumenta 
(ili 
dokumenta 
naspram 
baze 
dokumenata), 
jer 
bi 
se 
u 
tom 
slučaju detektovao samo prvi (Klon) i eventualno drugi (Ctrl-
C) slučaj. Već kod trećeg slučja (Pronađi-zameni), ovakav 
pristup u potpunosti ili delimično ne prepoznaje plagijat 
[2,3]. 
Uopšteno 
gledano, 
najčešći 
i 
najozbiljniji 
primeri 
"akademskog 
nepoštenja" 
su 
prva 
tri 
slučaja 
i 
ona 
su 
najčešće zastupljena kod studentskih radova koji su plagijati. 
Zarad 
uspešnijeg 
rešavanja 
ovog 
problema 
koriste 
se 
složene metode kao što su modelovanje vektorskog prostora, 
kosinusna 
mera 
sličnosti, 
vreća 
reči, 
frekvencija 
izraza, 
modelovanje teme i sl., o čemu će kasnije biti više reči. Zbog 
složenosti, ne postoji veliki broj kompletnih rešenja iz ove 
oblasti, 
a 
po 
raspoloživim 
informacijama, 
kao 
jedino 
besplatno, a opet efikasno po pitanju resursa (naročito RAM 
memorije), 
postoji 
samo 
GENSIM 
radni 
okvir. 
Od 
komercijalnih 
rešenja, 
po 
studiji 
rađenoj 
na 
Univerzitetu 
Luton, 
Engleska 
[2], 
izdvajaju 
se 
Findsame 
(Digital 
integrity), 
Eve2 
(CaNexus), 
Turnitin 
(iParadigms), 
CopyCatch 
(CFL 
Software 
developments), 
WordCHECK 
(Word CHECK systems). Dalja analiza upotrebne vrednosti 
ovih rešenja je veoma obimna i prevazilazi okvire ovog rada. 
U nastavku ovog rada prikazan je jedan od mogućih 
pristupa 
rešavanju 
ovog 
problema 
– 
detekcija 
plagijata 
upotrebom radnog okvira 
GENSIM. Prevashodna 
namena 
GENSIM 
je 
automatsko 
razvrstavanje 
(klasifikacija) 
dokumenta koji se prilikom dodavanja u bazu dokumenata, 
poredi sa već postojećim radovima. U skladu sa time biće 
razvrstan 
u 
najpribližniju 
kategoriju. 
Ova 
funkcionalnost 
GENSIM 
je 
iskorišćena 
pod 
pretpostavkom 
da 
će 
odgovarajući indikatori sličnosti novog dokumenta naspram 
već postojećih biti izuzetno visoki u slučaju da se radi o 
plagijatu. 
Pri 
tome 
u 
ovoj 
izvedbi 
GENSIM 
ne 
koristi 
mogućnost kategorizacije naspram dokumenta u bazi, već 
samo proveru sličnosti sa njima. 
III.
O
PIS 
P
RISTUPA 
R
EŠAVANJU 
P
ROBLEMA
A.
Modelovanje vektorskog prostora 
Prvi korak u praktičnom rešavanju datog problema je 
Modelovanje vektorskog prostora (Vector Space Modeling). 
Ovaj termin predstavlja algebarski model za reprezentovanje 
tekstualnih dokumenata (ili uopšte objekata). Koristi se za 
filtritranje informacija, prikupljanje informacija, indeksiranje 
i rangiranje relevantnosti [5]. Rangiranje se vrši na osnovu 
pretpostavki 
teorije 
sličnosti, 
poređenjem 
devijacije 
odstupanja 
uglova 
između 
vektora 
svakog 
dokumenta 
i 
vektora dokumenta za koji se vrši upit, pri čemu su svi 
predstavljeni istim tipom vektora. Praktična realizacija se 
lakše ostvaruje računanjem kosinusa ugla putem "Kosinusne 
mere 
sličnosti" 
(Cosine 
Similiarity), 
umesto 
samog 
ugla 
između dva vektora [4]. Nadalje, svaka reč, koja se u ovom 
kontekstu 
naziva 
token 
pretvara 
se 
u 
njenu 
numeričku 
reprezentaciju, da bi se mogla dalje smisleno obrađivati. 
Ovaj pristup je poznat pod nazivom "Vreća reči" (Bag-of-
words) i u njemu je značenje dokumenta zasnovano samo na 
rečima (bez određenog redosleda) ali samo u slučajevima 
gde ima dosta podataka u korpusu (kolekcija dokumenata). 
GENSIM radni okvir implementira modelovanje vektorskog 
prostora, upotrebom Numpy i SciPy numeričkih biblioteka 
realizovanih 
u 
programskim 
jezicima 
Python, 
C/C++ 
i 
Fortran. 
B.
Model vreće reči 
Model 
"vreće 
reči" je pojednostavljena 
reprezentacija 
koja 
se 
koristi 
u 
procesiranju 
prirodnog 
jezika 
(natural 
language processing) [6]. To je polje računarskih nauka, 
veštčke inteligencije i lingvistike koje se bavi interakcijama 
između računara i ljudskih (prirodnih) jezika, a samim time i 
interakcijom 
računar 
- 
čovek. 
Osnovni 
zadaci 
su 
razumevanje 
prirodnih 
jezika 
– 
drugim 
rečima 
omogućavanje računarima da izvuku značenje iz ljudskih 
reči. 
U 
ovom 
modelu 
se 
tekst, 
kao 
što 
su 
rečenice 
ili 
dokument, reprezentuje kao nesređena kolekcija reči bez 
obzira na gramatička pravila i redosled reči. U novije vreme, 
model se koristi i u oblasti računarskog vida [7]. Upotreba 
ovog modela je ustaljena u klasifikaciji dokumenata, gde se 
učestalost pojavljivanja svake reči koristi kao parametar za 
treniranje klasifikatora. Svaka reč iz svakog dokumenta se 
indeksira – bez ponavljanja, a zatim uvodi u rečnik. Na taj 
način 
svaka 
rečenica 
nanovo 
može 
biti 
predstavljena 
vektorom, pri čemu svaka stavka vektora predstavlja broj 
ponavljanja 
odgovarajuće 
stavke 
u 
rečniku. 
Ovakva 
reprezentacija ne čuva red reči u originalnoj rečenici, već 
samo ukazuje na njen sadržaj po pitanju iskorišćenih reči. 
Drugim 
rečima 
vektori 
dokumenata 
sadrže 
frekvencije 
pojave termina u tim dokumentima. U tehnici aportiranja 
informacija i tekstualne klasifikacije, dodeljuje se značaj 
svakom terminu (term weighting), putem različitih šema, od 
kojih je najpopularnija tzv. frekvencija izraza - inverzna 
frekvencija 
dokumenta 
(term 
frequency-inverse 
document 
frequency, tf-idf). 
C.
Frekvencija izraza 
Za svaki token u nizu tokena, računa se koliko puta se taj 
token pojavljuje u dokumentu [6]. Ako je neki dokument 
veći, 
imaće 
veći 
broj 
reči, 
pa 
usled 
toga 
toga 
i 
veću 
verovatnoću 
ponavljanja 
tokena 
u 
njemu. 
Da 
bi 
se 
ovaj 
proces 
normalizovao 
naspram 
dužine 
dokumenta, 
broj 
pojavljivanja 
se 
deli 
sa 
ukupnim 
brojem 
tokena. 
Tako 
dobijen broj predstavlja učestalost izraza (term frequency). 
D.
Inverzna frekvencija dokumenta 
Inverzna 
frekvencija 
dokumenta 
predstavlja 
relativnu 
važnost 
(zarad 
nedvosmislenosti 
u 
odnosu 
na 
druge 
dokumente) nekog tokena u odnosu na druge [8]. Za svaki 
token 
treba 
izbrojati 
broj 
dokumenata 
u 
kojima 
se 
on 
pojavljuje. Ukupan broj dokumenata u kolekciji se zatim deli 
sa ovim brojem. Primera radi, za pojedinačni token "zdravo" 
koji 
se 
pojavljuje 
u 
dokumentu 
3 
puta, 
ako 
je 
dužina 
dokumenta 100 tokena inverzna frekvencija dokumenta će 
iznositi 3/100=0,03. Ako je kolekcija dokumenta (korpus) 
veličine 300 dokumenata, a reč "zdravo" se pojavljuje u 120 
dokumenata, onda je log(300/120) = 0,398. Prema tome, 
tf*idf (term frequency*inverse document frequency) će biti 
0,01194 što predstavlja vrednost za pojedinačni token. U 
ovom primeru vrednost nije na visokom nivou što znači da 
se token pojavljuje u puno dokumenata [9]. 
E.
Euklidsko rastojanje 
Nakon 
što 
se 
dokument 
konvertuje 
u 
"vreću 
reči", 
dodeljivanjem numeričke vrednosti svakom tokenu, može da 
se izračuna udaljenost između ta dva dokumenta. Neka se 
jedan dokument nalazi u vektorskom prostoru. Taj vektorski 
prostor takođe sadrži sve druge dokumente, na odgovarajućoj 
udaljenosti, jednih od drugih. Za svaki token treba da postoji 
po jedna dimenzija. Za 20 tokena potrebno je 20 dimenzija. 
Usled toga se vektorski prostor, a za ovu primenu, smatra n-
dimenzionalnim. 
Nadalje, 
mogu 
se 
kalkulisati 
razdaljine 
samo između istih tokena u dva različita dokumenta. Za ovu 
kalkulaciju 
koristi 
se 
Euklidsko 
rastojanje 
(Euclidian 
distance). 
F.
Modelovanje tematike 
U 
mašinskom 
učenju 
i 
procesiranju 
prirodnog 
jezika 
modelovanje tematike (Topic modeling) je tip statističkog 
modela za otkrivanje apstraktinih tematika koje se javljaju u 
kolekciji 
dokumenata. 
Na 
primer, 
u 
dokumentu 
o 
vazduhoplovstvu 
će 
se 
najverovatnije 
pojavljivati 
termini 
"avion" 
i 
"nebo", 
a 
u 
dokumentu 
o 
pomorstvu 
"brod" 
i 
"more". 
Modelovanje 
tematike 
pretvara 
ova 
intiutivna 
očekivanja 
u 
matematički 
radni 
okvir, 
koji 
omogućava 
istraživanje 
skupa 
dokumenata 
i 
otkrivanje 
(bazirano 
na 
statistici svake reči) šta mogu biti teme i kakav je odnos tema 
u svakom dokumentu. Modelovanje tematike su prvobitno 
bili opisani i implementirani u kontekstu obrade prirodnih 
jezika (natural language processing). 
G.
Latentno semantičko indeksiranje 
Latentno 
semantičko 
indeksiranje 
(latent 
semantic 
indexing) je metod indeksiranja i prikupljanja informacija 
koji 
koristi 
matematičku 
tehniku 
zvanu 
dekompozicija 
karakterističnih vrednosti (singular value decompositiion), 
da bi se identifikovali obrasci u odnosima između pojmova i 
koncepata koji sadrže nestruktuirane kolekcije teksta. LSI je 
zasnovano na principu da reči koje su korišćene u istom 
kontekstu imaju tendenciju da imaju slično značenje. Ključna 
osobina LSI je sposobnost da se ekstrahuje konceptualni 
sadržaj tela teksta putem uspostavljanja asocijacija između 
tih termina koji se pojavljuju u sličnim kontekstima. Naziva 
se lateno semantičko indeksiranje zbog svoje sposobnosti da 
uskladi semantički srodstvene termine, koji su latentni u 
kolekciji tekstova. Ovaj metod se takođe naziva i latentna 
semantička analiza (latent semantic analysis). On takođe 
otkriva i prisutne, ali ne očigledne, na dubljim nivoima, 
semantičke strukture u korišćenju reči u telu teksta, i kako 
one mogu biti korišćene, a da se izvuče značenje teksta kao 
odgovora 
na 
upite 
korisnika. 
Zajednički 
se 
nazivaju 
"konceptualnim pretragama" (conceptual search). Upiti, ili 
konceptualne 
pretrage 
naspram 
seta 
dokumenata 
koji 
su 
prošli LSI vratiće rezultate koji su konceptualno slični po 
smislu prema kriterijumu pretrage čak i kada rezultati ne dele 
specifičnu reč ili reči u okviru kriterijuma pretrage [10,11]. 
LSI 
prevazilazi 
dva 
najproblematičnija 
aspekta 
upita 
putem 
ključnih 
reči, 
odnosno 
anulira 
neophodnost 
istovetnosti reči u pretrazi, time što omogućava pretragu: 
a)
više reči koje imaju slična značenja (sinonimi), 
b)
reči koje imaju više nego jedno značenje (polisemi). 
LSI se takođe koristi da obavi automatsku kategorizaciju 
dokumenata. Određen broj eksperimenata su demonstrirali 
da postoje brojne korelacije između načina na koji LSI i ljudi 
procesiraju 
i 
kategorišu 
tekst. 
Kategorizacija 
se 
u 
ovom 
slučaju 
odnosi 
na 
smeštanje 
dokumenata 
u 
neku 
od 
predefinisanih 
kategorija, 
a 
zasnovana 
je 
na 
njegovoj 
sličnosti konceptualnog sadržaja sa datom kategorijom. LSI 
koristi set reprezentativnih dokumenata da bi se oformila 
konceptualna 
baza 
za 
svaku 
kategoriju. 
Tokom 
procesa 
kategorizacije, koncepti koji se sadrže u dokumentima koji 
se 
kategorizuju, 
porede 
se 
naspram 
koncepata 
u 
reprezentativnim dokumentima i dokumentu se dodeljuje 
kategorija, na osnovu sličnosti između koncepata koje sadrže 
i koncepata koji su sadržani u rerezentativnim dokumentima. 
Pomoću 
LSI 
takođe 
može 
da 
se 
postigne 
"dinamičko 
klasterisanje" 
zasnovano 
na 
konceptualnom 
sadržaju 
dokumenata. Klasterisanje je način da se grupa dokumenata 
bazira na njihovoj konceptualnoj sličnosti – jedan naspram 
drugog bez korišćenja reprezentativnih dokumenata zarad 
ustanovljavanja konceptualnih osnova za svaki klaster. Ovo 
je veoma korisno kada se radi sa nepoznatom kolekcijom ili 
nestruktuiranim tekstom. LSI se automatski adaptira na novu 
i promenljivu tehnologiju, i veoma je tolerantan na "šum" 
koji 
u 
ovom 
slučaju 
mogu 
biti 
pogrešno 
otkucane 
reči, 
tipografske greške i sl. LSI takođe dobro radi sa podacima 
koji su dvosmisleni ili kontradiktorni [12,13]. Da bi LSI bio 
efektivan, tekst ne mora da bude u vidu rečenica. LSI može 
da 
radi 
sa 
listama, 
beleškama, 
elektronskom 
poštom, 
sadržajem sa veb stranica. Sve dok kolekcija teksta sadrži 
višestruke termine, LSI može da se koristi da identifikuje 
šeme i relacije između važnih termina i koncepata koji se 
sadrže u tekstu. 
Latentno semantičko indeksiranje koristi opšte tehnike 
linerane algebre da bi "naučilo" konceptualne korelacije u 
kolekciji tekstova. Proces se sastoji od kreiranja matrice na 
osnovu dokumenta u kome su pridodati faktori značajnosti 
terminima 
(term 
weighting). 
Matrica 
se 
konstituiše 
primenom 
dekompozicije 
karakterističnih 
vrednosti 
(Singular value decomposition) i korišćenjem matrice da se 
identifikuju koncepti sadržani u tekstu. LSI se sve češće 
koristi u elektronskom otkrivanju dokumenata (eDiscovery) 
da bi se omogućila priprema preduzeća za pravne sporove. 
Sposobnost eDiscovery da klasifikuje, kategorizuje i pretraži 
velike kolekcije nestruktuiranih podataka, samo na osnovu 
konceptualne 
osnove 
je 
od 
esencijalnog 
značaja. 
Konceptualne 
pretrage 
zasnovane 
na 
LSI 
se 
koriste 
u 
eDiscovery 
procesima 
od 
strane 
najvećih 
korisnika 
ovih 
usluga još od 2003 god. 
H.
Dekompozicija karakterističnih vrednosti 
U 
lineranoj 
algebri, 
dekompozicija 
karakterističnih 
vrednosti 
(Singular 
value 
decomposition) 
je 
faktorizacija 
realnih 
ili 
kompleksnih 
matrica, 
sa 
mnogim 
korisinm 
aplikacijama u signalnom procesiranju i statistici. Koristi se 
u velikoj meri u latentnoj semantičkoj analizi i indeksiranju. 
Posebnost GENSIM se ogleda u tome što se operacije sa 
matricama 
ne 
moraju 
nužno 
sve 
obavljati 
u 
operativnoj 
memoriji, 
čime 
je 
omogućena 
upotreba 
i 
na 
ličnim 
računarima 
prosečnih 
performansi. 
Treniranje 
korpusa 
na 
reprezentativnom 
uzorku 
veličine 
8.2GB 
komprimovanih 
svih članaka sa Wikipedia se na prenosnom računaru može 
obaviti za oko 9 časova. 
I.
Priprema dokumenata 
Pre rada sa GENSIM radnim okvirom, odnosno njegovim 
alatom za automatizaciju postupka - SIMSERVER - potrebno 
je pripremiti dokumente za rad sa njime. To 
se ostvaruje 
putem PHP skripte koja kreira i priprema potrebne datoteke, 
uklanjajući iz njih znakove interpunkcije. Nakon toga dodaje 
pročišćene tekstove u korpus, a zatim pokreće Python skriptu 
koja 
inicijalizuje 
SIMSERVER 
sa 
potrebnim 
parametrima. 
Izlaz iz SIMSERVER se prosleđuje u drugu PHP skriptu koja 
pravi pogodan prikaz u veb čitaču, koji u ovom slučaju ima 
funkciju interfejsa za rad sa korisnikom. 
IV.
GENSIM 
Navedni 
postupak 
rešavanja 
problema 
se 
može 
rešiti 
upotrebom samo jednog radnog okvira – GENSIM – otuda i 
njegov veliki značaj za ovu oblast. 
GENSIM 
je 
kolekcija 
alata, 
u 
vidu 
radnog 
okvira, 
otvorenog 
koda, 
objavljena 
pod 
LGPL 
licencom, 
za 
modelovanje vektorskog prostora i modelovanje tematike, 
implementirana 
u 
Python 
programskom 
jeziku, 
posebno 
namenjena radu sa velikim tekstualnim kolekcijama. 
Sastoji 
se 
od 
implementacija 
Tf-idf, 
nasumičnih 
projekcija, 
latentne 
semantičke 
analize, 
latentnog 
semantičkog 
indeksiranja 
i 
latentne 
Dirhleove 
alokacije. 
GENSIM se koristi u brojnim komercijalnim i akademskim 
aplikacijama 
[5] 
gde 
uspešno 
automatski 
određuje 
semantičke 
teme 
iz 
dokumenata. 
Prema 
raspoloživim 
informacijama 
GENSIM 
je 
jedina 
javno 
dostupna 
implementacija 
za 
latentnu 
semantičku 
analizu 
(latent 
sematic 
analysis) 
koja 
ne 
zahteva 
da 
matrica 
sa 
dokumentima 
koji 
sadrže 
termine, 
bude 
celokupno 
u 
memoriji. Usled toga, GENSIM kao Python implementacija 
numeričke 
biblioteke 
Numpy 
kojom 
se 
realizuje 
LSA 
je 
nezavisna od veličine korpusa. GENSIM radni okvir sadrži 
inkrementalne 
(memorijski 
efikasne) 
algoritme 
za 
frekvenciju izraza/inverznu frekvenciju dokumenata, latentnu 
semantičku 
analizu/indeksiranje, 
nasumične 
projekcije 
i 
latentnu Dirhleovu analizu [5]. 
Ograničenje se ogleda 
u tome što 
može da obrađuje 
isključivo obične tekstualne datoteke. Važno je napomenuti 
da korisnik nema interakciju sa ovim algoritmima, na njemu 
je smo da obezbedi adekvatan korpus – bazu dokumenata i 
da izabere algoritam po kojem želi da radi proveru. 
V.
I
MPLEMENTACIJA 
GENSIM 
Za 
praktičnu 
implementaciju 
radnog 
okvira 
GENSIM 
predlaže se automatizacija putem SIMSERVERA (Document 
Similiarity Server) [14]. SIMSERVER je dobar za digitalne 
biblioteke koje se uglavnom sastoje od tekstova. Olakšava 
anotaciju, 
organizaciju 
i 
navigaciju 
kroz 
dokumente 
na 
apstraktiniji način u poređenju sa jednostavnom pretragom 
putem ključnih reči. U opisanoj implementaciji iskorišćen je 
Debian 
GNU/Linux 
5.0 
Lenny 
server. 
Na 
njemu 
su 
instalirane 
sve 
potrebne 
komponente: 
Python 
programski 
jezik, 
set 
rutina 
za 
laku 
instalaciju 
biblioteka, 
razvojne 
biblioteke, 
GNU 
C, 
C++ 
i 
Fortran 
kompajleri, 
zatim 
numeričke biblioteke BLAS, NumPy, SciPy, i na kraju sam 
GENSIM 
radi 
okvir 
i 
pripadajući 
skup 
alata 
za 
automatizaciju rada SIMSERVER. 
Priprema 
dokumenta 
se 
ostvaruje 
putem 
PHP 
skripte, 
realizovane u formi internet stranice sa dijalozima [16], koja 
ukoliko već ne postoji, kreira praznu datoteku koja će biti 
budući korpus dokumenata. Zatim se za prvi i svaki sledeći 
dokument 
koji 
se 
dodaje 
u 
korpus, 
vrši 
uklanjanje 
specijalnih znakova i znakova interpukcije uopšte, tako da 
ostaju samo reči tj. u ovom slučaju tokeni. Tako "prečišćen" 
dokument 
se 
zatim 
dodaje 
kao 
string 
u 
korpus. 
Svaki 
dokument 
će 
predstavljati 
jedan 
string 
– 
jedan 
novi 
neformatirani red u datoteci koja predstavlja korpus. Ovo je 
uslovljeno načinom rada GENSIM/SIMERVER. Kada se u 
korpusu nalazi dovoljan broj dokumenata, može se pokrenuti 
poređenje korpusa sa datotekom koja je predmet provere da 
li je plagijat. 
Posebna, 
za 
ovu 
primenu 
napisana, 
Python 
skripta, 
priprema 
SIMSERVER 
za 
rad 
sa 
dokumentima. 
Skripta 
uključuje 
logovanje, 
pokreće 
instancu 
servera, 
definiše 
zaglavlje za dokumente (na osnovu GENSIM specifikacije), 
uvozi 
Python 
biblioteke 
potrebne 
za 
rad. 
Zatim, 
otvara 
korpus koji se sastoji od jedne obične tekstualne datoteke 
(plain text), a koja može i biti prazna. Nakon toga se definiše 
zaglavlje 
korpusa, 
sekvenca 
dokumenata 
i 
određuje 
se 
veličina blokova koji će se obrađivati u memoriji. U ovoj 
realizaciji odabrana je veličina od 1000 bajtova. Dodeljen je 
prostor na disku za privremeno skladištenje (swap). Nakon 
toga se bira mod treninga korpusa (Tf-idf, LSI), tekst se 
konvertuje u njegovu semantičku reprezentaciju i postavlja 
se veličina indeksa [14,15]. U narednom koraku se definišu i 
tzv. stop-words, koje predstavljaju uopštene reči u jeziku, 
koje se uvek pojavljuju bez obzira na tematiku dokumenta. 
Konačno, 
serveru 
se 
zadaje 
komanda 
kojom 
se 
pokreće 
pronalaženje najsličnijeg dokumenta iz korpusa dokumenta 
koji se nalazi na početnoj poziciji indeksa. U ovoj realizaciji 
svi dokumenti se nalaze učitani u korpus, uključujući i novi 
dokument 
za 
kojeg 
se 
tek 
želi 
ustanoviti 
kolika 
mu 
je 
sličnost sa ostalim dokumentima. 
Izlaz iz SIMSERVER-a pokrenutog opisanom skriptom, 
koja je opet pokrenuta od strane PHP skripte koja se nalazi u 
okviru veb forme, takođe će biti prikazan na veb stranici. 
Korisnik može, ukoliko prikazani rezultat pokazuje neobično 
veliku 
sličnost 
učitanog 
dokumenta 
sa 
već 
postojećim 
dokumentima 
u 
korpusu, 
odabrati 
drugi 
stepen 
provere, 
manuelni mod rada. Ovaj mod prikazuje na veb stranici 
sporni 
dokument 
naspram 
najpribližnijeg 
dokumenta 
iz 
korpusa, označavajući različitim koloritima delove koji su 
isti i koji su različiti. Za generisanje ovog uporednog prikaza 
korišćena 
je 
PHP 
klasa 
otvorenog 
koda 
CompareFiles 
autora R. Chauchan [17]. 
VI.
Z
AKLJUČCI I 
P
RAVCI 
D
ALJIH 
I
STRAŽIVANJA
Premda mu to nije prevashodna namena, GENSIM radni 
okvir se može uspešno primeniti i u detekciji plagijata, i to 
pokazuje ovde opisana implementacija putem SIMSERVER, 
iako na 
veoma pojednostavljnim skupovima podataka. U 
toku daljeg istraživanja primenjivosti ove tehnike, bilo bi 
potrebno 
napisati 
u 
Python 
umesto 
u 
PHP 
skripte 
za 
pripremu datoteka, jer bi to omogućilo bolju integrisanost sa 
već postojećim alatima. Takođe, trebalo bi detaljno izučiti i 
što više integrisati već postojeći GENSIM API, koji je veoma 
ekstenzivan. 
Autori 
ovog 
rada 
su 
posredstvom 
interneta 
stupili u komunikaciju kako sa autorom GENSIM tako i sa 
drugim autorima koji su uspešno implementirali taj radni 
okvir (Plone, CollectiveSimserver) [18] i od njih su dobili 
savet 
da 
je 
zarad 
postizanja 
punog 
efekta 
ovog 
rešenja 
potrebno 
napraviti 
početni 
korpus 
od 
barem 
20.000 
dokumenata. 
Pun 
efekat 
se 
očekuje 
na 
korpusu 
od 
oko 
100.000 
dokumenata. 
Usled 
toga, 
kao 
pravac 
daljeg 
istraživanja takođe predlažemo i instaliranje opisanog rešenja 
na zasebnom računarskom sistemu koji bi služio samo u te 
svrhe 
i 
koji 
bi 
bio 
svojevrsna 
digitalna 
biblioteka 
Univerziteta, 
kojoj 
bi 
onda 
putem 
odgovarajućeg 
veb 
interfejsa mogli pristupati zainteresovani korisnici, gde bi taj 
pristup bio sličan načinu na kojem su realizovana već dobo 
poznata komercijalna rešenja. 
L
ITERATURA
[1]
Prensky, 
M., 
"Digital 
Natives, 
Digital 
Immigrants: 
from 
On 
the 
Horizon", MCB Univesity Press, Vol. 9 No. 5, October 2001 
[2]
Bull, J., Collins, C., Coughlin, E., Sharp, D., "Technical Review of 
Plagiarism Detection Software Report", JISC, Univesity of Luton, 2001 
[3]
Group 
of 
Authors, 
"The 
Plagiarism 
Spectrum", 
White 
paper, 
iParadigms, Turnitin, 2013 
[4]
"Cosine Similiarity", Wikipedia, 29.05.2013 
http://en.wikipedia.org/wiki/Cosine_similarity, 
[5]
Rehurek, R., Sojka, P., "Software Framework for Topic Modelling with 
Large 
Corpora", 
Natural 
Language 
Processing 
Laboratory, 
Masaryk 
University, Faculty of Informatics, Brno, Czech republic, 2010 
[6]
"Bag-of-words-model", Wikipedia, 29.05.2013 
http://en.wikipedia.org/wiki/Bag-of-words_model, 
[7]
Sivic, Josef (April, 2009). "Efﬁcient visual search of videos cast as text 
retrieval". 
IEEE 
Transactions 
on 
pattern 
analysis 
and 
machine 
intelligence, VOL. 31, NO. 4. IEEE. pp. 591–605 
[8]
"Tf-idf", Wikipedia, 29.05.2013, http://en.wikipedia.org/wiki/Tf-idf 
[9]
"Tf-idf weighting", Infinova, 29.05.2013 
http://infinova.wordpress.com/2010/01/26/tfidf-weighting/ 
[10]
Giunchiglia, F., Kharkevich, U., Zaihrayeu, I., "Concept Search", In 
Proceedings of European Semantic Web Conference, 2009 
[11]
"Concept Search", Wikipedia, 29.05.2013. 
http://en.wikipedia.org/wiki/Concept_Search 
[12]
"Latent Semantic Indexing", Wikipedia, 
http://en.wikipedia.org/wiki/Latent_semantic_indexing 
[13]
Deerwester, S., et al, "Improving information retrieval with Latent 
Semantic Indexing", Proceedings of the 51st Annual Meeting of the 
American Society for Information Science 25, 1988, pp. 36–40 
[14]
Hurek, R., "Document Similiarity Server", 29.05.2013 
http://radimrehurek.com/gensim/simserver.html 
[15]
Hurek, R., "Gensim – Topic modeling for humans", 29.05.2013 
http://radimrehurek.com/gensim/ 
[16]
Bulatovic, N., Dević, D., Stanar, N., Orlić, D., Implementacija GENSIM 
u cilju detekcije plagijarizma, praktičan rad, FTN, Novi Sad, 2012 
http://ftniim.org/filecompare/ 
[17]
Chauhan, R., "FileCompare", PHP klasa, PHPClasses.org, 2012, 
http://www.phpclasses.org/browse/file/9696.html 
[18]
Ledremann, C., "Semantic Indexing for Plone Project", Plone Collective 
Simserver, 29.05.2013, http://plone.org/products/collective.simserver 
