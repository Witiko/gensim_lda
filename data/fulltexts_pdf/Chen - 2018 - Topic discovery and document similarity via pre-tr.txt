IN 
DEGREE PROJECT 
INFORMATION AND COMMUNICATION 
TECHNOLOGY,
SECOND CYCLE, 30 CREDITS
, 
STOCKHOLM SWEDEN 2018
Topic discovery and document 
similarity via pre-trained word 
embeddings
SIMIN CHEN
KTH ROYAL INSTITUTE OF TECHNOLOGY
SCHOOL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE
Abstract
Throughout the history,
humans continue to generate an
ever-growing volume of
documents about a wide range of
topics.
We now rely on computer programs to automat-
ically process these vast collections of
documents in vari-
ous applications.
Many applications require a quantitative
measure of
the document similarity.
Traditional
methods
first learn a vector representation for each document using
a large corpus, and then compute the distance between two
document vectors as the document similarity.
In contrast to this corpus-based approach,
we propose
a straightforward model
that directly discovers the topics
of a document by clustering its words, without the need of
a corpus.
We define a vector representation called normal-
ized bag-of-topic-embeddings (nBTE) to encapsulate these
discovered topics and compute the soft cosine similarity be-
tween two nBTE vectors as the document similarity.
In
addition,
we propose a logistic word importance function
that assigns words different importance weights based on
their relative discriminating power.
Our model is efficient in terms of the average time com-
plexity.
The nBTE representation is also interpretable as
it allows for topic discovery of
the document.
On three
labeled public data sets,
our model
achieved comparable
k-nearest neighbor classification accuracy with five state-
of-art baseline models.
Furthermore, from these three data
sets, we derived four multi-topic data sets where each label
refers to a set of
topics.
Our model
consistently outper-
forms the state-of-art baseline models by a large margin on
these four challenging multi-topic data sets.
These works
together provide answers to the research question of
this
thesis:
Can we construct an interpretable document represen-
tation by clustering the words in a document, and effectively
and efficiently estimate the document similarity?
Keywords
Document similarity;
document representation;
word embedding;
natural
lan-
guage processing; topic modeling;
Referat
Under hela historien fortsätter människor att skapa en väx-
ande mängd dokument om ett brett spektrum av publika-
tioner.
Vi
förlitar oss nu på dataprogram för att automa-
tiskt bearbeta dessa stora samlingar av dokument i
olika
applikationer.
Många applikationer kräver en kvantitativ-
mått av dokumentets likhet. Traditionella metoder först lä-
ra en vektorrepresentation för varje dokument med hjälp
av en stor corpus och beräkna sedan avståndet mellan two
document vektorer som dokumentets likhet.
Till
skillnad från detta corpusbaserade tillvägagångs-
sätt,
föreslår vi
en rak modell
som direkt upptäcker äm-
nena i ett dokument genom att klustra sina ord , utan be-
hov av en corpus. Vi definierar en vektorrepresentation som
kallas normalized bag-of-topic-embeddings (nBTE) för att
inkapsla de upptäckta ämnena och beräkna den mjuka co-
sinuslikheten mellan två nBTE-vektorer som dokumentets
likhet.
Dessutom föreslår vi
en logistisk ordbetydelsefunk-
tion som tilldelar ord olika viktvikter baserat på relativ
diskriminerande kraft.
Vår modell är effektiv när det gäller den genomsnittliga
tidskomplexiteten.
nBTE-representationen är
också tolk-
bar som möjliggör ämnesidentifiering av dokumentet.
På
tremärkta offentliga dataset uppnådde vår modell
jämför-
bar närmaste grannklassningsnoggrannhet med fem topp-
moderna modeller. Vidare härledde vi från de tre dataseten
fyra multi-ämnesdatasatser där varje etikett hänvisar till
en uppsättning ämnen.
Vår modell
överensstämmer över-
ens med de högteknologiska baslinjemodellerna med en stor
marginal av fyra utmanande multi-ämnesdatasatser. Dessa
arbetsstöd ger svar på forskningsproblemet av tisthesis:
Kan vi
konstruera en tolkbar dokumentrepresentation
genom att
klustra orden i
ett
dokument
och effektivt
och
effektivt uppskatta dokumentets likhet?
Acknowledgments
I would like to first express my gratitude to my supervisor Prof.
Sarunas Girdzi-
jauskas for his guidance and help throughout this research.
It was his full support
that enabled me to pursue my research interest and carry out the work of the chosen
research topic.
His insightful
feedback was invaluable to the achievements of this
research.
I would like to thank my examiner Prof.
Henrik Boström for his detailed feed-
back on this thesis.
His remarks not only made it possible to take the quality of
this thesis to a higher level but also helped me become a better researcher.
Finally, I would like to thank my family for supporting me in everything I need
during my persuasion of the master’s degree.
Contents
1
Introduction
1
1.1
Background .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1.1.1
Document representation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1.1.2
Word embeddings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1.1.3
From word embedding to document similarity .
.
.
.
.
.
.
.
.
3
1.1.4
Distance metrics for document similarity .
.
.
.
.
.
.
.
.
.
.
.
4
1.2
Problem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
1.2.1
A word clustering strategy .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
1.3
Purpose
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
1.4
Goal
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
1.5
Benefits, ethics, and sustainability
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
1.6
Methodology
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
1.7
Delimitation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
1.8
Outline
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
2
Extended background
9
2.1
Natural language processing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
2.2
Document representation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
2.2.1
Bag-of-word .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
2.2.2
Term-frequency inverse-term-frequency .
.
.
.
.
.
.
.
.
.
.
.
.
10
2.2.3
Latent Semantic Indexing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
2.2.4
Latent Dirichlet Allocation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
2.2.5
Common document similarity measures
.
.
.
.
.
.
.
.
.
.
.
.
15
2.2.6
Summary
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
2.3
Word embedding
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
2.3.1
Word2vec
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
2.4
Soft cosine similarity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
2.5
Word Mover’s Distance
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19
2.5.1
Word centroid distance
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
2.5.2
k-nearest neighbor classification .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
2.6
Clustering analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
2.6.1
k-means clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
2.6.2
Spectral clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
CONTENTS
2.6.3
Internal cluster validation criteria .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
3
Methodology
25
3.1
Research methods
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
25
3.2
Data collection
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
3.3
Data analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
3.3.1
Verifying the word clustering assumption
.
.
.
.
.
.
.
.
.
.
.
28
3.3.2
Evaluating the effectiveness
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
3.3.3
Evaluating the efficiency and interpretability with the algo-
rithms
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
3.3.4
Evaluating the interpretability with an experiment
.
.
.
.
.
.
29
3.4
Model design
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
3.5
Experimental design
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30
3.5.1
Experiment 1:
verifying the assumption .
.
.
.
.
.
.
.
.
.
.
.
30
3.5.2
Experiment 2:
effectiveness of the model .
.
.
.
.
.
.
.
.
.
.
.
35
3.5.3
Experiment 3:
interpretability of the document representation
37
4
Verifying the word clustering assumption
41
4.1
A simple synthetic document
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
4.2
A complex synthetic document
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
4.3
A short real-world document
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
4.4
A long real-world document
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45
4.5
Discussion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
48
5
The proposed model
51
5.1
Logistic word importance
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
51
5.1.1
Word cluster importance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
53
5.2
Topic modeling with word embeddings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
54
5.2.1
Notations
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
55
5.2.2
Building the nBTE representation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
56
5.2.3
A view of data compression .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
57
5.3
Soft cosine document similarity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
58
6
Performance evaluation
59
6.1
Efficiency
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
59
6.2
Effectiveness
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60
6.2.1
Overall results
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60
6.2.2
Comparing the baseline models with our model
.
.
.
.
.
.
.
.
60
6.2.3
Comparing WMD with the soft cosine similarity
.
.
.
.
.
.
.
62
6.3
Interpretability .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
64
6.3.1
Comparing with LDA .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
64
6.3.2
More sample documents
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
65
7
Conclusion
69
CONTENTS
7.1
Discussion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70
7.1.1
Word clusters in the word2vec vector space
.
.
.
.
.
.
.
.
.
.
70
7.1.2
Effective similarity measure
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71
7.1.3
Efficient similarity estimation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71
7.1.4
Interpretable document representation .
.
.
.
.
.
.
.
.
.
.
.
.
71
7.2
Drawbacks and future work .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
72
Bibliography
75
Appendices
80
A Sample documents
81
A.1
Sample document from BBCNews data set
.
.
.
.
.
.
.
.
.
.
.
.
.
.
81
A.2
Sample document from BBCSport data set
.
.
.
.
.
.
.
.
.
.
.
.
.
.
81
A.3
Sample document from news20group data set
.
.
.
.
.
.
.
.
.
.
.
.
.
82
B Used software
85
Chapter 1
Introduction
Written language enables humans to store complex information and communicate
with each other across time and space.
The ancient Chinese threw tortoise shells
into fire, and read the crack signs on the shell in order to tell the fortune [29].
We
could regard the tortoise shells as the earliest form of
Chinese documents,
which
talk about fortune.
Throughout the history, humans continue to generate an ever-
growing volume of
documents,
not only about the fortune,
but also our joys and
sufferings,
every destroyed and flourishing civilization,
every magnificent technol-
ogy,
thousands of
splendid ideology and religions,
our theory about the universe,
competing economic doctrines and more.
With the advancement of information technology, these documents are digitized
and maintained in large data centers and personal
devices with an unprecedented
speed.
Due to the fast increasing volume of documents, manual handling of various
document-related tasks is no longer feasible.
We now rely on computer programs
to automatically process the vast collections of documents in various applications.
Many of the applications involve comparing the similarity between documents.
A
common information retrieval
(IR) task is,
given a specific document,
querying a
ranked list of documents that are most similar to it.
This requires a quantitative
measure of
the document similarity.
Such measure is also useful
for other tasks
like document classification and clustering, where a natural strategy is to assign the
same label to the similar documents [2].
Due to its importance in real-life applications, document similarity is one central
research theme in the field of information retrieval and natural language processing
[6].
Researchers in these fields have proposed various models for document repre-
sentation and document similarity which have been successfully applied to tasks
like document retrieval
and document classification.
These models typically first
learn a vector representation for each document using a large corpus of documents
[31], and then measure the distance between two document vectors as the document
similarity.
Recently,
significant progress has been made on word representation models
[32][38].
These models are able to learn high-quality vector representation for mil-
1
2
CHAPTER 1.
INTRODUCTION
lions of
words using a corpus of
hundreds of
billions of
words [13].
These word
vectors are shown to accurately capture the similarity between words and are as-
sociated with abstract concepts [32][33].
Researchers have also made these learned
word vectors publicly accessible [13][21][11].
This sheds new light on a bottom-up
approach for document similarity:
we could directly infer the topics of documents
and compare the similarity using the publicly available word vectors,
without the
need of an external corpus.
This is intuitive since a document is constructed from
words.
If we understand the individual
words well,
we should also be able to un-
derstand a document without referencing to other documents.
In this thesis, we further investigate the feasibility of this bottom-up approach.
We aim to propose a new document representation model
that is useful
for topic
discovery and document similarity,
requiring only the publicly available word vec-
tors.
1.1
Background
In order to quantitatively measure the document similarity, we need 1) an appropri-
ate representation of documents,
and 2) a suitable distance metric.
Both of these
two ingredients are important for an effective similarity measure.
On one hand, the
representation of documents should capture as much information as possible.
On
the other hand, the distance metric should be meaningful for the chosen represen-
tation.
1.1.1
Document representation
Traditional
IR methods like BOW and TF-IDF,
as discussed in section 2.2.1 and
2.2.2, transform a document into a sparse vector representation whose components
refer to unique words [43].
In the past decades, machine learning researchers have
proposed more advanced models to learn dense vector representation of documents.
Two notable models are Latent Semantic Indexing (LSI) [7]
and Latent Dirichlet
Allocation (LDA) [4].
LSI achieves significant vector compression by eigendecom-
posing the word-document matrix.
Deerwester et al.
showed that the resulting
latent vector space leads to improved performance in the document retrieval
task
[7].
In contrast, LDA takes a probabilistic approach to modeling documents [4].
In
the LDA model, the words are probabilistically grouped into topics and a document
is modeled as a distribution over the underlying set of topics.
All these models transform the documents into vectors using a corpus.
We could
then compute the cosine similarity between two document vectors as the document
similarity [2].
1.1.2
Word embeddings
In recent years,
significant progress has been made on the problem of word repre-
sentation [32][38].
The celebrated Word2vec model [32] can efficiently learn a dense
1.1.
BACKGROUND
3
Figure 1.1:
Figure taken from [25].
An illustration of the word mover’s distance.
All non-stop words (bold) of both documents are embedded into a word2vec space.
The distance between the two documents is the minimum cumulative distance that
all words in document 1 need to travel to exactly match document 2.
vector representation of real values for each word.
Such vectors are also called word
embeddings.
Mikolov et al.
demonstrated that both syntactic and semantic relation-
ships between words are preserved in the word2vec vector space [32].
Semantically
similar words tend to have similar embeddings and such similarity is quantified us-
ing the cosine similarity.
A famous example that demonstrates the power of
the
word2vec embeddings is that,
vector
(”
king
”)
− vector
(”
man
”) +
vector
(”
woman
”)
results in a vector that is closest to
vector
(”
queen
”) in terms of
cosine similarity
[32].
This shows that the word2vec embeddings can not only capture the semantic
similarity between words but also associate each word with some abstract concept.
Besides its encouraging performance, the word2vec model is also efficient to train
[32][34].
Google has published its pre-trained word2vec embeddings for millions of
words using a corpus of up to hundreds of billions of words [13].
1.1.3
From word embedding to document similarity
Kusner et al.
made a successful
attempt at using the pre-trained Google word
embeddings to estimate the document similarity.
They proposed the Word Mover’s
Distance(WMD) [25], which measures the similarity between two documents as the
minimum amount of distance that the embedded words of one document need to
“travel” to reach the embedded words of another document.
An illustration of the
WMD distance can be found in Figure 1.1.
WMD models a document as a weighted cloud of its embedded words, therefore
requiring only the pre-trained word embeddings.
Kusner et al.
showed that WMD
achieved unprecedented low k-nearest neighbor document classification error rate
comparing to a set of state-of-art baseline models including LSI and LDA [25].
This
result suggests that it is not only feasible but also effective to compare document
similarity using only the pre-trained word embeddings.
4
CHAPTER 1.
INTRODUCTION
1.1.4
Distance metrics for document similarity
Kusner et al.
used the Earth Mover’s Distance(EMD) to estimate the distance be-
tween two weighted clouds of words.
The best average time complexity of solving
EMD is
O
(
n
3
log
n
), where
n
is the number of unique words in the respective doc-
uments [25].
However, there are other applicable distance metrics [44][19].
Grigori
Sidorov et al.
proposed the soft cosine similarity[44] of time complexity
O
(
n
2
) for
measuring document similarity.
The soft cosine similarity assumes that the simi-
larity between word features is known, which holds true for the word embeddings.
Grigori Sidorov et al.
showed that the soft cosine similarity is an effective distance
metric in a document retrieval case study, where they used the Levenshtein distance
to estimate word similarity [44].
1.2
Problem
The best average time complexity of WMD is
O
(
n
3
log
n
) [25].
The computational
overhead could become a significant issue considering that a document could contain
hundreds to thousands of words.
Another drawback of WMD is that the resulting
document representation has poor interpretability comparing to LDA.
The com-
ponents of the LDA vector representation refer to different topics and the entries
their weight,
therefore allowing us to interpret it as a mixture of different topics.
However, WMD simply models a document as a cloud of all its words.
It is hard to
interpret the resulting document representation to gain any structured information
about the content of the document.
1.2.1
A word clustering strategy
A natural
strategy to improve the time complexity of WMD is to use less embed-
ded vectors to represent a document.
This is feasible considering that there are
similar words that express the same contextual
meaning in a document,
e.g.
syn-
onyms("country", "nation", "state"), variants of the same root word("denote, deno-
tation, denoting") and words that describe the same topic ("elections", "parliament",
"politics").
If we could find such word clusters in a document,
we could then rep-
resent each cluster using a single embedded vector that is most representative of
the member words.
Therefore, we could represent a document as a weighted cloud
of
k
such vectors,
where
k
is the number of word clusters.
When
k
is significantly
smaller than
n
, e.g.
k
=
O
(
2
√
n
), the time complexity of WMD could be improved
to
O
(
n
1
.
5
log
n
),
which is sub-quadratic.
If we use the soft cosine similarity to es-
timate the similarity between two clouds of embedded words, the time complexity
can be further improved to
O
(
n
).
However,
we note that it is not trivial
to find
such semantically coherent word clusters.
Therefore the overall complexity depends
on the algorithms for finding these word clusters.
Another benefit of this word clustering approach is topic discovery.
We could
examine the topic of each cluster by querying its member words.
The normalized
1.3.
PURPOSE
5
size of each cluster may also be a naive estimation of its contribution to the con-
tent of the document
1
.
In this sense, we could produce an interpretable document
representation.
1.3
Purpose
To the best of our knowledge, the aforementioned word clustering strategy for doc-
ument representation and document similarity is not studied in any literature.
We
consider it worthy of further investigation due to its potential benefits in terms of
time complexity and interpretability.
This leads to our main research question:
Can we construct an interpretable document representation by clus-
tering the words in a document, and effectively and efficiently esti-
mate the document similarity?
The above research question is concerned with three aspects, namely 1) effectiveness,
2) efficiency and 3) interpretability.
To be more specific,
1.
effectiveness means the performance for document similarity is comparable
with the state-of-art models
2
.
2.
efficiency means the time complexity of the overall model for document sim-
ilarity should be
O
(
n
2
log
n
) so that it is an order of
exponent better than
WMD.
3.
interpretability means that the resulting document representation can be in-
terpreted as a mixture of multiple topics, each weighted by its contribution to
the content of the document.
Note that the term interpretability itself can be interpreted differently and may
concern many different aspects, e.g.
model interpretability.
In this thesis, we focus
on the effectiveness and efficiency aspects and only informally address the inter-
pretability aspect by limiting it to the ability of topic discovery of our model.
The answer to this research question will advance the field of document represen-
tation and enable further research of document understanding from words, without
the need of a corpus.
1.4
Goal
The goal of this thesis is in line with the purpose discussed in the previous section.
The purpose of this thesis is centered around the raised research question.
To be
able to fully answer the research question, we need to achieve the following goals,
1
We shall see in Chapter 4 that the normalized size of a cluster is not an appropriate estimation
of its contribution.
2
The selected state-of-art baseline models are listed in section 3.5.2.
6
CHAPTER 1.
INTRODUCTION
1.
propose a document representation model that is based on the word clustering
strategy, requiring only the pre-trained word embeddings but not a corpus.
2.
propose a similarity measure for the resulting document representation.
3.
Analyze and evaluate the proposed model
and similarity measure with re-
spect to the three concerned aspects, namely the efficiency, effectiveness, and
interpretability.
In this work,
we focus on the efficiency and effectiveness aspects,
while only
informally address the interpretability aspect.
To be able to fully address the inter-
pretability aspect,
we need a more sophisticated definition of
the interpretability.
The evaluation of the interpretability is also challenging since there lacks a consensus
on what interpretability implies.
Instead, we informally address the interpretability
aspect by analyzing the algorithmic steps of the proposed model in order to explain
how we can interpret the resulting document representation as a mixture of
top-
ics.
We also demonstrate its ability of topic discovery on one sample document
3
,
compared with LDA.
To address the efficiency aspect,
we analyze the average time complexity of
the proposed model
and document similarity measure.
To address the effective-
ness aspect, we evaluate the performance of the document similarity measure using
multiple public data sets, compared with a set of state-of-art baseline models.
1.5
Benefits, ethics, and sustainability
The potential benefit of our work is significant to many document-related applica-
tions.
Humans rely on documents to maintain our knowledge base about our world.
Take the academia for example, we continue to use papers to record every scientific
breakthrough.
Due to the ever-increasing volume of our knowledge base, we rely on
smart algorithms to retrieve desired information.
The effectiveness and efficiency of
the algorithms directly impact the productivity of human society as a whole:
the
less time we spend on retrieving the desired information, the more time we have to
spend on producing activities.
We could also take the perspective of a machine to view the potential
benefit.
With our proposed method,
machines are able to understand the documents in
absence of
an external
corpus.
This enables the machines to undertake a larger
set of document processing tasks where such corpus is often not available.
On the
other hand,
the knowledge of words learned by one machine can be transferred to
many other machines.
This makes the use of
the computational
resources much
more sustainable.
This work also indirectly addresses one central
ethical
issue of artificial
intelli-
gence(AI), that the AIs become biased when trained on biased data sets [5].
In the
3
We select the sample document which was used by Blei
et al.
to demonstrate the ability of
topic discovery of LDA [4].
1.6.
METHODOLOGY
7
context of
document understanding,
many models require either labeled training
data or an external
corpus.
These model
can be easily abused by the user if
he
chose to supply biased training data or corpus.
However,
our model
requires only
the pre-trained word embeddings.
The AI community could agree on one central
unbiased corpus with which the word embeddings are trained, e.g.
the corpus used
by Google [13],
Facebook [11]
and Stanford [21].
Therefore effectively minimizing
the risk of biased usage of our model.
1.6
Methodology
To the best of our knowledge,
there is no rigorous probabilistic explanation of the
word2vec vector space [12][27].
The most comprehensible study of
the statistical
implication of
the word2vec model
we found is from Levy et al.[27],
where they
argued that the word2vec model
is implicitly performing a matrix decomposition
on a word- cooccurrence statistics matrix.
However, there is still no formal proba-
bilistic model proposed for the word2vec embeddings.
Therefore a rigorous analytic
approach is not applicable.
Instead, the conclusions of this thesis will be based on a
scientific study using an empirical
and experimental
approach to achieve scientific
validity.
The word clustering strategy discussed in section 1.2.1 is based on the assump-
tion that semantically similar words form clusters in the word2vec vector space.
To
verify this assumption,
we conduct an exploratory experiment on both synthetic
and real-world documents.
This exploratory experiment aims to generate empirical
evidence that supports the word clustering assumption,
and also recommends for
the detailed design of
the proposed model.
We qualitatively evaluate the seman-
tic coherence of
the word clusters by intuitively examining their member words.
We also quantitatively evaluate the word clusters using several
cluster validation
criteria.
Based on the results of this exploratory experiment, we propose 1) a document
representation model
that is able to discover the topics of
a document,
and 2)
a similarity measure for the resulting document representation.
We present the
detailed algorithmic steps of 1) and 2) so that we can argue their efficiency in terms
of the average time complexity,
and the interpretability by explaining how we can
reveal
the topics of a document using the resulting document representation.
We
also qualitatively evaluate the discovered topics on several
sample documents to
demonstrate the interpretability of the resulting document representation in terms
of topic discovery.
We quantitatively evaluate the effectiveness of the proposed model for document
similarity in the context of the k-nearest neighbor document classification task, as
proposed by Kusner et al.
[25].
The k-nearest neighbor classification algorithm
determines the label
of
an unseen document
d
as the majority label
among its
closest neighbors.
If
the document similarity measure is indeed effective,
these
closest neighbors would be similar to
d
.
Therefore the classification error rate would
8
CHAPTER 1.
INTRODUCTION
also be low.
We compare the performance of our model
with a set of state-of-art
baseline models to argue its relative effectiveness.
We use publicly available data
sets so that the experimental results can be reproduced.
Since the topic discovery
is an important feature of
our model,
we selected three public data sets that are
suitable for topic discovery.
The details of the research methodology can be found in chapter 3.
1.7
Delimitation
In this thesis,
we only experiment with the pre-trained Google word embeddings.
Other pre-trained word embeddings are not considered.
This means that the per-
formance of
our model
will
be different when other pre-trained word embeddings
are used.
We only informally address the interpretability aspect of our model since there
lacks a consensus of what interpretability is, so does the evaluation method.
1.8
Outline
This thesis is organized as follows.
In Chapter 2, we introduce the relevant theory
and related work.
In Chapter 3, we present the details of the research methodology.
In Chapter 4,
we present and discuss the results of the first experiment.
We then
propose a document representation model
and a document similarity measure in
Chapter 5.
In Chapter 6, we present the evaluation results of the second experiment.
In Chapter 7, we discuss these evaluation results and conclude this thesis by directly
answering the raised research question.
We also propose several valid future works
in Chapter 7.
Chapter 2
Extended background
This chapter introduces the extended background of our research.
Document similarity is tightly connected with the problem of document repre-
sentation, which is a central research topic in the area of natural language process-
ing.
Therefore we first briefly introduce the area of
natural
language processing,
with a focus on the problem of
document representation.
We then introduce the
word2vec model
with a focus on its training objective.
After that,
we introduce
two document similarity measures that may be suitable for our model.
Finally, we
introduce the clustering analysis,
a task of
grouping similar objects into clusters.
We cover a number of
clustering algorithms that may be suitable for discovering
the word clusters in the word2vec vector space.
2.1
Natural
language processing
Natural language processing (NLP), a.k.a.
computational linguistics, is the sub-field
of computer science concerned with using computational techniques to understand
and reproduce human language content [16].
The earliest research of NLP may trace
back to 1950s when the idea of
automatic machine translation attracted research
attention [18].
The early NLP systems were mostly based on complex sets of hand-
written rules and not able to deal with variation, ambiguity, and context-dependent
interpretation of the language.
Starting in the late 1980s, with the increasing com-
putational power, more and more researchers started applying a variety of statistical
methods to automatically learn the abstract rules using a large corpus [16].
This
direction of research is also known as statistical NLP (SNLP).
At the core of
any NLP task is the important issue of
natural
language un-
derstanding [6].
The raw text needs to be transformed into an appropriate rep-
resentation that can be processed by the computer programs.
In this section,
we
extensively discuss the notable methods for document representation following a
chronological order.
9
10
CHAPTER 2.
EXTENDED BACKGROUND
2.2
Document representation
Researchers in the field of information retrievals(IR) have made significant progress
on the problem of document representation [2].
The basic paradigm proposed by IR
researchers is to learn a vector representation for the documents using an external
corpus [4].
Starting from the simplest Bag-of-word model,
we gradually discuss
more complex models for document representation.
2.2.1
Bag-of-word
The bag-of-words(BOW) model represents each document as a multi-set of words,
disregarding the grammar and ordering [43],
but preserving the number of occur-
rences of each word.
Take the following document for example,
Bob transferred eight bitcoins to Alice,
and Alice transferred back
five bitcoins.
its BOW multi-set representation is,
{Bob
2
, Alice
2
,transferred
2
, bitcoins
2
, eight, five, to, and, back}
where the upper indices equal
to the multiplicity of the respective word.
We can
then build a BOW vector of real values where each dimension refers to a unique word
w
and the entry refers to the frequency of
w
in the document.
The BOW vector
of the above document is (2
,
2
,
2
,
2
,
1
,
1
,
1
,
1
,
1),
assuming an ordered list of unique
words that match the components of the BOW vector.
It is common to normalize
the BOW vector by dividing each entry by the total
frequency.
The normalized
BOW vector of the above sentence is then (
2
13
,
2
13
,
2
13
,
2
13
,
1
13
,
1
13
,
1
13
,
1
13
,
1
13
).
The BOW model takes into account the word occurrences in a document, which
to some extent reflects the content of a document.
Intuitively thinking,
two doc-
uments with similar word occurrences should also be similar in content.
However,
in the BOW model,
all
words are equally important.
This is clearly not realistic
considering that, in certain domains, words like "economics" and "monetary" should
have much higher discriminating power than words like "therefore" and "although".
A remedy to this problem is weighting each word by its term-frequency inverse-
term-frequency, a method which we shall discuss in the next section.
2.2.2
Term-frequency inverse-term-frequency
The term-frequency-inverse-document-frequency(TF-IDF) method [43](1983) is a
step forward from BOW.
It assumes to have access to a corpus
D
consisting of
a collection of
documents.
It modifies the BOW vector by offsetting the word
frequency by the inverse document frequency,
which measures the discriminating
power of a word.
2.2.
DOCUMENT REPRESENTATION
11
Term-frequency
The term frequency
tf
(
w, d
) is defined as the frequency of word
w
in a document
instance
d
.
It is a measure of the contribution of a word to a document.
tf
(
t, d
) = frequency of
t
in
d
(2.1)
Inverse document frequency
The inverse term frequency is defined as,
idf
(
t, d
) = log
|D|
+ 1
|{d ∈ D
:
t ∈ d}
+ 1
|
(2.2)
The inverse term frequency is a measure of how much information a word pro-
vides.
A low inverse document frequency indicates that a word is rare and therefore
likely being more discriminating.
In contrast,
a high inverse document frequency
indicates that the word is common across the corpus,
therefore carries little dis-
criminating power.
TF-IDF
Combining the term frequency and inverse document frequency, we can produce a
composite weight for each term in a document, defined as,
tf idf
(
t, d, D
) =
tf
(
t, d
)
× idf
(
t, d, D
)
(2.3)
By its definition, the TF-IDF score of a word
w
in a document
d
is highest if
w
occurs rather frequently in
d
while occurring in only few documents in
D
.
In this
way, discriminating words are considered to be more important.
Term-document matrix
Let
R
be a dictionary consisting of all
the unique words in
D
.
We can then build
a term-document matrix X
∈ R
|R|×|D|
where the
j
th
column is the TF-IDF vector
representation of document
D
i
and the
i
th
row is the TF-IDF score of the
i
th
word
in all
the
|D|
documents.
In this way,
a document
d
of
arbitrary length can be
represented as a fixed-length
|R|
-dimensional
vector where
i
th
dimension refers to
the
i
th
word in dictionary
R
and the entry refers to the TF-IDF score of
the
i
th
word in
d
.
Comparing to BOW vector, TF-IDF is better at identifying the discriminating
words by incorporating the word-occurrence statistics in a corpus.
However, 1), the
TF-IDF vectors are rather sparse since its dimension equals the vocabulary size.
Such vector could easily reach hundreds of thousands of entries with a sufficiently
large corpus.
2),
It also reveals little intra and inter-document structure [4].
Syn-
onymous terms are regarded as totally different words even though they have the
same meaning, while polysemous terms are regarded as words expressing the same
12
CHAPTER 2.
EXTENDED BACKGROUND
meaning 3), Besides, it is often not suitable for document similarity measure due to
their frequent near-orthogonality [25].
The near-orthogonality of
TF-IDF vectors
means that a given querying TF-IDF vector may be far away from all other vectors
in terms of cosine similarity, therefore producing poor document similarity scores.
As we shall
discuss in the next section,
Latent Semantic Indexing model
tries
to address the aforementioned issues by constructing a latent vector space for the
documents and words.
2.2.3
Latent Semantic Indexing
The Latent Semantic Indexing model
(LSI) [7](1990),
sometimes referred to as
Latent Semantic Analysis (LSA),
finds a lower-rank approximation to the term-
document matrix
M ∈ R
v×d
using Singular Vector Decomposition (SVD). Without
diving into the details of the linear algebras, LSI finds a lower-rank approximation
C
0
that can be decomposed into three matrices,
C
0
=
U
Σ
V
(2.4)
where
U
has dimensions (
v × k
), Σ(
k × k
) and
V
(
k × d
).
We can then construct a new matrix
W
as
W
=
U
Σ
(2.5)
with dimension (
v × k
), where each row refers to a word vector, and a new matrix
D
as
D
= Σ
V
(2.6)
with dimension (
k × d
), where each column refers to a document vector.
The end result of
SVD is that all
documents and words are represented as
vectors of
reduced dimension of
k
.
In this way,
LSI is able to achieve significant
vector compression.
Deerwester et al.
also showed that the resulting latent vector
space of documents reveals better inter-document structure and the latent vector
space of words captures some linguistic notions such as synonymy and polysemy [7].
The LSI model
uses linear algebra techniques to achieve significant dimension
reduction,
with the hope that the resulting latent vectors can capture semantic
similarity.
Even though LSI has shown to be successful, it still lacks a solid theoret-
ical
foundation,
therefore hinders a more principled study of the model
itself [17].
To address this issue, researchers have proposed to use a probabilistic approach to
model
the latent topics of
a document,
which has a sound statistical
foundation
[17][4].
We shall discuss one such notable generative probabilistic model in the next
section.
2.2.4
Latent Dirichlet Allocation
The Latent Dirichlet Allocation(LDA) [4](2003) takes a probabilistic approach to
modeling documents.
The words are probabilistically grouped into a set of topics
T
2.2.
DOCUMENT REPRESENTATION
13
of size
k
and a document is modeled as a distribution over an underlying set of topics.
Similarly to LSI and TF-IDF, LDA assumes to have access to a corpus
D
consisting
of a collection of documents and a dictionary
R
consisting of all unique words in
D
.
LDA also assumes that the document is in the BOW multi-set representation [4],
therefore disregarding the grammar and word order in a document.
In the language
of probability theory,
this is the assumption of exchangeability for the words in a
document [1].
Before introducing the details of the generative process for a document, we first
give definition to several probability distributions.
Poisson distribution:
The Poisson distribution is a discrete probability distribution parameterized by
a positive value
λ
.
The probability mass function is given by
f
(
x|λ
) =
λ
x
exp
−λ
x
!
(2.7)
Dirichlet distribution:
The Dirichlet distribution Dir(
α
) is a family of continuous multivariate proba-
bility distribution parameterized by a vector
α
= (
α
i
, ...α
k
) of positive values.
The
order of
a Dirichlet distribution is
|α|
=
k
.
The probability density function of
Dir(
α
) of order
k
is given by:
f
(
x
1
, ...x
k
|α
1
, ...α
k
) =
1
B(
α
)
k
Y
i
=1
x
α
i
−
1
i
(2.8)
where
B(
α
) =
Q
k
i
=1
Γ(
α
i
)
Γ(
P
k
i
=1
α
i
)
(2.9)
and
k
X
i
x
i
= 1
∧ x
i
≥
0
, ∀i ∈
[1
, k
]
(2.10)
Multinomial distribution:
The Multinomial distribution is a generalization of binomial distribution parametrized
by a vector
θ
= (
θ
1
, ...θ
k
).
The probability mass function of Multinomial(
θ
) of order
k
is given by:
f
(
x
1
, ...x
k
|θ
1
, ...θ
i
) =
Γ(
P
k
i
x
i
+ 1)
Q
k
i
Γ(
x
i
+ 1)
k
Y
i
p
x
i
i
(2.11)
The overview of the generative process of LDA is as follows:
we first choose the
length
N
of a document
d
, then choose a topic distribution for
d
.
We then iteratively
generate a word by first choosing a topic according to the topic distribution,
and
then choosing a word instance according to the word distribution of
the chosen
topic.
14
CHAPTER 2.
EXTENDED BACKGROUND
Now we describe the probabilistic details of each of these steps.
The LDA model assumes that the length of a document follows Poisson(
λ
).
For
a document
d
,
we draw a random variable
N
from Poisson(
λ
) as the length of
the document.
Note that the Poisson assumption is not critical
to the following
generative process and more realistic length distribution can be used [4].
LDA uses the Dirichlet distribution as prior to assign a topic distribution to
a document
d
by drawing a sample
θ
from Dir(
α
),
where
θ
i
is the probability of
mentioning topic
T
i
in
d
.
Note that LDA assumes the number of topics
k
is known
and fixed.
The choice of using Dirichlet distribution as prior is based on a number
of
convenient mathematical
properties of
Dirichlet distribution including the fact
that it is a conjugate to the Multinomial distribution [4].
These properties facilitate
the parameter estimation algorithms used for training the LDA model.
For each word
w
i
in
d
,
the LDA uses Multinomial(
θ
) to draw a random topic
z
i
from the topic distribution
θ
.
We denote the resulting topics of
all
words as
z
= (
z
i
, ..., z
N
).
We have provided a probabilistic model
to select the topic distribution of
a
document,
thereafter select the topic of
each individual
word in the document.
However,
we still
have not yet generated any word instance for document
d
.
To
this end,
we define a matrix
β ∈ R
k×|R|
,
where
β
i,j
=
p
(
R
j
= 1
|T
i
).
Therefore,
given a topic
T
i
, we can sample a word
w
from Multinomial(
η
), where
η
is the word
distribution for topic
T
i
, determined by the
i
th
row of
β
.
As a summary, the LDA assumes the following generative process for any doc-
ument
d
in corpus
D
.
1.
Choose
N ∼
Poisson(
ε
) to be the number of words in
d
.
2.
Choose
θ ∼
Dir(
α
) to be the topic distribution of
d
.
3.
For each of the
N
words
w
i
:
a) Choose a topic
z
n
∼
Multinomial(
θ
).
b) Choose a word from
R
i
from
p
(
R
i
|z
n
, β
), a multinomial probability con-
ditioned on the topic
z
n
.
LDA uses variational Bayes approximation of the posterior distribution to infer
the hidden variable
θ
and
z
for each document [4].
Blei,
et.
al.
also introduced
an EM algorithm for empirical
Bayes parameter estimation of
α
and
β
[4].
The
variational inference and EM algorithm are not relevant to our research,
therefore
we do not introduce them in this thesis.
The detailed description of these methods
can be found in [4].
The resulting LDA representation for a document
d
is a
k
-dimensional
vector
where the
i
th
entry refers to the probability of
d
mentioning topic
T
i
.
We can
then interpret a document as expressing content about
k
topics with a different
weight assigned to each topic.
Besides,
we can examine the content of each topic
by querying the top words using the multinomial distribution matrix
β
.
Blei et.
al.
2.2.
DOCUMENT REPRESENTATION
15
demonstrated that on a sample document,
LDA learns topic groups that contain
mostly semantically coherent words [4].
Comparing to LSI,
LDA has a solid statistical
foundation [4].
It also provides
better interpretability and modularity.
LDA can be readily extended to contin-
uous data or other non-multinomial
data by modifying its probabilistic modules
[4].
Similarly to LSI,
we can view LDA as a dimension reduction technique which
compresses the length of the document vector to
k
.
2.2.5
Common document similarity measures
We have discussed various models for transforming documents into vectors in some
latent vector space.
This way of representing documents as fixed-length vectors is
known as vector space model.
In the vector space, each dimension refers to a unique
feature.
In the case of BOW and TF-IDF model, the dimensions refer to the unique
words and the entries of a vector capture the relative importance of these words in
a document.
As for LSI and LDA, the dimensions represent some latent concepts.
Using the vector space model, we can estimate the document similarity between
two documents as the distance between their vector representations.
The two most
common distance measures are the cosine similarity and Euclidean distance [43].
Euclidean distance
Given two vectors
v
and
w
of dimension
d
,
their Euclidean distance is defined
as,
Euclidean
(
v, w
) =
v
u
u
t
n
X
i
=1
(
v
i
− w
i
)
2
(2.12)
Cosine similarity
Given two vectors
v
and
w
, the cosine similarity is defined as
cosine
(
v, w
) =
v · w
kvkkwk
(2.13)
where
v · w
denotes the dot product between
v
and
w
, which is
P
n
i
=1
v
i
· w
i
, and
kvk
denotes the Euclidean length, which is
q
P
n
i
=1
v
2
i
.
The cosine similarity measures the difference in direction between two vectors.
From this view,
cosine similarity is preferred over the Euclidean distance in situa-
tions where we are only concerned with what concept a document is about, but not
how strong it is towards that concept [43].
2.2.6
Summary
We have discussed a wide range of notable document representation models.
Except
for the simplest BOW model, they all share the same paradigm which aims to build
a fixed-length vector representation for a document using a corpus.
Comparing to
BOW and TF-IDF, more advanced models including LSI and LDA achieve signifi-
cant dimension reduction and are able to capture the semantic similarity between
16
CHAPTER 2.
EXTENDED BACKGROUND
individual words.
In these models, words also represented as fixed-length vectors in
some latent vector space,
enabling us to calculate the semantic similarity between
words.
Besides the fixed-length paradigm, these models all share the BOW assumption
which disregards the word orders.
However, the word orders may provide valuable
contextual
information that is useful
for estimating semantic similarity between
words.
Let us consider two intuitive situations:
1),
if two words occur frequently
in neighboring positions in the document, we may conjecture that these two words
are often used to express the same contextual
meaning.
2),
if
two words occur
frequently at the same position in a sentence,
we may conjecture that these two
words are substitutable (synonyms).
These conjectures are formally formulated in
the distributional
hypothesis of
Harris [15],
which states that words appearing in
similar context have similar meanings.
In the next section,
we shall
exploit this line of
thought and discuss several
models that are specifically designed for building dense vector representation for
words, also known as word embedding.
2.3
Word embedding
A word embedding is a dense vector representation of a word [27].
We call a model
that transforms an input word to a fixed-length dense vector as a word embedding
model.
In section 2.2.3,
we showed that LSI can map a word into a latent vector
space where the semantic similarity between words is preserved.
We may regard
LSI as a successful word embedding model.
Many of the modern word embedding models use various training methods in-
spired by neural-network language modeling [27].
Such models were shown to per-
form significantly better than LSI fore preserving linear regularities among words
[35].
One of the earliest neural word embedding model was proposed by Bengio et
al.
[3]
(2003),
where a feedforward neural
network is used to learn word vector
representation using the linguistic context in which the word occurs.
Most of the
newest word embedding methods extend the work of Bengio and rely on a neural
network architecture [32].
One notable model
was recently proposed by Mikolov
et.al.
[32] (2013) that is both efficient to train and provide state-of-art results on
various linguistic tasks [27].
This model
is also known as word2vec.
In the next
section, we discuss the word2vec model in detail.
2.3.1
Word2vec
Like many other word embedding models,
the word2vec model
is based on the
distributional hypothesis in linguistic which states that semantically similar words
have similar distributions in large samples of language data [15].
In other words,
words that occur in the similar contexts have similar meanings.
In the word2vec model,
the context
c
of
a word
w
is defined as the list of
its neighboring words in a document,
fixed by a window size
k
.
For instance,
if
2.3.
WORD EMBEDDING
17
k
= 5, context
c
includes the five previous and five following words of
w
.
The vector
representation of words is then learned by maximizing the conditional
probability
p
(
c|w
) or
p
(
w|c
) over all (
w, c
) pairs.
Mikolov et al.
proposed two model
architectures,
namely the continuous bag-
of-words model (CBOW) which maximizes
p
(
c|w
), and continuous skip-gram model
(skip-gram) which maximizes
p
(
w|c
) [32].
Mikolov et al.
also proposed an efficient
training method called negative sampling for the skip-gram model
(SGNS) [33].
Mikolov et al.
argued that SGNS improves both the training speed and the quality
of the word embeddings [33].
Since Google used the SGNS model to train its word
embeddings, we focus on discussing the SGNS model.
Skip-gram
Given a corpus, we denote
W
to be the set of words and
C
to be the set of contexts
in the corpus.
We denote
C
(
w
) to be all the contexts where
w
appeared.
Now consider a word-context pair (
w, c
) where
w ∈ W
and
c ∈ C
.
Let
P
(
c|w
)
be the probability that (
w, c
) is observed in
D
given word
w
.
This conditional
probability is modeled as
P
(
c|w
) =
e
− ~
w·~c
P
c
i
∈C
e
− ~
w·~
c
i
(2.14)
where
~
w
and
~c
are the vector representations of
word
w
and context
c
to be
learned.
Note that this is essentially a multinomial
logistic model
that assumes
linear dependency between
~
w
and
~c
.
The objective of the skip-gram model is to find parameters
~
w
and
~c
that maxi-
mize the average log probability.
Therefore objective function is given by
arg max
~
w,~c
1
N
X
w∈W
X
c∈C
(
w
)
log
P
(
c|w
) =
X
w∈W
X
c∈C
(
w
)
(log
e
~
w·~c
−
log
X
c
i
∈C
e
~
w·~
c
i
)
(2.15)
Equation 2.15 makes an important assumption:
by optimizing this objective
function,
similar words would eventually have similar word vectors.
As noted by
Goldberg et al.
[12],
this assumption has no sound statistical
foundation and it is
unclear why it holds.
The objective function shown in equation 2.15 is expensive to compute because
of the term
P
c
i
∈C
e
~
w·
~
c
0
,
which iterates through all
contexts
c ∈ C
.
This becomes
extremely costly when the corpus contains billions of
sentences.
To address this
issue,
Mikolov et al.
proposed the negative sampling method which optimizes a
slightly different objective function.
Negative sampling
We again consider a word-context pair (
w, c
).
We denote #(
w, c
) to be the number
of times the pair is observed in the corpus, and #
c
the number of occurrences of
c
.
18
CHAPTER 2.
EXTENDED BACKGROUND
Let
P
(
D
= 1
|w, c
) be the probability that (
w, c
) is observed from the corpus.
This
conditional probability is modeled as
P
(
D
= 1
|w, c
) =
σ
(
w, c
) =
1
1 +
e
− ~
w·~c
(2.16)
This is essentially a binomial
logistic model.
The negative sampling method
aims to maximize
σ
(
w, c
) for observed word-context pairs while minimizing it for
k
randomly sampled unobserved pairs.
These unobserved pairs are regarded as
negative samples,
hence the name "negative sampling".
Note that an assumption
put here is that if a pair (
w, c
) is not observed from the corpus,
then it is indeed
a negative sample.
Furthermore,
the context
c
of the negative samples are drawn
according to the empirical Bernoulli distribution
P
noise
(
c
) =
Bernourlli
(
#
c
|Q|
), where
Q
is the set of observed (
w, c
) pairs.
Therefore the objective function is given by
`
=
X
w∈W
X
c∈C
(
w
)
#(
w, c
) log
σ
(
~
w, ~c
) +
k · E
c
i
vP
noise
(
c
i
)
[log
σ
(
−~
w, ~
c
i
)]
(2.17)
By optimizing equation 2.17,
we can expect that words appearing in similar
contexts to have similar embedding.
However,
a formal proof is not available [27].
Nevertheless,
empirical
results have shown that the cosine similarity between two
word embeddings is an effective measure of the semantic similarity [32][33].
2.4
Soft cosine similarity
In the previous section,
we saw that the semantic similarity between words can
be measured as the cosine similarity between their word2vec embeddings.
In this
section,
we discuss the soft cosine similarity which could utilize this property for
measuring document similarity.
Recall
that in the vector space of BOW and TF-IDF,
the dimensions refer to
unique word features.
This becomes problematic when two documents share no
common words even though the words are semantically similar.
Their BOW and
TF-IDF vectors would be orthogonal and their cosine similarity is zero.
To address this problem, Grigori et al.
proposed the soft cosine similarity [44].
Given two vectors
a
and
b
of length
n
, the soft cosine similarity between
a
and
b
is
defined as
sof t
_
cosine
(
a, b
) =
PP
n
i,j
=1
s
i,j
a
i
b
j
q
PP
n
i,j
=1
s
i,j
a
i
a
j
q
PP
n
i,j
=1
s
i,j
b
i
b
j
(2.18)
where
s
i,j
is the similarity between component
i
and
j
.
This formula is generalized from the cosine similarity,
taking into account the
basis vectors.
Let us denote the basis vectors to be
e
1
, e
2
, ...e
n
, we can then rewrite
vector
a
= (
a
1
, ...a
n
) as
2.5.
WORD MOVER’S DISTANCE
19
a
=
n
X
i
a
i
e
i
(2.19)
Since dot product is bilinear,
and
|e
i
|
= 1 in the BOW and TF-IDF vector space,
we have
a · b
= (
n
X
i
a
i
e
i
)
·
(
n
X
i
b
i
e
i
)
=
n
X
i
=1
n
X
j
=1
a
i
b
j
·
(
e
i
· e
j
)
=
n
X
i
=1
n
X
j
=1
a
i
b
j
· cosine
(
e
i
, e
j
)
· |e
i
| · |e
j
|
=
n
X
i
=1
n
X
j
=1
s
i,j
a
i
b
j
(2.20)
which leads to the numerator in formula 2.19.
Similarly, we can derive
a · a
and
b · b
which would lead to the denominator in formula 2.19.
The soft cosine similarity allows us to compute the similarity between two BOW
vectors taking into account the word similarity.
Grigori et al.
used the Levenstein’s
distance between two words as
s
i,j
.
Instead, we can use the cosine similarity between
the word2vec embeddings of the two words.
2.5
Word Mover’s Distance
In the previous section, we showed that the soft cosine similarity between two BOW
vectors can naturally take into account the word similarity.
Here we discuss another
method that is successful in measuring document similarity using the words of the
documents.
Word Mover’s Distance (WMD) is a distance function between documents, pro-
posed by Kusner et al [25] (2015).
WMD models a document as a weighted point
cloud of its words in the word2vec space.
The distance between document
A
and
B
is the minimum cumulative distance that the words from
A
need to travel to match
exactly the point cloud of
B
[25].
The weight of each point is the normalized term
frequency of the corresponding word.
Let
d
1
,
d
2
be the normalized BOW vectors of two documents of dimension
n
,
where component
i
refers to word
w
i
.
We denote
c
(
i, j
) to be the minimum distance
required for word
w
i
to move to
w
j
.
WMD assumes
c
(
i, j
) to be the Euclidean
distance between the word embeddings of
w
i
and
w
j
,
provided by a pre-trained
word embedding model.
Let us define a matrix
T ∈ R
n×n
where
T
i,j
is the "mass"
of
w
i
in
d
1
traveled to
w
j
in
d
2
.
The Word Mover’s Distance is estimated by solving
the following linear program.
20
CHAPTER 2.
EXTENDED BACKGROUND
minimize
n
X
i,j
=1
T
i,j
c
(
i, j
)
subject to :
n
X
j
=1
T
i,j
=
d
2
i
, ∀i ∈ {
1
, ..., n}
n
X
i
=1
T
i,j
=
d
1
j
, ∀j ∈ {
1
, ..., n}
(2.21)
The first constraint states that the incoming "mass" of words to
w
i
in
d
2
equals its
weight.
The second constraint states that the outgoing "mass" of words from
w
j
in
d
1
equals its weight.
These two constraints together guarantee that all the "mass"
of words in
d
1
is moved to the words in
d
2
while each word in
d
2
receives no more
mass than its own weight.
This optimization problem can be cast as calculating the Earth Mover’s Distance
(EMD) [42].
We do not discuss further the details of EMD since it is irrelevant to
our research.
However, we do note that EMD problem can be efficiently solved with
the best average time complexity of
O
(
n
3
log
n
).
2.5.1
Word centroid distance
The average word embedding (AWE) representation of
a document of
n
words is
defined as
AWE
=
1
n
n
X
i
~
w
i
(2.22)
which is simply the unweighted average of all the word embeddings of a document
Kusner et al.
defined the Word centroid distance (WCD) between two documents
as the distance between the corresponding AWE vectors [25].
It is shown that WCD
is a lower bound of WMD and has comparable performance with WMD [25]
in a
k-nearest neighbor document classification task.
2.5.2
k-nearest neighbor classification
Kusner et al.
argued the effectiveness of WMD in comparison with several baseline
models in a k-nearest neighbor document classification task.
In this task, a collection
of labeled documents is divided into a training set
D
and a test set
S
.
A k-nearest
neighbor classifier (k-NN) is then trained on
D
and predicts the labels of documents
in
S
.
The performance is evaluated in terms of error rate.
The k-NN classifier works by retrieving the top
k
most similar documents to
an unseen document
d
, and assigns it the majority label among the retrieved docu-
ments.
Thus the performance of the k-NN classifier depends on the effectiveness of
the document similarity measure.
Therefore the k-NN classifier is a surrogate for
evaluating the effectiveness of the models.
2.6.
CLUSTERING ANALYSIS
21
Kusner et al.
selected a set of
state-of-art baseline document representation
models including LSI and LDA. For all the baseline models, the distance metric for
the k-NN classifier is Euclidean distance.
It is shown that WMD outperforms all the
baseline models [25].
This result shows that we can effectively estimate document
similarity using only the pre-trained word embeddings.
2.6
Clustering analysis
Clustering analysis is a widely used technique in machine learning and pattern
recognition [36].
Clustering is an unsupervised task which groups a set of objects
in such a way that objects in the same group are more similar to each other than
those in other groups [8].
In section 2.3, we saw that the semantic similarity between
words can be estimated using their word embeddings.
This suggests that we could
find semantically coherent word clusters using suitable clustering algorithms.
There are many clustering algorithms available [10], such as k-means [24], spec-
tral
clustering [30],
Gaussian mixture models [39],
DBSCAN [9]
and so on.
The
purpose of this thesis is not to find the best clustering algorithms for word cluster-
ing.
Therefore we only introduce several notable clustering algorithms that may be
suitable for finding good word clusters.
2.6.1
k-means clustering
Assuming a set of d-dimensional vectors
v
=
{v
1
, ..., v
n
}
and the number of clusters
k
,
the k-means clustering aims to minimize the within-cluster sum of
variance.
Formally, the objective is to find
arg min
C
=
k
X
i
=1
X
v∈C
i
kv − c
i
k
2
(2.23)
The Lloyd-algorithm is commonly used to solve the above optimization problem
[24].
The algorithmic steps are shown below.
1.
randomly generate
k
centers
c
1
, ..., c
k
,
2.
for each vector in
v
, find its closest center
c
i
and assign it to cluster
C
i
3.
update each center
c
i
to be the centroid of all vectors in cluster
C
i
4.
repeat step 2) and 3) until cluster assignments do not change
By the definition of
the objective function,
the k-means algorithm defines the
dissimilarity between objects to be the Euclidean distance.
The WMD distance also
measures the semantic dissimilarity between words as the Euclidean distance be-
tween their word embeddings and achieved good performance in a k-nearest neigh-
bor document classification task.
Therefore we expect k-means to find clusters
containing semantically similar words.
The time complexity of k-means is
O
(
nkt
)
22
CHAPTER 2.
EXTENDED BACKGROUND
where
t
is the number of iterations of the center updating [46].
Since
k
and
t
are
typically much smaller than
O
(
n
), k-means is suitable for clustering a large number
of objects efficiently [46].
Due to the random initialization of
the
k
centers,
k-means algorithms is non-
deterministic.
The procedure of
Lloyd-algorithm does not guarantee the conver-
gence of global optimum [46] either.
It is shown that the k-means algorithm tends
to stuck at the local
optimum [30].
Besides,
it assumes the clusters to be convex,
which may not be true for word clusters in the word2vec space.
2.6.2
Spectral
clustering
Unlike k-means algorithm which directly operates on the vectors to be clustered,
spectral clustering algorithm first maps the vectors to a k-dimensional space using
the similarity matrix,
and then performs k-means clustering in this space.
This
leads to improved performance for non-convex clusters and allows for the use of
different similarity measures other than Euclidean distance [30].
The spectral
clustering algorithm works as follows.
Assume we have access to
the similarity matrix X
∈ R
n×n
of
the vectors
v
=
{v
1
, ..., v
n
}
,
where X
i,j
is the
non-negative similarity between
v
i
and
v
j
.
We can view matrix X as the weight
matrix of an undirected graph
G
whose vertices are
v
.
In
G
, two vertices
v
i
and
v
j
are connected if X
i,j
>
0, and the weight of the edge is
w
i,j
= X
i,j
.
The degree of
v
i
is defined as
d
i
=
P
n
j
=1
w
i,j
.
Now we define the diagonal degree matrix D
∈ R
n×n
where
D
i,i
=
d
i
.
Using
D
and
X
, we can derive a Laplacian matrix
L
of
G
which has some nice mathematical
properties that are useful
for the clustering problem.
We do not discuss them in
detail
since it is not relevant to this thesis.
It is sufficient to know that we can
use the
k
largest eigenvectors of
L
to generate a k-dimensional
representation
v
0
i
for each
v
i
∈ v
.
We then cluster
{v
0
1
, ..., v
0
n
}
into
k
clusters
{C
1
, ...C
k
}
using the
standard k-means clustering algorithms.
Finally,
for each
C
i
,
we replace
v
0
j
by
v
j
and obtain the discovered clusters of
v
.
Note that different types of
Laplacian
matrices can be used [30].
Ng et al.
demonstrated that the normalized Laplacian
matrix
L
=
D
−
1
/
2
XD
−
1
/
2
leads to good experimental results [36].
The choice of the similarity measure has a big influence on the discovered clus-
ters.
Unfortunately,
the theoretical
implication of
different similarity measure is
not clear [30].
Nevertheless, for word clustering, we use the cosine similarity which
captures the semantic similarity between words [32].
Due to the extra computation
for finding the k largest eigenvectors, the time complexity of the algorithm is
O
(
n
3
).
However,
there are many fast approximate spectral
clustering algorithms[48]
[47].
Yu et al.proposed an algorithm with time complexity
O
(
n
3
/
2
k
+
nk
2
) that guar-
antees near-global
optimum [48].
As we will
discuss in Chapter 4,
this algorithm
produces very good results in a number of word clustering tasks on both synthetic
and real-world documents.
2.6.
CLUSTERING ANALYSIS
23
2.6.3
Internal
cluster validation criteria
When there is no ground truth of
the clusters to be discovered,
we can use the
internal cluster validation criteria to evaluate the quality of the discovered clusters.
This type of indexes only rely on the information of the data itself [28],
therefore
not requiring a ground truth to be specified.
In this section, we introduce two internal cluster validation criteria that may be
useful for measuring the word cluster quality.
Within-sum of variance
One trivial
criterion is the within-sum of variance,
which is the objective function
of the k-means clustering algorithm.
It is essentially the sum of variance of all the
clusters, defined as
k
X
i
=1
X
v∈C
i
kv − c
i
k
2
(2.24)
where
C
i
is a cluster and
c
i
the centroid of
C
i
.
This is also known as inertia.
It measures the compactness of the clusters,
which means how closely related the
objects in a cluster are.
The closeness is defined as the Euclidean distance [28].
Silhouette coefficient
In contrast to the within-sum of
variance,
the Silhouette coefficient makes no as-
sumption on what defines the closeness.
It only assumes to have access to a pair-wise
distance matrix X
∈ R
n×n
where X
i,j
is the distance between object
i
and
j
.
The
Silhouette coefficient of a cluster
C
of size
n
is defined as
silhouette
(
C
) =
1
n
n
X
i
=1
b
i
− a
i
max (
a
i
, b
i
)
(2.25)
where
a
i
is the average distance between object
i
and all other objects in
C
, and
b
i
the maximal average distance between
i
and all other objects in a different cluster
[41].
The Silhouette coefficient of a set of clusters is then simply the average Silhouette
coefficients of all the clusters.
The Silhouette coefficient lies in the range of [
−
1
,
1].
A higher Silhouette coefficient indicates better quality of the clusters, while negative
Silhouette coefficient indicates that the objects are wrongly clustered.
For evaluating the word clusters, we use the pair-wise cosine distance matrix X
0
where X
0
i,j
= 1
− cosine
_
similarity
(
~
w
i
,
~
w
j
) and
~
w
i
is the word2vec embedding of
word
w
i
.
Chapter 3
Methodology
This chapter describes the detailed strategy to answer the research question raised
in section 1.3,
Can we construct an interpretable document representation by clus-
tering the words in a document, and effectively and efficiently esti-
mate the document similarity?
The above research question is concerned with three aspects, namely
1.
effectiveness:
the performance for document similarity is comparable with the
state-of-art models
1
.
2.
efficiency:
the time complexity of the overall
model
for document similarity
should be
O
(
n
2
log
n
), so that it is an order of exponent better than WMD.
3.
interpretability:
the resulting document representation can be interpreted as
a mixture of multiple topics, each weighted by its contribution to the content
of the document.
We note that this research question is based on the non-trivial
assumption that
semantically similar words form separable clusters in the word2vec vector space, as
discussed in section 1.2.
In order to fully answer the research question, we propose
a novel
document representation model
and a similarity measure for the resulting
document representation.
We then analyze and evaluate the proposed model
and
similarity measure with respect to each of the three concerned aspects.
3.1
Research methods
We refer to the taxonomy of
the most common research methods described by
Håkansson [20] to choose the appropriate research method in this work.
The choice
1
The selected state-of-art baseline models are listed in section 3.5.2.
25
26
CHAPTER 3.
METHODOLOGY
is centered around the research question.
To address each of the concerned aspects
of the research question, we may employ different research methods.
First of
all,
the non-trivial
assumption of
the research question is concerned
with the word clusters in the word2vec vector space.
As argued in section 1.6,
the word2vec model
lacks a solid statistical
foundation.
Therefore a rigorous an-
alytical
approach is inapplicable.
The fundamental
and applied research methods
are not suitable either since the purpose of
this work is not to formulate a new
theory or focus on a specific technological
domain.
The most appropriate method
is the experimental
research which studies the causes and effects.
We aim to ap-
ply different clustering algorithms and quantitatively and qualitatively evaluate the
semantic quality of the generated clusters so to provide empirical evidence that sup-
ports the word clustering assumption.
Therefore the inductive research approach
is used to draw the conclusion regarding the assumption by observing the collected
experimental results.
For the same reason,
we employ the experimental
research method to address
the effectiveness aspect.
We will quantitatively evaluate the performance of differ-
ent models using the same set of
data sets while keeping any other experimental
variable constant,
e.g.
the training and testing split and the parameter of
the k-
nearest neighbor classifier.
The inductive research approach is then employed to
draw the conclusion regarding the effectiveness of our model by analyzing the col-
lected experimental results.
As for the efficiency aspect,
the analytical
method is most appropriate since
the efficiency is concerned with the average time complexity.
We can rigorously
analyze the algorithmic steps of the model
to derive an exact upper bound of the
average time complexity,
which is then compared with that of WMD to argue the
relative efficiency.
This implies that the deductive research approach is employed to
address the efficiency aspect since the time complexity analysis follows a deductive
principle.
Similarly,
the interpretability aspect can be addressed by analyzing the algo-
rithmic steps of
producing the document representation.
We shall
explain how
the resulting document representation can be interpreted as a mixture of different
topics.
Therefore,
the deductive research approach is employed to address the in-
terpretability aspect.
Furthermore, we are interested in the topic discovery on the
real-world documents.
Therefore we also employ the experimental research method
in order to experiment with the topic discovery on several
real-world document.
This implies that the inductive research approach is also employed to address the
interpretability aspect.
As argued in section 1.3, a rigorous evaluation of the inter-
pretability is challenging and not the focus of this thesis.
Thus we only evaluate the
discovered topics qualitatively based on our own experience of the English language.
Therefore the reliability of the conclusion regarding the interpretability aspect is not
as scientifically strict as the other two aspects.
An illustration of the details of the research methods is shown in figure 3.1.
As
a summary, the word clustering assumption is firstly verified using an experimental
method.
Based on this assumption, we design a novel model and provide the detailed
3.2.
DATA COLLECTION
27
Figure 3.1:
The high-level
overview of the research methodology.
There are three
experiments, colored in blue.
algorithmic steps.
Afterwards,
we evaluate the model
with respect to the three
concerned aspects,
each employing different research methods.
Finally,
based on
the collected qualitative and quantitative results,
the conclusion is drawn to fully
answer the raised research question.
3.2
Data collection
In order to conduct the aforementioned experiments,
we first need to specify how
to collect the data for the experiments.
The term data is concerned with both the
data sets with which the experiments are conducted, and also the data which is the
result of the experiments.
For collecting the data sets,
we use the experiments data collection method
since there are a large amount of
public data sets of
documents available.
We
select multiple public data sets so that the experimental results can be reproduced
and also the performance of our model
can be evaluated on data sets of different
domains.
Similarly, for collecting the data which is the result of the experiments, we use
the experiments data collection method since such data is directly accessible af-
ter running the experiments.
For the experiment of verifying the word clustering
assumption,
the results include the discovered word clusters and their cluster val-
idation measures collected by running different clustering algorithms on different
documents.
For the experiment of
evaluating the effectiveness of
our model,
the
results include the classification accuracy collected by running different models on
different data sets.
For the experiment of evaluating the interpretability of the re-
sulting document representation,
the results include the discovered topic clusters
collected by running our model on several real-world documents.
Besides, the algo-
rithmic steps of the proposed model are presented, which enables the evaluation of
28
CHAPTER 3.
METHODOLOGY
the efficiency by analyzing the average time complexity and also the interpretabil-
ity by analyzing how the resulting document representation can be interpreted as a
mixture of topics.
3.3
Data analysis
After the results are collected
2
,
we need an appropriate data analysis method to
analyze the data.
Different data analysis methods may be used depending on the
chosen research method for addressing the three aspects of our model and verifying
the word clustering assumption.
3.3.1
Verifying the word clustering assumption
The experiment of verifying the word clustering assumption uses both qualitative
and quantitative research.
For qualitative results,
we use analytic induction data
analysis method since the experiment involves multiple iterations and the semantic
quality of the word clusters is analyzed subjectively based on our experience of the
English language.
For quantitative results, which are the cluster validation criteria,
we use the descriptive statistics data analysis method to objectively evaluate the
cluster validity.
The two cluster validation criteria are the with-in sum of variance
and Silhouette coefficient, as discussed in section 3.5.1.
3.3.2
Evaluating the effectiveness
The experiment of
evaluating the effectiveness of
our model
uses quantitative re-
search.
Therefore we use the descriptive statistics data analysis method.
We com-
pare the accuracy of
different models run on different data sets with different
k
for the k-nearest neighbor classifier to analyze whether the performance of
our
model
is comparable with the sate-of-art baseline models.
There are possible rig-
orous statistical
methods for testing the significance of
the difference in accuracy
between different models,
e.g.
Friedman test.
However,
the significance test only
tells whether the difference in performance is significant, yet a significant difference
is acceptable as long as such difference is not too big so that our model
has ac-
ceptable performance comparing to the state-of-art.
We note that the sense of the
comparable performance is subjective.
Therefore, in this work, a comparable perfor-
mance is defined as the difference in accuracy between different models is within
0.05, run on different data sets with different k for the k-nearest neighbor classifier.
Furthermore, when comparing the performance of model
M
1
with
M
2
, we calculate
the relative improvement in accuracy, as shown in equation 3.1.
improvement
=
accuracy
(
M
1
)
accuracy
(
M
2
)
−
1
(3.1)
2
The proposed model is an artifact of the research, therefore also a result of this work.
3.4.
MODEL DESIGN
29
The improvement statistic tells how much better or worse the
M
1
is comparing to
M
2
in terms of the relative improvement in accuracy.
3.3.3
Evaluating the efficiency and interpretability with the algorithms
The algorithmic steps of
the proposed model
are presented and analyzed to ad-
dress the efficiency and also the interpretability aspect.
We use the computational
mathematics data analysis method since the efficiency aspect is concerned with the
average time complexity,
which is analyzed by examining the algorithmic steps of
the proposed model, and the interpretability can be argued by examining the algo-
rithmic steps for producing the document representation and explaining how it can
be interpreted as a mixture of topics.
3.3.4
Evaluating the interpretability with an experiment
The experiment of
evaluating the interpretability of
the document representation
uses qualitative research.
The interpretability is a rather subjective property, thus
requiring an iterative evaluation of
the semantic quality of
the topic clusters dis-
covered from different documents.
Therefore we use the analytic induction data
analysis method.
As mentioned in section 1.4,
the focus of
this thesis is the ef-
ficiency and effectiveness aspects.
We only informally address the interpretability
aspect.
This is done by analyzing the semantic meaning of
the discovered topic
clusters based on our experience of the English language.
3.4
Model
design
The main purpose of this research is to show whether by clustering the words in a
document we can obtain a document representation that enables effective and effi-
cient document similarity measure.
Therefore, the proposed model will implement
this word clustering strategy to produce a document representation.
Notice that
this implies an inductive principle for the model design.
We assume that a document
talks about multiple topics, which will be discovered from the word clusters.
The efficiency and interpretability aspects are addressed by analyzing the algo-
rithmic steps of the proposed model.
Therefore the detailed algorithmic steps of the
model must be presented.
The design of the model should respect the efficiency and
interpretability aspects.
That means the chosen method for clustering the words
in a document should have an average time complexity of
O
(
n
2
log
n
).
The time
complexity of
other algorithmic steps should not exceed this upper bound either.
The algorithmic steps should also lead to an interpretable document representation
in the sense that it can be interpreted as a mixture of topics.
The model
should assume to have access to only 1) the pre-trained word em-
beddings and 2) the document for which a representation needs be built.
This is
an implicit constraint enforced by the raised research question.
One purpose of this
30
CHAPTER 3.
METHODOLOGY
work is to investigate the feasibility of discovering the topics of a document directly
from its words, without the need of a corpus.
Each design decision of the model should have either the theoretic or empirical
evidence for support.
Due to the lack of
statistical
foundation of
the word2vec
model,
rigorous theoretic evidence may not always be accessible.
Nevertheless,
when empirical evidence is hard to obtain, a loose and intuitive theoretic arguments
should still
be given to justify the design decision.
The arguments for the design
decisions are presented in Chapter 5.
3.5
Experimental
design
This section describe the detailed settings of the three experiments.
3.5.1
Experiment 1:
verifying the assumption
The first experiment serves two purposes:
1.
verify the assumption that semantically similar words form separable clusters
in the word2vec space.
This is a core assumption of our proposed model.
2.
reveal
insights of the word clustering structure that are useful
for document
representation and document similarity.
The detailed design of our model will
be substantiated by these insights.
The experiment is conducted iteratively.
At each iteration,
1.
We select a English document
d
.
2.
We use the selected clustering algorithms to generate word clusters from
d
.
3.
We evaluate the semantic quality of the the generated word clusters based on
our experience of the English language.
4.
Go to step 1 and select a new document that reflects different semantic aspects
based on the findings in step 3.
The iterative design of the experiment is crucial since there is no ground truth
of the word clusters.
A rigorous mathematical definition of the semantic quality is
not available either.
Due to the subjective nature of semantics,
the iterative eval-
uation should involve end-users [10].
However,
such evaluation introduces human
bias.
Users with varying background may disagree on what constitutes a good word
cluster.
Therefore we also calculate two internal
cluster validation measures.
We
describe the details of the evaluation metrics in section 3.5.1.
We present the details of the experimental process in the next section.
3.5.
EXPERIMENTAL DESIGN
31
Process
For a given document,
its raw text is firstly tokenized into a list of
words.
We
then map these words into the word2vec space using a pre-trained word2vec model.
Afterwards,
we generate word clusters using k-meanings and spectral
clustering
algorithms.
The semantic coherence of
each cluster is qualitatively evaluated by
querying its member words.
We also compute two internal cluster validation criteria
for a quantitative evaluation.
We discuss the results aiming to derive insights of
the word clustering structure in the word2vec space.
This experimental
process is
illustrated in figure 3.2.
The results and the discussion can be found in Chapter 4.
Figure 3.2:
The experimental steps for exploring word clusters of a document.
Data
In total,
there are four documents used in the experiment,
each for one iteration.
The description of the four documents are listed below:
1.
A simple synthetic document consisting of
words belonging to three topic
groups that we deem to be well distinguishable to ordinary readers.
The word
lists are shown in table 3.1.
2.
A complex synthetic document consisting of seven lists of words.
Six out of
the seven word lists belong to some distinct topic group, while one list consists
of stop words.
Besides, each list has a varying number of words, with the stop
word list being the largest.
The word lists are shown in table 3.2.
3.
A short real-world document randomly sampled from the BBCNews data set,
as shown in figure 3.3.
It is about a British politician disclosing the informa-
tion about a planned election.
However,
it is difficult to decide on the true
number of topics in this document.
32
CHAPTER 3.
METHODOLOGY
List ID
Topic
Words
#words
1
Economics
Economics, economy, finance,
capitalism, macroeconomics,
money, consumer, euro, dollar,
bank
10
2
Animals
Dog, cat, goat, cow, horse,
sheep, chicken, rabbit, fox,
tiger
10
2
Colors
Red, orange, yellow, green,
blue, purple, white, black,
pink, brown
10
Table 3.1:
Iteration 1:
Three lists of words of different topics.
Figure 3.3:
A document randomly sampled from BBC News data set.
4.
A long real-world document randomly sampled from the BBCNews data set,
as shown in figure 3.4.
It is about the business of a fashion brand.
However,
it is difficult to decide on the true number of topics in this document.
The complexity of
the four documents gradually increases.
This allows for a
thorough exploratory analysis of the strength and weaknesses of the two clustering
algorithms.
In the third and fourth iterations, the real number of topics in the doc-
ument is unknown.
This poses a significant challenges to the clustering algorithms
since they require the number of clusters to be specified.
As we shall see in Chapter
4,
the results of the four iterations reveal
different insights that are useful
for the
detailed design of the proposed model.
3.5.
EXPERIMENTAL DESIGN
33
List ID
Theme
words
Number of words
1
Economics
Economics, economy, finance,
capitalism, macroeconomics,
money, consumer, euro, dollar,
bank, rich
11
2
Animals
Dog, cat, goat, cow, horse,
sheep, chicken, rabbit, fox,
tiger
10
3
Colors
Red, orange, yellow, green,
blue, purple, white, black
8
4
Sports
Football, basketball, baseball,
tennis, swimming, archery
6
5
Variations
Denote, denotes, denotation,
denoted, denoting
5
6
Stopwords
about, above, because, between,
could, from, where, under,
until, their, same, is,
through, after, each
15
7
Fashion
fashion, design, dressing,
designer, stylish, artistic
6
Table 3.2:
Iteration 2:
Seven lists of words of different themes.
Figure 3.4:
A document randomly selected from BBC News data set.
34
CHAPTER 3.
METHODOLOGY
Choice of clustering methods
In section 2.6, we discussed a wide range of clustering algorithms.
Due to the time
constraint,
it is not feasible to experiment with each of them.
Therefore we select
two algorithms that suit our goal the best, based on three requirements,
1.
The average time complexity should be
O
(
n
2
log
n
), so it is better than WMD
by at least one order of exponent.
2.
The algorithm should not be overly complex in terms of
parameters,
so we
can limit the number of independent variables in our experiment.
3.
The algorithm should not be overly sensitive to the choice of parameters,
so
we can minimize the effect of the choice of parameters on the cluster quality,
which in turn helps the evaluation.
The k-means algorithm is a natural choice due to its simple design.
It has only
one parameter,
and when Lloyd algorithm is used,
the time complexity is
O
(
nki
)
which is linear in terms of
n
,
where
k
is the number of clusters and
i
the number
of
iterations for the algorithm to run [24].
By its design,
k-means calculates the
centroid of each cluster which is a suitable choice for the topic embedding.
However,
it is shown that k-means tends to stuck at the local
optimum which
harms the overall
cluster quality [14].
Besides,
k-means algorithm minimizes the
with-in sum of
variance,
which explicitly uses Euclidean distance as the distance
metric.
Yet Mikolov et al.
recommended cosine similarity for estimating word
similarity in the word2vec space.
Another drawback of k-means is that it assumes
each cluster to be convex, which may not be the case for word clusters.
This leads to the natural
choice of
the spectral
clustering algorithm,
which
discovers clusters using a pair-wise similarity matrix.
In our case, we can construct
the cosine similarity matrix between word pairs.
Y.
Ng et al.
also demonstrated
that the spectral
clustering algorithm is able to discover non-convex clusters and
clusters that are not cleanly separated [36].
As discussed in section 2.6.2, there are
a number of efficient approximate spectral clustering algorithms with sub-quadratic
time complexity.
We select the discrete spectral
clustering algorithm proposed by
Yu et al.
which has a time complexity of
O
(
n
1
.
5
k
+
nk
2
) and produces consistent
clusters [48].
Evaluation metrics
It is difficult to quantitatively measure the semantic quality of word clusters without
ground truth.
Due to the subjective nature of semantics, we qualitatively evaluate
the semantic coherence of
word clusters based on our experience of
the English
language.
Nonetheless, we calculate two internal cluster validation criteria:
with-in sum of
variance and Silhouette coefficient, to quantitatively evaluate the inherent structural
quality of the clusters.
However,
as suggested by Estivill-castro [10],
such criteria
3.5.
EXPERIMENTAL DESIGN
35
merely reflects the mathematical
property of
the data itself
but not the desired
cluster property,
e.g.
semantic coherence.
Therefore we should not consider the
internal cluster validation criteria to be more reliable than the subjective evaluation
of the users.
3.5.2
Experiment 2:
effectiveness of the model
The second experiment uses three labeled public data sets to evaluate the effective-
ness of the proposed model and similarity measure in a k-nearest neighbor document
classification task.
We use quantitative measure and compare the results against a
set of baseline models.
Choice of baseline methods
We compare our proposed model against the following baseline methods:
BOW, TF-
IDF, LSI, LDA and WCD. We are particularly interested in comparing our model
with LDA since both of them produce interpretable document representation.
LDA
is also considered as a state-of-art document representation model [25].
We do not
compare with WMD due to the limitation of computation power.
It is impractical
to evaluate WMD on all
the data sets using our local
computer due to the time
constraint.
Instead,
we compare with WCD which is a lower bound of WMD and
has comparable performance with WMD [25].
Process
The process of
this experiment is depicted in Figure 3.5.
All
the documents in
the corpus are first tokenized using the same procedure as described in the first
experiment.
After that, they are transformed using certain transformation method.
The pair-wise document similarity is then calculated using a suitable distance met-
ric.
Finally,
the performance of the model is evaluated in the context of k-nearest
neighbor classification.
For a fair comparison, we report the results with different
k
separately.
For all
baseline models,
cosine similarity is used as the distance metric for the k-nearest
neighbor classifier.
The hyperparameters of
LSI
and LDA are tuned using the
validation data set.
Furthermore, we removed all the stop words from the documents
for all baseline models.
The stop words are not removed for the propose model since
we expect our model to naturally take into account the influence of the stop words.
Our final
model
is denoted as SC_softcosine,
meaning we used the spectral
clustering algorithm for discovering word clusters and soft cosine similarity as the
distance metric.
We set the number of topics to be
d
log
ne
for SC_softcosine, where
n
is the number of unique words in a document.
Therefore SC_softcosine assumes
more topics in longer documents.
In addition,
we evaluate the performance of
WCD weighted by the proposed logistic word importance function
I
(
w
), denoted as
WCD_I. We also evaluate the performance of our model with WMD as the distance
metric, denoted as SC_wmd.
36
CHAPTER 3.
METHODOLOGY
Figure 3.5:
The experimental steps for evaluating model performance.
Data sets
We select three common document classification data sets.
They are BBCNews
3
,
BBCSport
4
and 20newsgroup
5
data sets.
For the 20newsgroup data set,
we use a
subset with five labels.
The brief description of these three data sets is shown below.
1.
The BBCNews data set consists of
2225 labeled documents from the BBC
News website
6
correspond to the news story in five topical areas.
The labels
include business, entertainment, tech, politics and sport.
2.
The BBCSport data set consists of 737 labeled documents from the BBC Sport
website
7
in five sports areas.
The labels include athletics,
cricket,
football,
rugby, and tennis.
3.
The 20newsgroup data set consists of 3568 labeled documents of the email ex-
changes between college students.
The labels refer to the subject of discussion
in the email.
All documents include the "From" and "To" email headers.
The
labels include hardware, autos, space, baseball and crypt.
The labels of these three data sets refers to only one topic.
In order to evaluate
the model
performance on multi-topic documents,
we create a new data set
X
_t
for each data set
X
.
Each new document in
X
_t consists of
t
randomly sampled
documents of different labels.
Therefore each new label
of
X
_t refers to a set of
three different topics.
Each document in
X
appears in only one new document
in
X
_t.
Furthermore,
we make sure that the difference between any two labels is
3
BBCNews data set:
http://mlg.ucd.ie/datasets/bbc.html
4
BBCSport data set:
http://mlg.ucd.ie/datasets/bbc.html
5
20newsgroup data set:
http://qwone.com/ jason/20Newsgroups/
6
BBC News:
https://www.bbc.com/news
7
BBC Sport:
https://www.bbc.com/sport
3.5.
EXPERIMENTAL DESIGN
37
only one topic.
As an example,
one label
in the BBCNews_3 data set is "busi-
ness+entertainment+sport" which means the document talks about all these three
topics.
The detailed data set description can be found in table 3.3.
The created multi-topic data sets have meaningful
and distinguishable labels.
In the case of single-topic data sets, we can tell the difference between two different
labels by comparing just two topics.
However, for multi-topic data sets, we need to
compare two sets of topics, which is a more challenging task.
For all
data sets,
we split the samples into training and test sets (.6/.4 split).
In case a model requires hyperparameter tuning, a subset of the training set is used
as the validation set (0.6/0.4 split).
We make sure that the labels are split equally.
We only report the performance evaluated using the test set.
Evaluation metrics
Following the experimental
settings used by Kusner et al.
[25],
we evaluate the
performance of the models in a k-nearest neighbor classification task.
The k-nearest
neighbor classifier (k-NN) works by retrieving the top
k
most similar documents
to an unseen document
d
,
and assigns it the majority label
among the retrieved
documents.
Thus the performance of the k-NN classifier depends on the effectiveness
of the document similarity measure.
Therefore the k-NN classifier is a surrogate for
evaluating the effectiveness of the models.
Since all data sets have approximately balanced labels, we use the accuracy as
the evaluation metric for the k-nearest neighbor classification task.
Kusner et al.
used the error rate as the evaluation metric in his work of
WMD [25],
which is
simply the accuracy subtracted by one.
Furthermore,
we report the evaluation results with different
k
for the k-NN
classifier separately.
The reason is that
k
is actually a hyperparameter of the k-NN
classifier.
If we only report the best performance of a model optimized with different
k
, we then introduce the bias of the k-NN classifer to the results.
This differs from
the work by Kusner et al.
[25]
where they also optimized
k
for all
models during
the training.
3.5.3
Experiment 3:
interpretability of the document representation
The third experiment uses several
sample real-world documents to evaluate the
interpretability of
the resulting document representation of
the proposed model.
We also compare our model with LDA on on one sample document which was used
by Blei
et al.
to demonstrate the ability of topic discovery of the LDA model
[4].
Note that all
stop words are removed before training LDA but not for our model.
The evaluation is done informally by presenting the discovered topic clusters and
subjectively argue their semantic meaning based on our experience of the English
language.
38
CHAPTER 3.
METHODOLOGY
Data set
Size
Labels
BBCNews
2225
{"business", "entertainment", "sport", "tech", "politics"}
BBCSport
737
{"athletics", "cricket", "football","rugby", "tennis"}
20newsgroup
3568
{"hardware", "autos", "space","baseball", "crypt"}
BBCNews_3
384
{"entertainment+politics+sport",
"entertainment+politics+tech",
"entertainment+sport+tech",
"politics+sport+tech"}
BBCNews_4
385
{"business+entertainment+politics+sport",
"business+entertainment+politics+tech",
"business+entertainment+sport+tech",
"business+politics+sport+tech",
"entertainment+politics+sport+tech"}
20newsgroup_4
590
{"hardware+autos+space+baseball",
"hardware+autos+space+crypt",
"hardware+autos+baseball+crypt",
"hardware+space+baseball+crypt",
"autos+space+baseball+crypt"}
20newsgroup_3
488
{"autos+space+baseball",
"autos+space+crypt",
"autos+baseball+crypt",
"space+baseball+crypt",
"space+baseball+crypt"}
Table 3.3:
Data set characteristics
Data
In total, four sample documents are used.
One of which is taken from the work of
Blei
et al.
with which they argued the ability of topic discovery of LDA [4].
This
sample document is shown in figure 3.6.
The other three documents are sampled
from the three data sets used in experiment 2, as listed below:
1.
A document randomly sampled from the BBCNews data set.
It is about the
business of a medical device company.
2.
A document randomly sampled from the BBCSport data set.
It is about an
international rugby tournament.
3.
A document randomly sampled from the 20newsgroup data set.
It is about
the internet privacy and the political impact of a new market event.
These three sample documents can be found in the appendix A.
The number of
topics is specified as
d
log
2
ne
for our model.
3.5.
EXPERIMENTAL DESIGN
39
Figure 3.6:
An example document and the discovered topics by LDA.
Each color
corresponds to a different topic:
"Arts" (red),
"budget" (green),
"Children" (blue),
"Education" (violet).
Figure taken from [4].
Evaluation metric
The semantic coherence of
the topic clusters is evaluated qualitatively based on
our understanding of the English language.
We note that this is only an informal
evaluation and does not guarantee scientific validity regarding the interpretability.
Chapter 4
Verifying the word clustering
assumption
This section presents the results of the first experiment.
4.1
A simple synthetic document
We repeatedly run the two clustering algorithms three times on
d
with
k
= 3.
The
results of both algorithms are consistent.
Both algorithms generated word clusters
that are identical to the three word lists given in table 3.1.
The visualization of
the embedded words projected in 2-dimensional
space is
shown in Figure 4.1.
We can see that words from different topic lists are distant
away while being close to their member words.
This result shows that semantically similar words in
d
do form clusters in the
word2vec space, and the two clustering algorithms are able to discover them.
4.2
A complex synthetic document
We repeatedly run the two clustering algorithms three times on
d
with
k
= 7.
The
results of k-means clustering are inconsistent.
In contrast,
the spectral
clustering
produces consistent results and the generated word clusters are identical to the word
lists given in table 3.2.
The visualization of
the embedded words projected in 2-
dimensional space is shown in Figure 4.2.
We can see that the boundaries between
word lists are not as clear as in the previous iteration, which is expected.
Besides,
we notice that the stop word lists are positioned in the center.
We report the results of k-means clustering in table 4.1.
In repeat 1,
words including "rich",
"dollar",
"euro",
and "denotation" are clus-
tered together with stop words into cluster 6.
The rest six clusters remain seman-
tically coherent,
except for cluster 2,
where "archery" is clustered together with
animal-related words.
41
42
CHAPTER 4.
VERIFYING THE WORD CLUSTERING ASSUMPTION
Figure 4.1:
Iteration 1:The visualization of the topic lists in 2D using PCA. Words
from the same theme list are visualized with the same color.
In repeat 2, all color related words are clustered together with stop words into
cluster 6.
So do words including "dressing",
"rich",
and "denotation".
Besides,
animal-related words are clustered into two different clusters, namely 2 and 3.
The
rest clusters remain semantically coherent.
In repeat 3,
all
clusters are semantically coherent,
except for cluster 6,
where
"rich" is clustered with stop words.
The results suggest that comparing to k-means clustering,
spectral
clustering
is better at discovering semantically coherent word clusters in the presence of stop
words.
Besides,
it is also able to produce consistent results.
On the contrary,
k-means is inconsistent and greatly influenced by the stop words.
So far, we have seen a very good performance of the spectral clustering algorithm
on two synthetic documents.
These two documents consist of
topic lists that are
clearly distinguishable to ordinary readers.
To some extent,
the second document
resembles the real-world documents which may discuss a wide range of topics and
contain many words that are not related to any apparent topic.
4.3
A short real-world document
We run the two clustering algorithms for one time with different settings of
k
.
The results of k-means clustering and spectral
clustering algorithms are shown in
4.3.
A SHORT REAL-WORLD DOCUMENT
43
Member words (k-means)
Cluster ID
Repeat 1
Repeat 2
Repeat 3
1
economy, consumer,
economics, money,
finance, capitalism,
macroeconomics,
bank
economy, consumer,
dollar, economics,
money, finance,
capitalism, euro,
macroeconomics,
bank
economy, consumer,
economics, money,
finance, capitalism,
macroeconomics,
bank
2
cow, tiger, sheep,
horse, fox, dog,
chicken, archery,
goat, rabbit, cat
cow, sheep, horse,
chicken, goat
cow, tiger, sheep,
horse, fox, dog,
chicken, archery,
goat, rabbit, cat
3
white, red, green,
purple, blue, black,
orange, yellow
tiger, fox, dog,
rabbit, cat
white, red, green,
purple, blue, black,
orange, yellow
4
football, swimming,
baseball, basketball,
tennis
football, archery,
swimming, baseball,
basketball, tennis
football, archery,
swimming, baseball,
basketball, tennis
5
denote, denoted,
denotes, denoting
denote, denoted,
denotes, denoting
denote, denotation,
denoted, denotes,
denoting
6
through, after, rich,
dollar, under, same,
denotation, above,
is, euro, where,
could, between,
each, until, about,
because, from,
their
white, through,
dressing, after, red,
rich, green, under,
same, purple,
denotation, above,
is, blue, where,
could, between,
black, each, until,
about, because, from,
their, orange,
yellow
through, after, rich,
under, same, above,
is, where, could,
between, each, until,
about,
because, from, their
7
fashion, dressing,
stylish, artistic,
designer, design
fashion, stylish, artistic,
designer, design
fashion, dressing,
stylish, artistic,
designer, design
Table 4.1:
Run 2:
Discovered seven word clusters using k-means clustering.
44
CHAPTER 4.
VERIFYING THE WORD CLUSTERING ASSUMPTION
Figure 4.2:
Run 2:
The visualization of
the embedded words in 2D using PCA.
Words from the same theme list are visualized with the same color.
table 4.2 and 4.3 respectively
1
.
For each cluster, we sort the words by their cosine
similarity to the cluster centroid in a descending order.
The k-means clustering algorithm produced highly unbalanced clusters.
Many
informative words are clustered together with stop words.
Some clusters contain
only one word.
In contrast,
spectral
clustering algorithm produced more balanced clusters.
When
k
= 3,
informative words and stop words are all
well
separated from each
other.
When
k ∈ {
5
,
7
}
, it discovered refined topics of politics (cluster 1), Britain
(cluster 2),
announcement (cluster 3) and non-informative words (cluster 4 & 5).
When
k
= 7, it further discovered a topic about hints.
These results show that the
spectral clustering algorithm is robust in the presence of stop words.
It also discov-
ers different levels of topics with different
k
.
This suggests that it is less sensitive
to
k
and does not rely on the exact number of topics to find good clusters.
This
property is desired considering that in a real-world document there is typically no
ground truth of the discussed topics.
As a quantitative evaluation, we also plot the Silhouette coefficient and within
sum of variance of the clusters with different
k
,
as shown in figure 4.3.
The with-
in sum of
variance drops steadily as
k
increases which is expected.
The plot of
Silhouette coefficients suggests that spectral clustering generated clusters of better
quality.
However, it also suggests that two clusters are best at describing the doc-
1
The k-means produces inconsistent clusters after each run,
while the spectral
clustering al-
gorithm is consistent.
For the sake of
readability,
we only report the result of
one run for both
clustering algorithms
4.4.
A LONG REAL-WORLD DOCUMENT
45
ID
k = 3
k = 5
k = 7
1
not, the, that, would,
he, this, then, we,
what, ask, my,
presume, in, will,
may, be, for, ready,
meant, expected,
told, on, shortly,
out, is, very, given,
hints, his, call,
announcement,
getting, at, election,
weekend, monday,
clarified, live, home,
general, fresh, has,
tipped, five, seat, poll,
an, announced, blair,
ex, constituency,
david, clues, radio,
queen, secretary, tony,
dissolve
the, would, not, that,
this, will, then, may,
in, expected, for, ask,
be, ready, shortly,
presume, on, meant, is,
hints, given, out,
announcement, call, at,
weekend, getting, his,
home, fresh, has, announced,
live, clarified, five, tipped,
monday, an, general, clues,
poll, ex, seat, radio, queen,
dissolve, tony
the, then, he, that, what,
this, my, we, in, ask, for,
his, out, on, at, ready,
shortly, told, home,
weekend, getting, very,
monday, call, live, fresh,
five, an, general, has,
blair, seat, tipped, poll,
ex, clues, queen, tony,
secretary
2
bbc
david, bbc, blair
bbc, david
3
parliament
we, he, my, what, very,
told, secretary
parliament, election,
constituency
4
constituency, election
would, may, will, not,
presume, meant, be,
expected, is, given,
clarified, hints
5
parliament
radio
6
announcement, announced
7
dissolve
Table 4.2:
Iteration 3:
Discovered word clusters using k-means clustering
ument, which contradicts our qualitative evaluation.
We have concluded that more
refined topics can be discovered when
k ∈ {
5
,
7
}
.
As we argued in section 3.5.1, the
internal cluster validation criteria do not necessarily reflect the semantic coherence
of clusters.
4.4
A long real-world document
For the sake of
readability,
we only qualitatively evaluate the word clusters dis-
covered by spectral
clustering algorithm with
k
=
d
log
ne
.
The spectral
clustering
46
CHAPTER 4.
VERIFYING THE WORD CLUSTERING ASSUMPTION
ID
k = 3
k = 5
k = 7
1
election, told,
announcement,
parliament, poll,
constituency, ask,
secretary, clarified,
general, announced,
seat, call, hints,
dissolve, tipped
election, constituency,
poll, parliament, seat,
secretary, general,
dissolve
election, constituency,
poll, parliament, seat,
secretary, general,
dissolve
2
david, blair, bbc,
monday, tony, my,
live, presume,
queen, ex, radio
david, blair, bbc,
monday, tony, queen,
live, ex, radio
david, blair, bbc,
monday, tony, queen,
live, ex, radio
3
the, that, not, this,
would, he, then,
we, in, will, for,
may, be, what,
ready, is, on, out,
meant, expected,
getting, very, at,
shortly, given, has,
his, weekend,
five, fresh, home,
an, clues
announced, hints,
expected, announcement,
told, tipped, shortly,
clarified, clues, ready, call
announced, announcement,
expected, told, clarified,
tipped, ready, call
4
not, would, that, may,
this, we, be, will, presume,
what, is, very, ask, meant,
given
hints, clues, fresh, his
5
the, he, in, then, my, his,
for, out, on, home, at,
getting, five, has,
weekend, fresh, an
then, in, the, shortly,
out, on, at, home,
weekend, five
6
would, is, may, will,
this, be, for, has, given,
an
7
not, we, what, that,
he, my, ask, very,
presume, meant, getting
Table 4.3:
Iteration 3:
Discovered word clusters using spectral clustering
4.4.
A LONG REAL-WORLD DOCUMENT
47
(a) Silhouette coefficient
(b) Within sum of variance
Figure 4.3:
Iteration 3:
Silhouette coefficient and within sum of
variance of
the
clusters discovered with different
k
algorithm produces consistent word clusters with different runs.
For each cluster,
we show the top 3 words that are closest to its centroid and 3 words that are far-
thest, as shown in table 4.4.
We also plot the Silhouette coefficient and within-sum
of variance with different
k
, as shown in figure 4.4.
ID
Top 3 words
Bottom 3 words
Normalized size
0
couture, fashion, clothing
goods, champagne, duty
0.1236
1
sold, purchased, bought
purchased, bought, sale
0.0449
2
profitable, businesses,
investment
houses, stay, us
0.1011
3
brand, brands, label
based, group, core
0.1236
4
paris, mr, louis
florida, christian, moet
0.0786
5
exact, details, planned
said, ready, focusing
0.0898
6
it, the, which
an, as, runs
0.1011
7
for, in, including
ranges, struggling, opening
0.2247
8
been, had, has
many, by, sum
0.1123
Table 4.4:
Iteration 4:
Discovered word clusters using spectral clustering
From the top 3 words we can clearly tell the topic of cluster 0, 1 and 3, namely
fashion,
expense,
business and brand respectively.
The topic of cluster 5 seems to
be about information.
Cluster 4 contains mostly names.
In contrast,
it is more
difficult to tell the topics from the bottom 3 words.
Cluster 6, 7 and 8 contain mostly non-discriminating words such as stop words.
We notice that the total
normalized size of
these three clusters is 0.4381,
while
cluster 0,1,2 and 3 together is only 0.3932.
This suggests that the normalized size
is not a good measure of the contribution of each cluster.
Instead,
we need a way
to assign the non-informative words with lower weight.
Motivated by this finding,
we propose a word importance function in Chapter 5.
48
CHAPTER 4.
VERIFYING THE WORD CLUSTERING ASSUMPTION
(a) Silhouette coefficient
(b) Within sum of variance
Figure 4.4:
Iteration 4:
Silhouette coefficient and within sum of
variance of
the
clusters discovered with different
k
As shown in figure 4.4, the Silhouette coefficient plot suggests that spectral clus-
tering performs better than k-means.
As
k
increases, spectral clustering maintains a
positive silhouette coefficient.
In contrast, the performance of k-means deteriorates
in general as k increases.
Notice that, as we argued in the previous iteration, such
internal cluster validation criteria do not necessarily reflect the semantic quality of
clusters.
4.5
Discussion
We evaluated the performance of
two notable clustering algorithms on both syn-
thetic and real-world documents.
We observed the superior performance of spectral
clustering comparing to k-means clustering.
Spectral clustering is able to discover
semantically coherent clusters in the presence of
stop words.
It is also able to
capture different levels of
topics with different
k
.
This implies that the spectral
clustering can discover good word clusters even when the true number of clusters is
unknown.
We note that the internal
cluster validation criteria do not necessarily reflect
the semantic quality of the word clusters.
It is also difficult to determine the true
number of
topics in a real-world document.
We attribute this to the subjective
nature of document understanding.
Different readers may have a different under-
standing of
the same word or word cluster due to the different experience of
the
English language.
We observed that real-world documents contain a big portion of non-informative
words like stop words.
As suggested by the TF-IDF model,
such words have little
discriminating power, therefore should be assigned with less importance.
In section
5.1, we propose a logistic word importance function that estimates the discriminat-
ing power of a word based on the empirical
cumulative probability distribution of
the cosine similarity between words in the word2vec vector space.
Finally, we conclude that the word clustering assumption put forward in section
4.5.
DISCUSSION
49
1.2 is valid.
Semantically similar words do form word clusters in the word2vec vector
space and appropriate clustering algorithm can consistently discover such clusters.
Chapter 5
The proposed model
This chapter presents the details of the proposed model.
We first propose a logistic
word importance function based on a loose theoretical foundation.
We then propose
a clustering-based topic model
that transforms a document into a set of weighted
topic embeddings.
We call the resulting document representation normalized bag-
of-topic-embeddings (nBTE) vector.
Finally, we propose to use the soft cosine
similarity as the distance metric between two nBTE vectors.
5.1
Logistic word importance
Results from experiment 1 show that real-world documents contain many non-
discriminating words which should not be treated equally [43].
We examine the
distribution of
cosine similarity between words in order to propose a model
for
estimating word importance.
Let
D
be the corpus of a collection of documents and
R
be the dictionary consist-
ing of all the unique words in
D
.
We regard "the" as the least discriminating word
among all words and expect words that are close to "the" are also non-discriminating.
This is a reasonable assumption considering "the" can occur in any context and pro-
vide little to none contextual
information.
Besides,
in the Brown Corpus
1
,
word
"the" counts for nearly 7% out of the 1 million word occurrences.
To verify this assumption, we retrieved the top 50 words that are closest to "the"
in the BBCNews data set.
These words are
"the, this, in, that, another, however, one, entire, its, which,
their, only, it, on, our, rest, whole, any, then, itself, they,
particular, current, for, also, same, last, his, just, actually,
all, them, now, time, first, each, he, perhaps, what, an,
second, merely, my, the, during, clearly, from, over, within"
which indeed have little discriminating power.
1
Brown Copus:
http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM
51
52
CHAPTER 5.
THE PROPOSED MODEL
Let us denote
cosine
(
w
) to be the cosine similarity between a word
w
and "the".
One way to model the word importance is then
1
− cosine
(
w
)
(5.1)
The above formula implies that the discriminating power decreases linearly with
cosine
(
w
).
However,
it is counter-intuitive to say "since" is about two times more
discriminating than "this" only because
cosine
(”this”)
cosine
(”since”)
=
0
.
59
0
.
32
≈
2.
We desire an im-
portance function that works in a way that if two words are both non-discriminating
enough,
then their importance is both very close to 0.
More importantly,
since
cosine
(
w
)
∈
[
−
1
,
1]
,
this formula assumes that when
cosine
(
w
) = 0,
word
w
is
considered equally discriminating and non-discriminating.
This assumption lacks
empirical and theoretical evidence for support.
Instead of
a linear function,
we propose a logistic word importance function.
We model
X
as a random variable whose value is the cosine similarity between
some random word
w
and "the".
We can then compute the empirical
cumulative
distribution function
F
(
x
),
which tells the probability of
some random word
w
having
cosine
(
w
)
< x
.
From a statistical
point of
view,
this means that for a
word with
cosine
(
w
) =
x
, it is closer to "the" than
F
(
x
) percents of all the words.
Therefore
F
(
x
) is a reasonable estimation of the relative discriminating power of a
word.
We calculate
F
(
x
) using all the three billion words provided by the pre-trained
Google word2vec model.
We then fit a logistic function
2
F
0
(
x
),
as shown in figure
5.1.
We can see that the fitted logistic function closely resembles
F
(
x
).
The choice of logistic function originates from the SGNS (skip-gram with neg-
ative sampling) model,
with which the Google word embeddings are trained.
The
SGNS models the probability of a word
w
occurring in context
c
as
P
(
w
occured in
c|w, c
) =
1
1 +
e
~
w·~c
(5.2)
which is essentially a logistic model.
The term
~
w · ~c
is also related to the co-
sine similarity since
cosine
_
similarity
(
~
w, ~c
) =
~
w·~c
k ~
wkk~ck
.
An intuitive interpretation
of
P
(
w
occured in
c|w, c
) is the probability of
word
w
expressing the contextual
meaning of
c
.
If we substitute "the" for
c
, it then tells the probability of a word
w
expressing the contextual meaning of "non-informativeness", assuming that "the" is
an appropriate representative of the "non-informativeness".
Based on the above arguments, we propose the logistic word importance model
I
given by
I
(
w
) = 1
− F
(
cosine
(
w
)) = 1
−
1
1 +
e
−k
(
cosine
(
w
)
−µ
)
=
1
1 +
e
k
(
cosine
(
w
)
−µ
)
(5.3)
where
k
and
µ
are the parameters of the logistic function.
2
Logistic function:
f
(
x
) =
1
1+
e
−k
(
x−µ
)
.
5.1.
LOGISTIC WORD IMPORTANCE
53
Figure 5.1:
Fitting a logistic function to
F
(
x
) using the Google pre-trained word
embeddings.
We used
I
(
w
) = 1
− F
0
(
x
) to calculate the importance of all words in the BBC-
News data set.
We then randomly sampled 20 words in different
cosine
(
w
) ranges
and show their importances in table 5.1.
We can see that most non-discriminating
words like stop words have close to zero importance,
while discriminating words
close to one.
5.1.1
Word cluster importance
We can now calculate the importance of a word cluster
c
of size
n
as
IC
(
c
) =
n
X
i
=1
I
(
c
i
)
n
(5.4)
which is the averaged importance of all the member words.
Given a set of clusters
C
of size
k
, we define the weight of cluster
C
i
as
weight
(
C
i
) =
IC
(
C
i
) log
|C
i
|
P
k
j
=1
IC
(
C
j
) log
|C
j
|
(5.5)
The term log
|C
i
|
is used to attenuate the influence of
the cluster size,
similarly
to the log term used in the TF-IDF model
[43].
This attenuation is important
since non-discriminating words may occur rather frequently.
Different attenuation
functions can also be used, e.g.
p
|C
i
|
,
3
p
|C
i
|
.
54
CHAPTER 5.
THE PROPOSED MODEL
cosine(w)
Mean importance
20 sample words
(0.4,1]
0.0024
that, in, entire, only, they, then, itself, this,
our, one, whole, inthe, which, any, on,
another, their, particular, its, rest
(0.3, 0.4]
0.0325
half, remainder, aa, meanwhile, outside,
first, now, second, never, them, we, front,
actual, around, roughly, promptly, total,
when, had, by
(0.2, 0.3]
0.2482
fact, bizarre, totality, favorable, attached,
indisputable, further, significantly, dynamics,
adjacent, faltering, specific, rightfully,
anyhow, year, putative, definite, calendar,
gather, coincidentally
(0.1, 0.2]
0.7239
mysterious, illusion, schools, predominate,
personal, transforming, damned, detonation,
enlarge, domestic, undetermined, pinball,
concerned, sneaked, rugby, ignoring, logistics,
frustration, scattered, condensed
(0, 0.1]
0.947
rockers, nodded, monsters, profiles,
modulation, skateboarding, bits, fractions,
reduce, 2mg, peacefully, certitude, grays,
scrummaging, bled, copyrighted, excell,
thinkers, cafes, overthrow
(-1, 0]
0.9933
ghettos, synth, harry, empt, tartaric, auctioneer,
tpm, nevi, dolce, predispose, daydreams, voom,
mic, powerband, jetski, ketones, serotype,
gameboy, gayle, enfish
Table 5.1:
Samples of words with different cosine score
5.2
Topic modeling with word embeddings
Recall that given a corpus, the word2vec model learns a dense vector representation
of words using the following objective function
arg max
~
w,~c
1
N
X
w∈W
X
c∈C
(
w
)
P
(
c|w
)
(5.6)
where
W
is the set of all
words and
C
(
w
) the set of contexts containing word
w
.
The task of the word2vec training is to find
~
w
that is best at predicting the context
c
given
w
.
Therefore, 1) words that occur frequently in the same context would have
similar embeddings [32],
and vice versa.
Mikolov et al.
demonstrated that cosine
similarity between two word embeddings is an effective measure of
the semantic
similarity [32].
5.2.
TOPIC MODELING WITH WORD EMBEDDINGS
55
The distributional hypothesis states that 2), words appearing frequently in the
same context express similar contextual meaning [15].
From 1) and 2), we can con-
clude that 3), words with similar embeddings should also express similar contextual
meaning, and vice versa.
We could then interpret the consine similarity as a measure of the probability
of two words talking about the same topic since 4),
similar topics express similar
contextual meaning.
Thus we can model a document as a mixture of words over an
underlying set of topics.
The higher the cosine similarity between two words,
the
higher the probability they talk about the same topic.
Therefore a group of words
close to each other in the word2vec has a high probability of talking about the same
topic.
A natural
strategy to find such topic groups is using clustering algorithms to
generate word clusters such that words in the same cluster are close to each other
while being distant from words in other clusters.
Furthermore,
we can regard the
centroid of the cluster members as the topic embedding.
The intuition is that if a
cluster
S
contains words about the same topic,
then these words would also have
similar word embeddings,
because of 3) and 4).
Thus the centroid of
S
,
which is
the averaged embedding, should also be similar to all the words in
S
.
Using the discovered topic clusters, we can then represent a document as a set
of topic embeddings weighted by their importance, calculated using formula 5.5.
In
the following,
we describe the details of the model
for constructing this document
representation.
5.2.1
Notations
We first introduce several notations.
Word embedding matrix X
A pre-trained word embedding model provides a word embedding matrix X
∈ R
d×n
for a finite size vocabulary of
n
words.
The
i
th
column of
X corresponds to the
d−
dimensional embedding of the
i
th
word in the vocabulary.
For the sake of read-
ability, we denote X[
w
] to be the embedding of word
w
.
Word importance model
A word importance model
I
estimates the discriminating power of
a word
w
as
I
(
w
).
We use the logistic word importance model
defined in equation 5.3.
The
parameters are fit using the Google pretrained word embeddings[13].
Bag-of-words (BOW)
For a BOW representation
M
, we denote
unique
(
M
) to be the list of unique words
in
M
, and
M
(
w
) to be the multiplicity of a word
w
.
56
CHAPTER 5.
THE PROPOSED MODEL
Normalized bag-of-topic-embeddings (nBTE)
Let
v
be an ordered list of
d−
dimensional topic embeddings and
r
an ordered list of
non-negative real values, such that
|v|
=
|r|
and
P
|r|
i
=0
r
i
= 1.
We define the nBTE
representation as a
|v|
-dimensional vector
V
whose
i
th
component refers to
v
i
and
V
i
=
r
i
.
We denote
V
(
E
) to be
v
and
R
(
E
) to be
r
.
Clustering
We denote Clustering(
k, V
) to be a function that takes two inputs,
k
the number of
clusters and
V
the objects to be clustered,
and returns a list of clusters.
Different
clustering algorithms may require more hyperparameters to be specified.
For the
sake of readability, we use this simplified notation.
5.2.2
Building the nBTE representation
We now describe how to transform a document
d
to the nBTE representation.
We assume that we have access to a word embedding matrix X and a logistic
word importance function
I
.
We use algorithm 1 to build the nBTE representation
for a document
d
.
At step 2, we first build the BOW representation of a document.
At step 3, we
transform all the words to their word2vec embeddings.
At step 4, we use a clustering
algorithm to discover the topic clusters.
The results of experiment 1 suggest to use
the approximate spectral clustering algorithm [48].
Note that we also need to first
construct the pair-wise cosine similarity matrix for spectral clustering.
From step 6
to 16, we calculate the topic embedding
v
i
and importance score
r
i
for each cluster.
Step 9 and 11 require traversing through a word list of varying size.
Finally, at step
17, we normalize the importance scores
r
.
It is important to note that the spectral
clustering algorithm requires a non-
negative similarity matrix, as discussed in section 2.6.2.
However, the cosine simi-
larity between word embeddings lies in the range of [
−
1
,
1].
For any negative cosine
similarity, we simply set it to zero.
This is intuitive since when the cosine similarity
between two words is close to zero, we can already be sure that they are irrelevant.
The word2vec model
also only assumes that similar words are close to each other,
but makes no further assumption regarding the distant words.
Therefore by setting
a negative cosine similarity to zero, we do not lose too much information about the
semantic similarity.
Due to the subject nature of semantics, we could not determine
k
automatically
based on a sound heuristic.
Nevertheless, we use
k
=
d
log
ne
as an empirical choice
for the evaluation.
Results from experiment 1 also show that the spectral clustering
algorithm can produce good clusters even if the true number of clusters is unknown.
In real-life applications, the
k
should be set based on the domain knowledge of the
documents.
5.2.
TOPIC MODELING WITH WORD EMBEDDINGS
57
Algorithm 1 Normalized bag-of-topic-embeddings representation
Input:
S
:
document;
k
:
# clusters;
X:
word embedding matrix;
Output:
Normalized bag-of-topic-embeddings representation of
S
1
Function
build_representation(
S
,
k
, X
)
:
2
M ←
BOW representation of d
3
embedded
_
words ← {
X[
w
]
|w ∈ unique
(
M
)
}
4
Clusters ←
Clustering(
k, embedded
_
words
)
5
v ← {}, r ← {}
6
for
i ←
0 to
k
do
7
n ←
0
8
words ← Clusters
[
i
]
9
v
i
← centroid
(
Clusters
[
i
])
10
importance ←
0
11
for
j ←
0 to
length
(
words
) do
12
importance ← score
+
I
(
words
[
j
])
13
n ← n
+
M
(
words
[
j
])
14
end
15
r
i
=
importance
n
×
log
n
16
end
17
for
i ←
0 to
k
do
18
r
i
←
r
i
sum
(
r
)
19
end
20
return nBTE(
v, r
)
5.2.3
A view of data compression
We can also view the clustering procedure as a way of compressing a point set in
the d-dimensional space.
The task is to find
k
points that are best at capturing the
overall geometric shape of the original point set.
From this point of view, it is not
crucial
to know the exact number of topics
k
∗
in a document.
If
k
is significantly
smaller than
k
∗
, there could be too much loss of the original geometric information.
But when
k
is greater than
k
∗
,
the extra clusters still
contain locally close points.
Therefore we should expect the extra clusters to capture more details of the local
geometric shape of the original
point set.
However,
k
should not be set too large.
Otherwise many of
the
k
centroids could be capturing the noisy local
geometric
details, therefore reveal less of the overall geometric information.
58
CHAPTER 5.
THE PROPOSED MODEL
5.3
Soft cosine document similarity
The soft cosine similarity extends the cosine similarity by taking into account the
component-wise similarity [44].
Therefore it is particularly suitable for the nBTE
vectors whose components refer to the topic embeddings.
The semantic similarity
between two topic embeddings can be estimated using the cosine similarity since
they are in the word2vec space.
Given two documents,
we use algorithm 2 to
compute the soft cosine document similarity.
The algorithm assumes that two
documents are in their nBTE vector representation, namely
E
1
and
E
2
.
Algorithm 2 Soft cosine document similarity
Input:
E
1
:
nBTE vector representation of document 1;
E
2
:
nBTE vector representation of document 2;
Output:
Soft cosine document similarity between document 1 and 2;
21
Function
soft_cosine_similarity(
E
1
,
E
2
)
:
22
U ← V
(
E
1
).append(
V
(
E
2
))
23
a ← R
(
E
1
)
24
b ← R
(
E
2
)
25
M ←
empty matrix of dimension
|U | × |U |
26
for
i ←
0 to
|U | −
1 do
27
for
j ←
0 to
|U | −
1 do
28
M
[
i, j
]
←
cosine_similarity(
U
i
, U
j
)
29
end
30
end
31
similarity
←
P
|a|
i
P
|b|
j
M
[
i,j
]
a
i
b
j
q
P
|a|
i,j
M
[
i,j
]
a
i
a
j
q
P
|b|
i,j
M
[
i,j
]
b
i
b
j
32
return similarity
In the work of [44], Grigori Sidorov et al.
used Levenshtein’s distance to estimate
the similarity between words.
This is inapplicable to latent topics that do not have
a straightforward word representative.
Also note that in the original
soft cosine
similarity measure,
the two input vectors are assumed to have equal
length.
We
can trivially extend it to vectors of varying length by altering the summations in
the numerator of the formula, as shown in step 11.
Chapter 6
Performance evaluation
This chapter presents the evaluation results of the proposed model with respect to
the three concerned aspects, namely efficiency, effectiveness and interpretability.
6.1
Efficiency
We refer to algorithm 1 and algorithm 2 to analyze the average time complexity
of the proposed document representation model and the similarity measure respec-
tively.
At step 2, we first build the BOW representation of a document.
This requires
traversing through a tokenized document and counting their multiplicity, thus takes
O
(
n
) time.
At step 3,
we transform all
the words to their word2vec embeddings,
which take
O
(
n
) time as well.
At step 4, we use a clustering algorithm to discover the
topic clusters.
The results of experiment 1 suggest to use the approximate spectral
clustering algorithm [48],
which takes
O
(
n
1
.
5
k
+
nk
2
).
Note that we also need to
first construct the pair-wise cosine similarity matrix for spectral
clustering,
which
takes
O
(
n
2
).
From step 6 to 16, we calculate the topic embedding
v
i
and importance
score
r
i
for each cluster.
Step 9 and 11 require traversing through a word list of
varying size.
In total,
this takes
O
(
n
) time since we visit each cluster containing
distinct words only once and each word occurs in only one cluster.
Finally, at step
17, we normalize the importance scores
r
, which takes
O
(
k
) time.
All together this
algorithm has an average running time complexity of
O
(
n
1
.
5
k
+
nk
2
+
n
2
).
If
we
assume that
k
is
O
(
√
n
), the overall time complexity is
O
(
n
2
).
The soft cosine similarity measure assumes that two documents are in their
nBTE vector representation, namely
E
1
and
E
2
.
Step 26 to 29 computes the pair-
wise cosine similarity between topic embeddings.
This takes
O
(
|a||b|
) time,
where
|a|
and
|b|
refers to the number of embeddings in
E
1
and
E
2
respectively.
Step 31
requires pairing all
embeddings,
therefore takes
O
((
|a|
+
|b|
)
2
) time.
If we assume
that the number of topic embeddings is
O
(
√
n
) for both documents, the overall time
complexity of this algorithm is then
O
(
n
).
Therefore, the overall average time complexity of the proposed model is
O
(
n
2
).
59
60
CHAPTER 6.
PERFORMANCE EVALUATION
6.2
Effectiveness
In this section,
we present the results of experiment 2.
The accuracy scores of all
models run on all data sets with different
k
for the k-nearest neighbor classifier are
given.
We also analyze the results to evaluate and compare different models.
6.2.1
Overall
results
The accuracy score of all
models on the single-topic and multi-topic data sets are
shown in figure 6.1 and figure 6.2 respectively.
Overall, all models achieved higher accuracy on the single-topic data sets.
The
performance dropped significanly on the multi-topic data sets.
6.2.2
Comparing the baseline models with our model
Single-topic data sets
The range and mean of the difference in accuracy between the baseline models and
the proposed model with different
k
for the k-nearest neighbor classifier on the three
single-topic data sets are shown in table 6.1.
BBCNews
BBCSport
20newsgroup
LSI
[0, 0.02] @0.01
[0.04, 0.07] @0.05
[-0.03, -0.02] @-0.026
LDA
[-0.01, 0.02] @0.006
[0.02, 0.06] @0.04
[-0.02, 0.02] @-0.002
TF-IDF
[0, 0.02] @0.012
[0.04, 0.08] @0.06
[0.02, 0.05] @0.036
WCD
[0.01, 0.03] @0.013
[-0.09, -0.01] @-0.06
[-0.04, -0.01] @0.002
BOW
[-0.02, 0] @-0.01
[0.03, 0.05] @0.044
[-0.04, -0.01] @-0.028
Table 6.1:
The range and mean of the difference in accuracy between the baseline
models and the proposed model with different
k
for the k-nearest neighbor classifier
on three single-topic data sets.
Examining the range, the difference in accuracy between all the baseline models
and our model
is
<
0
.
05 on the BBCNews and 20newsgroup data sets for all
k
,
while on the BBCSport data set, it is
>
0
.
05 for LSI, LDA and TF-IDF for some
k
.
Except for the BBCSport data set, no baseline model consistently outperforms our
model on all data sets with different
k
, and vice versa.
On the BBCSport data set,
our model
consistently outperforms WCD with different
k
,
while it is consistently
outperformed by the other four baseline models.
Examining the mean, except for TF-IDF, the difference in accuracy between all
the baseline models and our model
is
<
0
.
05 on all
data sets.
The difference is
>
0
.
05 for TF-IDF on the BBCSport data set.
6.2.
EFFECTIVENESS
61
Figure 6.1:
Accuracy scores using k-NN classifier with different
k
on three single-
topic data sets.
62
CHAPTER 6.
PERFORMANCE EVALUATION
Multi-topic data sets
The mean of the relative improvement in accuracy (%) of our model
compared to
the baseline models with different
k
for the k-nearest neighbor classifier on the four
multi-topic data sets are shown in table 6.2.
We do not report the difference in
accuracy since it is obvious from figure 6.2 that our model
vastly outperforms the
baseline models.
BBCNews_3
BBCNews_4
20newsgroup_3
20newsgroup_4
LSI
55
52
28
48
LDA
55
60
61
54
TF-IDF
50
29
21
42
WCD
13
15
14
25
BOW
67
52
31
36
Table 6.2:
The mean of
the relative improvement in accuracy (%) of
our model
compared to the baseline models with different
k
for the k-nearest neighbor classifier
on four multi-topic data sets.
As shown in table 6.2, our model consistently outperforms all the baseline models
by a large margin on all
multi-topic data sets.
It outperforms the LDA model
by
at least 55% and at most 61% across the four data sets.
The WCD has better
performance comparing to the other four baseline models.
6.2.3
Comparing WMD with the soft cosine similarity
The range and mean of the difference in accuracy between the SC_wmd and our
model with different
k
for the
k
-nearest neighbor classifier on all data sets are shown
in table 6.3.
SC_wmd
BBCNews
[-0.02, -0.04] @-0.034
BBCSport
[-0.01, -0.07] @-0.042
20newsgroup
[-0.05, -0.06] @-0.056
BBCNews_3
[-0.02, -0.07] @-0.044
BBCNews_4
[-0.02, -0.08] @-0.048
20newsgroup_3
[-0.07, -0.2] @-0.114
20newsgroup_4
[-0.08, -0.1] @-0.086
Table 6.3:
The range and mean of the difference in accuracy between the SC_wmd
and our model with different
k
for the k-nearest neighbor classifier on all data sets.
From table 6.3 we can see that our model
consistently outperforms SC_wmd
with different
k
on all data sets.
6.2.
EFFECTIVENESS
63
Figure 6.2:
Accuracy scores using k-NN classifier with different
k
on four multi-topic
data sets.
64
CHAPTER 6.
PERFORMANCE EVALUATION
Topic
Words
Arts
lincoln, opera, york, philharmonic, new, real,
mark, performing, act, bit, music, performing,
leading, supporter
Children
randolph, hearst, center, opportunity, make,
future, important, traditional, social, share,
young
Education
william, monday, school, taught
Budgets
foundation, million, board, grants, support,
services, president, announcing, building,
receive, annual, fund, research, public, facilities
None
give, metropolitan, julliard, arts, areas, health,
medical, consolidated, corporate, donation, usual, new
Table 6.4:
Topic groups discovered by LDA according to Blei et al.
[4].
Topic names
are assigned by Blei et al.
Topic
Weight
Words
Topic 1
0.353
arts, music, opera, philharmonic, artists, education,
school, taught, performing, young, house
Topic 2
0.277
services, facilities, grants, fund, provide, health,
medical, million, public, metropolitan, receive,
areas, social, corporate, research, consolidated,
center, annual, share
Topic 3
0.115
supporter, foundation, president, donation,
support, board, said, building
Topic 4
0.044
real, make, bit, important, opportunity, future,
usual, new, felt, traditional, leading, announcing,
mark, act
Topic 5
0.21
william, lincoln, york, monday, randolph
Table 6.5:
Topic groups discovered by our model with
k
= 5.
6.3
Interpretability
In this section, we present the results of the experiment 3.
6.3.1
Comparing with LDA
The topic clusters discovered by LDA and our model are shown in table 6.4 and 6.5
respectively.
We do not name the topics discovered by our model.
Instead,
we leave it to
the readers to decide since such naming is purely subjective.
Nevertheless,
we
evaluate the discovered topics based on our experience of the English language.
As
6.3.
INTERPRETABILITY
65
Topic
Weight
Words
Topic 1
0.228
opera, music, philharmonic, arts, artists, performing
Topic 2
0.08
grants, donation, receive, fund, million, provide, share
Topic 3
0.074
facilities, areas, services, metropolitan, center,
consolidated, building
Topic 4
0.13
health, education, social, medical, public, corporate,
research
Topic 5
0.116
supporter, president, foundation, support, board, said
Topic 6
0.07
taught, school, young, house
Topic 7
0.01
opportunity, important, felt, real, make, act
Topic 8
0.02
announcing, annual, new, traditional, leading,
future, mark
Topic 9
0.266
william, lincoln, york, monday, randolph
Topic 10
0.005
bit, usual
Table 6.6:
Topic groups discovered by our model with
k
= 10
shown in table 6.5,
topic 1 contains mostly words about arts education.
Topic 2
is about public affairs.
The topic of
cluster 3 is less obvious but it seems to be
about foundation.
Topic 5 contains mostly names.
And topic 4 contains mostly
non-informative words in the context of this document.
As expected,
this topic is
also assigned with the least weight.
Comparing to LDA, our model produced more
semantically coherent topic groups.
LDA failed to identify the informative words
like "metropolitan",
"arts",
"health",
"medical",
etc.
The topic groups also contain
non-informative words like "bit", "new" and "make".
To show the ability of
our model
to discover different levels of
topics,
we set
k
= 10 and present the discovered topic clusters in table 6.6.
These clusters are all
semantically coherent.
Furthermore, we discovered refined topics like grants (topic
2),
facilities (topic 3),
and education (topic 4).
The weights of
the clusters also
reflect the informativeness.
Topic 7 and 10 contain mostly non-informative words
and are assigned with the least weight.
6.3.2
More sample documents
BBCNews sample document
As we can see from table 6.7, the model successfully discovered the topics of medical
devices (topic 1) and healthcare business (topic 2).
These two topics also have
significantly higher weights than topic 3 and topic 4, which are non-informative.
BBCSport sample document
As can be seen from table 6.8, the model successfully discovered the topics of sport
(topic 1) and countries (topic 3).
Topic 2 and topic 4 are less informative and have
66
CHAPTER 6.
PERFORMANCE EVALUATION
Weight
Words
Topic 1
0.30
stents, stent, pacemakers, implant,
defibrillators, artery, heart, heartbeat,
tubes, rhythm, drug, medical, irregular,
electric, shock, overlapping
Topic 2
0.49
company, firms, industries, products,
business, antitrust, mergers, pharmaceutical,
regulators, price, costs, technology, patent,
firm, demand, analysts, 4bn, generic,
healthcare, giant, expirations, buy,
fragmented, share, offsetting, operations,
competition, reliance, equipment, producer
Topic 3
0.09
the, for, that, it, also, because, they, some,
when, while, or, as, which, after, on, out,
are, from, at, is, by, more, than, been,
meanwhile, keep, such, each, has, under,
us, give, number, open, key, small, widely,
used, an, closing, aimed, wednesday, johnson
Topic 4
0.12
likely, expected, could, agreed, said,
agrees, cut, facing, move, battling, deal,
pressure, say, slowing, pay, will, force,
increase, problems, pointed, shed, fierce,
detected, unblocked, combats
Table 6.7:
BBCNews sample document:
Discovered topic clusters and their weights
less weight.
Topic 3 contains mostly names and is has the highest weight.
20newsgroup sample document
As can be seen from table 6.9, the model discovered the topic of encryption (topic
1).
Topic 2 contains mostly words about the market and politics.
Topic 3 contains
mostly country names and abbreviations and has the highest weight.
6.3.
INTERPRETABILITY
67
Weight
Words
Topic 1
0.16
matches, players, player, team, play,
squad, captain, tennis, singles, round,
miss, career, cup, schedule, competing, ranking
Topic 2
0.11
really, we, not, that, he, them, like,
things, great, real, she, said, someone,
our, would, when, particular, explained,
insisted, opportunity, looking, holistically,
her, fully, awkward, fed, request, stuffed
Topic 3
0.65
india, alexander, australia, singapore,
stewart, john, april, zealand, samantha,
korea, delhi, taiwan, alicia, nicole, pratt,
china, robin
Topic 4
0.08
the, for, one, in, have, this, is, be, are,
all, other, these, considered, was, their,
on, consider, will, without, as, included,
part, from, number, new, after, plus, has,
first, date, up, year, behind, form, leading,
world, events, achieve, an, geographical,
location, nations, focusing, designed
Table 6.8:
BBCSport sample document:
Discovered topic clusters and their weights
68
CHAPTER 6.
PERFORMANCE EVALUATION
Weight
Words
Topic 1
0.26
hackers, encryption, spy, hacker, wiretap,
spying, encrypt, computer, phones, telephones,
phone, privacy, bugged, spooks, communications,
crypto, smart, diplomats, voice, chip, stunt
Topic 2
0.25
companies, governments, government, business,
market, foreign, domestic, distribution, internationally,
international, workers, alternative, citizens, jobs,
inexpensive, our, world, are, private, electronics,
require, core, import, standard, issue, rigs, bans,
silicon, lines, example, organization, distributed,
Topic 3
0.34
japanese, jim, dutch, german, korean, wright,
facto, edu, hart, shit, etc, taiwanese, com, nsa,
doesn, ucla, aren, thru, asic, screw, agora, stupidity,
de, fascist, don, jester, clipper
Topic 4
0.08
but, the, only, for, that, even, in, however, still,
when, this, where, on, be, perhaps, as, make, over,
time, own, from, may, out, since, give, about, their,
at, more, thus, well, put, has, is, assuming, including,
who, allowing, by, open, subject, privately, wide,
Topic 5
0.11
do, know, want, just, not, anybody, going, somebody,
what, anything, so, we, it, here, else, if, things, always,
them, need, can, good, why, talking, ourselves, much,
cares, talk, real, your, somewhere, dumb, hard, find,
true, lose, people, help, please, attitude, concerned
Table 6.9:
news20 sample document:
Discovered topic clusters and their weights
Chapter 7
Conclusion
We discussed a wide range of
text representation models in Chapter 2.
Most of
them share the same paradigm:
learning a vector representation of the text using
an external corpus.
From the simplest BOW vector to more sophisticated word2vec
embedding, the evolution of these models is centered around building a denser vector
representation while utilizing more refined word-
cooccurrence statistics derived
from the corpus.
The WMD model deviates from this corpus-based approach.
Instead of building
a dense document vector,
it uses the Google pre-trained word embeddings [13]
to
directly estimate the document similarity as the cumulative distance between the
words of
two documents.
Results from [25]
show that it outperforms the state-
of-art corpus-based models for document similarity.
Kusner et al.
attributed the
performance of WMD to the high-quality Google word embeddings which are trained
on 100 billions of words [13].
In this thesis,
we went one step further from WMD.
We showed that it is not
only feasible, but also effective to directly infer the content of a document from its
words in a straightforward way.
We proposed a clustering-based topic model
that
transforms a document into a vector whose component refers to a topic embedding.
This model
requires only the pre-trained word embeddings,
without the need of a
corpus.
We proposed to use the soft cosine similarity as the distance metric between
the resulting document representation.
The average time complexity of the model
and similarity measure together is
O
(
n
2
),
which is an order of
exponent better
than that of
WMD.
Our results show that it achieved comparable classification
accuracy with the state-of-art baseline models on the single-topic data sets.
It even
consistently outperforms the state-of-art baseline models by a large margin on four
challenging multi-topic data sets.
We compared the proposed model with LDA on
one sample document for topic discovery.
The results show that the proposed model
produced semantically coherent topic groups and is able to discover different levels
of topics.
We conclude this thesis by directly answering the research question raised in
Chapter 1:
69
70
CHAPTER 7.
CONCLUSION
Can we construct an interpretable document representation by clus-
tering the words in a document, and effectively and efficiently esti-
mate the document similarity?
This question is concerned with three aspects, namely the interpretability, effective-
ness and efficiency of the proposed model.
7.1
Discussion
We fully answer the research question by addressing each of the concerned aspect
separately.
7.1.1
Word clusters in the word2vec vector space
The central assumption of the proposed model is that similar words form clusters in
the word2vec space.
To verify this assumption, we conducted an iterative experiment
with multiple synthetic and real-world documents,
as presented in Chapter 4.
We
started from a simple synthetic document with clear topic groups to more complex
real-world documents where the topics are subtle.
We concluded that semantically
similar words do form word clusters in the word2vec vector space, and such clusters
can be consistently discovered using an approximate spectral
clustering algorithm
proposed by Yu et al.
[48].
In section 4.2,
we showed that the spectral
clustering algorithm consistently
produces word clusters containing words about the same topic,
variants of
words
of
the same root,
and even stop words.
In section 4.3 and 4.4,
we showed that
it consistently produces semantically coherent word clusters on two complex real-
world documents.
We also observed that with a different choice of
k
,
which is the
number of
clusters,
it discovers different levels of
topics.
As shown in table 4.3,
cluster 1 contains words about politics and announcement when
k
= 3.
When
k
is
increased to 5, cluster 1 is split into two clusters, one about politics and one about
announcement.
During this experiment, we noticed that the internal cluster validation criteria,
e.g.
Silhouette coefficient and with-in sum of variance, does not reflect the seman-
tic coherence of
the clusters.
Instead,
they contradict our qualitative evaluation
based on our experience of the English language.
This confirms the arguments put
forward by Estivill-Castro [10],
"...these indexes are merely more elaborate mathe-
matical formulations of the clustering criteria..." but do not reflect what constitutes
a desired cluster, e.g.
semantic coherence.
This shows the importance of an iterative
experiment where the end users are involved in the evaluation due to the subjective
nature of semantics.
7.1.
DISCUSSION
71
7.1.2
Effective similarity measure
We compared the performance of our model with a set of state-of-art baseline models
in a k-nearest neighbor classification task.
Our model has comparable performance
with all
the baseline models on two out of
the three single-topic data sets.
We
also noticed that comparing to WMD, the soft cosine similarity is a more suitable
distance metric for our model.
On these three data sets,
each label
refers to only
one topic.
This means that it is possible to predict the label
by looking at some
highly discriminating words that only appear around some specific topics.
Simple
models like BOW and TF-IDF can easily capture such words.
To demonstrate the ability of capturing multiple topics, we evaluated the models
on four challenging multi-topic data sets.
In these data sets, each label refers to a
set of topics.
Furthermore, two labels have only one topic in difference.
This poses a
significant challenge to the models since they need to capture the subtle difference in
the underlying set of topics of the documents.
On these four multi-topic data sets,
our model
consistently outperforms the baseline models by a large margin.
This
shows that our model is particularly useful when the label of a document refers to
a combination of multiple topics.
7.1.3
Efficient similarity estimation
We argued the efficiency of our model in terms of its time complexity, in comparison
with the WMD model
of
an average time complexity
O
(
n
3
log
n
) [25].
The time
complexity of our model is dominated by the spectral clustering algorithm of average
time complexity
O
(
n
1
.
5
k
+
nk
2
),
where
k
is the number of clusters.
As shown in
section 6.1,
the average time complexity of
our model
is
O
(
n
2
),
assuming that
k
=
O
(
√
n
).
In fact,
we evaluated our model
with
k
=
d
log
ne
,
which is an even
tighter bound than
O
(
√
n
).
Therefore, in terms of time complexity, our model is at
least an order of exponent faster than WMD.
7.1.4
Interpretable document representation
The resulting document representation of our model
is a normalized bag-of-topic-
embeddings (nBTE) vector.
Each component of an nBTE vector refers to a topic
embedding, which is the centroid of a word cluster discovered by the spectral clus-
tering algorithm.
We can examine the content of
a topic by querying its closest
words in the document.
Therefore we can interpret a document as a mixture of
an underlying set of topics specified by the components of the nBTE vector.
The
contribution of each topic is quantified by the corresponding entry.
As shown in table 6.5, we can interpret the nBTE vector of the document shown
in figure 3.6 as 35% of arts education, 27% of public affairs, 12% of foundation, 21%
of names, and 5% of some non-informative words in the context of the document.
It is important to note this contribution score takes into account the discriminat-
ing power of the words.
The results of experiment 1 show that real-world documents
contain a large portion of non-discriminating words like stop words.
However,
we
72
CHAPTER 7.
CONCLUSION
desire a document representation that reflects meaningful
topics.
To address this
issue, we proposed a logistic word importance function that assigns words different
weights based on their discriminating power,
as defined in equation 5.3.
It as-
sumes a logistic distribution of the relative discriminating power of a random word.
This assumption originates from the loss function of the SGNS model
[33],
which
models the probability of a word appearing in a context using a binomial
logistic
model.
The parameters of the distribution are fit using all the Google pre-trained
word embeddings.
The results in table 5.1 show that the proposed word importance
function assigns non-discriminating words like stop words close to zero weight, while
discriminating words are assigned close to one weight.
7.2
Drawbacks and future work
Despite the encouraging results, our model has several drawbacks.
As shown in section 6.2.2,
our model
has inferior performance on the domain-
specific BBCSport data set.
Its performance largely relies on the quality of
the
pre-trained word embeddings.
Yet the pre-trained word embeddings merely reflect
the knowledge in the corpus on which they are trained.
One promising remedy to
this drawback is to train a word embedding model
on the domain specific corpus.
This may seem to counteract the main advantage of
our model:
that we do not
require an external corpus.
However, since our model requires the pre-trained word
embeddings,
it does indirectly require an external
corpus.
The real
advantage of
our model is that it can transfer the knowledge of the pre-trained word embeddings
to other tasks like topic discovery and document similarity.
An intuitive way to
think of it is that the pre-trained word embeddings serve as a dictionary.
Like we
human can understand a document by only looking up an English dictionary,
the
machines can just look up the word embeddings to infer the content of a document
using our model.
Another drawback of our model
is that the topic weighting is sensitive to rare
words like names and abbreviations,
as shown in table 6.7 and 6.8.
Topic clusters
containing this type of words are assigned with much higher weight.
This is expected
since the importance of
a word is calculated based on its distance to the word
"the".
In the word2vec space, names and abbreviations are among the farthest from
the word "the".
Therefore they are also assigned with the highest weight.
One
future work is to improve the word importance function so that it has a more solid
statistical ground.
The major drawback of
our work is that we did not formalize a probabilistic
model.
Therefore we are not able to study the topic clusters in a more principled
way.
We only informally introduced a topic model
by arguing that the cosine
similarity between words reflects the probability of
two words talking about the
same topic.
But we do see the possibility of proposing a sound probabilistic model.
One future work could be comprehensibly studying the probabilistic implication of
the word2vec model.
We could also experiment with other word embedding models
7.2.
DRAWBACKS AND FUTURE WORK
73
that have a solid statistical
ground.
For example,
the Gaussian word embedding
maps each word in a space of Gaussian distributions [45].
It may provide us with
more flexible probabilistic tools for a probabilistic topic model.
Another promising future work is to experiment with other distance metric for
the nBTE vectors.
For example, the Hausdorff distance measures how far two finite
point sets are from each other.
In fact, an nBTE vector is essentially a weighted set
of points in the
d
dimensional space, where
d
is the length of the topic embedding.
Therefore the Hausdorff distance is an applicable distance metric for the nBTE
vectors.
It has already been successfully applied in image matching tasks [19][22].
Bibliography
[1]
David J.
Aldous.
Exchangeability and related topics.
In P.
L.
Hennequin,
editor,
École d’Été de Probabilités de Saint-Flour XIII — 1983,
pages 1–198,
Berlin, Heidelberg, 1985. Springer Berlin Heidelberg.
[2]
B.
Ribeiro-Neto Baeza-Yates,
R.
Modern information retrieval.
Addison-
Wesley, 1999.
[3]
Yoshua Bengio,
Réjean Ducharme,
Pascal
Vincent,
and Christian Janvin.
A
neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March
2003.
[4]
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
Latent dirichlet alloca-
tion.
J. Mach. Learn. Res., 3:993–1022, March 2003.
[5]
Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam
Kalai.
Man is to computer programmer as woman is to homemaker? debias-
ing word embeddings.
In Proceedings of the 30th International
Conference on
Neural Information Processing Systems, NIPS’16, pages 4356–4364, USA, 2016.
Curran Associates Inc.
[6]
Gobinda G.
Chowdhury.
Natural
language processing.
Annual
Review of
In-
formation Science and Technology, 37(1):51–89.
[7]
Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer,
and Richard Harshman.
Indexing by latent semantic analysis.
JOURNAL OF
THE AMERICAN SOCIETY FOR INFORMATION SCIENCE,
41(6):391–
407, 1990.
[8]
Richard O.
Duda,
Peter E.
Hart,
David G.
Stork,
C R.
O.
Duda,
P.
E.
Hart,
and D. G. Stork.
Pattern classification, 2nd ed, 2001.
[9]
Martin Ester,
Hans-Peter Kriegel,
Jörg Sander,
and Xiaowei
Xu.
A density-
based algorithm for discovering clusters a density-based algorithm for discover-
ing clusters in large spatial databases with noise.
In Proceedings of the Second
International
Conference on Knowledge Discovery and Data Mining, KDD’96,
pages 226–231. AAAI Press, 1996.
75
76
BIBLIOGRAPHY
[10]
Vladimir Estivill-Castro.
Why so many clustering algorithms:
A position pa-
per.
SIGKDD Explor. Newsl., 4(1):65–75, June 2002.
[11]
Fasttext.
English word vectors, 2017.
[12]
Yoav Goldberg and Omer Levy.
word2vec explained:
deriving mikolov et al.’s
negative-sampling word-embedding method.
CoRR, abs/1402.3722, 2014.
[13]
Google.
Word2vec, 2013.
[14]
Greg Hamerly and Charles Elkan.
Alternatives to the k-means algorithm that
find better clusterings. In Proceedings of the Eleventh International Conference
on Information and Knowledge Management,
CIKM ’02,
pages 600–607,
New
York, NY, USA, 2002. ACM.
[15]
Zellig S. Harris. Distributional Structure, pages 775–794. Springer Netherlands,
Dordrecht, 1970.
[16]
Julia Hirschberg and Christopher D. Manning.
Advances in natural language
processing.
Science, 349(6245):261–266, 2015.
[17]
Thomas Hofmann.
Probabilistic latent semantic indexing.
In Proceedings of
the 22Nd Annual
International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval,
SIGIR ’99,
pages 50–57,
New York,
NY,
USA, 1999. ACM.
[18]
W. John Hutchins.
Machine translation:
A brief history.
In Concise history of
the language sciences:
from the Sumerians to the cognitivists, Pergamon, pages
431–445. Press, 1995.
[19]
D.
P.
Huttenlocher,
G.
A.
Klanderman,
and W.
J.
Rucklidge.
Comparing
images using the hausdorff distance.
IEEE Transactions on Pattern Analysis
and Machine Intelligence, 15(9):850–863, Sept 1993.
[20]
A.
Håkansson.
Portal
of
research methods and methodologies for research
projects and degree projects.
DIVA, pages 67–73, 2013.
[21]
Christopher D.
Manning Jeffrey Pennington,
Richard Socher.
Glove:
Global
vectors for word representation.
[22]
Oliver Jesorsky,
Klaus J.
Kirchberg,
and Robert Frischholz.
Robust face de-
tection using the hausdorff distance.
In Proceedings of
the Third Interna-
tional
Conference on Audio- and Video-Based Biometric Person Authentica-
tion, AVBPA ’01, pages 90–95, Berlin, Heidelberg, 2001. Springer-Verlag.
[23]
Eric Jones, Travis Oliphant, Pearu Peterson, et al.
SciPy:
Open source scien-
tific tools for Python, 2001–.
BIBLIOGRAPHY
77
[24]
T.
Kanungo,
D.
M.
Mount,
N.
S.
Netanyahu,
C.
D.
Piatko,
R.
Silverman,
and A.
Y.
Wu.
An efficient k-means clustering algorithm:
Analysis and im-
plementation.
IEEE Transactions on Pattern Analysis
Machine Intelligence,
24:881–892, 07 2002.
[25]
Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger.
From
word embeddings to document distances.
In Proceedings of the 32Nd Interna-
tional
Conference on International
Conference on Machine Learning - Volume
37, ICML’15, pages 957–966. JMLR.org, 2015.
[26]
Dawid Laszuk.
Python implementation of empirical mode decomposition algo-
rithm., 2017–.
[Online; accessed <today>].
[27]
Omer Levy and Yoav Goldberg.
Neural
word embedding as implicit matrix
factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and
K. Q. Weinberger, editors, Advances in Neural Information Processing Systems
27, pages 2177–2185. Curran Associates, Inc., 2014.
[28]
Y. Liu, Z. Li, H. Xiong, X. Gao, and J. Wu. Understanding of internal clustering
validation measures.
In 2010 IEEE International
Conference on Data Mining,
pages 911–916, Dec 2010.
[29]
Michael
Loewe.
Sources of
shang history:
The oracle-bone inscriptions of
bronze age china.
The Antiquaries Journal, 60(2):364–365, 1980.
[30]
Ulrike Luxburg.
A tutorial
on spectral
clustering.
Statistics and Computing,
17(4):395–416, December 2007.
[31]
Rada Mihalcea,
Courtney Corley,
and Carlo Strapparava.
Corpus-based and
knowledge-based measures of
text semantic similarity.
In Proceedings of
the
21st National Conference on Artificial Intelligence - Volume 1, AAAI’06, pages
775–780. AAAI Press, 2006.
[32]
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
Efficient estima-
tion of word representations in vector space.
CoRR, abs/1301.3781, 2013.
[33]
Tomas Mikolov,
Ilya Sutskever,
Kai
Chen,
Greg Corrado,
and Jeffrey Dean.
Distributed representations of
words and phrases and their compositionality.
In Proceedings of
the 26th International
Conference on Neural
Information
Processing Systems - Volume 2, NIPS’13, pages 3111–3119, USA, 2013. Curran
Associates Inc.
[34]
Tomas Mikolov,
Ilya Sutskever,
Kai
Chen,
Greg Corrado,
and Jeffrey Dean.
Distributed representations of
words and phrases and their compositionality.
In Proceedings of
the 26th International
Conference on Neural
Information
Processing Systems - Volume 2, NIPS’13, pages 3111–3119, USA, 2013. Curran
Associates Inc.
78
BIBLIOGRAPHY
[35]
Tomas Mikolov,
Wen-tau Yih,
and Geoffrey Zweig.
Linguistic regularities in
continuous space word representations.
In HLT-NAACL, pages 746–751, 2013.
[36]
Andrew Y.
Ng,
Michael
I.
Jordan,
and Yair Weiss.
On spectral
clustering:
Analysis and an algorithm. In T. G. Dietterich, S. Becker, and Z. Ghahramani,
editors, Advances in Neural Information Processing Systems 14, pages 849–856.
MIT Press, 2002.
[37]
F.
Pedregosa,
G.
Varoquaux,
A.
Gramfort,
V.
Michel,
B.
Thirion,
O.
Grisel,
M.
Blondel,
P.
Prettenhofer,
R.
Weiss,
V.
Dubourg,
J.
Vanderplas,
A.
Pas-
sos,
D.
Cournapeau,
M.
Brucher,
M.
Perrot,
and E.
Duchesnay.
Scikit-learn:
Machine learning in Python.
Journal
of Machine Learning Research, 12:2825–
2830, 2011.
[38]
Jeffrey Pennington,
Richard Socher,
and Christopher D.
Manning.
Glove:
Global vectors for word representation.
In Empirical
Methods in Natural
Lan-
guage Processing (EMNLP), pages 1532–1543, 2014.
[39]
Carl
Edward Rasmussen.
The infinite gaussian mixture model.
In Proceed-
ings of
the 12th International
Conference on Neural
Information Processing
Systems, NIPS’99, pages 554–560, Cambridge, MA, USA, 1999. MIT Press.
[40]
Radim Řehůřek and Petr Sojka. Software Framework for Topic Modelling with
Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges
for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010. ELRA.
http:
//is.muni.cz/publication/884893/en
.
[41]
Peter Rousseeuw.
Silhouettes:
A graphical
aid to the interpretation and val-
idation of
cluster analysis.
J.
Comput.
Appl.
Math.,
20(1):53–65,
November
1987.
[42]
Y.
Rubner,
C.
Tomasi,
and L.
J.
Guibas.
A metric for distributions with ap-
plications to image databases.
In Sixth International
Conference on Computer
Vision (IEEE Cat. No.98CH36271), pages 59–66, Jan 1998.
[43]
Gerard Salton and Michael
J.
McGill.
Introduction to Modern Information
Retrieval.
McGraw-Hill, Inc., New York, NY, USA, 1986.
[44]
Grigori
Sidorov,
Alexander F.
Gelbukh,
Helena Gómez-Adorno,
and David
Pinto.
Soft similarity and soft cosine measure:
Similarity of features in vector
space model.
Computación y Sistemas, 18, 2014.
[45]
Luke Vilnis and Andrew McCallum.
Word representations via gaussian em-
bedding.
CoRR, abs/1412.6623, 2014.
[46]
Rui
Xu and D.
Wunsch.
Survey of clustering algorithms.
IEEE Transactions
on Neural
Networks, 16(3):645–678, May 2005.
BIBLIOGRAPHY
79
[47]
Donghui Yan, Ling Huang, and Michael I. Jordan.
Fast approximate spectral
clustering.
In Proceedings of the 15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’09, pages 907–916, New York,
NY, USA, 2009. ACM.
[48]
Stella X.
Yu and Jianbo Shi.
Multiclass spectral
clustering.
In Proceedings
of the Ninth IEEE International
Conference on Computer Vision - Volume 2,
ICCV ’03, pages 313–, Washington, DC, USA, 2003. IEEE Computer Society.
Appendix A
Sample documents
A.1
Sample document from BBCNews data set
J&J agrees $25bn Guidant deal
Pharmaceutical giant Johnson & Johnson has agreed to buy medical technology
firm Guidant for $25.4bn (£13bn).
Guidant is a key producer of equipment that combats heart problems such as
implant defibrillators and pacemakers.
Analysts said that the deal is aimed at offset-
ting Johnson & Johnson’s reliance on a slowing drug business.
They also pointed
out that more mergers are likely because the drug and healthcare industries are
fragmented and are under pressure to cut costs.
A number of Johnson & Johnson’s
products are facing patent expirations,
while the company is also battling fierce
competition from generic products.
Meanwhile,
demand for defibrillators,
which
give the heart a small
electric shock when an irregular heartbeat or rhythm is de-
tected, is expected to increase, analysts said.
The move by Johnson & Johnson has
been widely expected and the firm will
pay $76 for each Guidant share,
6% more
than Wednesday’s closing price.
Analysts say that US antitrust regulators could
force the firms to shed some overlapping stent operations.
Stents are tubes that are
used to keep an artery open after it has been unblocked.
A.2
Sample document from BBCSport data set
Saint-Andre anger at absent stars
Sale Sharks director of rugby Philippe Saint-Andre has re-opened rugby’s club-
versus-country debate.
Sale host Bath in the Powergen Cup on Friday, but the Frenchman has endured a
"difficult week" with six players away on England’s Six Nations training camp.
"It’s
an important game but we’ve just the one full
session.
It’s the same for everyone
but we need to manage it.
"If five players or more are picked for your country they
should move the date of the game," he told BBC Sport.
Unless the authorities agree
to make changes, Saint-Andre believes England’s national team will suffer as clubs
81
82
APPENDIX A.
SAMPLE DOCUMENTS
opt to sign foreigners and retired internationals.
"That’s not good for the politics
of
the English team or for English rugby," he argues.
It is an issue he has taken
up before, most notably during the autumn internationals when Sale lost all three
Zurich Premiership matches they played.
Now he fears it could derail the club’s hopes of cup silverware after eight players,
including captain Jason Robinson and fly-half
Charlie Hodgson,
were away with
their countries.
"We’re in the quarter-finals, it’s always better to play at home than
away and it’s a great opportunity," he added.
"But we have to be careful.
Bath
have just been knocked out of Europe and will make it a tough game.
It also comes
at the end of
a very,
very difficult week.
"Sebastien Bruno’s been with France,
Jason White with Scotland and there are six with England,
that’s eight players
plus injuries - 13 players out of a squad of 31.
"We’ll have just one session together
and will have to do our best to make that a good one on Thursday afternoon."
Gloucester have also been caught in a club-versus-country conflict after England
sought a second medical opinion on James Simpson-Daniel’s fitness.
The winger is
carrying a shoulder injury and the national
team management believe he requires
time on the sidelines.
As a result he misses the Cherry and White’s quarter-final
at home to Bristol.
"Under the Elite Player Squad agreement,
England wanted
a second opinion,
which they can do," director of
rugby Nigel
Melville told the
Gloucester Citizen.
"They obviously want him for international
rugby and we want him for club
rugby in what is a very important game for us.
There is a conflict of interests.
"The
surgeon who carried out his operation said he was fine for us but England say he
is still vulnerable to be damaged again and want him on a full rehab programme."
Simpson-Daniel added:
"I’ve said to Nigel I want to be back playing and that means
if everything goes well this week, I can target the Worcester game (on 29 January)
for a return."
A.3
Sample document from news20group data set
From:
jhartagora.rain.com (Jim Hart) Subject:
Screw the people, crypto is for hard-
core hackers & spooks only Organization:
Open Communications Forum Lines:
37
Since the AT&T wiretap chip is scheduled to be distributed internationally, al-
lowing the U.S. government to spy on foreign governments, companies and people as
as well as to wiretap domestic citizens, this is a world-wide issue.
Thus Distribution:
world.
ygolandwright.seas.ucla.edu (The Jester) writes:
>However assuming that I can still encrypt things as I please, who >cares about
the clipper chip?
Why do we hackers care about the Clipper chip?
Do we give a shit about
anybody’s privacy accept our own? And perhaps not even our own; are we so smart
that we always know when we’re talking to somebody who has a wiretap on their
phone?
A.3.
SAMPLE DOCUMENT FROM NEWS20GROUP DATA SET
83
I find the "call thru your computer" ideas may reflect this attitude.
Ideas that
are of, by, and for hackers, and don’t help anybody in the real world, aren’t going
to do anybody much good, including ourselves where voice phones are concerned.
We *do* need an alternative to NSA-bugged telephones, but we’re talking inex-
pensive *telephones* here, including hand-sized cellulars, that need strong crypto,
real privacy.
Make-shift computer hacker rigs that require living by your computer
to talk privately over the phone are just a dumb stunt that doesn’t do anything for
anybody’s privacy in the real world.
What we need is a true *privacy chip*.
For example, a real-time voice-encryption
RSA, silicon compile it and spit out ASIC. Put this chip on the market as a de facto
standard for international business, diplomats, and private communications.
If the
U.S.
bans it,
we make it somewhere else and import it.
The Japanese,
German,
Dutch,
Taiwanese,
Korean,
etc.
electronics companies don’t want the NSA spying
on them.
U.S. workers lose more jobs to government fascist stupidity.
jhartagora.rain.com
Appendix B
Used software
The programming language we use for implementing our model and the experiments
is Python 3.
We also use the following Python libraries.
1.
scikit-learn [37].
An open-source machine learning library that features var-
ious machine learning algorithms.
We use its pre-implemented clustering al-
gorithms for exploring word clusters.
To be specific, the two clustering algo-
rithms are k-means clustering and spectral clustering.
2.
gensim [40].
An open-source vector space modeling and topic modeling toolkit.
We use it to import the pre-trained Google word2vec model[13].
3.
pyemd [23].
An open-source Python wrapper for fast calculation of the Earth
Mover’s Distance.
4.
numpy [26].
An open-source scientific computing library for Python.
We use
it for a number of mathematical
functions such as cosine,
average,
variance,
logarithm, etc.
85
TRITA EECS-EX-2018:557
www.kth.se
