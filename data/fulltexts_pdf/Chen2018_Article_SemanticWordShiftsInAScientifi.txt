Semantic word shifts in a scientific domain
Baitong Chen
1
•
Ying Ding
2,3,4
•
Feicheng Ma
3
Received:
14 January 2018 / Published online:
13 July 2018
 Akade
´
miai
Kiado
´
,
Budapest,
Hungary 2018
Abstract
Understanding semantic word shifts
in scientific domains
is
essential
for
facilitating
interdisciplinary communication.
Using a data set
of
published papers in the field of
information retrieval (IR),
this paper studies the semantic shifts of words in IR based on
mining per-word topic distribution over time.
We propose that
semantic word shifts not
only occur over time, but also over topics. The shifts are examined from two perspectives,
the topic-level and the context-level.
According to the over-time word-topic distribution,
stable words and unstable words are recognized.
The diverging and converging trends in
the unstable type reveal
characteristics of the topic evolution process.
The context-level
shifts are further
detected by similarities
between word vectors.
Our
work associates
semantic word shifts with the evolving of topics, which facilitates a better understanding of
semantic word shifts from both topics and contexts.
Keywords
Word-topic distribution  Semantic shifts  Semantic analysis
Introduction
Interdisciplinary communication is a crucial issue for promoting the diffusion of innova-
tions and science development.
Today,
data are extremely rich but
overwhelming (Ding
and Stirling 2016), and different research areas are continually developing new ideas and
methods. During the evolution of scientific topics, different topics may view same words or
phrases in varied ways,
e.g.,
‘‘data science’’ in biomedical science mainly refers to gene
& Baitong Chen
baitongchen@shu.edu.cn
Ying Ding
dingying@indiana.edu
Feicheng Ma
fchma@whu.edu.cn
1
Shanghai University,
Shanghai,
China
2
Indiana University,
Bloomington,
USA
3
Wuhan University,
Wuhan,
China
4
Tianjin Normal University,
Tianjin, China
123
Scientometrics (2018) 117:211–226
https://doi.org/10.1007/s11192-018-2843-2
(0123456789().,-volV)(0123456789().,-volV)
codes or protein structure, in computer science it focuses more on algorithm designing, and
in information science it generally concerns people and society.
The phenomenon of the same word appearing in multiple topics is closely related to the
evolution of topics. In practice, a word shifts or extends its semantics while a topic evolves,
in which the same word can appear in multiple topics relating to different innovations and
applications.
In data mining and computational linguistic areas,
language processing tools,
such as
corpus-based semantic representations (i.e.,
word embedding) (Mikolov et
al.
2013a,
b),
have been developed to identify semantic word shifts.
However,
extant literature investi-
gates the semantic shifts of
words mainly to facilitate natural
language processing in
information retrieval
problems,
in which the relationship of words with topics has been
largely overlooked.
In our earlier work,
we discussed the evolving dynamics of the topics in a scientific
domain, including the splitting, merging and knowledge transfer between the topics (Chen
et al. 2017a). In this paper, we further our study to the word level, and study semantic word
shifts in a scientific domain through the phenomenon of
the same word appearing in
multiple topics.
A data set
in the field of
information retrieval
(IR)
is utilized in an
empirical
study.
A word’s semantic shifts via topic channels are herein defined by their
related innovations and applications. When the same word is distributed in different topics,
even if
it
is embedded in a stable context
(the linguistic semantics are based on the
distributional
hypothesis,
i.e.,
the neighbouring words),
it
is likely to be associated with
different innovations and applications due to the change of topics.
The remainder of this paper is organized as follows. The related work section presents a
summary of studies related to semantic word representation and semantic word shifts. The
methods section describes the data set,
the topic extraction approach,
our definition of
semantic word shifts, and the methods for detecting such shifts from two perspectives, the
topic-level
and the context-level.
The results and discussion section presents our under-
standing of
semantic word shifts
from the two perspectives.
The conclusion section
summarizes the findings and suggests directions for future work.
Related work
Semantic word representation
Representing data constitutes a fundamental problem for machine learning. Traditionally,
data
representation (i.e.,
features)
is
manually designed.
However,
in recent
years,
approaches have been proposed to learn features automatically (Bengio et al. 2013). This is
also the case for word representations.
In natural language processing, the representation of word semantics mainly focuses on
the co-occurrence of words and word contexts.
Harris (1954) proposed the distributional
hypothesis of word semantics for the first time, which states that the meaning of a word is
determined by its
context.
According to the distributional
hypothesis,
the association
between words can be extracted from their contexts.
The distributional
hypothesis provides the theoretical
basis for
word representation
based on distributional similarity,
in which word semantics are distinguished by learning
their contexts,
and are represented by the location in a vector space.
In this space,
word
vectors that are close to each other are likely to be semantically associated.
123
212
Scientometrics (2018) 117:211–226
Representing word semantics with spatial
vectors introduced statistical
properties to
semantic word representation.
One of the most
classic spatial
representations is Latent
Semantic Analysis (LSA)
(Landauer
and Dumais 1997).
The LSA model
is a high-di-
mensional linear associative model that generates a representation that captures the sim-
ilarity of
words and text
passages.
The similarity is identified based on word usage:
predicting words from their context, in which suggesting words with similar usage patterns
tend to appear together.
The LSA model
represents each word with a vector,
which is
located in the derived semantic space.
The semantic similarity between two words is
typically measured by the cosine of the angle between their spatial vectors.
Semantic word representation based on distributional
similarity can be classified into
two kinds:
the distributional
representation and the distributed representation.
The two
kinds of approaches both represent words with spatial vectors in the semantic space.
The
differences are that, in distributional representation, word representations are learned from
a word co-occurrence matrix.
Each row of the matrix corresponds to a word vector,
and
each dimension of the vector possesses physical significance.
Typical models of the dis-
tributional representation type are Point-wise Mutual Information (PMI), LSA, and Latent
Dirichlet Allocation (LDA) (Bouma 2009; Griffiths and Steyvers 2003).
On the other
hand,
distributed representation obtains
dense low-dimensional
word
vectors,
in which all
dimensions of
the vectors together
represent
the latent
semantic
features of the corpus as a whole, and an individual dimension of a word vector does not
possess physical
significance.
The modelling is usually conducted based on neural
net-
works,
and such representation is also referred to as neural-network word embeddings
(Collobert and Weston 2008). A typical example is the word2vec model that has become
especially popular in the embeddings generation (Mikolov et al.
2013a,
b).
Semantic word shifts
In machine learning or data mining, problems that involve semantic word shifts are known
as concept drift.
Concepts here has a broad meaning,
in which concepts not only exist in
the textual
information in the documents,
but
also refer to the quantity that
a learning
model is trying to predict, i.e., the variables (Wang et al. 2011). In the real world, concepts
and the underlying data distributions are often not
stable,
but
change with time,
e.g.,
weather prediction rules and customers’ preferences. These changes make the model built
on old data inconsistent with the new data streams, which requires regular updating of the
model. The problem of concept drifts complicates the task of learning a model from data.
Special approaches, such as instance selection, instance weighting, and ensemble learning
are proposed for learning in the context of non-stationary distributions.
In linguistics,
semantic word shifts refer to a change of one or more meanings of the
same concept
over time (Lehmann 1993).
Concepts are used to describe sets of objects
with shared characteristics. The meaning drift of the same concept over time has been the
focus of much research in recent
years.
To differentiate from concept
drift
in machine
learning, we intuitively term it semantic word shifts. Studies on semantic word shifts can
be viewed from two perspectives: synonymy detection and polysemy detection. Synonymy
detection monitors the use of different
words with the same meaning over time (Kenter
et al.
2015).
Based on a small set of input words in a certain time period,
ranked lists of
terms for a consecutive series of periods in time would be output. The words in the ranked
lists are meant to denote the same concept as the input words.
Studied more extensively,
polysemy detection monitors different meanings expressed by the same word over time. A
123
Scientometrics (2018) 117:211–226
213
word can change semantically in a way in which new meanings replace the old ones,
or
acquire additional
meanings in which the original
meaning may still
be widely used
(Wijaya and Yeniterzi
2011).
In polysemy detection,
distributional
semantic models
(Gulordava and Baroni 2011; Hamilton et al. 2016; Kim et al. 2014) are widely utilized for
quantitative measurement.
In these models,
the similarity between words is measured by
vector space models in which each word is associated with its context vectors.
Existing works
have investigated semantic word shifts
mainly to facilitate natural
language applications, e.g.,
time-aware query expansion for document retrieval tasks in a
historical corpus. Both in machine learning and linguistics, words are studied alone and are
not associated with topics.
In this study,
we propose that semantic word shifts occur not
only over time,
but
also over topics.
Specifically,
when the same word is distributed in
different topics, its related innovations and applications tend to change due to the change of
topics.
Methods
Data set and topic extraction
Information retrieval
(IR) is chosen as the target
domain.
Papers are collected from the
Web of Science for 1956–2014, comprising a total of 20,359 documents, with search based
on a set of IR-related terms. Search term selection refers to the paper by Xu et al. (2015),
which are INFORMATION RETRIEVAL,
INFORMATION STORAGE and RETRIE-
VAL,
QUERY PROCESSING,
DOCUMENT RETRIEVAL,
DATA RETRIEVAL,
IMAGE RETRIEVAL,
TEXT RETRIEVAL,
CONTENT BASED RETRIEVAL,
CON-
TENT-BASED RETRIEVAL,
DATABASE QUERY,
DATABASE QUERIES,
QUERY
LANGUAGE,
QUERY LANGUAGES,
and RELEVACE FEEDBACK.
The selected
document types include articles,
books,
book chapters,
and proceedings papers.
The title
and abstract fields are used as the text corpus for extracting topics.
Prior to extracting topics, all terms are stemmed using the Porter2 stemming algorithm.
A stop word list (Yan et al. 2012) is utilized to filter common words. Words with only one
letter or that appear less than five times are removed.
Latent
Dirichlet
Allocation (LDA)
proposed by Blei
et
al.
(2003)
is
applied for
extracting topics from the corpus. LDA is a three-layer Bayesian model that is now widely
used in discovering latent
topic themes in collections of
documents.
The LDA model
represents each document with a probability distribution over topics, in which each topic is
represented as a probability distribution over
words.
For
a detailed explanation of
the
algorithm, refer to, e.g., Blei (2012). The Gensim library (Rehurek and Sojka 2010) is used
for implementing the LDA model,
in which the parameters are set as the standard value
proposed by Gensim. Considering the size and duration of the data set, and the diversity of
the IR field,
five topics are extracted in consistent
with our earlier work (Chen et
al.
,
2017b).
Defining semantic word shifts
In linguistics, the meaning of a concept C at some moment in time t is defined as a triple
(label
t
(C),
int
t
(C),
ext
t
(C)),
where label
t
(C) is a String,
int
t
(C) refers to the intension of
C and is a set of properties, and ext
t
(C) refers to the extension of C and is a subset of the
123
214
Scientometrics (2018) 117:211–226
universe. Either of the three elements in the triple changes will cause the meaning change
of a concept (Wang et al.
2011).
In this study,
semantic word shifts refer to the extension change ext
t
(C),
relating to
different innovations and applications when the word is distributed in different topics.
The semantic shifts are examined from two perspectives,
the topic-level and the con-
text-level.
Topic-level
shifts are represented by the words being embedded in different
topics,
which are not simply a replicate of lexical information,
but rather an idea re-creation,
in
which one word can express multiple ideas.
The new ideas that
a word expresses are
closely related to the innovations and applications that are unique in the new topics.
Context-level shifts are reflected by the changing of the words with a similar context
compared to the target
word.
The distributional
hypothesis (Harris 1954) states that
the
meaning of a word is determined by its surrounding words,
and that
words with similar
meanings tend to share similar neighbouring words, and we call such neighbouring words
context.
Detecting topic-level
semantic word shifts
Mapping the overall
topic distribution of words
The topic distribution of each word in the corpus is measured based on the transformation
of
the LDA training results.
The LDA model
represents each topic with a probability
distribution over all words in the dictionary. In our corpus, there are 6580 distinct words.
We extract five topics in total,
in which each topic is represented by a probability distri-
bution over the 6580 words in the dictionary.
The five-topic word distribution can be viewed as
a matrix with dimensions
of
5 9 6580,
in which its rows are the five topics,
and the columns are the words.
As
presented in Table 1,
each row is a probability distribution and adds up to one.
We
transpose the topic-word matrix and then divide the values of the elements by the row sum
to obtain the word-topic matrix (Table 2). Table 2 presents the per-word topic distribution
that we require, in which each word in the dictionary is represented by a five-dimensional
vector,
indicating the word’s probability distribution over the five topics.
The LargeVis
(Tang et
al.
2016)
library is utilized to visualize the mapping of
the per-word topic
distribution.
Representing the over-time topic distribution of words
The topic distribution of a word in a document with a specific time stamp is parameterized
by the variational
parameter / in the LDA model
(Hoffman et
al.
2010).
The / value
Table 1
Topic-word distribution
Health
Image
Query
…
Topic 1
0.0045
0.0000
0.0000
…
Topic 2
0.0000
0.0635
0.0020
…
Topic 3
0.0000
0.0000
0.0367
…
Topic 4
0.0000
0.0000
0.0361
…
Topic 5
0.0000
0.0000
0.0128
…
123
Scientometrics (2018) 117:211–226
215
indicates the likelihood of a word belonging to a topic in terms of a particular document.
Each word in a document has five topic / values corresponding to the five global topics
(Table 3). After normalizing by the frequency of the word in the document, the sum of a
word’s / values is equal to 1.
The same word from two different documents usually has
two different sets of / values.
For a selected word,
we calculate its average / value for
each topic in each year. The average / values represent the topic probability distribution
for the word in that year. As the / values change over the years, the word-topic distribution
changes accordingly over time.
Detecting context-level
semantic word shifts
A word embedding technique,
the so-called word2vec,
is used to extract word represen-
tations. Based on the distributional hypothesis, the word2vec model (Mikolov et al. 2013b)
represents a word into vectors in a way that maximizes the conditional probability of the
surrounding words given the word (or maximizes the conditional probability of the target
word given the surrounding words).
As presented in Table 4,
the word2vec results represent each word with a vector with
fixed dimension numbers. Considering the size of our data set, which is not relatively large,
we consider that
a standard value of 100 dimensions proposed by word2vec would be
appropriate for training the vectors.
The word vectors are equivalent to the original words in the semantic space. The vectors
can be considered as the words’ coordinates. Words with close vectors usually have similar
semantics. The similarity between the vectors is measured with their cosine similarity. We
recognize words with similar semantics compared to a target
word based on the cosine
similarity of their vectors. The context-level semantic shifts are reflected by the changing
of the similar words of the target word over time.
Similar
words detected by word2vec include,
but
are not
limited to,
frequently co-
occurring words,
since frequently co-occurring words tend to have similar contexts.
The
semantic shifts are examined herein after 1990, because the IR field begins to thrive in the
1990s (Chen et al. 2017a, b), and earlier data do not contain sufficient textual information
to extract meaningful yearly word-topic distribution or semantically accurate word vectors.
The timeline is divided into five periods: 1991–1995, 1996–2000, 2001–2005, 2006–2010,
and 2011–2014.
The number of publications per period is presented in Table 5.
Table 2
Word-topic distribution
Topic 1
Topic 2
Topic 3
Topic 4
Topic 5
Health
0.9994
0.0002
0.0001
0.0002
0.0002
Image
0.0000
1.0000
0.0000
0.0000
0.0000
Query
0.0000
0.0231
0.4187
0.4120
0.1462
…
…
…
…
…
…
Table 3
An example of the normalized / values of a word in a particular document
Word
Topic 1
Topic 2
Topic 3
Topic 4
Topic 5
Sum
A
0.45
0.05
0.12
0.21
0.17
1
123
216
Scientometrics (2018) 117:211–226
Results and discussion
Topic extraction
The top 10 words with the highest probabilities of the five extracted topics are presented in
Table 6.
Topic 1 focuses on user-oriented problems, covering online information-seeking
behavior,
use of digital resources such as digital
libraries by research scholars,
and user
information needs, especially for health information searches. The main theme of topic 2
centers on multimedia information retrieval, especially image retrieval. Topic 3 and topic 4
both study queries for structured data sets, in which topic 3 mainly deals with traditional
query processing for relational and object-oriented databases, and topic 4 primarily focuses
on distributed query processing for spatial networks and communication networks. Topic 5
studies text retrieval for unstructured documents,
which involves document indexing and
Table 4
Word vectors
Word
Word vector (100 dimensions)
Query
(0.467552,
- 2.031277, - 1.712295, …,
0.518877, - 0.493766,
- 1.732974)
Image
(- 0.655203, 1.217866, - 1.284322, …,
0.616633, 0.592486,
0.036463)
Semantic
(0.675473,
0.315660, - 1.307766, …,
- 0.487085, - 0.749494,
- 0.882005)
Table 5
Number of publications
per period
Period no.
Period
No.
of pubs
Cumulative sum
0
1956–1990
1654
1654
1
1991–1995
1488
3142
2
1996–2000
2585
5727
3
2001–2005
5126
10,853
4
2006–2010
5642
16,495
5
2011–2014
3864
20,359
Table 6
Top words in the topics
Topic 1 User
study
Topic 2 Image
retrieval
Topic 3 Database
querying
Topic 4 Query
processing
Topic 5 Text
retrieval
Research
User
Data
Design
Library
Web
Find
Analysis
Medical
Access
Image
Feature
Content
Similarity
Visual
Music
Learn
Algorithm
Object
Color
Query
Data
Database
Language
Relational
Semantic
Integration
Structure
Object
Knowledge
Query
Data
Network
Algorithm
Time
Index
Distributed
Optimize
Computing
Tree
Document
Text
User
Relevance
Term
Query
Web
Evaluation
Rank
Word
123
Scientometrics (2018) 117:211–226
217
terminology processing problems,
such as term disambiguation,
query expansion,
and
cross-language retrieval.
Topic-level
semantic word shifts
Overall
word-topic distribution
Figure 1 presents an overview of the topic distribution status of all of the words. Each node
is a word, and is represented by a five-dimensional vector, which is equivalent to the per-
word topic distribution over the whole corpus. Words with a similar topic distribution are
closer to each other,
and words with a dissimilar topic distribution are further apart from
each other, which results in the clearly distinguished five clusters corresponding to the five
topics.
The per-word topic distributions of
the labelled words in Fig.
1 are shown in
Table 7. Words that are stable in one topic are far away from the center,
such as Health,
Patient,
and Library in User Study;
Color and Image in Image Retrieval;
Logic and
Expression in Database Querying; Sensor, Node, and Tree in Query Processing (especially
for
spatial
and communication networks);
and Word and Document
in Text
Retrieval.
Words that are distributed approximately evenly in multiple topics are closer to the center,
such as Web,
Context,
and Evaluation.
Words that
are distributed mainly between two
topics are correspondingly located between these two topics,
e.g.,
Language between
Database Querying and Text
Retrieval;
Resource between User
Study and Database
Querying; and Query between Database Querying and Query Processing.
Fig.
1
Per-word topic distribution
123
218
Scientometrics (2018) 117:211–226
Over-time word-topic distribution
Figure 2 shows the over-time topic distribution of
the top 10 words with the highest
probabilities in each topic over time. Because topics share some of the top words, there are
42 unique top words in total.
Considering the number of topics between which a word is mainly distributed,
words
can be categorized into two types: stable words and unstable words. Stable words are those
that always belong to only one topic over time. Examples are presented in Fig.
3, including
Image in Image Retrieval (Topic 2); Document in Text Retrieval (Topic 5); Library in User
Study (Topic 1);
and Tree in Query Processing (Topic 4).
Words that
belong to the
stable type are core words in the topic, which reflect the research theme or main method of
a topic.
Unstable words are distinguishable in two or more topics.
A typical
example is Lan-
guage in Database Querying (Topic 3) and Text Retrieval (Topic 5) shown in Fig.
4. When
in Database Querying,
it
is related with query language in databases.
When in Text
Retrieval, it is placed in the situation of cross-language retrieval. When the unstable words
distribute among multiple topics, there are multiple underlying ideas represented by these
words.
There are two kinds of notable trends in the unstable type: the diverging trend and the
converging trend. By tracing such trends, the evolving of topics is indicated by the topic-
level semantic word shifts.
Words with a diverging trend start
with an approximately even probability in several
topics,
but
the probabilities diverge in later
periods for
the word being assigned more
apparently to one or two topics.
For example (Fig.
4),
Web and Visual are typical unsta-
ble words with a divergent trend. Web starts with a probability of 0.3 distributed evenly in
User Study, Database Querying, and Text Retrieval. The probability in Text Retrieval then
Table 7
Per-word topic
distributions
Word
Topic 1
Topic 2
Topic 3
Topic 4
Topic 5
Health
0.9994
0.0002
0.0001
0.0002
0.0002
Patient
0.9994
0.0002
0.0002
0.0002
0.0002
Library
0.9706
0.0001
0.0261
0.0001
0.0031
Color
0.0001
0.9997
0.0001
0.0001
0.0001
Image
0.0000
1.0000
0.0000
0.0000
0.0000
Logic
0.0001
0.0001
0.9996
0.0001
0.0001
Expression
0.0149
0.0289
0.9012
0.0061
0.0490
Sensor
0.0001
0.0001
0.0001
0.9995
0.0001
Node
0.0001
0.0001
0.0001
0.9995
0.0001
Tree
0.0001
0.0490
0.0001
0.9507
0.0001
Word
0.0001
0.0244
0.0001
0.0001
0.9754
Document
0.0002
0.0000
0.0003
0.0001
0.9993
Web
0.2298
0.0000
0.2962
0.0000
0.4739
Context
0.1889
0.1469
0.2708
0.0166
0.3768
Evaluation
0.2026
0.1137
0.0806
0.1645
0.4385
Language
0.0000
0.0000
0.7027
0.0000
0.2972
Resource
0.4709
0.0001
0.3005
0.0735
0.1551
Query
0.0000
0.0231
0.4187
0.4120
0.1462
123
Scientometrics (2018) 117:211–226
219
increases and separates from the other two, as Web is studied more specifically under the
context
of
text
retrieval
in web environments
in later
periods.
Visual
begins
with a
probability of approximately 0.5 both in Image Retrieval and Database Querying, but later
diverges to almost only appear in Image Retrieval, since it is usually used in visual features
or human visual perception, rather than visual query language in Database Querying. The
diverging process of an unstable word indicates that the word is becoming topic-specific.
The topic that it mainly distributes towards becomes increasingly developed,
resulting in
the word being more studied under the specific context of this topic.
Words with a converging trend are obviously assigned to one topic at
first,
but
the
probabilities in the topics converge in later periods, resulting in the word being distributed
evenly in multiple topics.
Taking Semantic as an example (Fig.
4),
Semantic is assigned
mostly to Database Querying appearing as semantic query language at first, the word then
gradually becomes important in Text Retrieval and Image Retrieval, and the probabilities
in the above three topics converge in the 2010s.
The converging process of
an unsta-
ble word indicates the increasing importance of the word in the domain, for it is not only
studied in a specific topic, but developed various applications covering a broader range of
contexts in different topics. In the case of Semantic, it is not only studied as semantic query
language, as in the beginning, but also studied as semantic text retrieval in Text Retrieval
and image semantics in Image Retrieval in later periods.
Fig.
2
Topic-level semantic shifts represented by word-topic distribution over time
123
220
Scientometrics (2018) 117:211–226
Context-level
semantic word shifts
The context-level semantic shifts are presented by examining the changing of the words
with a similar context compared to the target word. Combining with the over-time word-
topic distribution, four kinds of situations exist, as presented in Table 8: topic unchanged,
context unchanged; topic unchanged, context changed; topic changed, context unchanged;
and topic changed, context changed. When a word’s topic is unchanged, the word belongs
to the stable type. In contrast, when a word’s topic changes, the word is an unstable type
that might express divergent or convergent topic distributions.
The semantics of a word are closely associated with the innovation attached to the word.
If the innovation or the application of a word extends,
its semantics will change accord-
ingly.
In the following,
four examples are presented corresponding to the four semantic
shift
situations.
Each example presents the top ten most
similar words compared to the
target word.
Topic unchanged,
context unchanged
Figure 5 presents the words with a similar context to Document over different periods (read
from left to right, row first). Document is a stable word in topic 5, Text Retrieval, which can
be seen from the topic distribution presented in the lower right in Fig.
5. Its semantics are
constantly related to textual content without substantial change.
Fig.
3
Examples of the stable words
123
Scientometrics (2018) 117:211–226
221
Fig.
4
Examples of the unstable words
Table 8
The four situations of
semantic shifts
Topic
Context
Word type
Unchanged
Unchanged
Stable words
Unchanged
Changed
Changed
Unchanged
Unstable words
Changed
Changed
Fig.
5
Words with a similar context to Document
123
222
Scientometrics (2018) 117:211–226
Topic unchanged,
context changed
Figure 6 presents the words with a similar
context
to Library over
different
periods.
Library is one of the core words in topic 1, User Study. Although Library remains stable in
topic 1,
its context
varies in different
time periods,
indicating the development
of
the
library towards digital resources,
and use in health and medical retrieval.
Topic changed,
context unchanged
Figure 7 presents the words with a similar context to Web over different periods.
Web is
mainly distributed among three topics,
but
its context
consistently centers on the World
Wide Web and the Internet, although it is worth noting that increasing attention is paid to
web engine interfaces and social tagging in web searches.
Topic changed,
context changed
Figure 8 presents the words with a similar
context
to Language in different
periods.
Language is distributed between topic 3 Database Querying as query language and topic 5
Text Retrieval in the context of cross-language retrieval. The context change of Language
is caused by a new concept
being created,
e.g.,
Xml.
Before Xml
was developed,
Lan-
guage’s context only focused on relational database query language processing. However,
after Xml
appeared,
Language became apparently related to xquery,
xpath,
and various
metadata description models, such as rdf and sparql. Moreover,
due to its usage in cross-
language retrieval, natural language processing achieved more importance in later periods.
Conclusion
Understanding semantic word shifts has been the focus of much research in recent years.
Finding word semantics over
time by the changing of
its direct
context
(neighbouring
words) is well explored in the extant literature. Most word semantic analysis is conducted
by the machine learning and linguistics community. Word semantics are studied alone to
promote the efficiency of natural
language processing.
However,
when we consider the
semantic shifts of words in the circumstances of topic evolution,
finding word semantics
solely from its direct context is insufficient. In practice, different topics may view the same
Fig.
6
Words with a similar context to Library
123
Scientometrics (2018) 117:211–226
223
word from varied perspectives. When the same word is distributed in several topics, even if
its direct
context
is stable,
its extension meaning that
relates to the new innovation and
application in the new topics is likely to be updated.
In this study, we proposed that semantic word shifts not only occur over time, but also
over
topics.
The semantic word shifts are investigated from two perspectives,
i.e.,
the
topic-level
and the context-level.
For the topic-level
shifts,
based on the per-word topic
distribution over time, stable and unstable words are recognized. Two kinds of trends in the
unstable type are especially worth noting,
the diverging trend and the converging trend,
which reveals the characteristics of the evolving of the topics. The context-level shifts are
examined through the changing of
words with a similar
context
to the target
word in
sequential
periods,
and the generated four
situations
are accordingly illustrated with
specific examples.
Our work contributes to a better understanding of semantic word shifts in the topic
evolution process, indicating that semantic word shifts not only occur over time, but also
over topics.
By studying semantic word shifts from the topic-level and the context-level,
the changing of word semantics has been associated with the evolving of topics.
Future
work could consider determining whether regular patterns or quantitative laws exist
in
word-topic distributions.
Fig.
7
Words with a similar context to Web
Fig.
8
Words with a similar context to Language
123
224
Scientometrics (2018) 117:211–226
Acknowledgements
This work is funded by the National Natural Science Foundation of China (Grant Nos.
71420107026 and 71704138).
The present study is an extended version of an article presented at the 16th
International Conference on Scientometrics and Informetrics,
Wuhan (China),
16–20 October 2017 (Chen
et al.
2017a).
References
Bengio,
Y.,
Courville,
A.,
& Vincent,
P.
(2013).
Representation learning: a review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
35(8),
1798–1828.
Blei,
D.
M.
(2012). Probabilistic topic models.
Communications of the ACM,
55(4),
77–84.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning
Research,
3,
993–1022.
Bouma, G. (2009). Normalized (pointwise) mutual information in collocation extraction. In Proceedings of
GSCL (pp. 31–40).
Chen, B., Ding, Y., & Ma, F. (2017a). Mapping the semantic word shifts in topics in the field of information
retrieval.
In Proceedings of
ISSI
2017—The 16th international
conference on scientometrics and
informetrics (pp.
1335–1341). Wuhan University,
China.
Chen, B., Tsutsui, S., Ding, Y., & Ma, F. (2017b). Understanding the topic evolution in a scientific domain:
an exploratory study for the field of information retrieval. Journal of Informetrics, 11(4), 1175–1189.
Collobert,
R.,
& Weston,
J.
(2008).
A unified architecture for natural
language processing:
Deep neural
networks with multitask learning.
In Proceedings of
the 25th international
conference on machine
learning (pp.
160–167).
New York,
NY,
USA: ACM.
Ding,
Y.,
& Stirling,
K.
(2016).
Data-driven discovery:
a new era of exploiting the literature and data.
Journal of Data and Information Science,
1(4),
1–9.
Griffiths,
T.
L.,
& Steyvers,
M.
(2003).
Prediction and semantic association.
In Advances
in Neural
Information Processing Systems (pp.
11–18). Cambridge, MA,
USA: MIT Press.
Gulordava,
K.,
& Baroni,
M.
(2011).
A distributional
similarity approach to the detection of
semantic
change in the Google Books Ngram Corpus.
In Proceedings of
the GEMS 2011 workshop on geo-
metrical models of natural language semantics (pp.
67–71).
Stroudsburg,
PA,
USA: Association for
Computational Linguistics.
Hamilton, W. L., Leskovec, J., & Jurafsky, D. (2016). Diachronic word embeddings reveal statistical laws of
semantic change. arXiv:1605.09096 [Cs].
Harris,
Z.
S.
(1954). Distributional structure.
Word,
10,
146–162.
Hoffman, M., Bach, F. R., & Blei, D. M. (2010). Online learning for latent dirichlet allocation. In Advances
in neural information processing systems (pp. 856–864).
Cambridge, MA,
USA: MIT Press.
Kenter, T.,
Wevers, M.,
Huijnen, P.,
& de Rijke, M.
(2015). Ad hoc monitoring of vocabulary shifts over
time.
In Proceedings of
the 24th ACM international
on conference on information and knowledge
management (pp.
1191–1200).
New York,
NY,
USA: ACM.
Kim,
Y.,
Chiu,
Y.-I.,
Hanaki,
K.,
Hegde,
D.,
& Petrov,
S.
(2014).
Temporal analysis of language through
neural language models.
arXiv:1405.3515 [Cs].
Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: the latent semantic analysis theory
of acquisition,
induction,
and representation of knowledge. Psychological Review,
104(2),
211.
Lehmann,
W.
P.
(1993).
Historical
linguistics:
An introduction (3rd edition).
London;
New York:
Routledge.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector
space.
arXiv:1301.3781 [Cs].
Mikolov,
T.,
Sutskever,
I.,
Chen,
K.,
Corrado,
G.
S.,
& Dean,
J.
(2013b).
Distributed representations of
words
and phrases
and their
compositionality.
In C.
J.
C.
Burges,
L.
Bottou,
M.
Welling,
Z.
Ghahramani, & K. Q. Weinberger (Eds.), Advances in neural information processing systems 26 (pp.
3111–3119). New York: Curran Associates Inc.
Rehurek,
R.,
& Sojka,
P.
(2010).
Software framework for
topic modelling with large corpora.
In Pro-
ceedings of the Lrec 2010 workshop on new challenges for Nlp Frameworks (pp.
45–50).
Tang,
J.,
Liu,
J.,
Zhang,
M.,
& Mei,
Q.
(2016).
Visualizing large-scale and high-dimensional
data.
In
Proceedings of
the 25th international
conference on world wide web (pp.
287–297).
Republic and
Canton of Geneva,
Switzerland: International World Wide Web Conferences Steering Committee.
Wang, S., Schlobach, S., & Klein, M. (2011). Concept drift and how to identify it. Web Semantics: Science,
Services and Agents on the World Wide Web, 9(3),
247–265.
123
Scientometrics (2018) 117:211–226
225
Wijaya,
D.
T.,
& Yeniterzi,
R.
(2011).
Understanding semantic change of words over centuries.
In Pro-
ceedings of
the 2011 international
workshop on detecting and exploiting cultural
diversity on the
social web (pp.
35–40). New York, NY,
USA: ACM.
Xu,
J.,
Ding,
Y.,
& Malic,
V.
(2015).
Author credit for transdisciplinary collaboration.
PLoS ONE,
10(9),
e0137968.
Yan,
E.,
Ding,
Y.,
Milojevic
´
,
S.,
& Sugimoto, C.
R.
(2012).
Topics in dynamic research communities: an
exploratory study for the field of information retrieval.
Journal of Informetrics,
6(1),
140–153.
123
226
Scientometrics (2018) 117:211–226
