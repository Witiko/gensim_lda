Relevance Feedback-based Query Expansion Model using Ranks Combining and
Word2Vec Approach
Jagendra Singh and Aditi Sharan
School of Computer and Systems Sciences, Jawaharlal Nehru University, New Delhi, India
ABSTRACT
Query expansion is a well-known method for improving the performance of information retrieval
systems.
Pseudo-relevance feedback (PRF)-based query expansion is a type of
query expansion
approach that assumes the top-ranked retrieved documents are relevant.
The addition of
all
the
terms of
PRF documents is not important or appropriate for expanding the original
user query.
Hence,
the selection of
proper expansion term is very important
for
improving retrieval
system
performance.
Various
individual
query expansion term selection methods
have been widely
investigated for improving system performance.
Every individual expansion term selection method
has its own weaknesses and strengths.
In order
to minimize the weaknesses and utilizing the
strengths of the individual
method,
we used multiple terms selection methods together.
First,
this
paper explored the possibility of improving overall
system performance by using individual
query
expansion terms selection methods. Further, ranks-aggregating method named Borda count is used
for combining multiple query expansion terms selection methods.
Finally,
Word2vec approach is
used to select semantically similar terms with query after applying Borda count rank combining
approach.
Our
experimental
results
on both data-sets
TREC and FIRE demonstrated that
our
proposed approaches
achieved signiﬁcant
improvement
over
each individual
terms
selection
method and other’s related state-of-the-art method.
KEYWORDS
Borda count ranking method;
Feature/term selection
methods; Information
retrieval system; Query
expansion; Semantic
similarity
1.
INTRODUCTION
Query expansion (QE) is one of the main research issues
in the ﬁeld of information retrieval
(IR) system.
One of
the
most
important
goals
of
query
expansion is
to
expand the original user query with words that describe
the user
intention or
a query that
is
more likely to
retrieve only the relevant documents.
The query can be
expanded manually,
semi-automatically,
and automati-
cally.
However, due to well-known difﬁculties of manual
and semi-automatic
query
expansion (AQE),
mainly
involving the user in the process of
query expansion,
AQE is generally preferred. It has been observed that the
volume
of
data
available
online
has
dramatically
increased while the number
of
query terms
searched
remained very less.
According to the authors in [1],
the
average query length was 2.30 words,
the same reported
10 years after in Van Rijsbergen [2].
While there has
been a slight increase in the number of long queries (of
ﬁve or more words),
the most prevalent queries are still
those of one,
two,
and three words.
In this situation,
the
need and the scope of
AQE have increased.
However,
AQE has its own problems.
The major difﬁculty of AQE is that it cannot work well
due to the inherent sparseness of the user query terms in
the high-dimensional
corpus.
Another problem is that
not all
the terms of
top retrieved documents (feedback
documents)
are
important
for
the
query
expansion.
Some of the query expansion terms may be redundant or
irrelevant. Some may even misguide the result, especially
when there are more irrelevant
query expansion terms
than relevant
ones.
Query expansion selection aims to
remove redundant
and irrelevant
terms from the term
pool
(top retrieved documents as feedback documents
for
selecting
user
query
expansion terms),
and the
selected query expansion terms set should contain sufﬁ-
cient and reliable information about the original
docu-
ment.
Thus,
query expansion terms selection should not
only reduce the high dimensionality of the feedback doc-
ument
corpus
(term pool),
but
also provide a better
understanding of
the documents,
in order to improve
the AQE result.
Feedback-based different query expan-
sion terms selection methods has been widely used in
AQE,
and it
has
been reported that
query expansion
terms selection methods can improve the efﬁciency and
accuracy of IR model.
The conventional query expansion terms selection meth-
ods for AQE are either corpus statistics-based or term
association-based,
depending on used algorithm in the
retrieval
model.
Term association-based terms selection
© 2016 IETE
IETE JOURNAL OF RESEARCH, 2016
VOL. 62, NO. 5, 591604
http://dx.doi.org/10.1080/03772063.2015.1136575
methods,
such as mutual information [3] and co-occur-
rence information like Jaccard co-occurrence measure
(JCM) and Dice co-occurrence measures (DCMs) [2,4]
estimate the goodness of each term based on the occur-
rence of terms in feedback documents (term pool).
Cor-
pus
statistics-based
query
expansion
term selection
methods,
such as
KullbackLeibler
divergence (KLD)
[5],
Information Gain (IG) [6] and Robertson selection
value (RSV)
[7]
estimate the goodness
of
each term
based on the distribution of terms across the corpus and
using the query term information present
in feedback
documents.
In literature survey,
the majority of research on the query
expansion terms selection approaches focused on the per-
formance
improvement
of
individual
query expansion
terms selection methods.
However,
it remains as a chal-
lenge to develop an individual
query expansion terms
selection method that
would outperform other methods
in most
cases.
Moreover,
as
multiple query expansion
terms selection methods are available, it is natural to com-
bine them for better performance by taking advantage of
their individual strength. In the past, experiments of com-
bining multiple query terms selection methods have been
conducted,
but
no theoretical
analysis
has
been done.
Combinations of
two uncorrelated and high-performing
query expansion terms selection methods have been tested
[8].
The author in [5] developed a naive combination of
co-occurrence and probabilistic kinds
of
method,
with
which
the
developed
approach obtained
results
that
improve those obtained with any of them separately.
This
result
conﬁrms
that
the information provided by each
approach is of
a different
nature and,
therefore,
can be
used in a combined manner.
The author in [9] discussed
various ranked search results aggregation methods such
as Borda, Condorcet, etc. and conﬁrmed the improvement
in search quality.
A set
of
expansion terms was obtained after applying
Borda count-based ranks aggregation approach,
then we
analysed that
some obtained expansion terms are not
related to user query semantically.
Therefore,
it became
compulsory
to
check
the
semantic
meaning
of
the
selected expansion terms with the user query for to avoid
query drifting problem. For this purpose, we use the con-
cept
of
Word2Vec-based semantic similarity with the
help of
Word2Vec.
After
applying
Word2Vec-based
semantic ﬁltering,
a reﬁned set of additional
expansion
terms was obtained. Thereafter, an approach of reweight-
ing was required that will provide high weight to original
query terms than the additional
expansion terms.
The
authors
in
[10]
present
a
new method
for
query
reweighting to deal with document retrieval. The method
reweights a user’s query vector,
based on the user’s rele-
vance feedback,
to improve the performance of
docu-
ment retrieval systems.
The experiments of
proposed model
are performed on
two well-known benchmark data-sets,
namely FIRE and
TREC-3.
For
performance
evaluation,
the
proposed
model
has been compared with Okapi-BM25 [11] and
Aguera
et
al.’s
model
[5].
The
proposed
methods
increase the precision rates and the recall rates of IR sys-
tems for dealing with document retrieval.
It also gets a
signiﬁ
cantly higher average recall rate,
average precision
(AP) rate, and F-measure on both data-sets.
The main contributions of this work can be summarized
as follows:

First,
we present
co-occurrence-based approaches
named Jaccard co-occurrence measure (JCM) and
Dice co-occurrence measures (DCMs)
along with
statistics-based approaches named KLD and RSV of
expansion terms selection for PRF-based AQE; with
this,
the
experimental
analysis
of
all
these
term
selection methods
are
presented with evaluation
parameter score.

Second,
we
present
ranks
aggregation
method
named Borda count for combining different ranked
list
of
expansion terms
selected by JCM,
DCM,
KLD, and RSV methods discussed in Step 1.

Third,
we propose Word2Vec-based semantic simi-
larity approach for
ﬁltering out
the irrelevant
and
redundant
expansion terms
with context
to user
query obtained from Step 2.
Next,
additional expan-
sion terms used after applying reweighting approach.

Finally,
statistical
test
named paired t-test
con-
ducted
between
our
proposed
approaches
and
other’s model considered as the baseline model.
The remainder of this paper is organized as follows.
In
Section 2,
we brieﬂy introduce pseudo-relevance feed-
back (PRF)-based document selection and four individ-
ual
query expansion terms selection methods.
Section 3
explains
our
proposed model
and its
algorithm with
Borda count-based ranking approach and Word2Vec-
based semantic similarity approach.
Section 4 presents
the experimental
results
of
different
query expansion
terms selection methods are compared and with each
other,
next
in this
section our
proposed approaches
results
are presented and compared or
analysed with
baseline approaches in terms of the precision, recall, and
F-measure on both FIRE and TREC data-sets.
Finally,
Section
5
presents
conclusion
and
future
research
directions.
592
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
2.
METHOD FOR SELECTING PRF DOCUMENT
AND QUERY EXPANSION TERMS
We
brieﬂy discuss
about
PRF-based documents
and
expansion terms used for expanding query in this sec-
tion.
In Section 2.1,
we discuss
about
the criteria for
selecting the relevant
documents that
provide a source
for selecting candidate expansion terms.
In rest
of
the
sections (2.2 and 2.3),
we explain four query expansion
term selection methods.
2.1 PRF-based documents selection
In
PRF-based
approaches,
the
candidate
terms
for
expanding the user query are selected from an initially
retrieved set of documents.
Therefore,
the initial
step is
to retrieve
top n documents
from the
corpus.
The
retrieved documents are considered to be relevant.
The
sole criteria for selecting the relevant documents depend
on the proper selection of
similarity measure to match
query and document.
We have used an efﬁcient Okapi
BM25
similarity
measure
for
selecting
initial
set
of
retrieved documents,
which is more efﬁcient than tradi-
tionally used Cosine
similarity measure.
The
Okapi-
BM25 measure is given by the following Equation (1)
[11]:
Okapi
Q; D
i
ð
Þ D
X
teQ \ D
i
w
k
1
C 1
ð
Þtf
K C tf
£
k
3
C 1
ð
Þqtf
k
3
C qtf
(1)
where Q is the query that contains terms,
tf is the term
frequency of term t in document D
i
and qtf is the term
frequency in query Q.
Next,
k
1
,
b,
and k
3
are constant
parameters, the value of parameter that we used is based
on research paper explained [11],
(k
1
D 1.2,
b D 0.75,
k
3
D 7.0).
The parameters K and w are calculated by
Equations (2) and (3):
K D k
1
1  b
ð
Þ C b:
dl
avdl




(2)
w D log
N  n C 0:5
ð
Þ
n C 0:5
ð
Þ
(3)
where k
1
,
b, and k
3
are constant parameters,
the value of
parameter that we used are based on research paper [11],
the values are k
1
D 1.2, b D 0.75, and k
3
D 7. Next, N is a
number of documents and n is a number of documents
containing the term t.
Parameters dl and avdl are docu-
ment length and average document length, respectively.
Once the top k document is selected using Okapi-BM25
similarity method,
a term pool
of
candidate expansion
terms is constructed that contains all
the unique terms
present in the index of top k documents. These terms are
ranked by individual
ranking/scoring methods available
to rank the terms.
These scoring methods are presented
in coming Section 2.2.
2.2 Individual term scoring/ranking methods
We categorized these individual
methods into two cate-
gories:
First,
co-occurrence or
term association-based
measures and second, statistical measures based on term
distribution in the corpus.
2.2.1 Co-Occurrence coefﬁcient-based measures
The most feasible method for selecting the query expan-
sion terms is to initially score the terms on the basis of
their co-occurrence with original
user query terms.
The
concept of term co-occurrence has been used since the
1990s for identifying some kind of
relationship among
terms in the document set.
According to [2],
the idea of
using co-occurrence statistics for ﬁnding the relationship
between document corpus and the query terms is based
on the hypothesis that “the term is identiﬁed by compant
it keeps”, the author used this idea to expand the original
user queries.
Three popularly used methods, Jaccard, Dice, and Cosine
co-occurrence measures, are given by Equations (4, 5 and
6) respectively:
Jaccard
t
i
; t
j


D
c
ij
c
i
C c
j
 c
ij
(4)
Dice
t
i
; t
j


D
2c
ij
c
i
C c
j
(5)
Cosine
t
i
; t
j


D
c
ij
ﬃﬃﬃﬃﬃﬃ
c
i
c
j
p
(6)
where c
i
and c
j
are the number of documents that contain
terms
t
i
and t
j
,
respectively,
and C
ij
is
the document
numbers that contain both the terms t
i
and t
j
together.
We can apply these coefﬁcients to measure the similarity
between the query terms and terms in the documents.
However, there is a danger in adding these terms directly
to the query. The candidate terms selected for expansion
could co-occur
with the original
query terms
in the
documents (top n relevant) by chance.
The higher its
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
593
degree is in whole corpus, the more likely it is that candi-
date term co-occurs with query terms by chance.
Keep-
ing this
factor
in mind,
inverse document
frequency
(IDF) of a term can be used along with above-discussed
similarity measures to scale down the effect
of
chance
factor.
Incorporating IDF given in Equation (9)
and
applying normalization deﬁne degree of
co-occurrence
of
a candidate term with a query term as follows
by
Equation (7):
Co  degree
q
i
;
c
ð
Þ D log
10
co q
i
;
c
ð
Þ C 1
ð
Þ£
idf
c
ð Þ
log
10
D
ð
Þ


(7)
coðq
i
;
cÞ D Jaccard q
i
;
c
ð
Þ
(8)
idf
c
ð Þ D log
10
N=N
c
ð
Þ
(9)
where N is the number of documents in the corpus, D is
the number of top ranked retrieved documents consid-
ered,
qi
is the ith query term,
and c is the candidate
expansion term.
Nc is the number of documents in the
corpus that contain term c.
And co(q
i
,
c) is the number
of
co-occurrences between q
i
and c in the top-ranked
documents that is calculated by Jaccard (q
i
,
c) given in
Equations (8) and (4).
Above formula can be used for ﬁnding similarity of
a
term c with individual
query term.
To obtain a value
measuring how good c is for whole query Q,
we need to
combine its degrees of co-occurrence with all individual
original query terms. So we use Equation (10) for ﬁnding
similarity between the candidate term and the query.
Equation (7) can be used for ﬁnding the similarity of a
term c with individual
query term q
i
.
To obtain a value
measuring how well c is for the whole query Q, there is a
need to combine its co-degree with all individual original
query terms present
in the query.
So we use following
equation [12]:
co
occurrence
f inal
Q; c
ð
Þ
D
Y
q
i in Q
co
degree q
i
; c
ð
Þ


(10)
Finally,
Equation (10) is used to ﬁnd the co-occurrence
coefﬁcient
score of
candidate expansion term.
When,
we ﬁnd candidate expansion term co-occurrence score
with JCM,
we call
it
Jaccard co-occurrence measure-
based query expansion (JCMBQE) and when the can-
didate
expansion terms
are
scored with DCM,
it
is
called Dice co-occurrence measure-based query expan-
sion (DCMBQE).
2.3 Statistics-based terms selection methods
2.3.1 KullbackLeibler divergence-based query
expansion
The KLD [5] is well-known in information theory [4].
KLD-based approach has been used in natural language
and speech processing applications based on statistical
language modelling in IR [7].
KLD can be used as a
term-scoring function that
is based on the differences
between the distribution of term in the collection of top
retrieved relevant documents and entire document col-
lection.
Equation (11) is used to ﬁnd the KLD score of
candidate expansion term:
KLD t
ð Þ D P
R
t
ð Þlog
P
R
t
ð Þ
P
C
t
ð Þ
(11)
where P
R
(t) is the probability of
presence of
candidate
expansion term t in top retrieved document collection R,
and P
C
(t) is the probability of the presence of candidate
expansion term t
in the entire document
collection C.
Top scored candidate
expansion terms
are
used to
expand the user original query. This type of query expan-
sion is called KLD-based query expansion (KLDBQE).
2.3.2 Robertson selection value-based query
expansion
The RSV method [7] is based on Swets model of IR sys-
tem performance [13]. The system is assumed to retrieve
items by ranking them according to some measure of
association with the query.
The principle idea of
the
Swets theory is to examine the distribution of values of
this match function over the document collection.
More
speciﬁcally,
it considers two such distributions,
one for
the relevant documents,
and one for the non-relevant.
If
the retrieval system is any good, the two distribution will
be different; in particular,
the match function values will
generally be higher for relevant documents than for non-
relevant.
In general,
the more the two distributions are separated,
the
better
the
performance
of
the
system will
be.
Other
things
being
equal,
the
higher
the
difference
d D m
R
- m
NR
between the means of the two distributions
(where m
R
is the mean of relevant documents and m
NR
is
the mean of
non-relevant
documents),
the better
the
performance.
For the case of
query expansion,
we can consider the
candidate term t with weight W
t
,
then those class that
contain the term t
will
have W
t
added to their match
function values. The new mean of relevant and non-rele-
vant document class is given by m
R
and m
NR
, respectively.
594
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
If P
tR
and P
tNR
correspond to the probability of the pres-
ence of term in relevant and non-relevant document col-
lection,
respectively,
the
m
R
(mean
of
relevant
document) relates term weight
with the probability of
term presence and mean of
relevant
documents m
R
by
Equation (12) as follows:
1  P
tR
ð
Þm
R
C P
tR
m
R
CW
t
ð
Þ D m
R
C P
tR
W
t
(12)
Similarly, the new mean for m
NR
(the non-relevant docu-
ments) is given by Equation (13) as follows:
D m
NR
C P
tNR
W
t
(13)
And
the
effectiveness
d’
deﬁned
as
by
Equations
(1416):
d
0
D m
R
C P
tR
W
t
 m
NR
 P
tNR
W
t
(14)
D m
R
 m
NR
CW
t
P
tR
 P
tNR
ð
Þ
(15)
D d C W
t
P
tR
 P
tNR
ð
Þ
(16)
If
differences
between two distributions
are very low,
then Equation (16) reduces to Equation (17):
d
0
DW
t
P
tR
 P
tNR
ð
Þ
(17)
where d is the original difference of m
R
and m
NR
. Finally,
the
weight
of
candidate
expansion term is
given by
Equation (18) as follows:
D
X
t2d
w t;
d
ð
Þ P
tR
 P
tNR
ð
Þ
(18)
where PtR is the probability of expansion term in rele-
vant documents and PtNR is the probability of
expan-
sion term in non relevant document or corpus. Equation
(18)
can be used to ﬁnd the RSV score of
candidate
expansion terms. Top scored candidate terms are used to
expand the user original query. This type of query expan-
sion is called RSV-based query expansion (RSVBQE).
3.
PROPOSED RANKS AGGREGATION AND
WORD2VEC SEMANTIC SIMILARITY-BASED
MODEL
Our proposed work can be categorized mainly in three
parts:

First,
ranks/score aggregation of
individual
terms
selection approaches named JCM,
DCM,
KLD,
and
RSV using Borda count scheme.

Second,
apply
Word2Vec
semantic
similarity
scheme for removing noisy or irrelevant terms.

Expansion terms reweighting.
Figure 1 shows the architecture of
our proposed AQE
retrieval model based on rank aggregation and semantic
ﬁltering schemes.
We start
with PRF-based QE,
where
Okapi-BM25 [11]
is
used for
selecting top k relevant
documents.
A term pool is constructed by including the
terms present in top retrieved documents.
The terms of
term pool are called candidate terms; various term rank-
ing methods,
JCM,
DCM,
KLD,
and RSV,
are used for
Figure 1:
Proposed ranks combination and Word2Vec semantic ﬁltering-based AQE model
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
595
ranking
candidate
terms
of
term pool.
Finally,
top-
ranked candidate terms
obtained from each of
these
methods are used for expanding or reformulating query
after
applying expansion terms
reweighting approach.
The QE approaches
based on JCM,
DCM,
KLD,
and
RSV terms scoring methods discussed in Section 2.2 are
called JCM-based query expansion (JCMBQE),
DCM-
based query expansion (DCMBQE),
KLD-based query
expansion (KLDBQE),
and RSV-based query expansion
(RSVBQE),
respectively.
The sequences of
algorithmic
steps used during the implementation of proposed model
are deﬁned in Table 1.
Next,
score combination method SumScore is used for
combining similar
type
of
ranking methods
together
such
as
co-occurrence
methods
(DCM and
JCM)
together and statistics methods (RSV and KLD) together
by rank combining module-1.
Further,
ranks combining
module-2 combines these two methods through Borda
ranks combining schemes discussed in Section 3.2.
This
approach of query expansion is called ranks aggregation-
based query expansion (RABQE).
Word2Vec-based semantic ﬁltering module (vector mod-
ule) is then used to ﬁlter irrelevant terms.
Top n semanti-
cally
similar
terms
with
user
query
are
used
for
expanding the user query,
while other terms are ﬁltered
out. These approaches of query expansion are called ranks
aggregation
and
semantic-based
query
expansion
(RASBQE).
After applying Word2Vec-based semantic ﬁl-
tering,
a reﬁned set of expansion terms is obtained.
After
that,
a reweighting approach is
used that
will
provide
high weight
to original
query terms than the expansion
terms.
In this work,
we applied beta version of Rocchio’s
algorithm [10],
which is
well-known term reweighting
approach that is brieﬂy discussed in Section 3.5.
A list of
new relevant
documents
retrieved using
reformulated
user query are then produced as ﬁnal output.
3.1 Combination of multiple terms selection
methods
In this section,
we discuss about
SumScore and Borda
ranks combination schemes. Basically, these schemes can
be divided in two types, score combining and ranks com-
bining methods.
These two are discussed in Sections.
3.1.1 and 3.1.2, respectively.
3.1.1 Score combination schemes
In this section,
we explain score combination method.
Let the list of ranked candidate terms is given by T D {t
1
,
t
2
,.,
t
m
}.
If
there are n term selection/scoring methods.
The similarity score given by a terms selection method i
to a candidate term t
j
is S
ij
. A list of the popular and sim-
ple score combination methods is explained by [9,14].
(A) SumScore ranking approach.
The combined value
of similarity score of each candidate t
j
will be the sum of
the similarities score from all
candidate selection meth-
ods. That can be explained by Equation (19) as follows:
SumScore
t
j
 
D
X
k
i
D 1
S
ij
(19)
Similarity score of candidate terms obtained from differ-
ent terms selection methods is normalized before combi-
nation.
In many research works,
it has been shown that
the SumScore scores
combination approach performs
better than other score combination approach in almost
all
cases.
Finally,
some
high-scored candidate
terms
obtained from SumScore method are used as an input to
ranks combining module-2.
3.1.2 Ranks combination schemes
In this
section,
we
explain some
ranks
combination
methods based on terms ranks.
These rank combining
schemes are based on the social choice theory. The social
choice theory (Kelly,
1988) is a study ﬁeld in which vot-
ing algorithms are used as a technique for making the
social or group decision. The rank combination schemes
selected by us are explained in the following section with
an appropriate example.
(A) Borda count ranking approach.
According to borda
rank combining approach, each voter has its own prefer-
ence
list
of
candidates.
For
each voter,
the
top ﬁrst
Table 1:
Implementation steps used for proposed AQE model
1. Apply Okapi-BM25 similarity function for retrieving ranked relevant
document with respect to user query for selecting PRF documents, it
execute only single time.
2. All the unique terms of top N retrieved documents obtained from step
no. 1 are selected to form term pool.
3. Different method used to score the unique terms of term pool to form
candidate terms, these are listed below.
i. Calculate JCM score.
ii. Calculate DCM score.
iii. Calculate KLD score.
iv. Calculate RSV score.
Top-scored candidate terms obtained from substeps i to iv of Step 3
used to expand the user query and called JCMBQE, DCMBQE, KLDBQE,
and RSVBQE, respectively.
4. Borda rank aggregation methods used to combine different candidate
term ranks obtained from sub steps i to iv of Step 3.
i. Borda aggregated ranked list of candidate term.
Some top n candidate terms obtained from substeps i of Step 4 used to
expand the user query and called RABQE.
5. Semantic ﬁltering approach used for ﬁlter out semantically irrelevant
expansion terms from expansion term set obtained from RABQE
approach. After applying Word2Vec-based semantic ﬁltering, this ranks
aggregation and semantic-based approach called RASBQE.
596
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
candidate obtains m points,
the top second candidate
obtains m-1 points,
and the top third candidate obtains
m-2 points, and so on. The sum value of obtained points
of
each voter gives the ﬁnal
points to each candidate.
There are few candidates who are unranked by a voter
(candidate
term selection
method),
then
remaining
points are divided among the unranked candidates.
The
candidate who has high points wins [9].
Example No.
1:
Here,
we used an example to illustrate
the working of Borda ranks combining approach.
Here,
we assume a combined single query expansion terms
selection method
with
ﬁve
following
ranked
query
expansion term selection methods,
which have ranked
four candidate terms P, Q, R, and S as following:
Candidate term selection method 1: P, Q, R, S
Candidate term selection method 2: Q, P, S, R
Candidate term selection method 3: R, Q, P, S
Candidate term selection method 4: R, Q, S
Candidate term selection method 5: R, Q
Now we denote the score of each candidate term t by
candidate score (t).
Borda ranking (For example No. 1): The score for each
candidate terms are as following:
Candidate score (P) D 4 C 3 C 2 C 1 C 1.5 D 11.5
Candidate score (Q) D 3 C 4 C 3 C 3 C 3 D 16
Candidate score (R) D 2 C 1 C 4 C 4 C 4 D 15
Candidate score (S) D 1
C 2 C 1 C 2 C 1.5 D 7.5
Thus, the ﬁnal ranking of candidate terms is: Q, R, P, S.
Some high-ranked candidate terms
selected by Borda
scheme are used for expanding the user query: This type
of query expansion is called RABQE.
3.2 Proposed Word2Vec-based semantic ﬁltering
approach
A list of candidate terms obtained after ranks combina-
tion modules.
In this candidate terms list,
we observed
that
some candidate terms as expansion terms are not
related to the original user query.
If we use these candi-
date terms
as
query expansion terms,
it
may retrieve
irrelevant documents. Thus, it is compulsory to ﬁlter out
these irrelevant
candidate terms.
In order to eliminate
the irrelevant and redundant candidate expansion terms,
we used the concept
of
Word2Vec semantic similarity
that
capture the semantically related terms with query
terms from the candidate terms list and ﬁlter-out seman-
tically non-related terms.
For applying semantic similarity,
we have used Word2-
Vec [15].
We used the publicly available Gensim library
provided in [16]
to train a Word2Vec distributional
word representation using the TREC-CDS scientiﬁc arti-
cle corpus.
Word2Vec is
a highly sophisticated deep-
learning neural
network architecture which learns how
to represent
words
as
multi-dimensional
vectors
[17].
The model operates without human supervision by con-
sidering the textual
context
surrounding words.
These
contexts
may
be
represented in a
variety
of
ways,
although in this work we utilize the Skip-gram model
which is
able
to
capture
discontinuous
multi-word
sequences.
Word2Vec attempts to project words onto a
multi-dimension vector space such that
the proximity
between two vectors
indicates
the semantic similarity
between their associated words.
We used this property
by associating each key-phrase with its vector representa-
tion and using the 10 most similar words in the vocabu-
lary for expansion purpose.
For example: Word2Vec expansions of fatigue are: tired-
ness, malaise, sickness, instability, neuropathy, and many
more.
The skip-gram model
used in Word2Vec is discussed
here.
In this model,
we are given a corpus of words w
and their
contexts
c.
We
consider
the
conditional
probabilities
p(cjw;u),
and given a
corpus
text,
the
goal
is
to set
the parameter
u of
p(cjw;
u),
so as
to
maximize the corpus probability as follows by Equa-
tion (20):
arg max
u
Y
weText
Y
ceC w
ð
Þ
p c=w; u
ð
Þ
2
4
3
5
(20)
In Equation (20),
C(w) is the set of contexts of word w.
Alternatively, Equation (21) is as follows:
arg max
u
Y
w; c
ð
Þ2D
p c=w; u
ð
Þ
(21)
Here, D is the set of all word and context pairs we extract
from the text.
We have selected the intersected terms between Word2-
Vec vocabulary and candidate expansion terms
for
a
query term.
Similarly,
intersected terms are selected for
all
query terms
in candidate expansion terms
of
that
query.
This
approach of
query
expansion is
called
RASBQE.
The
algorithmic
steps
of
our
proposed
Word2Vec
semantic-based query expansion approach are listed in
Table 2.
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
597
3.3 Methods for reweighting the expanded query
terms
After one of the query expansion terms selection meth-
ods described above has generated the list of candidate
terms,
the selected candidate terms that system adds to
the user query must be re-weighted.
Different methods
have been proposed for query expansion terms reweight-
ing.
We made a comparison analysis of
these methods
and tested which one is the most
appropriate for our
proposed query expansion modules.
The
most
tradi-
tional and simple approach of expansion term reweight-
ing is the Rocchio algorithm [10]. In this proposed work,
we used Rocchio’s beta version of Rocchio’s algorithm,
in which we requires only the b parameter.
Finally,
we
computed the new weight,
qtw,
of candidate terms used
as expansion terms with the original user query by Equa-
tion (22) as follows:
qtw D
qtf
qtf
max
C b
wðtÞ
w
max
ðtÞ
(22)
In Equation (22), parameter w(t) is the old weight of can-
didate term t and w
max
(t) is the maximum weight of the
expanded query terms using scoring function,
b is a set-
ting parameter,
qtf
is the query term t
frequency and
qtf
max
is the query term t maximum frequency present in
the query q.
The value of parameter b is ﬁxed to 0.1 in
our experiment. Finally, the selected candidate terms are
used after reweighting for expanding the user query.
4.
EXPERIMENTAL STUDY
All the experiments carried out in this paper are based on
the model proposed in Section 3.
First,
the performance
of
individual
methods
such as:
JCMBQE,
DCMBQE,
RSVBQE,
and KLDBQE are compared with each other
or with Okapi-BM25 [11].
Second,
the performance of
RABQE and RASBQE compared with Aguera et
al.’s
model (combining multiple term selection methods) [5]
using different performance evaluation parameters.
4.1 Data-sets
In this section, we describe two well-known benchmarks
test collections used in our experiments: TREC disk 1&2
and FIRE ad-hoc data-sets,
which are different
in size
and genre (TREC disc 1&2 size is 6 Gb, while FIRE data-
set is 3.4 Gb). The detailed descriptions of both data-sets
are given in Table 3.
Query numbers ranging from 126
to 175 are used for FIRE data-set
and query numbers
ranging from 151 to 200 are used for TREC data-set
(a different
collection of
50 queries are used for both
data-set).
The TREC disk 1&2 collections contain news-
wire article from different
sources,
such as Association
Press, Wall Street Journal, Financial Times, Federal Reg-
ister,
etc.,
which are considered as high quality text data
with minimum noise.
The FIRE ad-hoc data-set
is a medium size collection
that contains newswire article from two different sources
named The
Telegraph and BD News24 provided by
Indian Statistical Institute Kolkata, India.
In our experiments,
we use only title ﬁeld of TREC and
FIRE query sets for retrieval
task,
because this ﬁeld is
closer to the actual queries used in real time applications
and the usefulness of this ﬁeld is expected to be the most
useful for short-type queries mention here.
The last col-
umn of Table 3 presents the average documents length
in the corresponding TREC and FIRE data-sets.
Based on the performance,
Porter
steamer
is
used to
stem each term in the process of indexing and querying,
and a latest list of 420 stop words is used to remove the
stop words. In both FIRE and TREC data-set, top 10, 25,
and 50 retrieved documents are used to measure the AP,
recall, and mean average precision (MAP).
Table 2:
Algorithmic steps developed for
ﬁnding Word2Vec-
based semantic similarity between two concepts/words
1. Once the candidate term sets obtained from Step 4 of Table 1.
2. Train the Word2vec using TREC-CDS scientiﬁc article corpus with the
help of Skip-gram model.
3. Obtain vocabulary of query terms using Word2Vec up to top ten
dimensions.
4. Find the intersected terms between the candidate terms obtained from
Steps 1 and 3.
5. Find intersected terms for all the query terms similarly as Step 4.
6. Select top 15 candidate expansion terms from Step 1 for the query.
7. Combine terms of Steps 5 and 6 together for query expansion.
8. Apply Rocchio’s algorithm for reweighting expansion terms obtained
from Step 7.
9. Use reweighted expansion terms obtained from Step 8 for query
expansion ﬁnally, this approach is called RASBQE.
Table 3:
Summary of used data-sets and query numbers
Data-sets
Task
Queries
Docs
No of unique terms
Average document length
TREC-3 (disk1 and 2)
ad hoc
151200
7,41,856
14,83,71,200
349
FIRE
ad hoc
126175
4,56,329
6,27,56,468
273
598
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
4.2 Parameter setting
In order to investigate the optimal setting of parameters
for
fair
comparisons,
we
used the
training
method
explained in Diaz and Metzler
[18]
for our proposed
model,
which is
very popular
in IR’s
ﬁeld.
First,
for
parameters in PRF models, we used different numbers of
top feedback documents in both baseline and proposed
approaches (5,
10,
15,
25,
50),
to ﬁnd the optimal num-
ber of feedback documents for making the proper collec-
tion
of
expansion
terms
that
may
improve
the
performance of
IR system,
but we found our proposed
model
performing best for top 15 numbers of feedback
document, that is why we ﬁx top 15 feedback documents
to make the term pool
in our experiment.
Second,
we
select
different
number
of
top candidate
terms
from
ranked candidate terms based on similarity value with
query terms as expansion terms (10,
20,
30,
50,
75),
for
both baseline and proposed methods to ﬁnd the optimal
number of top expansion terms used for reformulating
query,
but
our proposed model
was found performing
best for top 30 candidate term,
that is why we ﬁx top 30
candidate terms to reformulate the original user query in
our experiment.
4.3 Evaluation parameters
Recall (R), precision (P), and F-measure are three param-
eters that are used to evaluate the performance of IR sys-
tem, Recall is given by Equation (23) as follows:
Recall
D
R
r
j
j
S
arel
j
j
(23)
where R
r
,
is the set of relevant documents retrieved and
S
arel
is the set of all relevant documents.
The Precision is
given by Equation (24) as follows:
Precision D
R
r
j
j
S
ret
j
j
(24)
where S
ret
is the retrieved documents set.
The AP is used as a standard measure to ﬁnd the quality
of a search system in information retrieval. Precision of a
document d is deﬁned as the fraction of relevant docu-
ments within the set of retrieved documents.
The MAP
for a relevant document sets is obtained as the mean pre-
cision of all
these documents that is given by Equation
(25) as follows:
MAP D
1
n
X
n
i
D 1
Precision R
S
ð
Þ
(25)
where R
S
is the relevant documents set.
In general,
there has to be a trade-off between precision
and recall
as
both of
them cannot
increase
simulta-
neously.
Depending on the
requirement,
we
may be
interested in higher precision or higher recall;
however,
if we want to evaluate the accuracy considering both pre-
cision and recall.
We may use the F-measure to evaluate
the accuracy of the result.
The F-measure is a Harmonic
combination of the precision (P
i
) and recall
(R
i
) values
of ith documents set used in information retrieval.
The F-measure can be calculated by Equation (26)
as
follows:
F
i
D
2P
i
R
i
P
i
C R
i
(26)
We use these evaluation metrics as the primary single
summary performance metric in our experiments that is
also the main ofﬁcial
evaluation metric in the corre-
sponding TREC and FIRE evaluations forum. In order to
make
more
conﬁrm the
superiority of
our
proposed
method results,
we used ﬁxed-level
interpolated preci-
sion,
recall
(the PR curve) curve for making the basic
comparisons
of
our
proposed
method
with
other
methods.
4.4 Experimental results of individual query
expanding terms selection methods
Tables 4 and 5 show the retrieval
performance of query
expansion term selection methods in terms of
AP and
recall
on FIRE and TREC data-sets and compared with
Okapi-BM25 retrieval
model.
Where,
Okapi-BM25 is a
state-of-the-art
probabilistic-based similarity matching
model [11].
In our experiment, we found that the performance of our
proposed query expansion term selection approaches
DCMBQE,
JCMBQE,
KLDBQE,
and RSVBQE achieved
signiﬁcant
improvement
over
basic
retrieval
model
Okapi-BM25.
We
also
note
that
the
improvements
achieved by the proposed model
on TREC disk1&2 are
little greater than the FIRE data-set.
This is
probably
because
that
the
disk 1&2 collections
contain news
articles, which are usually considered as high-quality text
data with less noise.
On the contrary,
FIRE ad-hoc data-
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
599
set are news as well as web collections that are more chal-
lenging and include multiple sources of
heterogeneous
set of document as well as more noise.
Tables 4 and 5 show that the performance of KLDBQE
terms selection method is higher than other term selec-
tion methods in all top-retrieved document sets on both
FIRE and TREC data-sets. Figure 2 shows the signiﬁcant
improvement by all used individual term selection meth-
ods over Okapi-BM25 and the superiority of
KLDBQE
over other individual methods on both FIRE and TREC
data-sets.
The 11-point precision recall curve of all used individual
term selection methods,
namely DCMBQE,
JCMBQE,
RSVBQE,
and KLDBQE with baseline approach Okapi-
BM25 are shown in Figure 3.
The 11-point
precision-
recall curve is a graph plotting the interpolated precision
of an IR system at 11 standard recall levels,
that is,
{0.0,
0.1,
0.2,…,1.0}.
The graph is widely used to evaluate IR
systems that return ranked documents,
which are com-
mon in modern search systems.
Figure 3 also shows the
signiﬁcant
improvement
of
individual
term selection
approaches
over
baseline approach on both data-sets.
This also indicates the superiority of KLDBQE over other
individual approaches.
4.5 Experimental results of borda ranks aggregation
and Word2Vec-based semantic similarity
methods
Tables 6 and 7 show the retrieval
performance of
our
proposed Borda
rank combination methods
with or
without semantic similarity in terms of AP and recall on
both FIRE and TREC data-sets
and compared with
Aguera et al.’s (combining three-query expansion terms
Table 4:
Comparison of different term selection approaches with Okapi-BM25 in term of MAP and average recall
for 50 queries
using top 15 feedback documents and top 30 expansion terms on the FIRE data-set
Top 10 retrieved documents
Top 25 retrieved documents
Top 50 retrieved documents
Methods
MAP
Average recall
MAP
Average recall
MAP
Average recall
Okapi-BM25
0.2217
0.1043
0.2175
0.1871
0.1839
0.2957
DCMBQE
0.2221
0.1176
0.2163
0.1965
0.1847
0.2963
RSVBQE
0.2380
0.1184
0.2201
0.1994
0.1895
0.3045
JCMBQE
0.2439
0.1286
0.2463
0.2150
0.2383
0.3200
KLDBQE
0.2517
0.1299
0.2481
0.2150
0.2397
0.3268
Table 5:
Comparison of different term selection approaches with Okapi-BM25 in term of MAP and average recall
for 50 queries
using top 15 feedback documents and top 30 expansion terms for the TREC data-set
Top 10 retrieved documents
Top 25 retrieved documents
Top 50 retrieved documents
Methods
MAP
Average recall
MAP
Average recall
MAP
Average recall
Okapi-BM25
0.2378
0.1172
0.2204
0.1911
0.1955
0.3012
DCMBQE
0.2381
0.1195
0.2216
0.1978
0.1990
0.3160
RSVBQE
0.2435
0.1193
0.2259
0.2096
0.2098
0.3159
JCMBQE
0.2505
0.1242
0.2486
0.2138
0.2310
0.3280
KLDBQE
0.2536
0.1304
0.2594
0.2191
0.2413
0.3315
Figure 2:
Average recall, MAP and F-measures values of each individual approaches on both FIRE and TREC data-set (for average of top
10, 25, and 50 retrieved documents, discussed in Tables 4and 5)
600
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
selection methods-based model)
models
[5].
Where,
Aguera et al.’s is a state-of-the-art multiple query expan-
sion terms selection combination-based retrieval
model.
Both Tables 6 and 7 also present the results of
Okapi-
BM25
and
KLDBQE
(best
performing
individual
method) methods for better comparisons.
In our experi-
ment,
Tables 6 and 7 show that the performance of our
proposed Ranks
aggregation based query
expansion
approach RABQE alone and with semantic similarity
RASBQE achieved signiﬁcant improvement over Okapi-
BM25 model,
KLDBQE (best individual expansion term
selection method) and Aguera et al.’s methods.
Figure 4
also shows the signiﬁcant improvement by RABQE and
RASBQE over Okapi-BM25 and KLDBQE on both FIRE
and TREC data-sets.
The
11-point
precision recall
curve
of
the
proposed
approaches,
namely
RABQE,
RASBQE,
and baseline
approaches Okapi-BM25 and Aguera et al., are shown in
Figure 5.
The 11-point precisionrecall curve is a graph
plotting the interpolated precision of an IR system at 11
standard recall
levels,
that
is,
{0.0,
0.1,
0.2,…,1.0}.
The
graph is widely used to evaluate IR systems that return
ranked documents, which are common in modern search
systems. Figure 5 also shows the signiﬁcant improvement
of
our
proposed
both
approaches
over
baseline
approaches.
This indicates that both the combination of
Borda rank aggregation scheme and semantic similarity
scheme are having the positive effect on improving the
quality of expansion terms.
4.6 Statistical analysis
4.6.1 Statistical signiﬁcance of proposed approaches
Following the observations of
our
proposed approach
that are giving better performance/results than the best
of individual terms selection or query expansion method
(KLDBQE) considered, paired t-test was applied to show
Figure 3:
Precision recall curve of each individual approaches on both FIRE and TREC data-sets
Figure 4:
Recall, precision and F-measures values of both proposed approaches on both FIRE and TREC data-sets (for average of top 10,
25, and 50 retrieved documents, discussed in Tables 6 and 7)
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
601
that
the
improvement
is
statistically signiﬁcant.
This
paired ttest compares one set of measurements with a
second set from the same sample.
Given two paired sets
X
i
and Yj
of n measured values,
the paired t-test deter-
mines whether they differ from each other in a signiﬁ-
cant
way
under
the
assumptions
that
the
paired
differences
are
independent
and identically normally
distributed.
The statistical paired t-test results obtained for FIRE and
TREC data-sets are tabulated in Tables 8 and 9. A paired
t-test is the most commonly used hypothesis test in IR.
In the present work,
the paired t-tests are conducted to
determine
whether
the
proposed
query
expansion
approaches are statistically different from KLDBQE (best
individual
method)
and Aguera et
al.’s model
or not.
These paired t-tests return the results in terms of h-value,
Table 6:
Comparison of our both proposed approaches with Aguera model in term of average precision and recall for 50 queries
using top 15 feedback documents and top 30 expansion terms for the FIRE data-set
Top 10 retrieved documents
Top 25 retrieved documents
Top 50 retrieved documents
Methods
MAP
Average recall
MAP
Average recall
MAP
Average recall
Okapi-BM25
0.2217
0.1043
0.2175
0.1871
0.1839
0.2957
KLDBQE
0.2517
0.1299
0.2481
0.2150
0.2397
0.3268
Aguera et al.’s model
0.2738
0.1342
0.2545
0.2363
0.2510
0.3382
RABQE (Proposed)
0.2932
0.1413
0.2769
0.2518
0.2656
0.3459
RASBQE (Proposed)
0.3165
0.1507
0.2812
0.2614
0.2685
0.3517
Table 7:
Comparison of our both proposed approaches with Aguera’s model in term of average precision and recall for 50 queries
using top 15 feedback documents and top 30 expansion terms for the TREC data-set
Top 10 retrieved documents
Top 25 retrieved documents
Top 50 retrieved documents
Methods
MAP
Average recall
MAP
Average recall
MAP
Average recall
Okapi-BM25
0.2378
0.1172
0.2204
0.1911
0.1955
0.3012
KLDBQE
0.2536
0.1304
0.2594
0.2191
0.2413
0.3315
Aguera et al.’s model
0.2774
0.1350
0.2581
0.2401
0.2593
0.3352
RABQE (proposed)
0.2948
0.1456
0.2795
0.2585
0.2696
0.3467
RASBQE (proposed)
0.3195
0.1598
0.2883
0.2676
0.2730
0.3628
Figure 5:
Precision recall curve of proposed both approaches on both FIRE and TREC data-sets
Table
8:
Paired
t-test
results
between
Borda
count
and
KLDBQE approach on FIRE and TREC data-sets
KLDBQE
Proposed
approaches
Data-sets
h-Value
p-Value
CI
RABQE
FIRE
1
0.0011
[¡0.1721, ¡0.1200]
TREC
1
0.0004
[¡0.1023, ¡0.0754]
RASBQE
FIRE
1
0.0005
[¡0.1643, ¡0.1045]
TREC
1
0.0008
[¡0.1754, ¡0.1170]
Aguera,s model
FIRE
1
0.0007
[¡0.1436, ¡0.0473]
TREC
1
0.0003
[¡0.1674, ¡0.1071]
Table
9:
Paired
t-test
results
between
both
proposed
approaches and Aguera et
al.’s approach on FIRE and TREC
data-sets
Aguera et al.’s model
Proposed
approaches
Data-sets
h-Value
p-Value
CI
RABQE
FIRE
1
0.0013
[¡0.1754, ¡0.1143]
TREC
1
0.0007
[¡0.1254, ¡0.0543]
RASBQE
FIRE
1
0.0002
[¡0.1687, ¡0.0752]
TREC
1
0.0010
[¡0.1434, ¡0.0598]
602
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
p-value, and conﬁdence interval (CI) values. The p-value
D 0 indicates that the null hypothesis is rejected and that
the mean of our data is signiﬁcantly different from other
approaches with 95% certainty and therefore,
the null
hypothesis (‘‘means are equal’’) cannot be rejected at the
5% signiﬁcance level (a D 0.05).
If the p-value D 1, then the performances are not statisti-
cally different and therefore the null
hypothesis '‘means
are equal'’
can be rejected at
the 5% signiﬁcance level
(a D 0.05).
The CI is the 95% conﬁdence interval of the mean based
upon the t-distribution. Table 8 clearly indicates that the
improvement
of
the proposed RABQE and RASBQE
approaches over KLDBQE method’s is statistically signif-
icant at a D 0.05 (p is almost zero for both the FIRE and
TREC data-set).
Table 9 shows paired t-test values between our proposed
both approaches and Aguera et al.’s model.
Tables con-
tain only the proposed approaches that pass the paired t-
test. In our experiment, we compared our both proposed
approaches with Aguera et
al.’s model.
Table 9 clearly
indicates
that
the
improvement
of
our
proposed
approaches RABQE and RASBQE over Aguera et
al.’s
model
is statistically signiﬁcant at a D 0.05 (p is almost
zero for both the FIRE and TREC data-sets).
4.7 Summary
Our
observations
on the
experimental
results
of
the
query expansion score combination and rank combina-
tion of the query expansion selection methods are sum-
marized as follows:

In general,
query expansion can improve the efﬁ-
ciency of
IR system provided proper
expansion
methods are used.

Individual
query expansion terms selection meth-
ods,
namely DCMBQE,
JCMBQE,
RSVBQE,
and
KLDBQE,
performing
better
than
Okapi-BM25
(non-query expansion method).
In all
used-term
selection methods, KLDBQE performed best among
DCMBQE, JCMBQE, and RSVBQE.

Proposed ranks-aggregating approach using Borda
count
RABQE achieved motivational
results
and
performed signiﬁcantly better
than Okapi-BM25
model,
KLDBQE (best
individual
expansion term
selection method), and Aguera et al.’s methods.

Proposed ranks-aggregating approach and Word2-
Vec-based semantic
ﬁltering
approach RASBQE
also
perform better
than
Okapi-BM25
model,
KLDBQE (best individual expansion term selection
method),
RABQE (best ranks aggregation method),
and Aguera et al.’s method.

The paired t-test results demonstrated statistical sig-
niﬁcance of our proposed approaches over baseline
approach in terms of h-value,
p-value,
and CI value
as shown in Tables 8 and 9.
Overall
analysis of
the experimental
results shows that
both ranks combination and Word2Vec-based semantic
ﬁltering have a positive impact on improving the perfor-
mance of PRF-based query expansion.
5.
CONCLUSION
In this paper,
we have proposed a hybrid QE approach
for improving the efﬁciency of PRF-based AQE. The ﬁrst
contribution of
this work was to explore the power of
combining
multiple
QE terms
selection methods
to
improve the performance of AQE of the user query. As a
second contribution,
we tried to combine the notion of
Word2Vec-based semantic similarity with corpus-based
information,
which strengthens the performance of cor-
pus-based query expansion. For combination of the rank
approaches,
four individual
approaches were combined
by using Borda count ranks combination approach.
For
semantic similarity,
Word2Vec was
used to ﬁlter
out
semantically irrelevant terms.
The experiments
were performed on two well-known
benchmark data-sets FIRE and TREC.
The results were
analysed using MAP, average recall, and recallprecision
curve. The paired t-test approach was also used to deter-
mine that improvements in results are statistically signif-
icant.
The
proposed
approach
results
are
quite
motivating.
In addition to the average analysis,
the per-
formance was also analysed for individual queries.
Con-
sidering all the proposed approaches,
the approach with
Borda count-based combination and Word2Vec seman-
tic ﬁltering provides the best result on average as well as
with respect to the individual query.
ACKNOWLEDGMENTS
The authors are grateful to the University Grants Commission,
New Delhi,
India,
for providing research scholarship during
the tenure of the work.
DISCLOSURE STATEMENT
No potential conﬂict of interest was reported by the authors.
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
603
REFERENCES
1.
M.
E.
Lesk,
“Word-word
associations
in
document
retrieval systems,” Am. Doc., 20(1), pp. 2738, Jan. 1969.
2.
C.
J.
V.
Rijsbergen,
“A theoretical
basis for the use of co-
occurrence data in information retrieval,” J.
Doc.,
vol.
33
(2), pp. 106119, Sept. 1977.
3.
K.
W.
Church and P.
Hanks,
“Word association norms,
mutual
information,
and lexicography,”
Comput.
Lin-
guist.., vol. 16(1), pp. 2229, Jan. 1990.
4.
T.
M.
Cover and J.
A.
Thomas,
Elements of
Information
Theory, 2nd ed. Hoboken, NJ: Wiley, 1991.
5.
J. R. P Aguera, and L. Araujo, “Comparing and combining
methods
for
automatic
query expansion,”
in Adv.
Nat.
Lang. Proc. Appl. Res. Comp. Sci,
Haifa, 2008, pp.
177188.
6.
M. T. M. Valdivia, M. C. D. Galiano, A. M. Raez, and L. A.
U.
L

opez,
“Using information gain to improve
multi-
modal information retrieval systems,” Info.
Proc.
Manag.,
vol. 44(3), pp. 11461158, May 2008.
7.
S.
E.
Robertson,
“On term selection for query expansion,”
J. Doc, 46(4), pp. 359364, Dec. 1990.
8.
M.
Rogati
and Y.
Yang,
“High-performing feature selec-
tion for text classiﬁcation,” in Proc.
11th ACM Int.
Conf.
Inf. Knowl. Manag., Graz, 2002, pp. 659661.
9.
E.
A.
Fox,
and J.
A.
Shaw,
“Combination of
Multiple
Searches,” in Proc.
2nd Text
Retr.
Conf.,
Gaithersburg,
MD, 1994, pp. 243252.
10.
J.
Miao,
X.
Huang,
and Z.
Ye,
“Proximity-based rocchio’s
model for pseudo relevance feedback,” in Proc. 35th Annu.
Int.
ACM SIGIR Conf.
Res.
Dev.
Inf.
Retr.,
Portland,
OR,
2012, pp. 534544.
11.
S.
E.
Robertson,
S.
Walker,
S.
Jones,
M.
M.
H.
Beaulieu,
and M.
Gatford,
“Okapi
at TREC-3,” in Proc.
third Text
Retr. Con, Gaithersburg, MD, 1995, pp. 109126.
12.
J.
Xu and W.
B.
Croft,
“Query expansion using local
and
global document analysis,” in Proc.
19th Annu.
Int.
ACM
SIGIR Conf.
Res.
Dev.
Inf.
Retr.
SIGIR’96,
Zurich,
1996,
pp. 411.
13.
J.
A.
Swets,
“Information retrieval
systems,” Science,
vol.
141(3577), pp. 245250, Jul. 1963.
14.
Z.
Wei,
W.
Gao,
T.
E.
Ganainy,
W.
Magdy,
and K.
F.
Wong,
“Ranking model
selection and fusion for effective
micro blog search,” in Proc.
1st Int.
Workshop Soc.
Media
Retr. Anal. SoMeRA’14, New York, NY, 2014, pp. 2126.
15.
Y.
Goldberg and O.
Levy,
“Word2vec explained:
Deriving
Mikolov
et
al.’s
negative-sampling
word-embedding
method,” In: arXiv preprint arXiv:1402.3722, 2014.
16.
R.
Rehurek and P.
Sojka,
“Software framework for topic
modelling with large corpora,” in Proc. LREC’10 Workshop
New Chall. NLP Frameworks, Valletta, 2010, pp. 4550.
17.
T.
Goodwin,
and S.
M.
Harabagiu,
“UTD at TREC 2014:
Query Expansion for Clinical
Decision Support,” in The
Twenty-third Text
Retr.
Conf.,
Gaithersburg,
MD,
2014.
TREC’14, NIST Special Publication, pp. 18.
18.
F. Diaz, and D. Metzler, “Improving the estimation of rele-
vance models using large external
corpora,” in Proc.
29th
Annu.
Int.
ACM SIGIR Conf.
Res.
Dev.
Inf.
Retr.,
Seattle,
2006, pp. 154161.
Authors
Jagendra Singh is a doctoral candidate at
School
of Computer and Systems Scien-
ces,
Jawaharlal
Nehru University,
India.
He received his Master’s degree in com-
puter engineering from School
of
Com-
puter
and Systems
Sciences,
Jawaharlal
Nehru
University,
New
Delhi.
His
research
interest
is
in
information
retrieval
system,
natural
language proc-
essing, opinion mining, text Mining and web Mining.
E-mail: jagendrasngh@gmail.com
Aditi
Sharan has
been working as
an
assistant professor for the past 12 years at
the School of Computer and Systems Sci-
ences,
Jawaharlal
Nehru
University,
India.
She has a doctoral degree in com-
puter science.
She is involved in teaching
undergraduate and graduate courses like
database
management,
information
retrieval,
data mining,
natural
language
processing
and semantic
web.
She
has
published several
research papers in international
conferences and journals of
repute.
E-mail: aditisharan@gmail.com
604
J. SINGH AND A. SHARAN: RELEVANCE FEEDBACK-BASED QUERY EXPANSION MODEL USING RANKS COMBINING AND WORD2VEC APPROACH
C
o
p
y
r
i
g
h
t
o
f
I
E
T
E
J
o
u
r
n
a
l
o
f
R
e
s
e
a
r
c
h
i
s
t
h
e
p
r
o
p
e
r
t
y
o
f
T
a
y
l
o
r
&
F
r
a
n
c
i
s
L
t
d
a
n
d
i
t
s
c
o
n
t
e
n
t
m
a
y
n
o
t
b
e
c
o
p
i
e
d
o
r
e
m
a
i
l
e
d
t
o
m
u
l
t
i
p
l
e
s
i
t
e
s
o
r
p
o
s
t
e
d
t
o
a
l
i
s
t
s
e
r
v
w
i
t
h
o
u
t
t
h
e
c
o
p
y
r
i
g
h
t
h
o
l
d
e
r
'
s
e
x
p
r
e
s
s
w
r
i
t
t
e
n
p
e
r
m
i
s
s
i
o
n
.
H
o
w
e
v
e
r
,
u
s
e
r
s
m
a
y
p
r
i
n
t
,
d
o
w
n
l
o
a
d
,
o
r
e
m
a
i
l
a
r
t
i
c
l
e
s
f
o
r
i
n
d
i
v
i
d
u
a
l
u
s
e
.
