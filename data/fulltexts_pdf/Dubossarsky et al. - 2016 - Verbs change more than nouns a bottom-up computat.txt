1 
VERBS CHANGE MORE 
THAN NOUNS: A BOTTOM-UP 
COMPUTATIONAL APPROACH 
TO SEMANTIC CHANGE 
H
AIM 
D
UBOSSARSKY 
D
APHNA 
W
EINSHALL 
E
ITAN 
G
ROSSMAN
A
BSTRACT
:
Linguists have identified a number of types of recurrent semantic 
change, and have proposed a number of explanations, usually based on 
specific lexical items. This paper takes a different approach, by using a 
distributional semantic model to identify and quantify semantic change 
across an entire lexicon in a completely bottom-up fashion, and by examining 
which distributional properties of words are causal factors in semantic 
change. Several independent contributing factors are identified. First, the 
degree of prototypicality of a word within its semantic cluster correlated 
inversely with its likelihood of change (the “Diachronic Prototypicality 
Effect”). Second, the word class assignment of a word correlates with its rate 
of change: verbs change more than nouns, and nouns change more than 
adjectives (the “Diachronic Word Class Effect”), which we propose may be 
the 
diachronic 
result 
of 
an 
independently 
established 
synchronic 
psycholinguistic effect (the “Verb Mutability Effect”). Third, we found that 
mere token frequency does not play a significant role in the likelihood of a 
word’s meaning to change. A regression analysis shows that these effects 
complement each other, and together, cover a significant amount of the 
variance in the data. 
K
EYWORDS
: semantic change, distributional semantics. 
1.
THE PROBLEM OF SEMANTIC CHANGE 
Lexical semantic change - change in the meanings of words - is a basic fact of 
language change that can be observed over long periods of time. For example, 
the English word girl originally indicated a child of either sex, but in contem-
porary English, it refers only to a female child. Bybee shows the turning point 
was the fifteenth century, after the conventionalization of the word boy to refer 
to a male child, which “cut into the range of reference for girl” (Bybee 2015: 
202). But semantic change is also “an undeniable and ubiquitous facet of our 
experience of language” (Newman 2015: 267), with words acquiring new 
5
LINGUE E LINGUAGGIO XV.1 (2016) 5–25
2 
senses, developing new polysemies, and entirely new meanings, in time-
frames that can be observed even by casual observation by speakers. For ex-
ample, recent changes in technology have led to novel meanings of words like 
navigate, surf, and desktop (Newman 2015: 266). Speakers and listeners may 
even be aware of “mini” semantic change in real time, when they experience 
an innovative use of an existing word. 
Linguists have identified some recurring types of semantic change. Some 
of the major types include the textbook examples of change in scope, e.g., 
widening (Latin caballus ‘nag, workhorse’ > Spanish caballo ‘horse’) or nar-
rowing (hound ‘canine’ > ‘hunting dog’), or in connotation (amelioration or 
pejoration). However, the systematic search for an explanatory theory of se-
mantic 
change 
was largely neglected until Geeraerts (1985, 1992) 
and 
Traugott & Dasher (2002), who both claimed that semantic change is over-
whelmingly regular. Moreover, both Geeraerts and Traugott have claimed that 
semantic change – like language change in general – is rooted in and con-
strained by properties of human cognition and of language usage. 
Contemporary research identifies different kinds of regularity in semantic 
change as tendencies of change, which are asymmetries with respect to the 
directions in which change is more likely to occur. For example, Traugott & 
Dasher (2002) propose that semantic change regularly follows the pathway: 
objective meaning > subjective meaning > intersubjective meaning. It has also 
been suggested that concrete meanings tend to develop into more abstract ones 
(Bloomfield 1933; Haspelmath 2004; Sweetser 1990). See the following ex-
amples: 
(1)
see 
‘visual perception’ > ‘understanding’ 
(2)
touch 
‘tactile perception’ > ‘feel’ 
(3)
head 
‘body part’ > ‘chief’ 
Another often-observed regularity is that semantic change overwhelm-
ingly tends to entail polysemy, in which a word or expression acquire new 
senses that co-exist with the older conventionalized senses (e.g., a new sense 
for surf has emerged since the 1990s). These new senses can continue to co-
exist stably with the older ones or to supplant earlier senses, thereby “taking 
over” the meaning of the word. 
The existence of such regularities and asymmetries, or “unidirectional 
pathways of change”, has been taken as evidence that language change is not 
random. Moreover, these asymmetries call for explanations that are plausible 
in terms of what we know about human cognition and communication. Nu-
merous such explanations have been offered, from Traugott & Dasher's (2002) 
influential Neo-Gricean account to other pragmatically-based accounts (for an 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
6
3 
overview, see Grossman & Noveck 2015). However, while such accounts may 
offer potentially convincing explanations for observed changes, there is to 
date no empirically-grounded theory that can explain – or predict – which 
words are likely to undergo semantic change, and why this is so, across an 
entire lexicon. 
This last point is the focus of the present article. While historical linguists 
have painstakingly accumulated much data about – and proposed explanations 
for – cross-linguistically recurrent pathways of semantic change (e.g., body-
part term > spatial term), the data and explanations are usually specific to a 
particular group of words. For example, the explanations proposed for the de-
velopment of body-part terms into spatial terms cannot necessarily be gener-
alized to words of other semantic classes. In fact, the question posed in this 
article – what are the specific properties of words that make them more or less 
prone to semantic change? – has been almost entirely neglected in historical 
linguistic research. Furthermore, most studies of attested pathways of change 
tend to focus on their descriptive semantics, and have tended to ignore their 
distributional properties. 
Nonetheless, some work in this direction can be found in earlier structur-
alist and cognitivist theories of semantic change, which emphasized the role 
of the structure of the lexicon in explaining semantic change. For example, it 
has often been assumed that changes in words’ meanings are due to a tendency 
for languages to avoid ambiguous form-meaning pairings, such as homonymy, 
synonymy, and polysemy (Anttila 1989; Menner 1945). On the other hand, 
when related words are examined together, it has been observed that one 
word’s change of meaning often “drags along” other words in the same se-
mantic field, leading to parallel change (Lehrer 1985). These seemingly con-
tradictory patterns of change lead to the conclusion that if ambiguity avoid-
ance is indeed a reason of semantic change, its role is more complex than ini-
tially assumed. 
However, what is common to both ideas – the putative tendency to avoid 
ambiguous form-meaning pairings and the equally putative tendency for 
words in the same semantic domain to change in similar ways – is the obser-
vation that changes in a word’s meaning may result from – or cause – changes 
in the meaning of a semantically related word. The idea that words should be 
examined relative to each other, and that these relations play a causal role in 
semantic change is elaborated by Geeraerts (1985, 1992), who maps related 
words into clusters, and based on Rosch’s prototype theory (1973), establishes 
which words are the prototypical or peripheral exemplars within each cluster. 
Geeraerts analyzes these clusters diachronically, finds characteristic patterns 
of change due to meaning overlap, and concludes that prototypical semantic 
areas are more stable diachronically than peripheral ones. While Geeraert’s 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
7
4 
ideas are promising for studies of semantic change, they are based on case-
studies hand-picked by the linguist, and are not based on large-scale corpora 
(Geeraerts 2010). This is a lacuna in the research field of semantic change, 
which we have addressed in a previous article (Dubossarsky et al. 2015) by 
articulating a method for identifying and quantifying semantic change across 
an entire lexicon, represented by a massive historical corpus. 
Our aim in the present article is to evaluate whether other distributional 
properties of words are indeed implicated in semantic change. Specifically, 
we examine whether words of different parts-of-speech or word classes 
change at different rates. We assume that the null hypothesis is that there is no 
difference between word class assignment and rate of change. However, we 
predict that there will indeed be differences, based on the fact that different 
word classes prototypically encode cognitively different things: nouns proto-
typically encode entities, verbs prototypically encode events, and adjectives 
prototypically encode properties. Moreover, different word classes can have 
significantly different collocational properties, i.e., they occur in different 
types and ranges of contexts. Finally, Sagi et al. (2009), one of the only studies 
to tackle this question, found that in 19th century English, a small selection of 
verbs showed a higher rate of change than nouns. 
It is important to stress that at no time do we, or any of the above works 
cited as far as we know, claim that semantic change is governed by a single 
factor. In fact, it is clear that previous work on semantic change is likely to be 
correct in supposing that social, historical, technological, cognitive, commu-
nicative, and other factors are implicated in semantic change. The question is 
how to tease them apart and understand their respective contributions. This 
paper demonstrates that an observable property of words, i.e., their part-of-
speech or word class assignment, is indeed implicated in semantic change. 
Moreover, we demonstrate that this effect is in addition to another effect 
which we have argued for earlier, namely, that the position of a word within 
its semantic cluster – interpreted as its degree of prototypicality. 
The structure of the paper is as follows: in Section 2, we sketch the meth-
odology used, and in Section 3, we describe the experiment conducted. In Sec-
tion 4 we discuss the results, and in Section 5 we analyze possible interactions 
with other factors. Section 6 is devoted to discussion on the results and their 
implications. Section 7 provides concluding remarks, focusing on directions 
for future research. 
2.
METHODOLOGY 
2.1
The role of input frequency 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
8
5 
There are numerous ways of representing lexical meaning. Computational 
models developed for representing meaning excel in what computational ap-
proaches do best and classical historical linguistics does poorly, namely, the 
large-scale analysis of language usage and the precise quantitative represen-
tation of meaning. At the heart of these models lies the “distributional hypoth-
esis” (Firth 1957; Harris 1954), according to which the meaning of words can 
be deduced from the contexts in which they appear. 
We employ a 
distributional semantic 
modeling (DSM) 
approach to 
represent word meanings. DSM collects distributional information on the co-
occurrence profiles of words, essentially showing their collocates (Hilpert 
2006; Stefanowitsch & Gries 2003), i.e., the other words with which they co-
occur in specific contexts. Traditionally, this is done by representing each 
word in terms of its collocates across an entire lexicon. This type of model has 
the advantage of providing an explicit (or direct) quantitative measure of a 
word’s meaning, and is informative in that it tells us which words do or do not 
occur with a given word of interest. However, since most words occur with a 
limited range of collocates, most of the words in a lexicon will co-occur with 
most 
other 
words 
in 
the 
lexicon 
zero 
times. 
As 
such, 
these 
kinds 
of 
representations are sparse. This can be seen in the following illustrative 
example bellow, where only ten words collocate with the word pan, while the 
rest of the vocabulary (i.e., surf, sky, dress, hat, call, etc.) does not. 
T
ABLE 
1.
W
ORDS COLLOCATIONS STATISTICS FOR THE WORD PAN 
(
ILLUSTRATIVE EXAMPLE
)
This type of representation is usually further analyzed, e.g., by normaliz-
ing the word counts to frequencies, or with more sophisticated statistical meth-
ods, e.g., tf-idf or point mutual information. However, for our purposes, such 
models are inadequate, because in the end they tell us only whether a word 
co-occurs with another word or not. In order to understand the relationship of 
a word with the rest of the words in an entire lexicon, other types of models 
are necessary. 
These models are the more recent ones that exploit machine-learning and 
neural network tools to learn the distributional properties of words automati-
cally. Unlike traditional models, they do so by representing words in terms of 
the interaction of multiple properties. However, the specific contribution of 
collocations 
pot 
fry 
cook 
egg 
bacon 
cake 
butter 
oil 
stir 
stove 
surf 
sky 
dress 
hat 
# 
87 
69 
61 
55 
51 
49 
23 
19 
17 
9 
0 
0 
0 
0 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
9
6 
each property, when taken on its own, is opaque; as such, the quantitative rep-
resentation of a word’s meaning is implicit. Of the available recent models of 
this type, we chose a recently developed skip-gram word2vec model (Mikolov 
et al. 2013c, 2013d). This word2vec model has been fruitfully applied to dis-
tributional semantic corpora research, and scores high in semantic evaluation 
tasks (Mikolov et al. 2013a). As we will show, proof-of-concept can also be 
found in our results. 
The word2vec model captures the meaning of words through dense vec-
tors in an n-dimensional space. Every time a word appears in the corpus, its 
corresponding vector is updated according to the collocational environment in 
which it is embedded, up to a fixed distance from that word. The update is 
carried out such that the probability in which these words predict their context 
is maximized (Figure 1a.). As a result, words that predict similar contexts 
would be represented with similar vectors. In fact, this is much like linguistic 
items in a classical structuralist paradigm, whose interchangeability at a given 
point or “slot” in the syntagmatic chain implies that they share certain aspects 
of function or meaning, i.e., the Saussurian notion of “value” (Figure 1b.). It 
is worth noticing that if taken individually, the vectors’ dimensions are 
opaque; only when the full range of dimensions is taken together do they cap-
ture the meaning of a word in the semantic hyper-space they occupy. 
F
IGURE 
1.
(
A
)
WORD
2
VEC SKIP
-
GRAM ARCHITECTURE
.
G
IVEN A WORD
,
W
(
T
),
THE MODEL PREDICTS THE WORDS THAT PRECEDE AND PROCEED IT IN A 
WINDOW OF 
4
WORDS
,
W
(
T
-2),
W
(
T
-1),
W
(
T
+1),
W
(
T
+2)
(M
IKOLOV ET AL
.
2013
B
). 
(
B
)
A
N EXAMPLE OF THE CLASSICAL STRUCTURALIST PARADIGM
. 
While it may be surprising for linguists that one would choose to rely on 
a model whose individual dimensions are opaque, this is not a major concern, 
since it is well-established that words assigned similar vectors by the model 
are in fact semantically related in an intuitive way; for a recent demonstration, 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
10
7 
see Hilpert & Perek (2015), which looks at the collocates of a single construc-
tion in English. The similarity between vectors is evaluated quantitatively, and 
defined as the cosine distance between the vectors in the semantic hyper-
space. Short distances are considered to reflect similarity in meaning: related 
words are closer to each other in the semantic space (Turney 2006; Mikolov 
et al. 2013d; Levy & Goldberg 2014). In fact, this is reflected in the words’ 
nearest neighbors in the semantic space that often capture synonymic, anto-
nymic or level-of-category relations. 
Although the model uses the entire lexicon for training, the accuracy of 
the meaning representations that is captured in the corresponding vectors is 
expected to diminish for less frequent words. This is simply because these 
words do not appear frequently enough for the model to learn their corre-
sponding contexts. Therefore, only the most frequent words in the corpus, ex-
cluding stop-words, are defined as words-of-interest and are further analyzed. 
These words represent the entire lexicon. 
2.2
Corpus 
A massive historical corpus is required to train distributional semantic models. 
This is because the words whose distributional properties we are interested in 
must appear frequently enough in each time period in order to collect enough 
statistical information about their properties. Clearly, the time resolution of 
any analysis on such models is limited by the nature of the historical corpus: 
the finer the tagging for time, the finer the analysis can be. 
Google Ngrams is the best available historical corpus for our purposes, as 
it provides an unprecedented time resolution – year by year – on a massive 
scale; the second largest historical corpus is about 1000 times smaller. Tens 
of millions of books were scanned as part of the Google Books project, and 
aggregated counts of Ngrams on a yearly resolution from those books are pro-
vided. 
We used a recently published syntactic-Ngram dataset (Goldberg & Or-
want 2013), where the words
1
are analyzed syntactically using a dependency 
1
The present study deals with word forms rather than lexemes. While this is possibly a short-
coming, it is shared by most NLP studies of massive corpora. Furthermore, the issue is less 
likely to affect English, with its relatively poor morphology, than other languages. Neverthe-
less, one might speculate about the effects of this. For example, it might be that the meaning of 
a specific verb forms in the corpus will be narrower than that of specific noun forms, overall, 
in an analysis based on word forms than in one based on lexemes. While it would be of consid-
erable interest to conduct an experiment to determine the effect of using word forms versus 
lexemes, the issue has never been dealt with explicitly in computational linguistics, as far as we 
know, and it is beyond the scope of the present paper. We thank an anonymous reviewer for 
bringing this issue to our attention. 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
11
8 
parser in their original sentences. The dataset provides aggregated counts of 
syntactic Ngrams on a yearly resolution that includes their part-of-speech 
(POS)
2
assignments as well. The dataset distinguishes content words, which 
are meaning-bearing elements, from functional markers
3
that modify the con-
tent words. Therefore, a syntactic Ngram of order N includes exactly N con-
tent words and few optional function markers. We used syntactic Ngrams of 
4 content words from the English fiction books,
4
and aggregated them over 
their dependency labels to provide POS Ngrams. The following is an example 
POS Ngram from the corpus. 
(4) and_CC with_IN sanction_NN my_PR tears_NN gushed_VB out_RB 
Verbs, nouns, and adjectives below a certain frequency threshold, and all 
the rest of the POS assignment, lose their tags. In this Ngram, only tears re-
tains it. 
The historical corpus is sorted diachronically, with 10 million POS 
Ngrams (about 50 million words) per year for the years 1850-2000. When the 
number of POS Ngrams in the corpus for a given year was bigger than that 
size, due to the increasing number of published and scanned books over time, 
a random subsampling process was conducted to keep a fixed corpus size per 
year. This resulted in a corpus size of about 7.5 billion words. Only the words-
of-interest, the most frequent words in the corpus, retain their POS assign-
ment, while the rest of the words reverted to their original word forms. All 
words were lowered case. 
2.3
Diachronic Analysis 
After initialization, the model is trained incrementally, one year after the 
other, for the entire historical corpus (POS-tagged and untagged words alike). 
In this way, the model’s vectors at the end of one year’s training are the start-
ing point of the following year’s training, which make them comparable dia-
chronically. The model is saved after each year’s training, so that the words' 
vectors could be later restored for synchronic and diachronic analyses. 
The words vectors are compared diachronically in order to detect semantic 
change. Based on the affinity between similarity in meaning and similarity in 
vectors described in §2.1, semantic change is defined here as the difference 
between a word’s two vectors at two time points. This allows us to quantify 
2
We use the term “part-of-speech” abbreviated POS, in the context of Natural Language Pro-
cessing tagging, and the term “word class” otherwise.
3
These include the following dependency labels: det, poss, beg, aux, auxpass, ps, mark, com-
plm and prt.
4
From the 2
nd
version of Google books. 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
12
9 
semantic change in a straightforward fashion: the bigger the distance between 
the two vectors of a given word, the bigger the semantic change that this word 
underwent over that period of time. Specifically, the comparison is defined as 
the cosine distance between the word’s two vectors according to equation 1, 
with 0 being identical vectors and 2 being maximally different. This is carried 
out for the entire lexicon. 
(
1
)
∆𝑤
𝑡
0
→𝑡
1
= 1 − 
𝑣
𝑤
𝑡
0
∙ 𝑣
𝑤
𝑡
1
‖
𝑣
𝑤
𝑡
0
‖
∙
‖
𝑣
𝑤
𝑡
1
‖
where 
𝑣
𝑤
𝑡
0
and 
𝑣
𝑤
𝑡
1
are the word’s 
w
vectors at two time points, 
t
0
and 
t
1
, 
respectively. 
In the following section, we present an experiment that investigates the 
relationship between word class assignments and likelihood of change. 
3.
EXPERIMENT 
In this experiment, we evaluate the hypothesis that different parts of speech 
change at different rates. As noted above, we assume that the null hypothesis 
is that there is no difference between part of speech assignment and rate of 
change. However, we predict that there will indeed be differences, based on 
the fact that different parts of speech prototypically encode cognitively differ-
ent things: nouns prototypically encode entities, verbs prototypically encode 
events, and adjectives prototypically encode properties. Moreover, different 
parts of speech can have significantly different collocational properties, i.e., 
they occur in different types and ranges of contexts. Finally, pilot studies of 
this question (Sagi et al. 2009) have indicated that some verbs show a higher 
rate of change than some nouns. 
The word2vec model
5
was initialized with the length of vector set to 52, 
which means that the words' contexts are captured in a 52-dimension semantic 
hyper-space. The model was trained over the POS-tagged English fiction cor-
pus (see §2.2), using the method described above (see §2.3). Words that ap-
peared less than 10 times in the entire corpus were discarded from the lexicon 
and were ignored by the model. 
The vectors of the 2000 most frequent verbs, nouns and adjectives (6000 
in total) as they appear in the corpus were defined as the words-of-interest, 
and restored from the model at every decade from 1900 till 2000. For each 
word, the cosine distances between its vectors at every two consecutive dec-
ades were computed using equation (1). This resulted in 6000x10 semantic 
5
We used genism python library for its word2vec implementation (Řehůřek & Sojka 2010). 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
13
10 
change scores that represent the degree of semantic change that each word 
underwent in every decade throughout the twentieth century (e.g., 1900-1910, 
1910-1920, until 1990-2000). The average semantic change scores of each 
POS assignment were compared between groups. 
4.
RESULTS 
Figure 2 shows the average semantic change for the different POS assignment 
groups at ten decades throughout the twentieth century. The results were sub-
mitted to a two-way ANOVA with POS assignment and decade as the inde-
pendent variables. The first main effect, also clearly visible, is that the POS 
assignment groups differ in their rates of semantic change over all the decades 
(F
(2,59970) 
= 6464, η = .177, p-value <.001). The second main effect is that the 
semantic change rate appears to differ throughout different decades across all 
POS assignment groups (F
(9,59970)
= 576, η = .08, p-value <.001). The interac-
tion between the variables was found to be significant as well (F
(18,59970)
= 
14.34, η = .004, p-value <.001). This means that the rate of semantic change 
along the decades is not uniform across the POS assignment groups. However, 
the effect size of the first two variables reported above is robust, accounting 
for 17.7% and 8% of the overall variance in the words semantic change, re-
spectively, which render these variables highly meaningful. In contrast, the 
effect size of the aforementioned interaction accounts for only 0.4% of the 
variance, which makes it unimportant, albeit statistically significant. 
In order to evaluate the source of the first main effect – the difference in 
the rate of semantic change between the POS assignment, we conducted per-
mutation tests as a post-hoc analysis on the pairs verbs-nouns and nouns-ad-
jectives. The permutation tests created null hypotheses for each pair by assign-
ing words to one of the two POS group randomly, then computing the differ-
ences between the averages of the two groups, and repeating the process 
10,000 times for each decade. These distributions were later compared to the 
real differences in the average semantic change in each decade, so that their 
statistical significance could be evaluated. The permutation tests corroborate 
what is visibly clear from the descriptive pattern of the results (all p-values 
<.001), that verbs change more than nouns, and nouns change more than ad-
jectives. 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
14
11 
F
IGURE 
2.
A
VERAGE SEMANTIC CHANGE RATES THROUGHOUT THE DECADES IN THE TWENTIETH 
CENTURY FOR DIFFERENT 
POS
ASSIGNMENT GROUPS
.
B
ARS REPRESENT STANDARD ERRORS
. 
5.
INTERACTION WITH OTHER FACTORS: FREQUENCY 
AND PROTOTYPICALITY 
In previous work, at least two observable properties of words have been 
argued to be causally implicated in semantic change, word frequency and 
prototypicality. We wanted to test their joint involvement in semantic change 
in light of the aforementioned findings. 
5.1
Frequency 
Frequency is often linked to language change, but its exact effects still remain 
to be worked out (Bybee 2006, 2010). While frequency clearly facilitates 
reductive formal change in grammaticalization and in sound change, it also 
protects morphological structures and syntactic constructions from analogy 
(e.g., irregular verbs forms are more frequent). Since no explicit hypothesis 
has been made regarding the role of frequency in semantic change per se, we 
set out to test the hypothesis that frequency plays some role in semantic 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
15
12 
change. The null hypothesis was that there is no correlation between words’ 
frequencies and their degree of semantic change. 
Token frequencies were extracted from the entire corpus (about 7.5 billion 
words) and served as the words frequencies. The degrees of semantic change 
were taken from the results reported in §4 above. 
In general, frequency was not found to correlate with the degree of words’ 
semantic change over the ten decades in the twentieth century. Only four 
decades (1900-1910; 1910-1920; 1950-1960; 1960-1970) showed significant 
(p-value <.01) correlations. However, such correlations are so small, with 
maximum correlation coefficient <.07, that in terms of their effect size they 
account for less than 0.5% of the variance in the semantic change scores. 
Similar results were obtained when the analysis was repeated for each POS 
assignment 
group 
separately. 
Most 
correlations 
were 
statistically 
insignificant, and the ones that were significant were very small. Overall these 
results suggest that frequency plays little or no role in semantic change. We 
think that this result is surprising, since frequency is often thought to correlate 
with the degree of entrenchment of linguistic items in the mental lexicon 
(Bybee 2010). As such, one might hypothesize that words with high token 
frequency 
might 
be 
“protected” 
from 
semantic 
change. 
However, 
this 
hypothesis is counter-indicated by the results of our experiment. It may be that 
token frequency is, in the end, mainly responsible for coding asymmetries 
(Haspelmath 2008) and does not contribute much to semantic change per se. 
5.2
Prototypicality 
One of the model’s inherent properties is that similar words have similar 
vectors (see §2.1). This makes the vectors ideal for clustering, where each 
cluster captures the words’ “semantic landscape,” as Hilpert & Perek (2015) 
call it. Importantly, it turned out that these clusters exhibit an internal 
structure, with some words closer to the center and others further away. In 
Dubossarsky et al. (2015) we analyzed this structure, and interpreted the 
distance 
of 
a 
word 
from 
its 
cluster 
center 
to 
reflect 
its 
degree 
of 
prototypicality, which is the degree by which a word resembles its category 
prototype. Crucially, this prototypicality was found to play an important role 
in semantic change, as the further a word is from its category’s prototype, the 
more likely it is to undergo change. 
We employ the methodology described in Dubossarsky et al. (2015) to the 
current dataset. Specifically, for each decade we cluster the 6000 word vectors 
using 1500 clusters, and compute the words’ distances from their cluster 
centroids. This resulted in ten “prototypicality scores” for each word. 
In Table 2, we present two clusters as examples. In each cluster, the words 
are sorted in prototypicality order (distance from their cluster’s center). As a 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
16
13 
result, said and chamber/room, appear at the tops of their lists, and constitute 
the most prototypical exemplars in their clusters, verbs of utterance and 
enclosed habitats for humans (see Dubossarsky et al. 2015 for further 
examples). 
said_VB, 0.06 
chamber_NN, 0.04 
exclaimed_VB, 0.08 
room_NN, 0.04 
answered_VB, 0.08 
drawing_NN, 0.05 
added_VB, 0.11 
bedroom_NN, 0.06 
whispered_VB, 0.13 
kitchen_NN, 0.07 
cried_VB, 0.14 
apartment_NN, 0.1 
murmured_VB, 0.15 
growled_VB, 0.16 
repeated_VB, 0.2 
muttered_VB, 0.25 
T
ABLE 
2.
T
WO WORD CLUSTERS
,
WITH 
POS
TAGS AND DISTANCES FROM THEIR CENTROID
,
SORTED IN ASCENDING ORDER OF THE LATTER
. 
We used this approach to extend our previous finding that focused on 
semantic change in only one decade (1950-1960) to the entire twentieth 
century. Indeed, prototypicality at the beginning of each of the ten decades 
was related to the semantic change the words underwent by the end of that 
decade. Correlation coefficients ranged between r=.27 and r=.35, with average 
coefficient of r=.32 (all p-values <.001). This means that the farther a word is 
from the prototypical center of its category, the more likely it is to undergo 
semantic 
change, 
and 
attests 
to 
the 
meaning-conserving 
nature 
of 
prototypicality in semantic change. This could be called the “Diachronic 
Prototypicality Effect”. 
5.3
Regression analysis 
It is intuitively clear that semantic change is not induced solely by a single 
factor, and that different factors may also be involved. Therefore, we wanted 
to evaluate the interaction between the two factors that were proven to be in-
volved in semantic change, word class assignment and prototypicality. 
In order to discern the contribution of these two factors, whether they com-
plement each other or are to a large extent redundant, they were submitted to 
a multiple linear regression analysis. Prototypicality, as distance from cen-
troid, and POS assignment were the independent variables, and the semantic 
change scores was the dependent variable. Regression analyses were con-
ducted for these variables at each of the ten decades, and also pulled over all 
the decades. 
Table 3 shows the contribution of each of the two variables in accounting 
for the semantic change in each of the ten decades examined as well as overall 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
17
14 
the decades (all the results reported were statistically significant p-value <.01). 
The results show that the two variables account for a fair amount of the vari-
ance in semantic change, between 21%-29%. Although both variables account 
for a large part of semantic change when taken individually, POS plays a 
larger role. Prototypicality, despite playing a lesser role, accounts for a sub-
stantial amount of the variance in semantic change as well, which exactly re-
flects its correlation coefficients' values reported above. 
Crucially, prototypicality’s unique contribution to the variance in seman-
tic change, over and above what is being explained by POS, is smaller than its 
individual contribution. This indicates that the two variables overlap to a cer-
tain degree, and are not fully independent. However, the fact that prototypi-
cality adds a substantial and unique explanatory power to the regression model 
suggests that different independent causal elements are involved in semantic 
change. Our variables are unable to capture these elements in a fully independ-
ent form, but different choice of variables, at a different linguistic level, per-
haps could. Nevertheless, the results support the hypothesis that the different 
factors involved in semantic change can be ultimately teased apart. 
Decades 
Variables 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
pulled 
POS + Prototypicality 
29 
24 
28 
25 
23 
22 
23 
19 
21 
22 
22 
POS 
24 
17 
23 
19 
19 
16 
20 
12 
14 
17 
17 
Prototypicality 
10 
12 
10 
11 
7 
11 
8 
12 
12 
9 
10 
∆ Prototypicality 
5 
7 
5 
6 
4 
6 
3 
7 
7 
5 
5 
T
ABLE 
3.
P
ERCENTAGES OF THE EXPLAINED VARIANCE IN SEMANTIC CHANGE WITH DIFFERENT 
COMBINATIONS OF VARIABLES THROUGHOUT THE 
10
DECADES
,
AND PULLED OVER THE DECADES
. 
6.
DISCUSSION 
In the above section, we have argued that the word class assignment of a word 
is a distinct and significant contributing factor to the likelihood for its meaning 
to change over time. While, as we have noted above, the null hypothesis is 
that part of speech assignment does not play a role in semantic change, it is 
nonetheless reasonable that verbs change at a faster rate than nouns, and that 
both change at a faster rate than adjectives. 
For an explanation, we turn to psycholinguistic research that indicates that 
in particular contexts, verb meanings are more likely to be reinterpreted than 
noun meanings. In this section, we restrict ourselves to the noun-verb asym-
metry, leaving adjectives for future research. Early work on this topic 
(Gentner 1981) identified a processing effect known as “verb mutability” 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
18
15 
which basically says that “the semantic structures conveyed by verbs and other 
predicate terms are more likely to be altered to fit the context than are the 
semantic structures conveyed by object-reference terms” (Gentner & France 
1988: 343). Broadly speaking, this effect states that when language users are 
confronted with semantically implausible utterances, e.g., the lizard wor-
shipped, they are more likely to reinterpret the verb’s meaning than that of the 
collocate noun. While it would have been possible for lizard to be reinter-
preted as meaning slimy man, in fact, experimental subjects preferentially re-
interpreted the verb as meaning, e.g., look at the sun or some other action that 
lizards actually do.
6
Similarly, given the utterance the flower kissed the rock, 
English speakers did not reinterpret the meaning of the nouns, e.g., a flower-
like and rock-like person kissing, but rather of the verb, interpreting kissed as 
describing an act of gentle contact (Gentner & France 1988: 345). 
The verb mutability effect requires explanation. Several types of explana-
tions have been proffered which mostly have to do with the inherent semantic 
and formal properties of nouns as opposed to verbs: 
1.
Nouns outnumber verbs in utterances (Gentner & France 1988). 
2.
Verbs are typically more polysemous than nouns (Gentner & France 
1988). 
3.
Verbs are typically predicates, while nouns establish reference to ob-
jects (Gentner & France 1988). 
4.
Nouns concepts are more internally cohesive than verb representations 
(Gentner & France 1988). 
5.
Nouns are learned earlier than verbs, and presumably for this reason are 
more stable (Gentner & Boroditsky 2001). 
However, all of these explanations have problems (Gentner & France 
1988; Fausey et al. 2006; Ahrens 1999). 
Our results do not allow us to take a position on the ultimate causal factors 
underlying the verb mutability effect, nor do we assume that it is universal.
7
6
Another line of research that may contribute to an explanation of this phenomenon is generally 
known as coercion, in which the meaning of a construction is “type-shifted” in appropriate 
contexts. For example, while the verb know in English has a stative default interpretation, when 
combined with an adverb like suddenly, e.g., Suddenly, she knew it, it takes on an inchoative 
meaning. Michaelis (2004) has provided a detailed theory of coercion in the framework of Con-
struction Grammar, focusing on aspectual coercion. What we observe from the literature on 
coercion, although the point is not made explicitly therein, is that it is the event whose semantics 
is adjusted to fit the context, rather than the referring expressions.
7
For example, Ahrens (1999) shows that the verb mutability effect observed in Mandarin is 
different from that observed in English, and Fausey et al. (2006) found that Japanese does not 
show a robust noun-verb asymmetry.
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
19
16 
Rather, we opportunistically embrace the observation that in English, the lan-
guage investigated here, this effect has been shown to be robust. Under the 
assumption that diachronic biases are ultimately rooted in synchronic “online” 
performance or usage, we expect that the tendency of verbs’ meanings to be 
more frequently adapted to contexts of semantic strain than the meanings of 
their noun collocates should show up as a diachronic bias. 
In fact, this is the leading hypothesis in most theories of semantic change: 
the interpretive strategies of language users, specifically listeners, are what 
lead to semantic reanalysis. For example, Bybee et al. (1994) propose that 
listeners’ inferences cause some types of semantic change observed in gram-
maticalization. Traugott & Dasher (2002) make a similar argument, couching 
their theory in Neo-Gricean pragmatics. Detges & Waltereit (2002) propose a 
“Principle of Reference”, according to which listeners interpret contextual 
meanings as coded meanings, and Heine (2002) talks about “context-induced 
reinterpretation”. However, closest to the type of effect discussed here is Re-
gina Eckardt (2009) principle of “Avoid Pragmatic Overload”, which says that 
when listeners are confronted with utterances with implausible presupposi-
tions, they may be coerced into a form-meaning remapping.
8
Essentially, all of these theories argue that the ways in which listeners in-
terpret semantically implausible utterances lead to biases in semantic change, 
and, ultimately, the appearance of “pathways” of semantic change. The verb 
mutability effect identified by Gentner (1981) may be one kind of synchronic 
interpretative bias implicated in the diachronic asymmetry observed in the 
present article: in terms of synchronic processing, verbs are more semantically 
mutable than nouns; correspondingly, in terms of diachronic change over time, 
verbs undergo more semantic change than nouns. However, the bridge be-
tween synchronic processing and diachronic change is not an obvious one. 
What does seem to be clear is that one would need an appropriate model of 
memory that would allow individual tokens of utterances, with their contex-
tual meanings, to be stored as part of the representation of a word; for an ex-
ample, see the exemplar-based model proposed in detail by Bybee (2010). 
We would like to point out that we do not think that it is necessarily the 
word class as a structural label that is implicated in semantic change. Rather, 
we suspect, along with previous researchers, that this is but a proxy for another 
asymmetry: verbs, nouns, and adjectives prototypically encode different con-
cepts, with verbs prototypically denoting events, nouns denoting entities, and 
adjectives denoting properties (Croft 1991, 2000, 2001). It is highly plausible 
8
Grossman et al. (2014) and Grossman & Polis (2014) have applied the latter to long-term 
diachronic changes in Ancient Egyptian, which provides some necessary comparative data from 
a language other than the well-studied western European languages.
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
20
17 
that the diachronic asymmetry observed in this article is the result of the se-
mantics of the concepts prototypically encoded by a word class rather than the 
formal appurtenance to a word class per se. 
7.
CONCLUSIONS 
In this paper, we have proposed that a computational approach to the problem 
of semantic change can complement the toolbox of traditional historical lin-
guistics, by detecting and quantifying semantic change over an entire lexicon 
using a completely bottom-up method. Using a word2vec model on a massive 
corpus of English, we characterized word meanings distributionally, and rep-
resented it as vectors. Defining the degree of semantic change as the cosine 
distance between two vectors of a single word at two points in time allowed 
us to characterize semantic change. While in earlier work (Dubossarsky et al. 
2015), we argued that the degree of semantic change undergone by a word 
was found to correlate inversely with its degree of prototypicality, defined as 
its distance from its category’s center, in the present article we argued that the 
degree of semantic change correlates with its word class assignment: robustly, 
verbs change more than nouns, and nouns change more than adjectives. A re-
gression analysis showed that although these effects are not entirely independ-
ent from each other, they nevertheless complement each other to a large ex-
tent, and together account for about 25% of the variance found in the data. 
Interestingly, token frequency on its own did not play a role in semantic 
change. 
These results are both reasonable and surprising. They are reasonable be-
cause part-of-speech assignment is probably a proxy for the prototypical 
meanings denoted by the different parts of speech. While verbs, nouns, and 
adjectives are formal categories of English (“descriptive categories,” Haspel-
math 2010), and as such, may encode non-prototypical meanings (e.g., the 
English word flight denotes an event rather than an entity), the majority of 
frequently encountered nouns are likely to denote entities, verbs to denote 
events, and adjectives to denote properties. Our results indicate that the inher-
ent prototypical semantics of parts-of-speech does indeed influence the likeli-
hood of word meanings to change, individually and aggregately across a lex-
icon. 
We have addressed one part of the diachronic data observed, by relating 
the diachronic noun-verb asymmetry to the findings of experimental psychol-
ogy: verbs not only change more than nouns over time, their meanings are also 
more likely to be changed in online synchronic usage, especially under condi-
tions of “semantic strain,” i.e., when language users are confronted with se-
mantically implausible collocations. Under the assumption that semantic 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
21
18 
change over time is the result of “micro-changes” in synchronic usage, we 
think it is plausible that the “verb mutability effect” may be part of a real 
causal explanation for the diachronic noun-verb asymmetry. To the extent that 
this assumption is correct, it provides further evidence for the need for rich 
models of memory, possibly along the lines of Bybee's exemplar-based model. 
Obviously, much remains for future research. The findings presented here 
are for a particular language over a particular time period. The most urgent 
desideratum, therefore, is cross-linguistic investigation. Since the computa-
tional tools used here require massive corpora, such cross-linguistic research 
would demand either larger corpora for more languages, or the development 
of computational tools that could deal adequately with smaller corpora. An-
other direction for future research is to continue to identify and tease apart the 
causal factors implicated in semantic change: while our findings account for 
a considerable amount of the variance found in the data, they hardly account 
for all of it. It is likely that further causal factors will be found both in purely 
distributional factors, the semantics of individual lexical items (given a finer-
grained semantic tagging), and extra-linguistic factors. For example, our re-
sults show a lack of uniformity in the total amount of change across decades 
in the twentieth century, a finding that may be related to that of (Bochkarev et 
al. 2014), which showed that the total amount of change in the lexicons of 
European languages over the same time period correlated with actual histori-
cal events. 
Despite the preliminary and language-specific nature of our results, we 
believe that this study makes a real contribution to the question of semantic 
change, by showing that a bottom-up analysis of an entire lexicon can identify 
and quantify semantic change, and that the interaction of the causal factors 
identified can be evaluated. 
REFERENCES 
Ahrens, K. (1999). The mutability of noun and verb meaning. Chinese Language and 
Linguistics 5. 335–371. 
Anttila, 
R. 
(1989). 
Historical 
and 
comparative 
linguistics. 
Amsterdam: 
John 
Benjamins. 
Bloomfield, L. (1933). Language. New York: Henry Holt. 
Bochkarev, V., V. Solovyev & S. Wichmann (2014). Universals versus historical 
contingencies in lexical evolution. Journal of The Royal Society Interface. 1–23. 
Bybee, J. (2006). Frequency of Use and the Organization of Language. Oxford: 
Oxford University Press. 
Bybee, J. (2010). Language, Usage and Cognition. Cambridge, UK: Cambridge 
University Press. 
Bybee, J. (2015). Language change. Cambridge, UK: Cambridge University Press. 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
22
19 
Bybee, J., R. Perkins & W. Pagliuca (1994). The evolution of grammar: Tense, aspect, 
and modality in the languages of the world. Chicago: University of Chicago Press. 
Croft, W. (1991). Syntactic categories and grammatical relations: The cognitive 
organization of information. Chicago: University of Chicago Press. 
Croft, W. (2000). Parts of speech as language universals and as language-particular 
categories. In P. Vogel & B. Comrie (eds.), Approaches to the typology of word 
classes, 65–102. Berlin: Mouton de Gruyter. 
Croft, W. (2001). Radical construction grammar: Syntactic theory in typological 
perspective. Oxford University Press. 
Detges, U. & R. Waltereit (2002). Grammaticalization vs. reanalysis: A semantic-
pragmatic 
account 
of 
functional 
change 
in 
grammar. 
Zeitschrift 
für 
Sprachwissenschaft 21(2). 151–195. 
Dubossarsky, H., Y. Tsvetkov, C. Dyer & E. Grossman (2015). A bottom up approach 
to category mapping and meaning change. In V. Pirrelli, C. Marzi & M. Ferro 
(eds.), Word Structure and Word Usage. Proceedings of the NetWordS Final 
Conference, 66–70. Pisa. 
Eckardt, R. (2009). APO—avoid pragmatic overload. In M.-B. Mosegaard Hansen & 
J. Visconti (eds.), Current trends in diachronic semantics and pragmatics, 21–
42. Bingley: Emerald. 
Fausey, C. M., H. Yoshida, J. Asmuth & D. Gentner (2006). The verb mutability 
effect: Noun and verb semantics in English and Japanese. In Proceedings of the 
28th annual meeting of the Cognitive Science Society, 214–219. 
Firth, J. R. (1957). Papers in Linguistics 1934–1951. London: Oxford University 
Press. 
Geeraerts, D. (1985). Cognitive restrictions on the structure of semantic change. In J. 
Fisiak (ed.), Historical Semantic, 127–153. Berlin: Mouton de Gruyter. 
Geeraerts, D. (1992). Prototypicality effects in diachronic semantics: A round-up. In 
G. Kellermann & M. D. Morissey (eds.), Diachrony within Synchrony: language, 
history and cognition, 183–203. Frankfurt am Main: Peter Lang. 
Geeraerts, D. (2010). Theories of Lexical Semantics. Oxford: Oxford University Press. 
Gentner, D. (1981). Some interesting differences between verbs and nouns. Cognition 
and brain theory 4(2). 161–178. 
Gentner, D. & L. Boroditsky (2001). Individuation, relativity, and early 
word 
learning. In M. Bowerman & S. C. Levinson (eds.), Language acquisition and 
conceptual development, 215–256. Cambridge, UK: Cambridge University Press. 
Gentner, D. & I. M. France (1988). The verb mutability effect: Studies of the 
combinatorial semantics of nouns and verbs. In S. L. Small, G. W. Cottrell & M. 
K. 
Tanenhaus 
(eds.), 
Lexical 
ambiguity 
resolution: 
Perspectives 
from 
psycholinguistics, neuropsychology, and artificial intelligence, 343–382. San 
Mateo, CA: Kaufmann. 
Goldberg, Y. & J. Orwant (2013). A Dataset of Syntactic-Ngrams over Time from a 
Very Large Corpus of English Books. In Second Joint Conference on Lexical and 
Computational 
Semantics 
(*SEM), 
Volume 
1: 
Proceedings 
of 
the 
Main 
Conference and the Shared Task: Semantic Textual Similarity, 241–247. Atlanta, 
Georgia, USA. 
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
23
20 
Grossman, 
E., 
G. 
Lescuyer 
& 
S. 
Polis 
(2014). 
Contexts 
and 
Inferences. 
The 
grammaticalization of the Later Egyptian Allative Future. In E. Grossman, S. 
Polis, A. Stauder & J. Winand (eds.), On Forms and Functions: Studies in Ancient 
Egyptian Grammar. Hamburg: Kai Widmaier Verlag. 
Grossman, E. & I. Noveck (2015). What can historical linguistics and experimental 
pragmatics offer each other? Linguistics Vanguard 1(1). 145–153. 
Grossman, E. & S. Polis (2014). On the pragmatics of subjectification: the emergence 
and modalization of an Allative Future in Ancient Egyptian. Acta Linguistica 
Hafniensia 46(1). 25–63. 
Harris, 
Z. 
S. 
(1954). 
Transfer 
Grammar. 
International 
Journal 
of 
American 
Linguistics 20(4). 259–270. 
Haspelmath, 
M. 
(2004). 
On 
directionality 
in 
language 
change 
with 
particular 
reference to grammaticalization. In O. Fischer, M. Norde & H. Perridon (eds.), 
Up and down the cline: The nature of grammaticalization, 17–44. Amsterdam: 
Benjamins. 
Haspelmath, M. (2008). Creating economical morphosyntactic patterns in language 
change. In J. Good (ed.), Language universals and language change, 185–214. 
Oxford: Oxford University Press. 
Haspelmath, 
M. 
(2010). 
Comparative 
concepts 
and 
descriptive 
categories 
in 
crosslinguistic studies. Language 86(3). 663–687. 
Heine, B. (2002). On the role of context in grammaticalization. In I. Wischer & G. 
Diewald (eds.), New reflections on grammaticalization, 83–101. Amsterdam & 
Philadelphia, PA: John Benjamins. 
Hilpert, M. (2006). Distinctive collexeme analysis and diachrony. Corpus Linguistics 
and Linguistic Theory 2(2). 243–256. 
Hilpert, M. & F. Perek (2015). Meaning change in a petri dish: constructions, semantic 
vector spaces, and motion charts. Linguistics Vanguard 1(1). 339–350. 
Lehrer, A. (1985). The influence of semantic fields on semantic change. In J. Fisiak 
(ed.), Historical Semantic, Historical Word formation, 283–296. Berlin: Mouton. 
Levy, O. & Y. Goldberg (2014). Dependencybased word embeddings. In Proceedings 
of the 52nd Annual Meeting of the Association for Computational Linguistics, vol. 
2, 302–308. Baltimore, Maryland. 
Menner, R. J. (1945). Multiple meaning and change of meaning in English. Language 
21. 59–76. 
Michaelis, L. A. (2004). Type shifting in construction grammar: An integrated 
approach to aspectual coercion. Cognitive linguistics 15(1). 1–68. 
Mikolov, T., K. Chen, G. Corrado & J. Dean (2013a). Efficient Estimation of Word 
Representations in Vector Space. In Proceedings of the International Conference 
on Learning Representations (ICLR). Scottsdale, Arizona, USA. 
Mikolov, T., Q. V. Le & I. Sutskever (2013b). Exploiting Similarities among 
Languages for Machine Translation. ArXiv preprint arXiv:1309.4168. 
Mikolov, T., I. Sutskever, K. Chen, G. Corrado & J. Dean (2013c). Distributed 
Representations of Words and Phrases and their Compositionality. In Advances 
in Neural Information Processing Systems, 3111–3119. 
Mikolov, T., W. Yih & G. Zweig (2013d). Linguistic Regularities in Continuous 
HAIM DUBOSSARSKY, DAPHNA WEINSHALL AND EITAN GROSSMAN
24
21 
Space Word Representations. In Proceedings of the 2013 Conference of the North 
American Chapter of the Association for Computational Linguistics: Human 
Language Technologies, 746–751. 
Newman, J. (2015). Semantic shift. In N. Rimer (ed.), The Routledge Handbook of 
Semantics, 266–280. New York: Routledge. 
Řehůřek, R. & P. Sojka. (2010). Software Framework for Topic Modelling with Large 
Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for 
NLP Frameworks, 45–50. Valletta, Malta: ELRA. 
Rosch, E. H. (1973). Natural categories. Cognitive psychology 4(3). 328–350. 
Sagi, E., S. Kaufmann & B. Clark (2009). Semantic Density Analysis : Comparing 
word meaning across time and phonetic space. In Proceedings of the EACL 2009 
Workshop on GEMS: Geometrical Models of Natural Language Semantics, 104–
111. 
Stefanowitsch, A. & S. T. Gries (2003). Collostructions: Investigating the interaction 
of words and constructions. International Journal of Corpus Linguistics 8(2). 
209–243. 
Sweetser, E. (1990). From etymology to pragmatics: Metaphorical and cultural 
aspects of semantic structure. Cambridge, UK: Cambridge University Press. 
Traugott, E. C. & R. B. Dasher (2002). Regularity in Semantic Change. Cambridge, 
UK: Cambridge University Press. 
Turney, P. D. (2006). Similarity of semantic relations. Computational Linguistics 
32(3). 379–416. 
Haim Dubossarsky 
The Edmond and Lily Safra Center for Brain Sciences (ELSC) 
Hebrew University of Jerusalem 
Jerusalem 91905 
Israel 
email:
haim.dub@gmail.com 
Daphna Weinshall 
School of Computer Science and Engineering 
Hebrew University of Jerusalem 
Jerusalem 91905 
Israel 
email: 
daphna@cs.huji.ac.il 
Eitan Grossman 
Department of Linguistics 
Hebrew University of Jerusalem 
Jerusalem 91905 
Israel 
email: 
eitan.grossman@mail.huji.ac.il
VERBS CHANGE MORE THAN NOUNS: A BOTTOM-UP COMPUTATIONAL APPROACH TO SEMANTIC CHANGE
25
