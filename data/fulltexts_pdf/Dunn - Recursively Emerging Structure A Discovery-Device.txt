Recursively Emerging Structure:
A Discovery-Device CxG
Jonathan Dunn
Illinois Institute of Technology
1
Abstract
Construction Grammar (CxG) views language as a network of constraint-based slot-
filler constructions at different levels of representation and abstraction that emerge
from observed usage. These constructions are self-similar in the sense that the same
processes and forms are posited to repeat themselves at multiple levels of represen-
tation: rather than posit a morphology and a constituent grammar and a dependency
grammar that each exhibit individual behaviors,
CxG posits a single grammatical
unit that generalizes across these levels. Unfortunately, the CxG paradigm has been
unable to formulate an emergence mechanism for constructions.
Although con-
straint
mechanisms have been proposed,
these only apply to existing structures.
This is a critical problem because any exception or unexplained grammatical phe-
nomenon can become an item-specific construction within CxG. Without an emer-
gence mechanism capable of predicting such item-specific representations,
con-
structions are ultimately ad hoc:
produced in a one-off manner to solve specific
descriptive challenges.
This paper describes a discovery-device grammar that re-
cursively learns CxGs from large unannotated corpora,
providing an emergence
mechanism capable of making falsifiable predictions.
The focus of the paper is
on searching through potential constructions and inheriting structure over iterations
across different levels of representation.
1
2
Constructions as Self-Similar Structures
Construction Grammar (CxG) is a grammatical
framework that
views linguistic
structure as a set of meaningful constructions emerging from observed usage (Hop-
per, 1987; Langacker, 1987; Fillmore, 1988; Kay & Fillmore, 1999; Goldberg, et
al.,
2004;
Bybee,
2006;
Goldberg,
2006;
Langacker,
2008).
These constructions
are self-similar in the sense that the same processes and forms are posited to re-
peat themselves at multiple levels of representation and abstraction.
For example,
rather than posit a morphology and a phrase structure grammar and a dependency
grammar that
each exhibit
individual
behaviors,
CxG posits a single mechanism
that generalizes across these different levels of description. For the purposes of this
paper,
this means that a single unsupervised algorithm is used to learn idioms,
a
phrase structure grammar,
and a full construction grammar that combines lexical,
syntactic, and semantic constraints while allowing complex constituents to fill in-
dividual slots.
The algorithm is recursive in the sense that each pass contributes
1
Contact information: jdunn8@iit.edu. Code and data are available at www.jdunn.name
its representations to subsequent passes.
Thus, the final representations have been
built iteratively over multiple passes.
Previous computational work on CxG (Steels, 2004; Bryant, 2004; Chang, et al.,
2012; Steels, 2012) has relied on introspection-based representations that require a
linguist to manually determine the optimum set of constructions.
From a linguis-
tic perspective, these representations are not replicable or falsifiable and cannot be
used to test hypotheses about the mechanisms of emergence. From a computational
perspective,
these representations are not scalable across domains and languages
and are subject to all the problems of purely knowledge-based systems. This paper
reformulates CxG as a discovery-device grammar (Chomsky, 1957) whose gener-
alizations are made at the level of learning.
In computational terms, this takes the
form of an entirely unsupervised algorithm for learning nested,
self-similar con-
structional representations.
Three aspects of the CxG paradigm are especially im-
portant here:
First, CxG views grammar as self-similar in the sense that the same grammati-
cal system describes all levels of linguistic structure. The goal of CxG is to produce
a single structural description capable of capturing lexical, morphological, and syn-
tactic generalizations.
Further, each of these structures should have the same sort
of semantics;
for example,
distributional
representations of each level
should be
possible using more or less the same methods.
Second, CxG views grammar as a description of language at multiple levels of
representation and abstraction:
a construction can contain lexical constraints (i.e.,
fixed idioms), can contain syntactic constraints (i.e., a slot filled by any constituent
of a given syntactic type),
and can contain semantic constraints (i.e.,
a slot filled
by any item from a given semantic domain).
On the one hand,
this follows from
the self-similar nature of linguistic structure.
On the other hand, CxG argues that
individuals are able to store and use different constructions that overlap: “John gave
Peter a hand” is a specific form of the more general ditransitive “John gave Peter an
apple.” CxG argues that, in this case, both representations are stored in a speaker’s
grammar. This is important because a CxG has no exceptions or irregular forms but
only regularities at different levels of abstraction.
Third,
CxGs view grammar as a meaningful
symbolic representation of lan-
guage: constructions are morphosyntactic forms that are linked with specific mean-
ings just as lexical items are linked with specific meanings.
Traditional grammars
separate syntax and semantics by viewing linguistic form as independent of mean-
ing; CxG posits a single lexical-grammatical continuum. This is important because
it maintains the idea that lexical items and syntactic structures are fundamentally
instances of the same self-similar structure.
These properties of
CxGs make them robust
and elegant
representations of
grammar.
Unfortunately,
these properties also mean that CxGs have struggled to
show learnability, reproducibility, and falsifiability.
First, we know that millions of
children learn similar grammars while being exposed to very different observations
of language use; the learnability problem is that a (discovery-device) grammar must
be able to produce relatively consistent representations regardless of the particular
data observed.
CxG has struggled to show learnability because no mechanism has
been put forward to move from observed usage to generalized constructions.
Fur-
ther,
the hypothesis space of potential constructions is much larger than for other
formalisms (i.e.,
phrase structure grammars) because CxGs use lexical,
syntactic,
and semantic representations.
Second, CxGs have struggled with standards for reproducibility and falsifiabil-
ity because,
in practice,
any exception or unexplained phenomenon becomes an
item-specific construction.
This is because CxG’s robust representations are used
to fill descriptive gaps. The problem is that these representations can be ad hoc and
produced in a one-off manner to solve any given descriptive challenge.
Can these
item-specific constructions be discovered by reproducible methods? Can a theory
of CxGs provide a single falsifiable set of constructions for a given corpus? In com-
putational terms, the problem is that no gold-standard CxG is possible, even given
unlimited resources,
which means that traditional supervised evaluation methods
cannot be used to measure grammar quality.
This paper formalizes a recursive CxG induction algorithm that makes gener-
alizations about the relationship between observed utterances and emergent struc-
tures.
The core of the algorithm is a Minimum Description Length (MDL) evalua-
tion of competing grammars using a tabu search with Association-Based Grammar
Sampling to reduce the search space to only those constructions likely to compress
the encoding of the dataset.
The focus of this paper is the sampling and search
problems, both of which are made more difficult by the large hypothesis space of
CxGs (for more information about measuring grammar quality,
see Dunn,
2018).
Another line of work considers how these learned grammars can be used to provide
a dynamic feature space for dialectometry (Dunn, Forthcoming).
3
Representations and Notations
This section introduces the symbolic notation used to represent CxGs.
The algo-
rithm recognizes three distinct types of representation as atomic units in its descrip-
tions: Lexical representation consists of tokenized word-forms (in lowercase); Syn-
tactic representation consists of part-of-speech categories (defined using the Univer-
sal POS tagset, Petrov, et al., 2012, and computed using RDRPosTagger, Nguyen,
et al., 2016); Semantic representation consists of clusters of distributionally similar
words meant to represent semantic domains and computed using GenSim’s imple-
mentation of word2vec (Rehurek & Sojka, 2010, using skip-grams with 500 dimen-
sions; these are segmented into domains using k-means clustering, where
k = 100
).
The idea behind these three types of representation is that a particular slot in a con-
struction can be defined or constrained at the lexical, syntactic, or semantic level.
These representations form the basic alphabet of the algorithm; with the exception
of the part-of-speech tagger, the pipeline is fully unsupervised.
Constituents are taken as complex structures that behave as a single unit.
In
other words, constituents are sequences of units whose internal structure is ignored
by higher-level representations.
The two main questions for constituents are (i) the
location of their boundaries and (ii) the type of their governing heads. The heads are
especially important here because, in the final CxG grammar, complex constituents
can be treated as individual units taking on the syntactic and semantic properties
of their head.
Previous work has used supervised dependency parsers to identify
constituents as part of CxG learning (i.e., Dunn, 2017). This paper takes the claims
of CxG a step further and uses the same algorithm to identify phrase structure rules
that it uses to identify idioms and constructions. This is important given the posited
self-similar nature of grammatical structures.
In the notation used here constructions are sequences containing a certain num-
ber of slots, as in (1a) with four individual slots. Each construction is surrounded in
brackets and each slot within a construction is separated by a dash (“—”). Each slot
in a construction is represented or defined by constraints that govern which units can
occupy that slot.
Lexical constraints are indicated using single quotes (e.g., ‘gave’
in 1b).
Syntactic constraints are indicated using part-of-speech tags in uppercase
(e.g.,
NOUN
in 1b).
Semantic constraints are indicated within parentheses with the
identifier for the semantic domain (e.g., (animate) in 1b). Thus, the construction in
(1b) describes the utterance in (1c) but not the utterance in (1e); the construction in
(1d) describes the utterances in both (1c) and (1e).
(1a) [
SLOT
1 —
SLOT
2 —
SLOT
3 —
SLOT
4]
(1b) [
NOUN
— ‘gave’ — (animate) — ‘a hand’]
(1c) Bill gave Peter a hand.
(1d) [
NOUN
— (transfer) — (animate) —
NOUN
]
(1e) Bill sent Peter a package.
In this paper, the term construction refers to the grammatical description (e.g.,
1b) and the term construct refers to a member of the set of utterances which that
construction represents (e.g.,
1c).
For a given grammar,
the set of constructions
is closed but
the set
of constructs is open.
A construct
or utterance can be rep-
resented by multiple constructions:
representations like (1b) that are more item-
specific alongside representations like (1d) that are more schematic.
This leads to
relationships between constructions: (i) an inheritance hierarchy in which (1b) is a
child of (1d) and (ii) nesting in which constructions can fill slots in other construc-
tions.
Nested constructions here result from the recursive nature of the algorithms:
idioms occupy single slots in phrase structure rules which occupy single slots in
full constructions. Currently, however, full constructions are not allowed to occupy
slots in other constructions.
4
Recursive CxG Learning: Accumulating Structure
A recursive algorithm is used to learn CxGs across levels of abstraction:
each it-
eration uses the same algorithm and the same slot-filler principles but operates on
different types of representation.
The first pass operates on lexical representations,
learning idioms.
Here, idioms are sequences of lexical items that have been fused
together so that their internal structure can be ignored; for example, “could be” and
“will be” are identified as single units when the algorithm is applied to English so
that later passes view these sequences as one lexical item with one syntactic type.
The second pass operates on syntactic representations, treating previously iden-
tified idioms as single units. Here, syntactic constructions are phrase structure rules;
for example, when applied to English the sequence [
VERB
—
NOUN
] is identified
as a phrase structure rule, with
VERB
as its head.
In later passes, these sequences
are converted into constituents that can be treated as a single unit.
Thus, complex
constituents from phrase structure rules can occupy single slots in constructions.
The third pass operates on all three levels of representation, treating both idioms
and constituents as individual units. For convenience we refer to these iterations as
CxG
LEX
,
CxG
SY N
, and
CxG
F ULL
respectively, as shown in Figure 1.
The algo-
rithm accumulates structure in the sense that constructions output from a previous
pass become atomic units that can fill individual slots in the current pass.
CxG
SYN
(Phrase Structure) 
CxG
LEX
(Idioms) 
CxG
FULL
(Constructions) 
Figure 1: Discovery-Device CxG Overview
This recursive learning algorithm depends on the central insight of MDL (Rissa-
nen, 1978, 1986; Grünwald & Rissanen, 2007): a grammar is a method for encoding
observed linguistic utterances and the learner is searching for the smallest adequate
encoding method (c.f.,
Goldsmith,
2001,
2006).
Explanation here is a matter of
prediction: can the grammar produce the utterances observed in held-out test-sets?
This is a whole-grammar approach to the productivity of constructions and MDL
provides a way of quantifying productivity across large numbers of constructions.
While the MDL metric can be used as an objective function for selecting one
grammar over another (c.f.,
Dunn,
2018),
the algorithm faces three challenges in
implementing this principle:
(i) to define the search space of potential grammars;
(ii) to sample from and explore this very large search space in an efficient manner;
(iii) to determine whether the grammar which minimizes the MDL metric actually
provides meaningful generalizations. This paper focuses on the first two problems.
At its core,
CxG learning is a combinatorial optimization problem in which a
large but discrete number of possible representations (i.e., constructions) need to be
reduced to a single grammar (i.e., one set of constructions). The algorithm performs
a tabu search (Glover, 1989, 1990a) using Association-Based Grammar Sampling
(c.f., Section 6) to avoid searching the full space of potential grammars by sampling
only those which contain sequences sufficiently probable to aid in compressing
the dataset.
This becomes a two-part
optimization problem of first
learning the
best sampling parameters for reducing the search space and then finding the best
grammar within that search space.
Rather than operating directly on the search space of potential grammars con-
taining potential constructions,
Association-Based Grammar Sampling filters this
space by only making grammars that do not contain improbable sequences avail-
able to the tabu search algorithm.
This is done using association measures:
the
tabu search is across the parameters of the sampling algorithm (types of associa-
tion measures and their thresholds) rather than directly on all potential candidates.
Within the tabu search,
the MDL metric is used to evaluate sampled grammars.
Once the search space has been reduced using the optimum sampling parameters, a
direct tabu search explores this much-reduced space to find the optimum grammar.
To avoid reaching local optima, the search is restarted
n
times, each with a unique
test set for calculating the size of the data as encoded by each potential grammar.
The final evaluation, on a unique held-out test set, generalizes the MDL metric by
finding the grammar’s rate of compression over an unencoded baseline. The goal is
to implement an MDL metric for grammar quality without exploring those parts of
the (very large) search space that are unlikely to improve compression while at the
same time avoiding local optima.
5
Defining the Space of Potential Grammars
For each sub-type of grammar (e.g.,
CxG
LEX
) the algorithm is purely surface-
oriented: any observed sequence of allowed representations is a potential construc-
tion and any set of constructions is a potential grammar.
We use
S
to represent
the space of observed sequences.
S
L
restricts this to sequences with a maximum
length of
L
and S
L
R
further restricts this to sequences of representation type
R
. Thus,
S
L=3
R=Lex
represents the space of all observed sequences with maximum length 3 (i.e.,
unigrams,
bigrams,
and trigrams) with a lexical representation (i.e.,
word-forms).
We use
G
k
∈
S
L
R
to represent some potential grammar
G
k
drawn from the space of
potential constructions in
S
; given even a moderately sized dataset,
S
is very large
but remains finite and countable.
(2) [
NOUN
— ‘gave’ — (animate) — ‘a break’]
The number of sequences, and thus the number of potential constructions, grows
quickly as both the sequence window size,
L
,
and the types of atomic units con-
sidered,
R
,
increase.
This is especially true across representation types (i.e.,
for
CxG
F ULL
) because sequences can contain mixed representations. For example, the
ditransitive in (2) contains three representation types; in order to discover this con-
struction, the search space must include S
L=4
R=Lex,Syn,Sem
.
Such a search space contains
a very large number of potential constructions. Many of the potential constructions
in this search space, however, are unlikely to aid in encoding the dataset.
6
Sampling the Space of Potential Grammars
Given the size of the search space, a brute force search across all potential grammars
G
k
∈
S
L
R
to find the one which minimizes the MDL metric is not feasible.
We can
reduce the search space using the observation that the most frequent or probable
sequences will be most useful for encoding the data.
In other words,
a potential
construction that occurs only 10 times in a corpus of a million words is not likely
to significantly compress the description of that corpus.
Instead of exploring the
entire search space, then, we use the probability of potential constructions to filter
out those potential grammars that contain improbable constructions.
Put another
way,
the MDL metric requires that
a construction used to encode one utterance
(increasing model complexity) is capable of encoding many utterances (reducing
the final
encoding size).
We need a measure of the relative usefulness of each
potential construction across all observed utterances.
The starting point
for such a measure is frequency:
infrequent
constructions
by definition will be able to encode only a few utterances.
At the same time, fre-
quency alone is not sufficient because we are interested in the boundaries between
constructions:
sequences with highly frequent units will dominate in a frequency-
based metric regardless of the accuracy of their borders.
For example, consider the
noun phrase in (3a), whose frequency is in part a consequence of the high frequency
of “the.” Frequent units will also make sequences like (3b) frequent, but frequency
alone may not distinguish between (3a) and (3b).
(3a) “the house”
(3b) “build the”
Instead of pure frequency, then, we use association measures in order to jointly
select sequences for border quality and overall probability. Specifically, we use the
∆P
measure (Ellis, 2007; Gries, 2013):
P (X
P
|Y
P
) − P (X
A
|Y
P
)
For a sequence,
XY
, this represents the probability of
X
and
Y
occurring to-
gether reduced by the baseline probability of
Y
occurring without
X
.
This is a
directional association measure which represents the probability of the sequence
while controlling for the presence of highly frequent units.
The goal is to sample
from those sequences that are more likely to occur together than to occur indepen-
dently; this prevents highly frequent units from promoting sequences with improba-
ble borders. At the same time, we want to find the grammar which best compresses
the observed language.
Sequences of relatively rare units can have high associa-
tion strength but low impact on compression.
Thus, we also provide the sampler a
measure in which the
∆P
of sequence
XY
is weighted by the frequency of
XY
.
Unfortunately, the base
∆P
only measures association for sequences of length
two (e.g.,
XY
).
We need a series of multi-unit directional association measures
based on the
∆P
in order to provide equivalent
representations of sequences of
more than two units.
We draw from recent work (Dunn, 2017) and use a series of
multi-unit directional association measures based on the
∆P
. Maintaining indepen-
dent measures for each direction is important here (instead of using, for example,
pointwise mutual information) because we do not assume in advance which direc-
tions are most relevant for the language we are observing.
(4a) [A — B — C — D — E — F — G]
(4b) [B — C — D]
(4c) [C — D — E — F — G]
(4d) [A — B]
Each measure is used to describe sequences as in (4a), where each letter indi-
cates a unit.
Each sequence has two end-points;
here the left end-point is A and
the right end-point is G. Each sequence is considered in isolation. This is important
because the sequence in (4a) contains many sub-sequences which are themselves
possibly associated, as in (4b) through (4d), and are also present in the search space.
Because each sequence is treated independently, rather than being iteratively pro-
cessed (c.f.,
Jelinek,
1990),
the measures need to be able to indicate when a sub-
sequence is more associated than the entire sequence. A sequence is non-optimal if
its sub-sequences are more associated than the sequence as a whole.
The pairwise
∆P
measure is calculated as follows:
Let
X
be a unit
of any
representation and
Y
be any other unit of any representation, so that
X
A
indicates
that
unit
X
is absent
and
X
P
indicates that
unit
X
is present.
The left-to-right
(LR) measure is
p(X
P
|Y
P
) − p(X
P
|Y
A
)
and right-to-left (RL) is
p(Y
P
|X
P
) −
p(Y
P
|X
A
)
.
This is the conditional probability of association in the given direction
adjusted by the conditional probability with only part of the sequence (i.e.,
of
Y
occurring without
X
).
In its original formulation,
the
∆P
was meant to indicate
the probability of an outcome given a cue,
p(X
P
|Y
P
)
, reduced by the probability
of the outcome in the absence of the cue,
p(X
P
|Y
A
)
.
The direction of association
measured is notated using a sub-script, as in
∆P
LR
and
∆P
RL
.
Given the multi-unit association measures from Dunn (2017),
we have a vec-
tor of 30 association values representing each potential construction.
We expect
that frequency and association indicate that a potential construction is worth con-
sidering but we do not know,
for any given language or dataset,
which measures
and which thresholds will be most indicative of construction quality.
Association-
Based Grammar Sampling operates on top of this vector representation to provide
candidate grammars to the larger tabu search algorithm. This is important because,
given the size of the overall search space and the fact that improbable sequences are
unlikely to contribute to the compression of the data set, we do not want to search
across all possible grammars.
Given an association measure listed and a threshold for that measure used to
determine which potential constructions do not need to be considered as part of the
search space,
θm
, we use C
A
θ
to indicate the set of potential constructions satisfying
all thresholds in
θ
for all parameters in
A
.
We can then use the notation C
A
θ
S
L
R
to
indicate the potential grammar sampled from the search space given measures
Ak
and thresholds
θm
.
For each measure
Ak
there are two allowed states: AND, OR.
For the first state, AND, all measures must be satisfied for an individual construction
to be sampled.
The second state, OR, enables a single measure to overrule all the
others and allow an individual construction to be sampled.
We can think about this as a query across association measures with two terms:
in the first term, all thresholds must be satisfied; in the second term, the satisfaction
of any threshold is sufficient for a given construction to be sampled.
The purpose
of this dual query is to allow the tabu search algorithm the flexibility to use a single
measure to break out of a local optima: without the OR state, a single conservative
Ak
could dominate the local search space.
On its own, this sampling is a heuristic
for estimating which potential constructions should be considered.
When coupled
with the MDL-based learning algorithm, it provides a theoretically-motivated guide
for the selection of the optimum grammar.
7
Searching for the Optimum Grammar
The search algorithm has three components:
(i) randomly initializing the starting
state, (ii) an indirect tabu search for moving toward the optimum grammar sampler,
and (iii) a direct
tabu search across constructions selected by the grammar sam-
pler.
Each possible C
A
θ
S
L
R
produces a single sampled grammar; thus, the search and
estimation tasks are focused on determining the optimum measures,
Ak
, and thresh-
olds,
θm
, for sampling grammars.
Once the grammar sampler has been optimized,
the search space is reduced to those constructions present in that sampled grammar.
The final stage searches within this reduced space using a direct tabu search; this
search is only possible once the main tabu search and the grammar sampler have
reduced the very large initial search space to a more manageable space of potential
constructions.
Repeated over 
n 
restarts
Random 
Starting Point 
Tabu Search 
(Sampling Parameters)
Tabu Search 
(Across Constructions)
Figure 2: Overview of the Search Algorithm
The first tabu search takes a randomly initialized starting state and searches for
improved grammars by exploring different sampling parameters. The essential idea
of tabu search is (i) to define the set of possible moves from the current grammar
state to a new grammar state and (ii) to combine tabu restrictions and aspiration
criteria to allow the algorithm to search promising areas of the overall search space
that
are not
directly reachable from the current
state.
We divide the parameter
space between the maximum and minimum observed values into
n
discrete values
for each
Ak
. This provides a finite set of possible moves from any given state.
For each turn, the algorithm iterates across all measures in order to determine
the best available move; this is shown in Figure 3.
“Best” here is defined using the
MDL-metric: the best move is that which provides the smallest MDL-metric of all
possible moves.
“Available” here is defined as a move that is either (i) not on the
tabu list or (ii) satisfies an aspiration criteria that overrules the tabu list.
The tabu list is a short-term memory item that contains the last
n
moves.
Each
move is represented using the association measures in
A
that have been changed.
For practical reasons,
n
is set at 7 (c.f., Glover, 1990b); this means that for any given
turn the best move cannot involve a sampling parameter that has been changed over
the last 7 turns.
This prevents the algorithm from cycling between local optima.
The aspiration criteria used is that the grammar produced by a move is not only
the best available grammar but also the best observed grammar: a new global min-
imization of the MDL metric.
Thus, the tabu against changing a recently changed
Ak
can be overruled if that change creates a new best grammar. The use of such an
aspiration criteria makes intuitive sense: the tabu search is designed to prevent cy-
cling between previously visited states, but a grammar which reaches a new global
minimization of the MDL metric has by definition not previously been visited.
Three types of moves are available at each turn: (1) The feature in question can
be turned OFF and removed from the current grammar; this allows the tabu search
Generate Potential Moves 
From Current State 
Check 
Stopping Criteria 
Update Tabu List and 
Aspiration Criteria 
Make 
Best 
Available 
Move 
Evaluate Potential Moves 
With MDL Metric 
Figure 3: Tabu Search Across Sampling Parameters
to eliminate sampling parameters that reduce grammar quality.
(2) The feature in
question, in state AND, can be grouped with
n
randomly sampled changes to other
features; this allows the tabu search to explore states similar to the current grammar.
(3) The feature in question can be turned to OR and allowed to overrule all other
features; this allows the tabu search to move toward better but more distant states.
In addition to recently visited parameter changes, the tabu list includes all changes
to an OR state that do not fulfill aspiration criteria.
Because OR features overrule
all other features,
a change to OR that does not reach a new global optimum will
not
contribute to a future global
optimum.
Each turn iterates over each feature
(the 30 association values) and evaluates each of these types of moves as follows:
one move with the feature turned OFF but the grammar otherwise unchanged; two
OR moves above and two below the feature’s current threshold, serving as escape
hatches; and
n
randomly sampled moves that include the current feature and
1. . . k
other features.
There are two parameters here:
n
the number of moves per feature
and
k
the maximum number of features that can be changed per move.
The stopping criteria is that a new best grammar has not been observed for
n
turns.
Here
n
is set at 14, or twice the size of the tabu list.
This stopping criteria
is an intermediate memory item that monitors the general direction of the search.
The intuition is that, if a new optimum grammar has not been reached within two
complete cycles of the short-term tabu list, such a grammar is unlikely to exist. It is
important to keep in mind that each turn evaluates a wide range of possible moves.
This means that a large number of potential grammars are evaluated in determining
each move.
Thus,
given the size of the space reachable from any given state and
the number of states visited during the tabu search, it is unlikely that a significantly
more optimal grammar exists after 14 moves have failed to reach it.
The main tabu search across sampling parameters reduces the overall
search
space to a size that can be navigated by a final direct tabu search. This search has a
different set of moves available at each turn: rather than choosing constructions by
their association values, constructions are chosen directly. Each potential construc-
tion has two settings in respect to the grammar: IN and OUT. The same aspiration
criteria and stopping conditions are used as above.
The tabu list holds the set of
constructions that were changed within the last 14 moves and only allows them to
be changed if a new optimum grammar is reached; the expanded size of the tabu list
is a result of the larger range of possible moves available to the direct tabu search.
0
5
10
15
20
25
English
Spanish
French
Italian
German
Sampling Search (CxG syn)
Sampling Search (CxG full)
Figure 4: Iterations Required for Sampling Search
The essential
difference between the direct
and indirect
tabu searches is the
moves that are available to each turn: here each turn evaluates
n
moves that reverse
1. . . k
randomly selected constructions.
To reverse means to turn ON features to
OFF and vice-versa.
The direct tabu search is simpler,
but is only made possible
by the reduced search space provided by the indirect
tabu search over sampling
parameters.
0
250
500
750
1000
1250
1500
1750
2000
English
Spanish
French
Italian
German
Direct Search (CxG syn)
Direct Search (CxG full)
Figure 5: Iterations Required for Direct Tabu Search
The evaluation uses web-crawled corpora from the WaC (Baroni, et al., 2009)
and Aranea (Benko, 2014) projects:
English (ukWac), German (Aranea), Spanish
(Aranea),
Italian (itWac),
and French (frWac).
The number of iterations required
for the search algorithm to reach the best (discovered) state is shown in Figures
4 (for sampling) and 5 (for the direct search).
The two passes shown,
CxG
SY N
and
CxG
F ULL
,
have very different hypothesis spaces but the sampling algorithm
reaches its best state in under 20 iterations for each language in both cases. For both
searches,
approximately 500 states are evaluated per move.
In the direct search,
however, the search algorithm takes under 200 turns per language for
CxG
SY N
but
rises to roughly 2,000 turns per language for
CxG
F ULL
. Thus, the size of the search
space directly influences this search in a way it does not influence the search for the
optimum sampling parameters.
0
0.2
0.4
0.6
0.8
English
Spanish
French
Italian
German
Sampling Search
Direct Search
Figure 6: Reductions in Hypothesis Space Per Search Type for
CxG
F ULL
At the same time, Figure 6 shows the percentage of the hypothesize space that
is discarded by each search for
CxG
F ULL
.
This shows that the sampling search is
able to remove a large portion of the candidates quite quickly while the direct tabu
search takes longer to ultimately remove a smaller portion of the space.
This also
shows that only a small number of candidates are identified as constructions by the
algorithm, with most discarded by the sampling algorithm.
8
Typing Constructions
Constructions from both
CxG
LEX
and
CxG
SY N
are passed on to future iterations
as atomic units that take on the properties of their head. In order for this to happen,
these constructions must be assigned to a head category.
All constructions are se-
quences as in (5) below. The main learning algorithm in
CxG
SY N
identifies phrase
structure rules from the very large search space of potential syntactic sequences and
the algorithm described here assigns each discovered sequence to a head.
We start
by assuming that the head of both idioms and constituents occupies one of its two
end-points: here, either A or E.
(5) [A — B — C — D — E]
Because we are using the Universal POS tagset (v.
1; Petrov, et al., 2012), we
can make assumptions across languages based on the type of units we are interested
in.
Some types of units cannot head phrases; these are listed by tag in Table 1.
For
example,
we are interested in verb phrases as complex constituents within larger
constructions but are not interested in adverb phrases. A sequence must have a head
unit occupying one of its end-points in order to qualify as a phrase structure rule.
In most cases, this simple procedure succeeds in assigning phrase structure rules to
head categories. In some cases, however, both left and right end-points can function
as heads.
In these cases the hierarchy in Table 1 is used to determine dominance:
for example, verbs take precedence over nouns and subordinating conjunctions take
precedence over verbs.
Thus, if a sequence contains a noun in one end-point and a
verb in the other, it is categorized as a verb phrase. For idioms, a further constraint is
that the same head type must occupy both end-points. This simple typing algorithm
is required to allow the algorithm to recursively incorporate smaller structures into
larger structures.
Tag
Unit Name
Head Status
Head Rank
sconj
Subordinating Conj.
Yes
1
verb
Verb
Yes
2
adp
Adposition
Yes
3
propn
Proper Noun
Yes
4
noun
Noun
Yes
5
pron
Pronoun
Yes
6
conj
Coordinating Conj.
No
n/a
num
Numeral
No
n/a
det
Determiner
No
n/a
part
Particle
No
n/a
intj
Interjection
No
n/a
aux
Auxiliary Verb
No
n/a
adv
Adverb
No
n/a
adj
Adjective
No
n/a
Table 1: Unit Types
9
Stability Over Sub-sets
We evaluate the stability of
CxG
F ULL
for English against held-out data from the
ukWac corpus using the MDL metric (c.f., Dunn, 2018).
The dataset here contains
ten subsets of the corpus with 500k sentences each; this allows us to observe vari-
ations in grammar quality across subsets.
The first question is quality:
does the
grammar provide generalizations above the baseline (the baseline is 0 compression;
this measure is not relative to gold-standard annotations).
The second question is
stability:
does the grammar describe each mutually-exclusive subset of the corpus
equally well? As shown in Figure 7, grammar quality is both significant and stable
across different test sets.
10
Discussion
A major problem for the usage-based CxG paradigm is that
it
has been unable
to model the emergence of constructions from observed language use.
While the
paradigm allows for item-specific and irregular representations,
it has previously
0
0.05
0.1
0.15
0.2
0.25
0.3
1
2
3
4
5
6
7
8
9
10
MDL Compression, Full Grammar
Figure 7:
CxG
F ULL
Quality Over Subsets of ukWac
been unable to make predictions about
the relationship between observed usage
and irregular representations.
This paper has approached this problem using a
discovery-device CxG,
specifically focusing on the problems of defining the hy-
pothesis space of potential constructions and searching through these potential con-
structions in order to find a single grammar. While much work remains to be done,
these results suggest that a discovery-device CxG is able to achieve stable and sig-
nificant descriptive adequacy across different test corpora.
This, in turn, suggests a
way forward in resolving these difficulties faced by the usage-based CxG paradigm.
Resources.
The code and models for this work and related work are available at jdunn.name
and github.com/jonathandunn/c2xg
Acknowledgements.
This research was supported in part by an appointment to the Visiting
Scientist Fellowship at the National Geospatial-Intelligence Agency administered by the Oak Ridge
Institute for Science and Education through an interagency agreement between the U.S. Department
of Energy and NGA.
The views expressed in this presentation are the author’s and do not imply
endorsement by the DoD or the NGA.
References
Baroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E. 2009. The WaCky Wide Web: A Collec-
tion of Very Large Linguistically Processed Web-crawled Corpora.
Language Resources and
Evaluation, 43: 209-226.
Benko, V. 2014.
Aranea:
Yet Another Family of (Comparable) Web Corpora.
In Proceedings of
Text, Speech and Dialogue. 17th International Conference, TSD 2014. Springer International
Publishing. 257-264.
Bryant, J. 2004.
Scalable construction-based parsing and semantic analysis.
In Proceedings of the
Workshop on Scalable Natural Language Understanding (HLT-NAACL): 33-40.
Bybee,
J.
2006.
From usage to grammar:
The mind’s response to repetition.
Language,
82(4):
711-733.
Chang, N.; De Beule, J.; & Micelli, V. 2012.
Computational construction grammar:
Comparing
ECG and FCG.
In Steels,
L.
(ed.),
Computational Issues in Fluid Construction Grammar.
Berlin: Springer. 259-288.
Chomsky, N. 1957.
Syntactic Structures.
The Hague: Mouton de Gruyter.
Dunn, J. 2017.
Computational Learning of Construction Grammars.
Language & Cognition, 9(2):
254-292.
Dunn, J. 2018.
Modeling the Complexity and Descriptive Adequacy of Construction Grammars.
In Proceedings of the Society for Computation in Linguistics.
Dunn, J. Forthcoming.
Finding Variants for Construction-Based Dialectometry:
A Corpus-Based
Approach to Regional CxGs.
Cognitive Linguistics.
Ellis,
N.
2007.
Language Acquisition as Rational Contingency Learning.
Applied Linguistics,
27(1): 1-24.
Fillmore, C. 1988. The Mechanisms of ’Construction Grammar.’ In Proceedings of the Fourteenth
Annual Meeting of the Berkeley Linguistics Society. 35-55.
Glover, F. 1989.
Tabu Search, Part 1.
ORSA Journal on Computing, 1(3): 190-206.
Glover. F. 1990a.
Tabu Search, Part 2.
ORSA Journal on Computing, 2(1): 4-32.
Glover, F. 1990b.
Tabu Search: A Tutorial.
Interfaces, 20(4): 74-94.
Goldberg,
A.
2006.
Constructions at work:
The nature of generalization in language.
Oxford:
Oxford University Press.
Goldberg, A.; Casenhiser, D.; & Sethuraman, N. 2004.
Learning argument structure generaliza-
tions.
Cognitive Linguistics, 15(3): 289-316.
Goldsmith, J. 2001.
Unsupervised learning of the morphology of a natural language.
Computa-
tional Linguistics, 27(2): 153-198.
Goldsmith, J. 2006. An algorithm for the unsupervised learning of morphology. Natural Language
Engineering, 12(4): 353-371.
Gries, S. 2013.
50-something years of work on collocations:
What is or should be next.
Interna-
tional Journal of Corpus Linguistics, 18(1): 137-165.
Grünwald, P. & Rissanen, J. 2007.
The Minimum Description Length Principle.
Cambridge, MA:
The MIT Press.
Hopper, P. 1987.
Emergent grammar.
In Proceedings of the 13th Annual Meeting of the Berkeley
Linguistics Society, 139-157.
Jelinek, F. 1990.
Self-organizing language modeling for speech recognition.
In A. Waibel & K.
Lee (eds.), Readings in Speech Recognition. San Mateo, CA: Morgan Kaufmann. 450-506.
Kay, P. & Fillmore, C. 1999. Grammatical constructions and linguistic generalizations: The What’s
X Doing Y? construction.
Language, 75(1): 1-33.
Langacker, R. 1987.
Foundations of Cognitive Grammar.
Stanford: Stanford University Press.
Langacker, R. 2008.
Cognitive Grammar: A basic introduction.
Oxford: Oxford University Press.
Nguyen,
Dat Quoc;
Nguyen,
Dai Quoc;
Pham,
Dang Duc;
& Pham,
Son Bao 2016.
A Robust
Transformation-Based Learning Approach Using Ripple Down Rules for Part-Of-Speech Tag-
ging.
AI Communications, 29(3): 409-422.
Petrov, S.; Das, D.; & McDonald, R. 2012.
A Universal Part-of-Speech Tagset.
In Proceedings of
the Eight International Conference on Language Resources and Evaluation (LREC’12).
Rehurek, R. & Sojka, P. 2010.
Software Framework for Topic Modelling with Large Corpora.
In
Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks.
Rissanen, J. 1978.
Modeling by the shortest data description.
Automatica, 14: 465-471.
Rissanen, J. 1986.
Stochastic complexity and modeling.
Annals of Statistics, 14: 1,080-1,100.
Steels, L. 2004.
Constructivist development of grounded construction grammar.
In Proceedings of
the 42nd Meeting of the Association for Computational Linguistics: 9-16.
Steels, L. 2012.
Design methods for fluid construction grammar.
In Steels, L. (ed), Computational
Issues in Fluid Construction Grammar. Berlin: Springer. 3-36.
