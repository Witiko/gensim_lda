Application of Neural Networks in
Class-Based Language Models for
Automatic Speech Recognition
Jan-Pieter D’Anvers
Thesis voorgedragen tot het behalen
van de graad van Master of Science
in de ingenieurswetenschappen:
elektrotechniek, optie Ingebedde
systemen en multimedia
Promotor:
Prof. dr. ir. P. Wambacq
Academiejaar 2014 – 2015
Master of Science in de ingenieurswetenschappen: elektrotechniek
Application of Neural Networks in
Class-Based Language Models for
Automatic Speech Recognition
Jan-Pieter D’Anvers
Thesis voorgedragen tot het behalen
van de graad van Master of Science
in de ingenieurswetenschappen:
elektrotechniek, optie Ingebedde
systemen en multimedia
Promotor:
Prof. dr. ir. P. Wambacq
Assessoren:
Prof. dr. ir. S. Moens
Prof. dr. ir. H. Van hamme
Begeleider:
J. Pelemans
Academiejaar 2014 – 2015
c
Copyright KU Leuven
Without written permission of the thesis supervisor and the author it is forbidden
to reproduce or adapt in any form or by any means any part of this publication.
Requests for obtaining the right to reproduce or utilize parts of this publication should
be addressed to ESAT,
Kasteelpark Arenberg 10 postbus 2440,
B-3001 Heverlee,
+32-16-321130 or by email
info@esat.kuleuven.be
.
A written permission of the thesis supervisor is also required to use the methods,
products, schematics and programs described in this work for industrial or commercial
use, and for submitting this publication in scientific contests.
Zonder voorafgaande schriftelijke toestemming van zowel
de promotor als de au-
teur is overnemen, kopiëren, gebruiken of realiseren van deze uitgave of gedeelten
ervan verboden.
Voor aanvragen tot of
informatie i.v.m.
het overnemen en/of
gebruik en/of realisatie van gedeelten uit deze publicatie, wend u tot ESAT, Kas-
teelpark Arenberg 10 postbus 2440, B-3001 Heverlee, +32-16-321130 of via e-mail
info@esat.kuleuven.be
.
Voorafgaande schriftelijke toestemming van de promotor is eveneens vereist voor het
aanwenden van de in deze masterproef beschreven (originele) methoden, producten,
schakelingen en programma’s voor industrieel of commercieel nut en voor de inzending
van deze publicatie ter deelname aan wetenschappelijke prijzen of wedstrijden.
Preface
Deze masterproef zou niet tot stand gekomen zijn zonder de hulp van verscheidene
mensen.
Daarom zou ik graag iedereen bedanken die mij het voorbije jaar geholpen
en gesteund hebben bij het maken van dit werk.
Allereerste zou ik graag mijn promotor
professor
Patrick Wambacq bedanken.
Hij
stond vanaf
de eerste bijeenkomst klaar met een vriendelijk onthaal
en zeer
gewaardeerde hulp.
Daarnaast zou ik graag mijn begeleider Joris Pelemans bedanken voor zijn interesse,
zijn vriendelijkheid en zijn ongelofelijke enthousiasme.
Hij was steeds aanwezig om
technische hulp te geven, om te brainstormen over nieuwe ideëen en om deze tekst
kristisch te na lezen.
Mijn dank gaat ook uit naar de andere doctorandi
in de SPEECH groep,
voor
de hartelijke ontvangst en de leuke gesprekken op weg naar het koffieapparaat.
Voorts wil
ik graag de assessoren professor Sien Moens en professor Hugo van
Hamme bedanken voor het lezen van deze tekst en het bijwonen van de presentatie.
Ik zou ook graag een paar zeer goede vrienden bedanken voor de zeer geappre-
cieerde hulp:
Toon Borghgraef en Sander Smets voor het rigoureus nalezen van deze
tekst, Alexander Vanhulsel voor de linguïstische hulp en Paul Lacko voor het Engelse
advies en de leuke gesprekken over het maken van een thesis.
Tenslotte zou ik graag mijn ouders en zussen bedanken voor hun geweldige steun en
het leuke jaar dat we achter de rug hebben.
Jan-Pieter D’Anvers
i
Contents
Preface
i
Abstract
iv
Samenvatting
v
List of Figures and Tables
ix
List of Abbreviations and Symbols
xi
1
Introduction
1
1.1
Overview .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
1.2
Research goal
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
2
N-gram Language Models
7
2.1
N-grams
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
2.2
Data sparsity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
2.3
Class-based models .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
2.4
Evaluation of language models
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
2.5
Conclusion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
3
Neural Network-based Word Similarity
13
3.1
Neural networks
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
3.2
Skip-gram neural network .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
3.3
Conclusion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
4
Dependency-based model
25
4.1
Overview .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
25
4.2
Implementation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
4.3
Data sparsity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
4.4
Conclusion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
5
Word Clustering
29
5.1
Perplexity clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
5.2
Word similarity clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
31
5.3
Part of speech tagging .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
5.4
Uncommon words .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
5.5
Acoustic word similarity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
35
5.6
Conclusion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
6
Evaluation
39
ii
Contents
6.1
Setup
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
39
6.2
Clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
6.3
POS clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
6.4
Uncommon words .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
6.5
Acoustic word similarity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
46
6.6
Dependency-based model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
47
6.7
Final model:
integrating all information sources .
.
.
.
.
.
.
.
.
.
.
.
48
6.8
Conclusion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50
7
Conclusion and Further Research
51
7.1
Conclusion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
51
7.2
Further research
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
52
Bibliography
55
iii
Abstract
Automatic Speech Recognition (ASR) can be decomposed into two parts:
an acoustic
model (AM), which represents the probability of a speech fragment given a word
sequence, and a language model (LM), which represents the a priori probability of
the word sequence itself.
The language model is often trained with N-gram language
modelling.
One of the drawbacks of this technique is data sparsity:
uncommon words
do not occur enough to reliably estimate their N-gram probabilities.
In this thesis
we investigate class-based language models, which attempt to alleviate this problem
by aggregating statistics for similar words.
More specifically, we investigate whether
and how Artificial Neural Networks (ANNs) can be used to cluster similar words.
Recent developments in the field of machine learning have lead to ANNs that
can learn vector representations for words:
the skip-gram model,
introduced by
Mikolov et al., generates word vectors that have both semantic and syntactic word
properties, while the dependency-based model, introduced by Levy and Goldberg,
focuses more on syntactic information.
The space in which these vectors reside,
allows for computing similarities between words, which are well suited to generate
classes that may be used in a class-based N-gram model.
Words can be clustered with several techniques, such as hierarchical agglomerative
clustering and k-means.
Hierarchical clustering offers the possibility of incorporating
extra information in the clustering by adapting individual word similarities, but the
disadvantage of this approach is a large memory usage.
K-means clustering is much
more memory efficient, but does not allow the incorporation of extra information.
A
hybrid method is introduced, which combines the advantages of both models.
Several
models to include extra information in the clustering are proposed,
such as word
frequency, part of speech (POS) and acoustic similarity of words.
The models were evaluated on Dutch data with perplexity and word error rate
(WER). The results show that POS and word frequency information are useful in the
language model, and that acoustic word similarity does not yield any improvement.
The importance of more syntactic classes is confirmed by the superior results of the
dependency-based model over the skip-gram model.
However, the dependency-based
model is more sensitive to data sparsity, which results in worse results compared to
the skip-gram model when the syntactic advantage decreases, e.g.
due to the inclusion
of POS information.
The final model, which combines an optimized set of features,
performs slightly worse than a baseline word-level N-gram model, from which we
conclude that it’s not a good idea to use ANN word vectors for the clustering of
classes in a class-based language model.
iv
Samenvatting
Automatische spraakherkenning,
waarbij
gesproken woorden door een computer
worden herkend, is een technologie met veel toepassingen in tal van gebieden, zoals
automatische ondertiteling,
transcriptie van spraak voor dove mensen en mens-
computer interactie.
Bij deze toepassingen is het zeer belangrijk dat de transcriptie
van goede kwaliteit is, maar de state-of-the-art spraakherkenning moet echter nog
altijd onderdoen voor het menselijke gehoor.
In spraakherkenning wordt de kans geschat dat een bepaalde zin overeenkomt met
een spraakfragment, waarbij de beste kandidaat wordt geselecteerd als uiteindelijke
transcriptie.
Deze kans kan worden berekend aan de hand van twee componenten:
een akoestisch model, dat de kans van het spraakfragment gegeven de zin schat, en
een taalmodel, dat de a priori kans van het voorkomen van de zin schat.
Deze thesis
richt zich specifiek op het verbeteren van het taalmodel.
Het taalmodel schat de kans van het voorkomen van een sequentie van woorden
in natuurlijke taal.
Dit kan met de kettingregel worden herschreven als een vermenig-
vuldiging van de kans van elk afzonderlijk woord, gegeven de voorafgaande woorden.
Om hiervan een goede schatting te kunnen maken, wordt de kans van een woord,
gegeven alle voorafgaande woorden, benaderd door de kans van het woord gegeven de
N-1 vorige woorden, wat verder de geschiedenis van het woord zal genoemd worden.
Deze benadering wordt een N-gram benadering genoemd, waarbij de opeenvolging
van de geschiedenis en het woord zelf de naam N-gram krijgt.
N-grammen zijn een
veelgebruikte techniek binnen taalmodellering.
De kans van elk woord kan nu geschat worden met de maximum likelihood
estimator,
wat praktisch gezien betekent dat de kans wordt benaderd door het
relatieve voorkomen van het woord na zijn geschiedenis in een trainingstekst.
Een
probleem bij deze aanpak is dat er niet genoeg voorbeelden zijn voor elk N-gram
om tot een goede schatting te komen, wat data sparsity wordt genoemd.
Sommige
woorden zullen na een bepaalde geschiedenis zelfs een nulkans krijgen omdat het
N-gram niet voorkwam in de trainingstekst.
Dit zorgt ervoor dat deze opeenvolging
nooit zal
kunnen voorkomen in de transcriptie,
ongeacht of
het taalkundig een
correcte opeenvolging is of niet.
Er zijn verschillende technieken om data sparsity te verminderen, zoals smoothing,
back-off en klassegebaseerde modellen.
Bij smoothing wordt de kansmassa herver-
deeld, zodat nulkansen toch een kleine kans worden toegekend.
Hiervoor bestaan
verschillende technieken.
Bij back-off wordt de geschiedenis van N-grammen met
nulkansen verkleind, totdat er voorbeelden zijn om het relatieve voorkomen te bere-
v
Samenvatting
kenen.
Klassegebaseerde N-gram taalmodellen groeperen gelijkaardige woorden in
klassen, waarna kansen niet enkel worden bepaald met de woordgeschiedenis, maar
ook met gelijkaardige geschiedenissen.
Om verschillende taalmodellen te vergelijken worden twee technieken gebruikt
in deze thesis:
word error rate (WER) en perplexiteit.
Om de word error rate te
bepalen wordt een referentiespraakfragment getranscibeerd,
waarna het relatieve
aantal fouten een maat geeft voor de kwaliteit van het taalmodel.
Deze test geeft
een goed idee van de perfomantie van het model in praktijk, maar is rekenintensief.
Perplexiteit is een snellere test die vaak een goede indicatie geeft van het eindresultaat.
In deze test wordt geen spraakdata gebruikt, maar wordt het taalmodel getest op
een evaluatietekst.
De perplexiteit is de inverse genormaliseerde kans van de tekst
gegeven het taalmodel, wat intuitief te begrijpen is als het gemiddelde aantal woorden
dat volgens het taalmodel
kan volgen op een woord in een tekst,
als ze allemaal
dezelfde kans zouden hebben.
De eerste stap bij het opstellen van een klassegebaseerd taalmodel is de clustering
van woorden.
Een bestaande techniek hiervoor is perplexiteitsclustering,
waarbij
woorden worden geclusterd zodat de perplexiteit van een trainingsset geminimaliseerd
wordt.
Dit is een proces met een zeer slechte schaalbaarheid naar grotere vocabularia.
In deze thesis wordt een nieuwe manier van clusteren voorgesteld, die gebruik
maakt van state-of-the-art artificiële neurale netwerken, ontworpen om de gelijkenis
tussen woorden te bepalen.
Een artificieel neuraal netwerk is een algoritme dat kan
leren op basis van trainingsdata, om daarna voorspellingen te maken over gelijkaardige
data.
Het is gebaseerd op de werking van menselijke hersenen.
Recent werd een
neuraal netwerk architectuur ontwikkeld om te voorspellen welke woorden samen
voorkomen in een tekst, genaamd skip-gram.
Na het trainen van dit netwerk stelde
men vast dat de interne vectorrepresentatie van een woord, een goede voorstelling
was van zijn eigenschappen.
Zo schat het intern product van de genormaliseerde
vectoren de woordgelijkenis, en kunnen wiskundige operatoren zoals optellingen en
aftrekkingen gebruikt worden om analogieën te bepalen.
Het zijn deze vectoren die
in deze thesis zullen gebruikt worden om een nieuwe woordclassificatie te ontwerpen.
Er zijn verschillende technieken om de woorden op te delen in klassen,
zoals
bijvoorbeeld k-means en hiërarchische agglomeratieve clustering met minimum, ge-
middeld of maximum linkage.
K-means start met het toewijzen van random vectoren
aan de verschillende klassen, waarna elk woord wordt toegewezen aan de dichtstbij-
zijnde klassevector.
De klassevectoren worden dan opnieuw bepaald als het midden
van hun woordvectoren.
De laatste twee stappen worden herhaald tot een vooraf
bepaald criterium is bereikt.
Het voordeel van k-means is de goede schaalbaarheid
van het geheugenverbruik voor grote vocabularia.
Voor hiërarchische clustering
wordt de afstand tussen elk woordpaar berekend en opgeslagen in een afstandsmatrix.
Daarna worden woorden geclusterd, waarbij telkens de dichtstbijeenliggende woorden
samengenomen worden totdat het aantal klassen bereikt is.
Om de afstand tussen
klassen (al gegroepeerde woorden) te bepalen, wordt een linkage criterium gebruikt.
Bij maximum en minimum linkage wordt respectievelijk het verste en dichtstbijzijnde
woord beschouwd, bij gemiddeld linkage wordt het gemiddelde van alle afstanden
genomen.
Het grote voordeel van hiërarchische clustering is dat er extra informatie
vi
Samenvatting
kan verwerkt worden in de clustering, door de afstandsmatrix aan te passen.
Hiervan
kan gebruik gemaakt worden om de clustering te verbeteren.
Om de voordelen van extra informatie te combineren met een lager geheugenver-
bruik wordt een hybride clustering geïntroduceerd, waarbij de woorden eerst met
k-means clustering worden voorgeclassificeerd in een beperkt aantal groepen, waarna
hiërarchische clustering wordt toegepast op elk van de groepen.
Bij een vergelijking van deze clusteringstechnieken op de testdataset,
zien we
dat de hiërarchische clustering met gemiddelde linkage de beste word error rate
behaalt met 29
.
8%, op de voet gevolgd door hiërarchische clustering met maximum
linkage met 30
.
2%.
De hybride methode met een preclustering in 3 groepen en
hiërarchische clustering met maximum linkage geeft ook zeer goede resultaten 30
.
2%,
maar de resultaten worden slechter voor preclustering in 10 groepen met 30
.
8%.
Het
geheugenverbruik van de afstandsmatrix verminderde hierbij van 149 Gb naar 18
Gb voor preclustering met 3 groepen en 3.3 Gb voor preclustering met 10 groepen.
De klassegebaseerde modellen zijn echter slechter dan het niet-klassegebaseerde
model,
dat een word error rate van 27
.
9% behaald op dezelfde test.
Om tot een
betere clustering te komen kunnen we gebruik maken van extra informatie,
zoals
de woordsoort,
de hoeveelheid van voorkomen en de akoestische gelijkenis tussen
woorden.
De woordgelijkenis van het skip-gram neurale netwerk is zowel semantisch als
syntactisch van aard.
Men kan redeneren dat woorden die als gelijk worden beschouwd
in de geschiedenis, dezelfde functie moeten hebben in de zin, en dat de syntactische
gelijkenis dus belangrijker zal
zijn dan de semantische.
Door het inbrengen van
informatie over de woordsoort kunnen we ervoor zorgen dat de klassen een meer
syntactisch verband hebben, in tegenstelling tot het eerder semantische verband bij
de skip-gram vectoren.
Om gebruik te maken van de woordsoort wordt de trainingstext eerst gelabeled,
waarna voor elk woord het meest voorkomende label wordt geselecteerd als gebruikte
woordsoort.
Deze laatste stap is nodig omdat sommige woorden als meerdere
woordsoorten kunnen voorkomen, zoals bijvoorbeeld het homoniem
plant
, dat zowel
een werkwoord als een zelfstandig naamwoord kan zijn.
Woorden zullen nu enkel
geclusterd worden als ze dezelfde woordsoort hebben.
Een goede verdeling van het
aantal klassen over de verschillende woordsoorten is hierbij cruciaal.
Door gebruik
te maken van woordsoortinformatie kan de word error rate teruggedrongen worden
van 30
.
2% naar 29
.
2% voor clustering met maximum linkage.
Het vocabularium
bestaat voor ongeveer 80% uit substantieven, die bovendien beter geschikt zijn voor
clustering dan andere woordsoorten.
Daarom zal de rest van de thesis zich vooral
richten op het clusteren van de substantieven.
Het data sparsity probleem dat we proberen te verlichten,
manifesteert zich
vooral bij weinig voorkomende woorden, omdat deze minder trainingsvoorbeelden
hebben.
Daarom kan het de clustering ten goede komen als deze weinig voorkomende
woorden meer geneigd zijn om te clusteren.
Hiervoor werden drie mogelijke modellen
uitgewerkt:
smooth, threshold en two-high adaptatie.
Bij smooth adaptatie wordt
elk element van de afstandsmatrix vermenigvuldigd met een factor die bepaald is
door het aantal voorkomens van de woorden waartussen de afstand bepaald is, en
vii
Samenvatting
een externe parameter.
Thresholdadaptatie zal ervoor zorgen dat enkel woorden die
minder dan een bepaalde drempelwaarde voorkomen, worden geclusterd.
Infrequente
woorden kunnen echter veel leren van gelijkaardige frequente woorden, waardoor het
voordelig kan zijn om een frequent woord te klasseren bij infrequente woorden.
Bij
two-high clustering zullen daarom woorden enkel geclusterd kunnen worden als een
van beide minder frequent is dan een bepaalde drempelwaarde.
Het nadeel hiervan
kan zijn dat veelvoorkomende woorden iets minder nauwkeurig geschat worden.
Bij
experimenten blijkt dat informatie over de woordfrequentie de clustering ten goede
komt.
Vooral thresholdadaptatie en smooth adaptatie verbeteren de word error rate,
die in beide gevallen van 28
,
6% naar 28
,
2% daalt.
Een derde mogelijke bron van informatie is de uitspraak van de woorden.
Als
woorden een gelijkaardige uitspraak hebben, zullen ze meer verward worden door het
akoestisch model, waardoor het interessant zou kunnen zijn om akoestisch gelijkende
woorden minder snel te clusteren.
Analoog aan de clustering voor weinig voorkomende
woorden kan dit gebeuren met smooth adaptatie of
thresholdadaptatie.
Beide
adaptaties maken gebruik van extra parameters om de invloed van het akoestisch
model
te bepalen.
Bij het optimaliseren van deze parameters blijkt dat de beste
resultaten worden behaald voor waardes die de invloed van het akoestisch model
minimaliseren.
Hieruit kan besloten worden dat informatie over de akoestische
gelijkenis van woorden geen voordeel biedt bij het clusteren.
De resultaten bij het toevoegen van extra informatie over de woordsoort toonden
aan dat een meer syntactische clustering voordelig is.
Dit kan ook bekomen worden
door meer syntactische woordvectoren te gebruiken, wat enkele aanpassingen vereist
aan de training en de architectuur van het neurale netwerk.
In tegenstelling tot
de skip-gram training,
waarbij
woorden die in elkaars buurt voorkomen worden
voorspeld, zal dit neurale netwerk woorden proberen voorspellen die een syntactisch
verband hebben.
Bovendien is het ook de taak van het netwerk om het syntactische
verband te voorspellen.
Dit netwerk zal clusteren tot meer syntactische vectoren,
maar ook in een grotere data sparsity voor vectoren van infrequente woorden, omdat
er minder trainingsvoorbeelden voorhanden zijn.
Deze meer syntactische woordvectoren resulteren in een beter taalmodel:
een
word error rate van 29% ten opzichte van 30
.
2% voor de skip-gram woordvectoren.
Echter, als er woordsoortinformatie wordt toegevoegd, vermindert de word error rate
naar 28
.
8% ten opzichte van 28
.
6% voor de skip-gram woordvectoren.
Een verklaring
hiervoor zou kunnen zijn dat het voordeel
van de syntactische informatie in de
vectoren minder belangrijk wordt door de toevoeging van de woordsoortinformatie,
en dat de data sparsity van het tweede model
zal
doorwegen in de uiteindelijke
resultaten.
In een finaal model werden de beste technieken gecombineerd:
hiërarchische clus-
tering, toevoegen van woordsoortinformatie en bevoordeling van weinig voorkomende
woorden.
Er werd gezocht naar een optimaal aantal klassen en de beste setup voor
het trainen van de woordvectoren.
Het uiteindelijke model haalde een word error
rate van 28
.
1%, wat iets slechter is dan de baseline met 27
.
9%.
Hieruit kunnen we
concluderen dat het niet loont om neurale netwerk vectoren te gebruiken voor de
clustering van klassen in een klassegebaseerd taalmodel.
viii
List of Figures and Tables
List of Figures
1.1
Overview of speech recognizer [28]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1.2
Hidden Markov Model [28]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
3.1
Model of a neuron [7]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
3.2
A sigmoid function .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
3.3
A feedforward neural network .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
3.4
A feedforward neural network without hidden layer .
.
.
.
.
.
.
.
.
.
.
.
16
3.5
The skip-gram neural network [15]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
4.1
Dependency parsed tree, with red dependency relations and black POS
tags [26]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
4.2
All word context pairs that can be extracted from the example sentence
26
5.1
Plot of the acoustic word adaptation with different values of parameter
a
and
b
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
6.1
Perplexities and WERs for the various clustering algorithms .
.
.
.
.
.
.
42
6.2
Perplexities and WERs for POS clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
6.3
WERs on the development set for uncommon words clustering with
smooth adaptation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
44
6.4
WERs on the development set for uncommon words clustering with
threshold adaptation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
44
6.5
WERs on the development set for uncommon words clustering with
two-high threshold adaptation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45
6.6
Perplexities and WERs for uncommon word adaptation
.
.
.
.
.
.
.
.
.
46
6.7
WERs on the development set for acoustic word similarity clustering
with smooth adaptation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
46
6.8
WERs on the development set for acoustic word similarity clustering
with threshold adaptation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
47
6.9
Perplexities and WERs for clustering with dependency-based word vectors
47
6.10 WERs on the development set for various word vector setups clustered
with maximum linkage hierarchical clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
48
ix
List of Figures and Tables
6.11 WERs on the development set for various word vector setups clustered
with average linkage hierarchical clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
49
6.12
Perplexities and WERs for hierarchical clustering with maximum linkage
without ’
Maximum
’ and with ’
F inal
’ optimization and extra
information sources .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50
List of Tables
3.1
The five most similar words to reference words according to word2vec
.
23
4.1
The five most similar words to reference words according to the
dependency-based model .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
4.2
Amount of words that have only a certain amount of training examples
28
5.1
Occurrences of POS in the vocabulary and the corpus as tagged by
FROG [25]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
5.2
The five most acoustically similar words to reference words according to
the acoustic word similarity model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
36
6.1
The OOV rates for the different corpora .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
40
6.2
Perplexities and WERs for the baseline model on the text evaluation set,
and on the speech evaluation and development set
.
.
.
.
.
.
.
.
.
.
.
.
41
x
List of Abbreviations and
Symbols
Abbreviations
DCT
Discrete Cosine Transform
FFT
Fast Fourier Transform
GT
Good-Turing
HMM
Hidden Markov Model
KN
Kneser-Ney
LM
Language Model
MFCC
Mel Frequency Cepstral Coefficients
MLE
Maximum Likelihood Estimator
POS
Part of Speech
PPL
Perplexity
SPEC
Special token
WER
Word Error Rate
xi
List of Abbreviations and Symbols
Symbols
C
count of occurrences of input
Cl
amount of classes
E
energy function
L
likelihood
N
size of N-gram approximation
N
i
number of N-grams with i occurrences
S
sigmoid function
T
training dataset
T
0
noise dataset
V
vocabulary size
W
word sequence
Y
acoustic utterance
a
ij
HMM transition probability from state i to state j
β
ik
bias of neuron i in layer k
b
i
HMM probability of acoustic feature vector in state i
b
k
k
th
bit in binary representation
c
k
class of k
th
word in word sequence
d
desired output value
dr
dependency relation
e
error
hd
head of a sentence
h
history of word k in word sequence
m
modifier of a word in a sentence
n
w
bit representation of word w
o
jk
output from neuron j in layer k
p
w
i
i
th
phone of word
w
s
surrounding of a word in a word sequence
σ
softmax function
v
ijk
synapse weight from neuron i to neuron j in layer k
w
k
k
th
word in word sequence
x
ijk
neuron input from neuron i to neuron j in layer k
y
t
acoustic feature vector at time t
V
k
matrix of synapse weights in layer k
β
k
bias vector of neurons in layer k
o
k
output vector of neurons in layer k
v
w
vector representation of word w
xii
Chapter 1
Introduction
Automatic speech recognition is the conversion of speech into the corresponding text
by a computer.
The importance of this technology is marked by the widespread use
of it in several applications, for example Human Computer Interaction, which ensures
that speech technology is included in every modern smartphone.
Other applications
include the transcription of speech for subtitles in television or the large YouTube
collection,
transcription of speech for deaf people,
audio search for news archives
and automatic speech translation.
All of these applications depend on high quality
transcriptions, but unfortunately the current state of the art has not yet reached the
accuracy of human speech recognition.
Automatic speech recognition is a complex
problem that has been studied for many decades and will probably not be solved
soon.
This introduction briefly explains the basic workings of
a speech recognition
system in section 1.1,
based on the overview in [
28
],
after which the goal
of this
thesis is covered in section 1.2
1.1
Overview
To process a speech signal, it is first converted into feature vectors by parametrization
at regular time intervals.
From this collection of
feature vectors
Y
,
the speech
recognizer has to determine the most probable word sequence
ˆ
W
:
ˆ
W
=
argmax
W
P
(
W |Y
)
(1.1)
With Bayes’ rule, this can also be written as:
ˆ
W
=
argmax
W
P
(
W
)
P
(
Y |W
)
P
(
Y
)
(1.2)
The probability of the observed feature vectors
P
(
Y
) is independent of
W
and
has thus no importance in the maximization to find
ˆ
W
.
Therefore this term can
be ignored and the maximization problem can be decomposed into two separate
sub-problems:
the language model
P
(
W
),
which computes the a priori
chance of
1
1.
Introduction
word sequence
W
, and the acoustic model
P
(
Y |W
), which represents the probability
of
observation
Y
given word sequence
W
.
An overview of
this decomposition is
represented in figure 1.1.
The best word sequence is finally found by considering all possible word sequences
and selecting the most probable outcome.
Smart implementations of
this word
sequence search (also called decoding) are needed to limit the required computation
power.
Figure 1.1:
Overview of speech recognizer [28]
1.1.1
Front-end parametrization
To convert a speech signal into a feature vector, a fragment is selected each 10 msec,
using a sliding window with a length of typically 25msec.
One can assume that the
signal is stationary within this fragment, which means that the speech has constant
properties.
From this fragment,
the feature vectors are computed with Fourier
analysis or linear prediction.
There are different ways to achieve this conversion.
A typical setup is that of the Mel-Frequency Cepstral Coefficients (MFCC), a
parametrization that aims at approximating the frequency resolution of the human
ear.
First the high frequencies are pre-emphasized to compensate the loss due to lip
radiation.
Then the signal is transformed to the frequency domain by means of a Fast
Fourier Transform (FFT). The frequency spectrum is integrated under triangular
filters that are arranged according to a non-linear scale called the Mel-scale, aimed at
mimicking the frequency resolution of the human ear.
The logarithm is taken from
the resulting parameters and finally a Discrete Cosine Transform (DCT) is applied
2
1.1.
Overview
to compress the spectral information in the lower bands and to decorrelate the result.
A vector of 13 parameters is now constructed, consisting of the signal energy and
the 12 first DCT coefficients.
The parametrization can be improved with dynamic information about the
coefficients by including delta features:
on top of the MFCC parameters, the first
and second derivatives are added to the feature vector, computed with a simple linear
differential filter, using the 2 preceding and the 2 following vectors.
The resulting
feature vector has 39 components.
1.1.2
Acoustic model
The goal of the acoustic model is to calculate the probability of the feature vector
sequence given the word sequence
P
(
Y |W
).
To find a model
for the sound of
the word sequence,
the words are split into phones,
single speech segments with
distinct properties.
These phones can be represented by a Hidden Markov Model
(HMM), presented in figure 1.2, which is a Finite State Machine with different states
representing the sound of the utterance.
At each time interval, when a new feature
vector is processed, a transition from state
i
to a new state
j
is made with probability
a
ij
.
In each state
i
, the model has a probability distribution of the acoustic feature
vector
y
observed in this time interval
t
, written as
b
i
(
y
t
).
Figure 1.2:
Hidden Markov Model [28]
A phone is commonly represented by 3 states.
Word sequences are acoustically
modelled by the concatenation of the HMM’s of their phones.
This model allows to
calculate how good the observed speech
Y
fits with the acoustics of the proposed
word sequence, which enables the computation of the term
P
(
Y |W
).
It can be greatly
improved for speed and accuracy with several techniques such as triphones, where
the phone representation depends on the previous and next phone in the word.
3
1.
Introduction
1.1.3
Language model
The language model (LM) is needed to calculate the probability of the word sequence
P
(
W
) with
W
=
w
1
w
2
...w
n
, which can be expanded as:
P
(
w
1
w
2
...w
n
) =
P
(
w
1
)
P
(
w
2
|w
1
)
P
(
w
3
|w
1
w
2
)
...P
(
w
n
|w
1
..w
n−
1
)
(1.3)
The probability can thus be factored into multiple conditional word probabilities,
where the word probability for
w
k
can be calculated by
P
(
w
k
|w
1
...w
k−
1
).
The most
common way to approximate the conditional word probabilities is by only looking at
the N-1 preceding words
P
(
w
k
|w
k−N
+1
...w
k−
1
), which is known as N-gram language
modelling.
In the context of speech recognition, 3-gram to 5-gram language models
are commonly used.
Chapter 2 contains an in-depth discussion of
the N-gram
language model.
1.1.4
Decoding
With the acoustic and language model we have the probabilities needed to find the
best word sequence
ˆ
W
from a collection of proposed word sequences
W
.
To make
sure that the best solution is found, all possible word sequences would have to be
tried, but this calculation is too computationally demanding.
Another strategy for
searching the whole word sequence space is required.
The computational work can
be greatly improved by using Viterbi decoding, in which only the best path through
the HMM is considered.
This approximation leads to a breadth-first search in time,
where only the best path to a given state has to be taken into account.
Furthermore,
if the probability of being in a given state is low compared to that of being in other
states, the path through that state can be neglected.
This is called beamforming.
Using these techniques,
the most probable word sequence can be calculated with
reasonable computational power.
1.2
Research goal
The goal of this thesis is to design a language model combining class-based N-grams,
with state-of-the-art neural
networks,
that are capable of
estimating similarities
between words.
The work will focus on reducing the data sparsity problem, which
causes the language model probabilities to deviate from the real probabilities.
The
language models are specifically designed for usage in speech recognition and the
proposed language models will
be trained on Southern Dutch text corpora,
and
tested on Southern Dutch text and speech benchmarks.
In chapter 2, the N-gram language model is discussed, with a special focus on
the class-based N-gram model.
At the end of the chapter, two evaluation metrics
are introduced.
Chapter 3 starts with a brief overview of neural networks, followed
by an in-depth examination of the skip-gram neural network model, which makes
it possible to estimate word similarities.
A more syntactic word similarity model is
presented in chapter 4, which is then compared to the model of chapter 3.
These word
similarities are used in chapter 5 to cluster words into word classes, which can be
4
1.2.
Research goal
used in the class-based N-gram model.
Several clustering techniques are introduced
at the start of this chapter.
The rest of the chapter proposes ways to incorporate
extra information to the clustering, such as word frequency, pronunciation and the
part of speech of a word.
All the proposed models are evaluated on both text and
speech data in chapter 6, after which a conclusion is drawn in chapter 7.
5
Chapter 2
N-gram Language Models
As seen in chapter 1, an acoustic model
P
(
Y |W
) and a language model
P
(
W
) are
required to find the best word transcription of a speech signal.
The goal
of this
chapter is to find a model for the probability of a word sequence
P
(
W
).
Section 2.1 handles the N-gram approximation.
The data sparsity problem is
described in section 2.2, followed by a number of techniques to reduce this problem,
such as smoothing,
back-off and interpolation.
Section 2.3 digs into class-based
language models, which are used in this thesis to reduce the data sparsity.
Finally,
two ways to compare and evaluate language models are proposed in chapter 2.4.
2.1
N-grams
As discussed in subsection 1.1.3 the word sequence probability
P
(
W
) can be decom-
posed as:
P
(
W
) =
P
(
w
1
w
2
...w
k
) =
P
(
w
1
)
P
(
w
2
|w
1
)
P
(
w
3
|w
1
w
2
)
...P
(
w
k
|w
1
..w
k−
1
)
(2.1)
where the individual word probability depends on all previous words.
One can assume
that words that are further away have less influence on the end result.
The N-gram
approximation therefore only considers the N most recent words for the calculation
of the probability.
The individual word probabilities are thus approximated with
only a limited history consisting of the N-1 last words:
P
(
w
k
|w
k−
1
1
)
≈ P
(
w
k
|w
k−
1
k−N
+1
)
(2.2)
with
w
k−
1
k−N
+1
=
w
k−N
+1
...w
k−
1
, the history of the word.
The Maximum Likelihood Estimator (MLE) can be used to determine an expres-
sion for the probability of N-grams.
This estimator chooses the N-gram probabilities
so that the likelihood of training observations
W
is maximized, while making sure
that the N-gram probabilities add to one.
First an expression for the probability of the training observations according to
the language model is composed.
These observations exist of a so-called training
corpus:
a large collection of text
W
=
w
1
...w
K
.
The probability of the text is the
multiplication of the individual conditional probabilities:
7
2.
N-gram Language Models
P
(
w
1
w
2
...w
K
) =
K
Y
k
=1
P
(
w
k
|w
k−
1
1
)
(2.3)
≈
K
Y
k
=1
P
(
w
k
|w
k−
1
k−N
+1
)
(2.4)
The logarithm of this expression is taken, to avoid working with a multiplication.
Since the log function is strictly increasing, the maximum of the probability is also
the maximum of the log of the probability, and the resulting estimation will be the
same.
The terms
P
(
w
k
|w
k−
1
k−N
+1
) that have the same words
w
k
and history
w
k−
1
k−N
+1
(further written as
h
k
), are grouped and replaced with the probability multiplied by
their count:
log

K
Y
k
=1
P
(
w
k
|h
k
)

=
K
X
k
=1
log

P
(
w
k
|h
k
)

(2.5)
=
X
w,h∈T
C
(
w, h
)
log

P
(
w|h
)

(2.6)
with
C
(
w, h
) the count of the N-gram consisting of word
w
and history
h
.
For each history, the probabilities should add up to one.
This gives rise to a set
of constraints
P
w
P
(
w|h
) = 1, for each history
h
.
The Lagrange function can be set
up as:
X
w,h∈T
C
(
w, h
)
log

P
(
w|h
)

+
X
h∈T
λ
h

X
w∈T
P
(
w|h
)
−
1

(2.7)
=
X
h∈T
X
w∈T
C
(
w, h
)
log

P
(
w|h
)

+
λ
h

X
w∈T
P
(
w|h
)
−
1

!
(2.8)
Probabilities with a different history
h
are independent and the optimization can
thus be performed for each history separately, giving for one specific history
h
:
X
w∈T
C
(
w, h
)
log

P
(
w|h
)

+
λ

X
w∈T
P
(
w|h
)
−
1

(2.9)
The MLE can be derived by differentiating this Lagrange function to
λ
and
P
(
w|h
) (for each
w
), and setting the resulting equations equal to zero.
By solving
the resulting system of equations to the possibilities
P
(
w|h
), the following results
are obtained:
P
(
w|h
) =
C
(
w, h
)
P
w
0
∈T
C
(
w
0
, h
)
=
C
(
w, h
)
C
(
h
)
(2.10)
The Maximum Likelihood Estimator is thus equal to the relative frequency of the
N-gram in the training corpus.
A full overview of N-gram language models is given
in [8].
8
2.2.
Data sparsity
2.2
Data sparsity
The Maximum Likelihood N-gram Estimator has one big drawback:
data sparsity.
If the count of the N-gram is high, the relative frequency will be a good estimate.
However, for infrequent N-grams, the estimation can greatly deviate from the real
probability.
Some existing N-grams could even be missing from the training corpus,
which would result in a zero probability, eventhough the N-gram could be perfectly
acceptable.
This problem is caused by the limited size of the corpora and amplified by the
bad scaling of N-grams:
for a vocabulary of
V
words, there are
V
N
possible N-grams.
Assuming there are 20 000 distinct words in a corpus, this gives 400 000 000 possible
2-grams, which is already the same order of magnitude as a corpus (e.g.
there are
128 000 000 words in the training corpus used for this thesis).
The problem only gets
worse when 3-grams are used.
Because of this, but also to reduce computational complexity, a limited vocabulary
is used in language models, in which words that are used least frequently, and that
thus would have a really bad probability estimation, are not included in the language
model.
These words are instead replaced by the tag <UNK>.
2.2.1
Smoothing
To prevent existing N-grams from having zero probabilities, the probability mass can
be redistributed with a technique called smoothing:
N-grams with zero probability
will receive a small probability from the N-grams with non-zero probability.
In add-one smoothing, also known as Laplace smoothing, the problem of zero
probabilities is tackled by adding one to the count of every N-gram.
In the denomi-
nator of formula 2.10, the amount of words in the vocabulary
V
is added, to make
sure the probabilities still add to one:
P
(
w|h
) =
C
(
w, h
) + 1
C
(
h
) +
V
(2.11)
While this smoothing technique does not perform well
[
4
],
it illustrates the
intuition behind smoothing.
To improve the redistribution, other smoothing techniques can be used, such as
Good-Turing (
GT
) and Kneser-Ney (
KN
) smoothing.
A full derivation is beyond
the scope of this thesis, but we will briefly explain the intuitions behind them.
In
Good-Turing,
the probabilities of N-grams that are only observed once are used
to predict the probabilities of
unseen N-grams.
If
N
c
is defined as the number
of N-grams that occur
c
times,
and
N
tot
is the total
number of observations,
the
smoothing is performed as follows [6]:
9
2.
N-gram Language Models
P
GT
(unseen N-grams) =
1
N
tot
N
1
N
0
!
(2.12)
P
GT
(N-gram with count c) =
1
N
tot
(
c
+ 1)
N
c
+1
N
c
!
(2.13)
To avoid problems with empty
N
c
bins, the distribution can be approximated
with a linear regression,
where the parameters
a
and
b
are chosen to fit the
N
c
distribution as well as possible [3]:
log
(
N
c
) =
a
+
b · log
(
c
)
(2.14)
To redistribute the counts, Kneser-Ney uses a technique called discounting, in
which a small portion
d
is taken from every count, that can be redistributed to avoid
zero counts.
Following the assumption that words that appear in many different
contexts, are more likely to appear in a new one, Kneser-Ney gives more probability
mass to these words.
This can be illustrated with the following example [8]:
I can’t see without my reading ...
Here,
glasses
seems more likely than
F rancisco
, however
F rancisco
has is more
frequent and is thus preferred in the simple unigram case.
Since
F rancisco
is only
frequent after
San
and since the word
glasses
appears in many different contexts,
Kneser-Ney smoothing will give more probability to
glasses
.
With
α
as normalization
parameter, this becomes [11]:
P
KN
(
w|h
) =



C
(
w,h
)
−d
C
(
h
)
if
C
(
w, h
)
>
0
α
(
h
)
|h
:
C
(
w,h
)
>
0
|
P
w
|h
:
C
(
w,h
)
>
0
|
otherwise
(2.15)
2.2.2
Interpolation and back-off
Data sparsity problem caused two problems:
zero probabilities for existing N-grams,
which can be dealt with using smoothing, and bad estimates for infrequent N-grams.
There is another source of
information that can be used to alleviate the second
problem:
if a certain N-gram probability estimate is inaccurate,
the lower order
(N-1)-gram probability can be used as extra information.
This can be achieved in
two ways:
interpolation and back-off.
In interpolation, the N-gram probability is combined with the lower order proba-
bilities typically in a linear way:
P
interpolation
(
w
k
|w
k−
1
k−N
+1
) =
N
X
i
=1
λ
i
P
(
w
k
|w
k−
1
k−i
+1
)
(2.16)
10
2.3.
Class-based models
To make sure that the probabilities are normalized, the
λ
coefficients should add
to one.
A set of
λ
coefficients can be chosen independently for every context,
by
optimizing them over a held-out corpus:
a part of the training corpus that is not
used in the training of the probabilities
P
.
In back-off, the information of the lower order (N-1)-gram is only used when the
N-gram count is lower than a certain threshold.
This backing off is continued until
an N-gram is found that has some counts.
In Katz back-off, probability mass is taken
from the non-zero probabilities, to be redistributed to the zero probabilities, similar
to formula 2.15 in the Kneyser-Ney smoothing.
This redistribution is based on the
(N-1)-gram probability, with normalizing factor
α
(
w
k−
1
k−N
+1
) [9]:
P
Katz
(
w
k
|w
k−
1
k−N
+1
) =





C
(
w,w
k−
1
k−N
+1
)
−d
C
(
w
k−
1
k−N
+1
)
if
C
(
w, w
k−
1
k−N
+1
)
>
0
α
(
w
k−
1
k−N
+2
)
P
Katz
(
w
k
|w
k−
1
k−N
+2
)
otherwise
(2.17)
2.3
Class-based models
Another way to deal with data sparsity is the class-based approach, as introduced by
Brown et al.
[
1
].
First, words
w
in the vocabulary are assigned to relevant classes
c
,
so that semantically and syntactically similar words are allocated to the same class.
Now, predictions given a certain history can also be based on similar histories.
The
assumption of class-based models is that the probability only depends on the word
classes of the history:
P
(
w
k
|w
k−
1
k−N
+1
)
≈ P
(
w
k
|c
k−
1
k−N
+1
)
(2.18)
According to Bayes’ Theorem, this can be rewritten as:
P
(
w
k
|c
k−
1
k−N
+1
) =
P
(
w
k
|c
k
, c
k−
1
k−N
+1
)
P
(
c
k
|c
k−
1
k−N
+1
)
(2.19)
≈ P
(
w
k
|c
k
)
P
(
c
k
|c
k−
1
k−N
+1
)
(2.20)
P
(
c
k
|c
k−
1
k−N
+1
) is the probability of a class given the word classes of the history.
This is computed with the same techniques as the simple N-grams,
where all the
words in the corpus are replaced with their corresponding classes.
P
(
w
k
|c
k
) is the
probability of a word given the word class.
The Maximum Likelihood Estimator of
this term is the relative frequency of the word in relation to the frequency of the
class, giving:
P
(
w|c
) =
C
(
w, c
)
P
w
0
C
(
w
0
, c
)
=
C
(
w
)
C
(
c
)
(2.21)
The problem of sparse data is alleviated because the amount of possible N-grams now
scales with
O
(
Cl
N
), with
Cl
the amount of classes, instead of
O
(
V
N
).
However, with
less classes, the class approximation in formula 2.18 is less accurate.
A good balance
in the amount of classes is needed.
In order to get a good class approximation, it
is of great importance that words are grouped together into relevant classes.
This
thesis will explore new ways to make a good classification.
11
2.
N-gram Language Models
2.4
Evaluation of language models
There are two main ways to evaluate a language model within the context of automatic
speech recognition:
an extrinsic and an intrinsic evaluation.
For the extrinsic
evaluation, the test corpus consists of a speech fragment and its transcription.
A
speech recognizer is used to transcribe a speech fragment, after which the transcription
is compared to a reference transcription by computing the word error rate (WER).
To calculate the word error rate, the word-level Levenshtein distance between
the transcription and the output of the recognizer is calculated [
27
].
First we define
3 types of
errors:
substitution,
where a word is wrongly recognized as another
word, insertion, where a word appears in the recognized text that wasn’t originally
there, and deletion, where a word is not recognized while being in the transcription.
The Levenshtein distance is defined as the smallest number of errors between the
transcription and the recognized text.
Ideally,
one would always test a model
on the task at hand.
In case of auto-
matic speech recognition however, running several comparative experiments is costly,
therefore an approximation is used to get a quick sense of the results:
the intrinsic
evaluation known as perplexity (PPL). In this measure, the test corpus consists of
a text
W
=
w
1
..w
K
.
The perplexity
P P L
(
W
) measures how good the model is at
predicting the test corpus
W
and is defined as the inverse probability
P
(
W
)
−
1
of
the test corpus given by the language model.
The result is normalized to make it
independent from the test set length:
P P L
(
W
) =
N
s
1
P
(
W
)
=
N
v
u
u
t
K
Y
k
=1
1
P
(
w
k
|w
k−
1
k−N
+1
)
(2.22)
Perplexity can be intuitively understood as the average number of possible next
words that can follow any word,
if the probability distribution of the next words
would be uniform.
The lower the perplexity, the better the model predicts the actual
language.
2.5
Conclusion
N-gram language models are an easy way to make a basic language model.
The
probabilities of the N-grams can be estimated by means of the maximum likelihood
estimator, leading to the relative frequency of the words, given the history.
A problem that arises when using N-gram language models,
is data sparsity,
which causes the estimated probabilities to deviate from the real probabilities when
dealing with uncommon words,
or even zero probabilities for word combinations
that are perfectly acceptable.
These problems can be alleviated with a number of
techniques, such as smoothing, back-off, interpolation or a class-based approach.
In
this last approach, words are grouped in relevant classes.
The class-based approach
will be the basis of this thesis.
12
Chapter 3
Neural Network-based Word
Similarity
Neural networks are the state of the art in various applications of machine learning.
In this chapter, the power of neural networks is applied to estimate word similarities,
which can be used to generate classes for the class-based language model from chapter
2.
In the first section 3.1, a brief introduction on neural networks is given, based on
the overview in [
7
].
Chapter 3.2 describes a neural network specifically designed to
estimate word similarities.
3.1
Neural networks
The most powerful
learning system that we know is the human brain:
a massive
interconnection of individual
neurons,
that efficiently processes huge amounts of
information.
An artificial neural network is a type of learning algorithm inspired by
the biological neural network.
It is a system with typically a large number of inputs
and outputs and an inner system consisting of a vast amount of simple processing
elements (neurons) with interconnections of a certain weight between them.
These
weights are adapted while training, after which the network can predict new data
based on the learned information.
First a model of a neuron will be explained in
subsection 3.1.1.
Then,
in subsection 3.1.2,
the neurons are interconnected in a
neural network.
Finally, a basic training algorithm will be discussed in subsection
3.1.3.
3.1.1
Model of a neuron
A single neuron consists of 3 parts:
synapses, a combiner and an activation function.
The synapses are connecting links, transferring signals
x
i
from inputs or from other
neurons
i
to their neuron.
The synapses have weights
v
i
that are used to scale the
signal.
Secondly, a linear combiner assembles these signals and sums all the input
signals from the links.
In some networks, a constant is added to this sum called the
bias
β
.
Finally the result is transferred to the activation function
f
, a function that
13
3.
Neural Network-based Word Similarity
converts the signal from the combiner to an output value
o
.
A schematic overview of
the neuron can be found in figure 3.1.
x
2
v
2
Σ
Combiner
f
Activation
function
o
Output
x
1
v
1
Synapses
x
3
v
3
Weights
β
Bias
Inputs
Figure 3.1:
Model of a neuron [7]
This can be expressed mathematically as:
o
=
f

X
i
v
i
x
i
+
β

(3.1)
The activation function is typically a nonlinear function with saturation, such as
the sigmoid function
S
in formula 3.2 and figure 3.2 and as the softmax function
σ
in
formula 3.3.
Both convert a signal to an output value between 0 and 1.
The softmax
function has the special property that the output of the involved neurons sums up
to one.
Therefore this function is frequently used to model a discrete probability
distribution, in which every output neuron represents a possible instantiation for the
random variable.
The value of the output gives the probability of this result.
S
(
t
) =
1
1 +
e
−t
(3.2)
σ
j
(
t
) =
e
−t
j
P
K
k
=1
e
−t
k
(3.3)
−
6
−
4
−
2
0
2
4
6
0
0
.
2
0
.
4
0
.
6
0
.
8
1
input
output
Figure 3.2:
A sigmoid function
14
3.1.
Neural networks
3.1.2
Neural networks
A single neuron is limited in its processing ability.
In order to enhance the decision
capability of the network, a large number of neurons are put in a layered network.
Such a neural network can be visualized as a graph, as depicted in figure 3.3, in which
the nodes represent the neurons and the edges are the interconnections between
the nodes.
A network without loops is called feedforward, otherwise it is called a
recurrent network.
All models that are used in this thesis are feedforward neural
networks.
Input
layer
Hidden
layer
Output
layer
Input 1
Input 2
Input 3
Input 4
Input 5
Output 1
Output 2
Figure 3.3:
A feedforward neural network
As can be seen in figure 3.3,
the neurons are arranged in consecutive layers:
the input layer, the hidden layers and the output layer.
The input layer is where
input signals are inserted into the system.
No computations are performed in this
layer.
The input layer is sometimes followed by one or more hidden layers, in which
the input is transformed through multiple calculations of neurons with activation
function
f
.
At the end, the output layer performs a last calculation and returns the
result.
The output values
o
jk
of neuron
j
in layer
k
can be written in an output vector
o
k
, which contains the values from each neuron.
For each layer
k
, the weight of the
interconnection from neuron
i
in layer
k −
1, to neuron
j
in layer
k
can be represented
as
v
ijk
, which can be assembled in the interconnection matrix
V
k
.
The bias of neuron
j
in layer
k
can be written as
β
jk
, or combined in a vector
β
k
.
The output of the
linear combiner of neuron
j
in layer
k
can be calculated as:
X
i
v
ijk
o
i
(
k−
1)
+
β
jk
(3.4)
The output of the linear combiner is then fed to the activation function
f
k
to get
the output of neuron
j
in layer
k
:
15
3.
Neural Network-based Word Similarity
o
jk
=
f
k

X
i
v
ijk
o
i
(
k−
1)
+
β
jk

(3.5)
These calculations can be vectorized by assuming that the activation function
f
k
takes a vector as input and works on each element of the vector individually, thus
outputting a vector of the same size as the input vector.
The sum over all weights
can be written as the inner product of the input vector
o
with the row of matrix
V
corresponding to the neuron.
The output of layer
k
can now be written as:
o
k
=
f
k

V
k
o
k−
1
+
β
k

(3.6)
Input
layer
Output
layer
Input 1
Input 2
Input 3
Output
Output
Figure 3.4:
A feedforward neural network without hidden layer
Figure 3.4 depicts a neural network with input layer
in
and output layer
out
but
without a hidden layer.
This network can be written mathematically as:
o
out
=
f
out
(V
out
o
in
+
β
out
)
(3.7)
Figure 3.3 represents a neural
network with one hidden layer,
which can be
represented as:
o
out
=
f
out

V
out
f
hidden
(V
hidden
o
in
+
β
hidden
) +
β
out

(3.8)
3.1.3
Training
A single training phase of a neural network consists of 2 steps:
the forward and the
backward step.
In the forward step, the neural network is stimulated by training
data.
Input data
x
is fed into the neural network, and the outputs
o
are calculated,
which can then be compared to the desired output values
d
.
In the backward step,
the weights of
the network are adjusted and the network will
behave differently.
Adjustments are made to minimize the error
e
between the output and the desired
16
3.2.
Skip-gram neural network
data in function of the training data.
To obtain a measure of the error reduction, a
cost function is defined, which is typically the quadratic error function:
E
=
1
2
X
i
e
2
i
=
1
2
X
i
(
d
i
− o
i
)
2
(3.9)
A minimum of this function can be found by applying the method of steepest
descent,
in which the weights
v
of the network are adapted with the gradient of
the error function
dE
dv
, scaled by the learning rate
α
.
This method only guarantees
conversion to a local minimum.
The weights of the neural network are thus adapted
as follows:
dE
dv
=
−
X
i
e
i
de
i
dv
=
−
X
i
e
i
do
i
dv
(3.10)
∆
v
=
α
dE
dv
=
−α
X
i
e
i
do
i
dv
(3.11)
The full
training of the neural
network consists of a large amount of training
phases to ultimately converge to a local minimum.
In each step, the weights of the
network are adapted in the steepest descent direction.
3.2
Skip-gram neural network
In this section we will describe the continuous skip-gram neural network, the best
model
in the word2vec family [
15
],
proposed by Mikolov et al
[
16
]
[
15
],
which is
trained to predict the surroundings of
a word in a sentence.
After training this
network, it turns out that the interconnection weights in the hidden layer can be used
to define a good measure for word similarities.
The described networks are optimized
to improve this similarity measure.
In subsection 3.2.1, the skip-gram neural network
is described, for which two training algorithms are derived in subsection 3.2.2.
The
properties of the trained word vector representations are discussed in 3.2.3.
3.2.1
The skip-gram network
The skip-gram neural
network will
try to predict the surrounding words
s
of the
input word
w
.
A graphical representation of the network can be found in figure 3.5.
The input is coded as a one-hot coding, i.e.
the amount of inputs equals the size
of the vocabulary,
every input node represents one word and an input signal 1 is
given to activate a word.
The input is then fed to a hidden layer without activation
function:
the projection layer.
This layer typically contains around 300 neurons and
can be represented as:
o
hidden
= V
hidden
o
in
(3.12)
The output layer also has the size of the vocabulary, where every neuron represents
one word.
The desired output is the estimated probability of the word
s
being in
17
3.
Neural Network-based Word Similarity
the surrounding of the input word
w
.
Using the softmax activation function, the full
neural network can be characterized as follows:
o
out
=
σ
(V
out
o
hidden
) =
σ
(V
out
V
hidden
o
in
)
(3.13)
Note that the network only predicts the surrounding words, and not the position
of the words.
Input
layer
Projection
layer
Output
layer
w(t)
w(t-2)
w(t-1)
w(t+1)
w(t+2)
Figure 3.5:
The skip-gram neural network [15]
3.2.2
Skip-gram training
The training of the skip-gram is very computationally demanding because of the
normalization in the softmax output layer:
for every training example, the denom-
inator of
the softmax in formula 3.3 has to be calculated,
which sums over the
whole vocabulary.
To overcome this, two approximations have been proposed in [
14
]:
negative sampling and the hierarchical softmax.
Negative sampling
In negative sampling, which is based on noise-contrastive estimation [
17
] and further
explained in [
14
] [
5
], the output neurons are changed to sigmoid functions to avoid
dependency on other neurons.
Now, one training example is taken from the data
set, and k noise examples are generated with the same input word
w
but a random
surrounding word
s
.
The training will try to maximize the output if the example is
from the data set, and minimize it if the data is a noise example.
This approximates
the softmax because the output will
be large if a word has high probability,
and
because of the noise examples, the output will be low if the probability is low.
As
opposed to the softmax, the outputs will no longer sum to one.
However, since we
only use the internal vector representation of the words, this is not an issue.
Technically, the output of the neurons is seen as the probability that the word
w
and context
s
come from the training set
T
, noted as
P
(
T
= 1
|w, s
).
Context words
from noise examples are assumed to be from a noise set
T
0
and will be noted with an
apostrophe.
The probability that the data is from the noise set is then
P
(
T
= 0
|w, s
0
)
or 1
− P
(
T
= 1
|w, s
0
).
The optimization objective now becomes:
18
3.2.
Skip-gram neural network
argmax
θ
log

P
(
T
= 1
|w, s
)
Y
s
0
∈T
0
p
(
T
= 0
|w, s
0
)

=
argmax
θ
log

P
(
T
= 1
|w, s
)
Y
s
0
∈T
0
1
− p
(
T
= 1
|w, s
0
)

=
argmax
θ
log

P
(
T
= 1
|w, s
)

+
X
s
0
∈T
0
log

1
− P
(
T
= 1
|w, s
0
)

(3.14)
Because of the one-hot coding of the input words, the output of the hidden layer
is the column of the
V
in
matrix corresponding to the input word.
The column can
be seen as the vector representation of the word and will be denoted as
v
w
.
Similarly,
for the output probability of one proposed context word, only one row of the
V
out
matrix is used, which will be referred to as the context vector representation of that
word
v
s
.
Now the probability of
P
(
T
= 1
|w, s
), with a sigmoid output function
S
,
can easily be written as:
P
(
T
= 1
|w, s
) =
1
1 +
e
−
v
w
v
s
(3.15)
and this can be used in the objective function:
argmax
θ
log

1
1 +
e
−
v
w
v
s

+
X
s
0
∈T
0
log

1
−
1
1 +
e
−
v
w
v
s
0

=
argmax
θ
log

1
1 +
e
−
v
w
v
s

+
X
s
0
∈T
0
log

1
1 +
e
+v
w
v
s
0

=
argmax
θ
log

S
(v
w
v
s
)

+
X
s
0
∈T
0
log

S
(
−
v
w
v
s
0
)

(3.16)
The gradient of this objective function determines the step towards a minimum.
This gives for v
w
:
∂
∂
v
w
log

S
(v
w
v
s
)

+
X
s
0
∈T
0
log

S
(
−
v
w
v
s
0
)

=v
s

1
− S
(v
w
v
s
)

+
X
s
0
∈T
0
v
s
0

S
(
−
v
w
v
s
)
−
1

=v
s

1
− S
(v
w
v
s
)

+
X
s
0
∈T
0
v
s
0

− S
(v
w
v
s
)

(3.17)
An equivalent derivation can be made for the
v
c
weights.
Equivalent with formula
3.11, the adaptation with learning rate
α
becomes:
∆v
w
=
α
v
s

1
− S
(v
w
v
s
)

+
X
s
0
∈T
0
v
s
0

− S
(v
w
v
s
)

!
(3.18)
19
3.
Neural Network-based Word Similarity
Still one problem should be addressed now.
Words that naturally occur more
frequently in the training data, will tend to have higher probabilities because they
appear more in the positive
S
(
v
w
v
s
) term.
To counter this effect, the noise contexts
are drawn in relation to their unigram probabilities.
Heuristically, it has been shown
in [
14
] that a noise data distribution equal to the unigram probability raised to the
3/4 power gives the best results:
p
(
s
0
|w
)
∼ p
(
s
0
)
3
4
(3.19)
Hierarchical softmax
Another way to reduce the computational requirements is the hierarchical softmax,
as proposed in [
18
].
The computational complexity of the softmax output layer is
due to the normalization term in the denominator, summing over all output neurons.
In the hierarchical softmax, the output neurons are grouped and the normalization is
only needed over the members of the groups.
To achieve this, every word context
s
is
grouped in one class
Class
=
G
(
s
).
The neural network tries to predict the context
s
based on the input word
w
, written as
P
(
s|w
).
This equation is then expanded to:
P
(
s|w
) =
X
i
P
(
s, Class
=
i|w
)
(3.20)
Since every context only belongs to one class, only this class can have a non-zero
probability.
The sum in the equation above simplifies to:
P
(
s|w
) =
P
(
s, Class
=
G
(
s
)
|w
)
=
P
(
s|Class
=
G
(
s
)
, w
)
P
(
Class
=
G
(
s
)
|w
)
(3.21)
Now, two probabilities and two normalizations have to be calculated.
However,
the amount of terms in these normalizations has decreased to the square root of the
amount of words in the vocabulary.
For example, if there are 10 000 output neurons
and 100 evenly distributed classes, this results in 2 normalizations over 100 terms,
instead of 1 normalization over 10 000 terms.
This reasoning can be extended by introducing more layers of
classes,
which
will again reduce the terms in the normalization sum.
Ultimately, there are only 2
elements per class, which can be represented by a binary tree.
Every context word
s
is represented by a specific path through the tree,
and every node in the tree
contributes to the probability of that word.
This way, every context word can be
given a unique binary path representation
bin
(
s
) =
b
1
..b
n
,
with
b
= 1 or
b
=
−
1.
Similar as before, the expansion can be derived:
P
(
s|w
) =
P
(
s|b
1
..b
n
, w
)
n
Y
j
=1
P
(
b
j
|b
1
..b
j−
1
, w
)
(3.22)
20
3.2.
Skip-gram neural network
Since every context word has one unique binary representation, the probability of
the context word given its representation equals one.
P
(
s|w
) = 1
·
n
Y
j
=1
P
(
b
j
|b
1
..b
j−
1
, w
)
(3.23)
The specific binary representation of node
j
in the context path will be written
as
n
s
(
j
) =
b
1
..b
j−
1
.
As in the previous subsection, the input word can be represented
by its word vector
v
w
and similarly, for every node
n
, a vector
v
n
is introduced.
The
probability of a choice in a node can now be calculated with the sigmoid activation
function.
P
(
b
j
= 1
|n
s
(
j
)
, w
) =
S
(v
w
v
n
s
(
j
)
)
(3.24)
P
(
b
j
=
−
1
|n
s
(
j
)
, w
) = 1
− P
(
b
j
= 1
|n
s
(
j
)
, w
)
= 1
− S
(v
w
v
n
s
(
j
)
)
=
S
(
−
v
w
v
n
s
(
j
)
)
(3.25)
Equation 3.24 and equation 3.25 can be combined into:
P
(
b
j
|n
s
(
j
)
, w
) =
S
(
b
j
v
w
v
n
s
(
j
)
)
(3.26)
and the full probability can now be written as:
P
(
s|w
) =
n
Y
j
=1
S
(
b
j
v
w
v
n
s
(
j
)
)
(3.27)
Before training the network,
a tree has to be constructed.
It has been shown
that a binary Huffman tree based on the occurrence counts of the words gives good
results [18].
The training of the hierarchical softmax aims at maximizing the log probability
of the observed context given the word.
The gradient of the log probability gives the
steepest descent direction to a local minimum:
∂
∂
v
w
log

P
(
s|w
)

=
∂
∂
v
w
log

n
Y
j
=1
S
(
b
j
v
w
v
n
s
(
j
)
)

=
n
X
j
=1
∂
∂
v
w
log

S
(
b
j
v
w
v
n
s
(
j
)
)

=
n
X
j
=1
b
j
v
n
s
(
j
)

1
− S
(
b
j
v
w
v
n
s
(
j
)
)

(3.28)
For
v
n
s
(
j
)
, an equivalent derivation can be made.
With the gradient, a step can
now be taken in the direction of steepest descent, equivalent to equation 3.11.
∆v
w
=
α
n
X
j
=1
b
j
v
n
s
(
j
)

1
− S
(
b
j
v
w
v
n
s
(
j
)
)

(3.29)
21
3.
Neural Network-based Word Similarity
According to [
14
], the results of negative sampling and hierarchical softmax are in
general comparable, but negative sampling sometimes performs slightly better on the
semantic and syntactic tasks.
The training time of both networks are comparable.
Training example selection
A training example for the skip-gram neural
network consists of
two words,
the
target word at the input and one of the context words at the output.
The window
size parameter
k
is one of the settings for the training.
This parameter determines
the window size, which is 2
k
+ 1, with a typical value of 5 to capture broad topical
information, or a smaller value for more focused information about the input word
[
12
].
Context words are taken from the
k
words before and the
k
words after the
target word.
To give more importance to close words, a reduced window is used.
For
every target word a random number between 1 and
k
, with a uniform distribution,
is used as real window size, resulting in more close-by context words in the training
data, and thus more influence from the nearest neighbouring words.
The most common words,
such as articles,
occur so frequently that they are
in the context of nearly every word.
Therefore, they usually contribute less useful
information.
To give more importance to the less common, but more informative
words, the training data can be subsampled, a process in which words are randomly
discarded with a probability depending on the frequency of their occurrences.
A
subsampling probability, that aggressively discards words with high frequencies, while
preserving less frequent words, was heuristically proposed in [14]:
P
discard
(
w
i
) = 1
−
s
t
f
(
w
i
)
(3.30)
with
t
a threshold,
typically around 10
−
5
and
f
(
w
) the frequency of word
w
.
However, in the actual implementation of word2vec in C [
14
] and Python [
22
], the
following formula is used:
P
discard
(
w
i
) = 1
−

s
t
f
(
w
i
)
+
t
f
(
w
i
)

(3.31)
3.2.3
Word representations
The final goal of the skip-gram neural network is to deliver good word representations.
As described in subsection 3.2.2, the columns of the
V
hidden
matrix can be seen as a
vector representation of the corresponding word, due to the one-hot coding used as
input.
The cosine similarity of the vectors gives a good indication of the similarity
of their corresponding words.
The higher the cosine similarity value, the higher the
word similarity.
cos
(v
1
,
v
2
) =
v
1
·
v
2
|
v
1
||
v
2
|
(3.32)
22
3.3.
Conclusion
Leuven
zomer
Gates
KU
herfst
Bill
Leuvense
lente
Microsoft-topman
Gent
winter
Microsoft-stichter
Kortrijk
week
Microsoft-baas
Hasselt
voorjaar
Ballmer
Table 3.1:
The five most similar words to reference words according to word2vec
An example of these similarities is given in table 3.1, where the five most similar
words to
Leuven
,
zomer
(summer) and
Gates
are given,
based on the skip-gram
model and trained on the training corpus used in this thesis as described in section
6.1.1.
In case of the city
Leuven
, the most similar words are cities with universities,
words related to the university of Leuven and the adjective
Leuvense
(from Leuven),
in case of
the season
zomer
,
the results are all
seasons or similar words,
and in
case of
Gates
, the results are synonyms for CEO of Microsoft, or names of CEOs
of Microsoft.
It is interesting to note the similarity between
Leuven
and
KU
and
between
Gates
and
Bill
, because they frequently appear in the same context as a
name:
KU Leuven
and
Bill Gates
.
Word2vec vectors also have the possibility to perform analogy reasoning, such as
determining the plural of a word (
apple
:
apples
), changing the gender (
king
:
queen
)
and determining the capital of a country (
F rance
:
P aris
), by giving an example
analogy.
The vector offsets between pairs forming a certain analogy, are similar to
each other (
man − woman ≈ king − queen
).
Thus, analogies can be performed by
adding the analogy vector to the given word, after which the result is compared to
all the word vectors, to find the most similar.
woman
+ (
king − man
)
≈ queen
(3.33)
3.3
Conclusion
Neural networks are a state-of-the-art technique in machine learning.
They consist of
a large interconnection of simple processing units known as neurons.
These networks
can be trained by providing training examples, determining the error and adapting
in the direction of steepest descent according to a cost function.
Word2vec is a family of neural networks that provide vector representations for
words.
One of the best performing models, known as skip-gram, is trained to predict
the surrounding words of
a target input word in a sentence.
Since training the
standard skip-gram model is computationally demanding, two alternative training
methods are used in practice:
negative sampling and the hierarchical softmax.
In
the end, the weights of the hidden layers determine the vector representations of the
words.
The cosine similarity of these vectors give a good indication of the similarity
of their corresponding words.
23
Chapter 4
Dependency-based model
The similarity determined by the skip-gram model is based on both semantic and
syntactic properties.
In the class-based approach, words are replaced by their class
in the history.
Therefore one could reason that class members should have a similar
syntactic function in the sentence, which would mean that the syntactic part of the
similarity is more important than the semantic part.
To achieve more syntactical
similarity vectors, two modifications to the skip-gram model are discussed in section
4.1 as described in [
12
].
The implementation of this model is discussed in section 4.2.
The data sparsity problem of this approach is discussed in section 4.3.
4.1
Overview
Two adaptations to the skip-gram model are proposed, in order to generate more
syntactic word embeddings.
Firstly,
in the skip-gram model,
the context size is
independent to the context information.
This can lead to the inclusion of context
words that have no direct functional dependence, such as words in dependent clauses,
or the exclusion of words with a direct functional dependence, but that are too far
from the target word.
To get more accurate results, only words that have a direct
impact should be used as context words.
Secondly,
extra information about the
syntactic structure can be added, by including tags about the function of a context
word in relation to the targeted word.
These two improvements lead to a new word/context model.
First, the training
sentences are parsed into a tree structure, with at each node a phrase, which is a
group of one or more words that make up a grammatical unit.
In each subtree, one
phrase is selected as head
hd
.
The other phrases are modifiers
m
, each with a specific
dependency relation
dr
to the head.
A graphical representation of this tree can be
seen in figure 4.1.
For a certain target word
w
, the context will include the head
hd
of its phrase,
complemented with the inverse dependency relation
dr
−
1
of the target word towards
its head.
If the word is the head of a phrase, the context will also include all of its
modifiers
m
1
..m
k
, and for every modifier, the respective dependency relation
dr
1
..dr
k
25
4.
Dependency-based model
Figure 4.1:
Dependency parsed tree, with red dependency relations and black POS
tags [26]
is added.
In the end,
this leads to the context
hd
_
dr
−
1
for the relation with the
head and
m
1
_
dr
1
, ..., m
k
_
dr
k
for the relation with the modifiers of word
w
.
In the example of figure 4.1, the contexts of the word "weer" would be:
Het
_
det
,
mooie
_
mod
and
maakt
_
subj
−
1
.
All possible word context pairs are represented in
figure 4.2, where an arrow symbolizes a dependency relation, and where the inverse
of an arrow gives an inverse dependency relation.
Figure 4.2:
All word context pairs that can be extracted from the example sentence
A head or modifier can be a phrase consisting of multiple words, in which case
there is no one word representation for that phrase.
In this case,
the used word
representation is the head of the underlying phrase.
This can be done recursively, so
if the head of the phrase still consists of multiple words, the head of its underlying
phrase is taken.
The dependency relation however,
will
stay that of the original
modifier.
4.2
Implementation
Alpino, a dependency parser for Dutch [
26
], can be used to generate a dependency
tree for every sentence.
This tree is annotated with information such as dependency
and part of speech tags for every word or combination of words.
The tree is formatted
as an XML file, as in figure 4.1.
26
4.3.
Data sparsity
For each level/subtree in the tree, there is one head, recognizable by one of the
dependency tags:
hd,
rhd,
whd,
cmp,
crd,
dlink.
The head of
that part of
the
sentence is considered as basis, and all sibling nodes are modifiers with their own
dependency tags.
With this information, the context for the head can be constructed
as
m
1
_
dr
1
...m
k
_
dr
k
,
and all
the inverse relations
h
_
dr
−
1
i
can be added to the
context of the respective modifiers
i
.
Sometimes, when searching for the word representations, the children of a node
contain no head, because multiple children are considered equally important.
This
is the case when a multi word unit or a conjunction is present, with the respective
tags MWP or CNJ. In this case, all the words with these tags are considered as head
nodes.
One of the word representations for a modifier, could be a preposition, tagged
with VZ, in which case the context information would be limited when only using
the word representation of the preposition.
To make sure that all the information is
used, the modifier of the preposition is also considered as a context.
When all
the word-context pairs are assembled,
the skip-gram model
can be
trained.
A minor adaptation to the output layer size is needed to train the network.
The input layer of
the neural
network will
stay the same size,
equal
to the size
of
the vocabulary,
but the output layer will
be much bigger due to the possible
combinations of context words with labels.
Both negative sampling and hierarchical
softmax training algorithms are available for training.
Leuven
lente
Gates
KU
herfst
Bill
Gent
lente
Hughes
Kortrijk
winter
Edwards
Hasselt
najaar
Lewis
Luik
kerstperiode
Harrison
Table 4.1:
The five most similar words to reference words according to the dependency-
based model
The most similar words according to the dependency-based mosel are shown in
table 4.1.
These results can be compared with the results of the skip-gram network
in table 3.1.
Leuven
and
lente
have comparable results, except for the fact that the
adjective
Leuvense
is no longer similar to
Leuven
because of their different syntactic
function.
In case of the word
Gates
, the most similar words are now other names,
which have similar syntactic functions, instead of the more semantically related words
to Microsoft in the skip-gram model.
4.3
Data sparsity
A disadvantage of the dependency-based model
is that it suffers more from data
sparsity:
the amount of extracted training examples will
be smaller than for the
skip-gram, while the amount of needed training data is bigger.
27
4.
Dependency-based model
In the standard skip-gram model as explained in 3.2.2, the amount of training
data for one word is the count of the word multiplied by the average amount of words
within the window size.
This includes both useful context words and useless words
that do not contribute much information such as articles, that are in the vicinity of
nearly every word and carry less information.
In the dependency based model, a single occurrence of a word results in only one
training example, if only one dependency relation is available, which is for example
the case when the word is not the head of a phrase.
Some occurrences don’t have
any training examples due to an inaccurate parsing.
There will thus be less training
examples available for training.
This can be illustrated when looking at the training
examples extracted from the corpus of "De Standaard",
which is used as training
corpus in this thesis as described in section 6.1.1.
In this case, a large part of the
vocabulary is affected by data sparsity:
295 words, or 0
.
145% of the vocabulary, have
no training example, and thus no vector representation.
Other words have only a
limited training, which can lead to bad word vector representations.
A summary of
the amount of words, that have only a certain amount of training examples is given
in table 4.2:
train ex
words
percentage
0
295
0.1475%
1-5
1195
0.5975%
6-10
9405
4.7025%
11-20
31261
15.6305%
Table 4.2:
Amount of words that have only a certain amount of training examples
On the other hand, since there are more possible contexts, there is a bigger need
for data.
This will make data sparsity a bigger problem for the dependency-based
model,
than for the standard skip-gram model,
which could lead to worse word
similarities for infrequent words.
The effect will be slightly countered because word
context pairs that are used as training examples in the dependency-based model, will
carry more information than in the standard model, which alleviates data sparsity.
Data sparsity could be a problem when clustering infrequent words, especially when
the uncommon words clustering is used as described in section 5.4.
4.4
Conclusion
Since the class-based approach looks at similar contexts, defined by the classes, it
is beneficial
if words in these classes have the same function in a sentence.
The
dependency-based model
is introduced,
based on the skip-gram model,
in which
modifications are done to generate more syntactic similarity vectors.
The disadvantage
of this approach is the reduction in training examples and the bigger size of the
neural network, which leads to more data sparsity and thus worse representations
for infrequent words.
28
Chapter 5
Word Clustering
In chapter 2, the class-based language model has been discussed, but a method to
create good word classes has not yet been described.
Chapter 3 described a way to
measure word similarities.
In this chapter, these word similarities are used to make
a classification algorithm for a class-based language model.
Usually classes are generated with perplexity-based clustering algorithms, which
is discussed in section 5.1.
However,
class generation is also possible with other
techniques.
Section 5.2 discusses different clustering algorithms that make use of
similarity vectors.
Also, a method to incorporate extra information is proposed.
The
next sections describe models for adding this extra information:
section 5.3 uses part
of speech tags, section 5.4 uses the frequency of words as extra information and in
section 5.5, acoustic similarity is modelled and integrated in the classification.
5.1
Perplexity clustering
Perplexity clustering is a method to assign words to classes, described in [
1
].
In this
method, clustering is done in order to maximize the likelihood
L
, which is defined as
the log probability of a training corpus
T
as modelled by the class-based N-gram
approach.
The likelihood can thus be constructed using the class-based language
model as defined in formula 2.20:
L
=
log

P
(
w
1
w
2
...w
K
)

=
log

K
Y
k
=1
P
(
w
k
|c
k
)
P
(
c
k
|c
k−
1
k−N
+1
)

=
K
X
k
=1
log

P
(
w
k
|c
k
)
P
(
c
k
|c
k−
1
k−N
+1
)

(5.1)
This sum can be simplified by combining the probabilities involving the same
word
w
k
and class context
c
k−
1
k−N
+1
:
29
5.
Word Clustering
L
=
X
w
k
,c
k−
1
k−N
+1
∈T
C
(
w
k
, c
k−
1
k−N
+1
)
log

P
(
w
k
|c
k
)
P
(
c
k
|c
k−
1
k−N
+1
)

(5.2)
in which
C
(
w
k
, c
k−
1
k−N
+1
) is the count of the occurrences of
c
k−
1
k−N
+1
followed by
w
k
in the corpus.
The logarithm can be split into a word dependent and a class dependent term:
L
=
X
w
k
,c
k−
1
k−N
+1
∈T
C
(
w
k
, c
k−
1
k−N
+1
)
log

P
(
w
k
|c
k
)
P
(
c
k
)
P
(
c
k
|c
k−
1
k−N
+1
)
P
(
c
k
)

=
X
w
k
,c
k−
1
k−N
+1
∈T
C
(
w
k
, c
k−
1
k−N
+1
)
log

P
(
w
k
|c
k
)
P
(
c
k
)

+
X
w
k
,c
k−
1
k−N
+1
∈T
C
(
w
k
, c
k−
1
k−N
+1
)
log

P
(
c
k
|c
k−
1
k−N
+1
)
P
(
c
k
)

=
X
w
k
∈T
C
(
w
k
)
log P
(
w
k
)
(5.3)
+
X
c
k
,c
k−
1
k−N
+1
∈T
C
(
c
k
, c
k−
1
k−N
+1
)
log
P
(
c
k
, c
k−
1
k−N
+1
)
P
(
c
k
)
P
(
c
k−
1
k−N
+1
)
!
(5.4)
The first term 5.3 is the unigram entropy of the corpus, which is independent of
the classes and which can thus be ignored.
The best classification assigns words to
classes such that the second term 5.4 of the likelihood is maximized:
classification =
argmax
classification
X
c
k
,c
k−
1
k−N
+1
∈T
C
(
c
k
, c
k−
1
k−N
+1
)
log
P
(
c
k
, c
k−
1
k−N
+1
)
P
(
c
k
)
P
(
c
k−
1
k−N
+1
)
!
(5.5)
To find the global
optimum,
all
possible class combinations would have to be
considered, which is computationally infeasible.
Several heuristic approaches have
been described such as agglomerative clustering [
1
] and exchange clustering [
13
] [
24
].
The described algorithms work with a 2-gram approximation.
3-gram versions are
also available,
but have worse computational
requirements and are therefore not
recommended for large data sets.
Agglomerative clustering is a bottom-up method.
At the start,
each word is
defined as a class.
In each step, two classes are greedily combined to maximize the
likelihood.
This process is repeated until the required number of classes is reached.
O
((
V − i
)
2
/
2) possible class merges are investigated in the
i
th
step, each requiring
to sum over the
O
((
V − i
)
2
) terms in equation 5.5,
giving a total
computational
complexity of
O
(
V
5
) [
1
].
Two adaptations to this algorithm are proposed in [
1
], in
30
5.2.
Word similarity clustering
which information is reused over the different calculations, leading to a complexity
of
O
(
V
3
) and
O
(
V Cl
2
), in which
Cl
is the amount of classes.
Exchange clustering starts with the assignment of the
Cl −
1 most common words
to
Cl −
1 classes.
The rest of the words are clustered in the last class.
Each iteration
step, a word is greedily swapped from one class to another, so that the likelihood in
that step is maximized.
The swapping is performed until a certain stopping criterion
is reached, such as a local optimum or more commonly a limited number of iterations.
This method has a complexity of
O

I · Cl ·
(
Bi
+
V
)

[
24
],
with
I
the number of
iterations and
Bi
the number of distinct bigrams in the training corpus, which is
always smaller than
V
2
.
An adaptation of the exchange algorithm can be used to
perform distributed computing.
It can be concluded that even the best perplexity
clustering methods do not scale well to large amounts of data.
5.2
Word similarity clustering
The class-based approach requires similar words to be grouped into classes.
These
classes can be generated using a global criterion, such as perplexity, or with a local
criterion, defined by a distance metric between individual words.
Skip-gram word
embeddings can be used to generate such a metric.
As discussed in subsection 3.2.3,
the cosine product of these word vectors is a good measure for the similarity.
To use
this similarity measure in the clustering, the cosine distance will be used, in which
the distance between two word vectors
v
1
and
v
2
is calculated as follows:
dist
cos
(v
1
,
v
2
) = 1
−
v
1
·
v
2
|
v
1
||
v
2
|
(5.6)
Two standard clustering algorithms are proposed:
k-means clustering,
which
is a commonly used algorithm for clustering of
skip-gram word vectors [
14
]
and
agglomerative hierarchical
clustering.
Finally,
to overcome certain limitations,
a
hybrid version is proposed.
In k-means clustering, word vectors are assigned to the cluster with the closest
cluster mean.
To get the best position of these cluster means, an iterative algorithm
is followed.
First, the cluster means are placed at random.
Then all the data points
are assigned to the nearest cluster.
Now the cluster mean is updated as the mean of
all data assigned to its cluster.
The last two steps are repeated until convergence.
The advantage of k-means clustering is a good scaling for big data sets,
in time
O
(
V · Cl · I
) and memory complexity.
The disadvantage is that k-means clustering
only uses word similarities as information,
while other information,
such as word
frequency,
acoustic word similarity and syntactic information could improve the
clustering.
To overcome this disadvantage, we propose the usage of hierarchical agglomerative
clustering.
With this clustering technique, it will be possible to incorporate extra
information on top of the word similarities.
Every iteration,
the two classes that
are closest, determined by the cosine similarity, are clustered.
The distance between
31
5.
Word Clustering
classes (frequently consisting of multiple words) is determined by the linkage criterion.
Some commonly used criteria are maximum,
average and minimum linkage.
In
maximum and minimum linkage, respectively the furthest and nearest points are
used as distance measure.
In case of average linkage,
the average distance of the
points is chosen as reference.
Minimum linkage can lead to long-stretched classes
due to so called chaining.
In this case, clusters are linked together because of two
close points, even though many other elements are far apart.
On the other hand,
the maximum linkage criterion prefers compact classes.
Average clustering finds a
middle ground between maximum and minimum clustering.
One can expect that
in this application, compact classes are preferred to long stretched classes, so that
points on the outsides of a class are still closely related.
To perform agglomerative clustering,
the distances between all
the points are
calculated and stored in an upper triangular matrix, which is the starting point for
the clustering.
Extra information can now be added by adapting the elements of this
matrix:
distances between words can be enlarged or reduced based on extra features.
This property will be used in later sections, to improve the clustering.
The disadvantage of agglomerative clustering is worse memory scaling for large
data sets in comparison to k-means clustering.
The size of the distance matrix is
O
(
V
2
),
with
V
the size of the vocabulary,
which can lead to large matrices.
The
time complexity of the agglomerative clustering is
O
(
V
2
) in the implementation of
[19].
To overcome the bad memory scaling, but keep the advantage of matrix adaptation,
we introduce a hybrid method.
In this model, the data is first clustered with k-means
in just a few sets.
In each of these sets, agglomerative clustering is used to generate
the classes.
The data sets used in the agglomerative clustering are therefore smaller.
The clustering however could be slightly suboptimal,
because classes can not be
formed across the set boundaries, even if words have good similarity.
5.3
Part of speech tagging
While clustering words with the goal of using a class-based approach as language
model, words that are grouped together will be regarded as equal in the history
h
.
One can therefore assume that it is important that the function of the clustered
words in sentences is the same.
The similarity of the skip-gram vectors are based on
both syntactic and semantic properties, but often the semantic similarity is dominant,
which can lead to clusters of words that have the same semantic meaning, but where
the syntactic function differs.
Such clustering could be be harmful for the language
model.
Therefore, it makes sense to only cluster words if they belong to the same part
of speech, which can be reformulated as a double clustering analogue to the hybrid
clustering in section 5.2:
first, the data is partitioned according to its part of speech
(POS) tag into different sets, then the data is clustered within these sets.
A problem can arise when dealing with homographs:
words that have the same
orthography,
but have a different meaning.
Some of these homographs will
have
32
5.4.
Uncommon words
different POS tags,
such as the word bear,
which can be a verb (to carry) or a
noun (the animal).
However,
in the class-based model,
a hard decision is needed.
Therefore, these words are sorted in the set of their most common POS tag.
Other
alternatives are also possible, such as not clustering words that have ambiguous POS
tags, or only clustering them when the frequency of the most common POS tag is
above a certain threshold.
POS
Vocabulary
Corpus
Noun
46.7 %
20.2 %
Special Token
33.2 %
7.9 %
Verb
11.6 %
20.3 %
Adjective
7.8 %
7.7 %
Adverb
0.4 %
5.89 %
Pronoun
0.1 %
8.87 %
Preposition
< 0.1 %
14.04 %
Article
< 0.1 %
11.2 %
Others
0.2 %
8.9 %
Table 5.1:
Occurrences of POS in the vocabulary and the corpus as tagged by FROG
[25]
In table 5.1,
the relative occurrences of
words in the vocabulary and in the
training corpus used in this thesis, as described in subsection 6.1.1, are given.
The
vocabulary consists of the words that appear at least 5 times in the corpus.
The
POS tag in the vocabulary is the most common POS for that word as described in
the previous paragraph.
The Special Token POS tag consists mainly of proper nouns
and words with this tag can thus in many circumstances be considered as nouns.
The category Others include numerals, conjunctions and interjections.
The amount of classes in which every POS set is partitioned, has to be divided
over the different sets.
This division can be done proportionally to the occurrences
of the POS tags in the vocabulary.
However, as can be seen in table 5.1, some sets
(e.g.
prepositions and articles) have little unique words, but emerge frequently in
the corpus.
These words will be less susceptible to the data sparsity problem.
Other
classes (e.g.
nouns and special tokens) contain a lot of different words, but do not
have a proportionate amount of training data available.
One could also reason that
prepositions and articles are more discriminative regarding which words surround
them.
It could therefore make sense to increase the amount of clustering of nouns
and special tokens with respect to the clustering of prepositions and articles.
5.4
Uncommon words
The data sparsity problem manifests itself especially with uncommon words:
because
the available training examples are rare, the probability estimates will not be accurate.
Thus, one could assume that uncommon words should be clustered more often, to
increase the number of their class-based N-gram counts and thus to improve their
33
5.
Word Clustering
probability estimates.
On the other hand,
their vectors are less reliable.
When
clustering based on the similarity vectors, there is no distinction between common
and uncommon words.
We introduce three approaches to give priority for clustering
uncommon words:
smooth adaptation,
simple threshold clustering and two-high
threshold clustering.
5.4.1
Smooth adaptation
To make a smooth adaptation,
the number of
word occurrences in the training
corpus are counted.
This count
C
(
w
) is then transformed by a function to obtain the
adaptation coefficient for that word.
A proposed function is a power function, where
the count is raised to a certain power
a
.
Finally, every distance in the agglomerative
clustering matrix is adapted by multiplying it with the adaptation coefficients of the
two words that define the distance.
dist
uncommon
(v
w
1
,
v
w
2
) =
C
(
w
1
)
a
C
(
w
2
)
a
dist
cos
(v
w
1
,
v
w
2
)
(5.7)
Smooth adaptation will favour the clustering of uncommon words, if the power is
high enough.
On the other hand, it is possible that words with good similarity are
not clustered because of the disturbance in the distance matrix.
This disturbance
might occur if the uncommon word adaptation favours infrequent words too strongly
and the adaptation of the matrix becomes dominant over the word similarity.
5.4.2
Simple threshold
To avoid the disturbance in the distance matrix,
the threshold approach can be
considered.
In this approach,
only the words with a count lower than a certain
threshold are clustered, which will not disturb the distances.
However, words are
not clustered if the count of one of the words exceeds the threshold, even though the
similarity is high and the clustering could be beneficial.
This method reduces the
amount of words that can be clustered, which leads to a smaller distance matrix and
a lower memory usage.
5.4.3
Two-high threshold
Words that are frequent will lead to good probability estimates, hence require less
clustering.
However, words with low frequency could benefit from the information of
similar words with high frequency.
This information is rarely used when performing
smooth adaptation and not used when using a simple threshold.
To incorporate this
information, a third technique is proposed, in which words are only clustered when at
least one of their occurrence counts is under a certain frequency.
The disadvantage
of this technique is that the probability estimates of the frequent words could be
distorted, due to their clustering with infrequent words.
34
5.5.
Acoustic word similarity
5.5
Acoustic word similarity
Words that sound similar,
are easily confused by the acoustic model
of a speech
recognizer.
To be able to recognize the correct word, the language model becomes the
dominant factor.
One could suspect that words with acoustic similarity should not
be clustered together, to maximize their distinctiveness.
In subsection 5.5.1, a model
to calculate the acoustic similarity of certain words is introduced.
In subsection 5.5.2,
the distance matrix is adapted to incorporate the acoustic information.
5.5.1
Acoustic similarity model
In this subsection,
a model
for acoustic word similarity will
be derived.
For this
model, it is presumed that example utterances for every word are not available.
The
model will be based on the phonetic representations of the words, as used in 1.1.2,
and phone confusion probabilities, as described in [
20
].
These probabilities are an
indication of the probability of confusion between two phones.
It should be noted
that the phone confusion probabilities are not symmetric.
The proposed measure is an adaptation of the Levenshtein distance as discussed
in section 2.4, and as described in [
27
].
Analogue to the Levenshtein distance, we
define three possible dissimilarities:
the substitution of a phone by another phone,
the deletion of a phone and the insertion of a phone.
These dissimilarities will have
an error coefficient equal to the corresponding phone confusion probability, where
the deletion and insertion have a special phone confusion probability.
Because the acoustic similarity will
be used as a distance measure,
it should
be commutative.
It can be seen that,
if
the phone confusion probabilities are
commutative, the similarity will also be commutative.
Commutativity is achieved by
taking the geometric mean of the phone confusion and its commuted counterpart.
The acoustic similarity between words can be defined as the smallest error distance
between their phone representations.
To calculate this distance between the words
w
and
v
, with lengths
l
(
w
) and
l
(
v
), they are replaced by their phone representation
p
w
1
..p
w
l
(
w
)
and
p
v
1
..p
v
l
(
v
)
.
Then a matrix
E
is initialized with size [
l
(
w
) + 1][
l
(
v
) + 1].
Every element
e
i,j
in the matrix represents the distance between prefixes
p
w
1
..p
w
j−
1
and
p
v
1
..p
v
i−
1
.
The first element
e
1
,
1
equals zero.
The rest of the cells are filled row
by row, using dynamic programming and the following rule:
e
i,j
=
min







e
i−
1
,j
+
insert
(
v
i−
1
)
e
i,j−
1
+
delete
(
w
j−
1
)
e
i−
1
,j−
1
+
substitute
(
w
j−
1
, v
i−
1
)
(5.8)
where the substitution error equals zero when the phones
p
w
j−
1
and
p
v
i−
1
are the
same.
When the whole matrix is filled, the lower right cell holds the similarity value.
This matrix needs to be calculated for every word pair,
in which each word
has multiple pronunciations, giving
O
(
V
2
·
(
µ
2
pron
+
σ
2
pron
)) acoustic word similarity
calculations, with V the vocabulary size and
µ
pron
and
σ
pron
respectively the mean
35
5.
Word Clustering
and standard deviation of the amount of pronunciations per word.
Luckily, these
calculations have no dependence on each other,
and can thus run in parallel
on
multiple cores.
In the end, the results can be easily combined in one acoustic distance
matrix.
To reduce the amount of calculations, an approximation is made by using
only one possible pronunciation of each word, which leads to a complexity of
O
(
V
2
).
Table 5.2 provides an overview of the most acoustically similar words to the Dutch
words "Leuven", "zomer" and "olifant", according to this model.
Words with uppercase
first letter that have their corresponding lowercase version in the most similar list,
are filtered in this table.
Leuven
zomer
olifant
leugen
zomert
olifanten
leven
Sommer
olifantje
gleuven
zomers
Olivan
sleuven
Homer
olievat
leunen
stomer
olieland
Table 5.2:
The five most acoustically similar words to reference words according to
the acoustic word similarity model
5.5.2
Acoustic similarity adaptation
To incorporate acoustic similarity,
the elements in the distance matrix will
be
multiplied with an acoustic adaptation coefficient.
This coefficient depends on the
value of the acoustic similarity.
Multiple functions can be used to transform the
acoustic similarity to the acoustic adaptation coefficient.
A linear function can be
used, but if words do not sound the same, it does not matter how low their acoustic
similarity is.
In this case, an acoustic adaptation would only disturb the distances.
For this reason, a negative exponential function with saturation for low similarities
is proposed as a first technique to incorporate acoustic similarity.
The adaptation
function is shown in formula 5.9 and a plot for several values of
a
and
b
is presented
in figure 5.1.
dist
ac
(v
w
1
,
v
w
2
) = (1 +
e
−a·sim
ac
(
w
1
,w
2
)
)
b
· dist
cos
(v
w
1
,
v
w
2
)
(5.9)
with adaptation parameters
a
and
b
and acoustic word similarity
sim
ac
.
Parameter
a
determines the different approach between acoustically similar and dissimilar words,
while the overall influence of the acoustic model in the language model is regulated
by parameter
b
.
To avoid disturbance in the distance matrix, analogue with section 5.4, threshold
adaptation is proposed as a second technique.
In this case, words are only clustered
if their acoustic similarity is less than a certain threshold.
36
5.6.
Conclusion
0
0
.
5
1
1
.
5
2
2
.
5
3
3
.
5
4
0
1
2
3
4
word similarity
acoustic adaptation coefficient
a
= 1
b
= 1
a
= 0
.
4
b
= 1
a
= 0
.
2
b
= 1
a
= 1
b
= 2
a
= 0
.
4
b
= 2
a
= 0
.
2
b
= 2
Figure 5.1:
Plot of the acoustic word adaptation with different values of parameter
a
and
b
5.6
Conclusion
The commonly used technique to obtain word classes is perplexity clustering, where
words are clustered so that the likelihood of a training corpus is maximized.
However,
other clustering methods, based on word similarities, are also possible.
Two clustering
methods are proposed:
k-means,
with good complexity scaling,
and hierarchical
clustering, which makes it possible to incorporate extra information.
A third hybrid
method is introduced to combine the advantages of both methods.
The extra information can be embedded in the hierarchical clustering by modifying
the distance matrix.
Three models have been proposed to incorporate information:
POS tagging, uncommon words clustering and acoustically word similarity clustering.
In POS tagging, syntactic information is added by only clustering words with the
same POS tag,
uncommon words clustering will
favour clustering words that are
infrequent and thus have a bad probability estimate, and acoustic words similarity
improves the distinction between acoustic similar words by not clustering them.
37
Chapter 6
Evaluation
In this chapter the models that were introduced in the previous chapters are evaluated.
All
models were trained on the same training data,
after which they were tested
for perplexity and word error rate.
Parameters were optimized on a development
set.
Section 6.1 describes the setup used in the experiments.
The following sections
evaluate the several
proposed techniques:
the different clustering techniques are
compared in section 6.2, POS clustering is evaluated in section 6.3, uncommon word
adaptation is assessed in section 6.4, followed by acoustic word similarity adaptation
in section 6.5 and finally dependency-based word similarities are evaluated in section
6.6.
The best techniques are bundled and compared in section 6.7.
6.1
Setup
6.1.1
Data
The text corpora are taken from the Mediargus archive between 1999 and 2004.
This archive includes normalized texts from the most important Flemish newspapers
and magazines.
The training corpus consists of the full
corpus of the newspaper
"De Standaard", which contains nearly 128 million words and more than 7.5 million
sentences.
This corpus is used to train both the N-gram language models and the
neural networks.
The evaluation corpus is a selected part of the Mediargus text from
the newspaper "De Morgen", which has approximately 6000 sentences and 100 000
words.
On this corpus, the perplexity is calculated as explained in section 2.4.
To calculate the word error rate, an evaluation and development set are selected
from the N-Best evaluation benchmark [
10
].
The fragments consist of 55 minutes of
development data and 1h55 of evaluation data in Southern Dutch as used in [
20
].
The WER is calculated using the ESAT N-Best speech recognizer [2].
6.1.2
Vocabulary
The training corpus consists of 1.1 million unique words, most of which only occur a
few times.
A vocabulary of 200 000 words, that can be used in the language model,
39
6.
Evaluation
is selected.
These are the words that occur most frequently in the training corpus.
The least frequent words in the vocabulary occur 8 times in the training corpus.
The use of a limited vocabulary results in out-of-vocabulary (OOV) words, which
are not in the language model and thus have no probability.
The OOV rate for the
different corpora can be found in table 6.1.
corpus
OOV-rate
Mediargus training
1
,
5%
Mediargus evaluation
1
,
9%
Nbest development
3
,
0%
Nbest evaluation
2
,
3%
Table 6.1:
The OOV rates for the different corpora
6.1.3
N-gram training
The N-gram language models are generated and evaluated with the SRILM (Stanford
Research Institute Language Modelling) toolkit [
23
], which provides a range of useful
programs and scripts.
The functionality of SRILM includes the generation of N-gram
language models, the generation of class based N-gram models and the calculation of
perplexities of language models.
Using SRILM, N-gram models can be generated with interpolation, back-off and
several smoothing techniques, including add-one, Kneser-Ney and Good-Turing.
A
restrictive vocabulary can be applied.
With a given partition of the vocabulary in
classes, the SRILM toolkit can generate a language model with the same smoothing
and interpolation techniques as a word-level N-gram model.
The toolkit can also
generate classes with agglomerative perplexity clustering as described in section
5.1.
However,
due to the time complexity of
O
(
V · Cl
2
) and the large size of the
vocabulary, this method is not feasible within a reasonable time.
Perplexity calculations are calculated using formula 2.22, which includes the begin
and end of sentence tags <s> and </s>.
Out-of-vocabulary words are ignored in the
calculation of the perplexity.
The baseline language model is chosen as a word-level
N-gram language model with the limited vocabulary described in subsection 6.1.2.
A
Katz back-off model is used to cope with zero probabilities, as described in section
2.2.2.
The results of the baseline model are shown in table 6.2.
All results in this
chapter are obtained using a model with N-gram size 5.
40
6.2.
Clustering
N-Gram size
Perplexity
WER evaluation
WER development
2
347
36,39 %
15,62 %
3
251
29,14 %
11,86 %
4
233
28.79 %
11.79 %
5
232
27,94 %
11,80 %
6
232
28.79 %
11.75 %
7
233
28.83 %
11,84 %
Table 6.2:
Perplexities and WERs for the baseline model on the text evaluation set,
and on the speech evaluation and development set
6.1.4
Skip-gram training
To train the skip-gram neural network, two implementations are used:
the original C
version [
14
] and a Python version [
22
].
These versions implement the same training
algorithm and have overall similar functionality, however the C version has built in
k-means clustering,
while the Python version is faster [
21
] and makes it easier to
extract the trained word vectors.
Both hierarchical softmax and negative sampling
are included as training algorithms.
Except when explicitly denoted,
the training iterates 5 times over the whole
training corpus, the window size is 5 words to the left and right and the training uses
no subsampling of frequent words, as described in subsection 3.2.2.
The resulting
word vectors consist of 300 elements.
When training with negative sampling, 5 noise
words are drawn.
6.2
Clustering
In this section, the different clustering algorithms from section 5.2, are compared.
The skip-gram model is trained with negative sampling, since it is often slightly better
than hierarchical softmax as discussed in section 3.2.2, and the standard settings
from section 6.1.4 are applied.
The vocabulary is divided in 10000 classes with
the proposed clustering algorithms:
k-means, hierarchical clustering with minimum,
average and maximum linkage and the hybrid method with maximum linkage, where
preclustering is done respectively in 3 (
K
3) and 10 (
K
10) classes.
The results are
compared to the baseline model in figure 6.1.
From these results we can see that all similarity-based clustering models perform
worse than the baseline.
The best results are obtained with the hierarchical clustering
with average linkage, closely followed by hierarchical clustering with maximum linkage.
The results of minimum linkage clustering are disastrous.
The hybrid method with a preclustering in 3 sets gives similar results as the
hierarchical clustering, and with a preclustering in 10 sets, the results are slightly
worse.
The RAM memory usage of the distance matrix decreased from 149 gigabyte
in the hierarchical
clustering to a maximum of 18 gigabyte in preclustering with
3 sets, and 3.3 gigabyte in preclustering with 10 sets.
The memory scaling of the
41
6.
Evaluation
Baseline
K-means
Minimum
Average
Maximum
K3
K10
0
200
400
600
800
232
316
703
284
279
286
309
300
300
300
300
300
300
300
Perplexity
Baseline
K-means
Minimum
Average
Maximum
K3
K10
25 %
30 %
35 %
40 %
45 %
30
30
30
30
30
30
30
27
.
9
32
.
2
40
.
2
29
.
8
30
.
2
30
.
2
30
.
8
WER
Figure 6.1:
Perplexities and WERs for the various clustering algorithms
hybrid method is close to the theoretical quadratic scaling, but not fully quadratic
because the preclustering does not guarantee subsets of equal size.
Similar results are obtained when choosing other settings, such as other values
for the amount of classes or the order of the N-grams.
However,
in some setups,
hierarchical clustering with maximum linkage gives better results than clustering
with average linkage, and in some setups, pre-clustering in 3 sets gives slightly worse
results as compared to hierarchical clustering.
Hierarchical clustering with maximum
linkage is used as the standard clustering method throughout the rest of this chapter.
6.3
POS clustering
In this section, the results of POS clustering from section 5.3 is investigated.
All
words are tagged with their most frequent POS tag using the Dutch POS tagger
FROG [
25
], and clustering happens only between words with the same POS tag.
In
a first model, denoted with POS, the amount of classes is approximately 10000, the
same amount of classes as in the reference model.
The division of the amount of
classes over the POS sets is proportional to their frequency in the vocabulary, with
the exception of sets that have less than 300 members, which are not clustered.
As previously discussed in section 5.3, nouns could be expected to be the best
POS group to cluster.
Therefore, the clustering in this thesis mostly focuses on nouns.
A second model, named POS-N, is trained in which the clustering only focuses on
nouns.
Special
tokens (SPEC) are included in the clustering because this group
consists mainly of nouns such as proper nouns or nouns that are uncommon.
The
resulting set amounts to nearly 80% of the vocabulary.
This set is clustered with
maximum hierarchical clustering in 7500 classes, approximately proportional to the
part of the vocabulary that is clustered and the amount of classes (10000) in the
maximum hierarchical clustering without POS tagging.
The remaining part of the
42
6.4.
Uncommon words
vocabulary is assigned one class for each word, practically resulting in a word-level
language model.
The results are summarized in figure 6.2.
Baseline
Maximum
POS
POS-N
220
240
260
280
232
279
266
243
232
232
232
232
Perplexity
Baseline
Maximum
POS
POS-N
27 %
28 %
29 %
30 %
30
30
30
30
27
.
9
30
.
2
29
.
2
28
.
6
WER
Figure 6.2:
Perplexities and WERs for POS clustering
The POS clustered model
gives better results than clustering without POS
tagging, which supports the theory that more syntactic classes lead to better results.
However,
the baseline model
still
outperforms both clustering techniques.
Again,
these results are generalizable with other amounts of classes and other N-gram sizes.
Care must be taken when interpreting the result from the POS-N clustering, due
to unequal amount of classes between the different clustering methods.
Throughout
the rest of this chapter, only words that are tagged as noun or SPEC are clustered,
as is done in the POS-N model.
6.4
Uncommon words
In this section, uncommon words are favoured during clustering, as introduced in
section 5.4.
The evaluated model is based on a combination of POS-N clustering,
as used in the previous section, and uncommon words clustering.
This means that
only words that are tagged as noun or SPEC are clustered, after being adapted for
uncommon word clustering.
Three approaches for uncommon words clustering are
compared:
smooth adaptation, threshold clustering and two-high threshold clustering.
The words are clustered in 7500 classes.
The models are first trained with various values for the input parameters, after
which they are compared on the development data.
The best value for the parameter
of each model is selected and used in the final comparison in the last subsection.
6.4.1
Smooth adaptation
In smooth adaptation, the count of the words in the training corpus is raised to a
certain power, which is determined by the adaptation parameter
a
as can be seen in
43
6.
Evaluation
formula 5.7.
The resulting WER on the development set with different values for the
parameter can be seen in figure 6.3.
A parameter value of 2
−
3
is chosen for the final
comparison of the uncommon word adaptation techniques in subsection 6.4.4.
2
−
5
2
−
4
2
−
3
2
−
2
2
−
1
12 %
12
.
2 %
12
.
4 %
12
.
6 %
12
.
36
12
.
14
12
.
04
11
.
94
12
.
14
12
.
07
12
.
54
a (adaption parameter)
WER
Figure 6.3:
WERs on the development set for uncommon words clustering with
smooth adaptation
6.4.2
Threshold
For the uncommon word threshold approach,
a certain threshold is chosen,
as
described in subsection 5.4.2.
Words that appear more frequently are assigned one
class each, and are thus not clustered, words that appear less are clustered in the
remaining classes.
The minimum threshold value in these experiments is 537,
in
which case the infrequent words are divided in only 12 classes.
A lower threshold
would leave no classes for the infrequent words.
Following the results in figure 6.4, a
threshold value of 800 is be used to evaluate threshold clustering.
537
600
700
800
900
1
,
000
12 %
12
.
5 %
13 %
13
.
5 %
13
.
07
12
.
37
12
11
.
96
12
.
02
12
.
11
threshold
WER
Figure 6.4:
WERs on the development set for uncommon words clustering with
threshold adaptation
44
6.4.
Uncommon words
6.4.3
Two-high threshold
In two-high threshold clustering, words are not clustered if both their occurrences
are above a certain threshold, as described in subsection 5.4.3.
Similar to threshold
clustering, there is a minimum threshold to guarantee that the words can be clustered
into the required amount of classes.
This threshold is the same as in the previous
subsection,
because the amount of
classes that can be generated with a certain
threshold differs only with one class between these approaches.
The influence of the
adaptation parameter on the WER of the development set is depicted in figure 6.5.
In the end, a threshold value of 700 is selected.
537
600
700
800
900
1
,
000
12
.
1 %
12
.
2 %
12
.
3 %
12
.
14
12
.
22
12
.
04
12
.
05
12
.
12
12
.
22
threshold
WER
Figure 6.5:
WERs on the development set for uncommon words clustering with
two-high threshold adaptation
6.4.4
Overview
In the previous subsections, the optimal parameter settings for the different uncom-
mon word models were determined on the development set.
The resulting models
are compared on the evaluation data set and the results are depicted in figure 6.6.
From these results we can conclude that uncommon word information is beneficial
for word clustering.
These results also support the theory of Kneser-Ney smoothing
and Katz back-off, where the probabilities are only slightly adapted when enough
training data is available, but more thoroughly when the amount of training examples
is low, as can be seen in formula 2.15 and formula 2.17.
Within uncommon word
clustering,
threshold and smooth adaptation clustering are the best adaptation
techniques
45
6.
Evaluation
Baseline
POS-N
Smooth
Thres
2-high
230
235
240
245
232
243
241
242
242
232
232
232
232
232
Perplexity
Baseline
POS-N
Smooth
Thres
2-high
28 %
28
.
5 %
27
.
9
28
.
6
28
.
2
28
.
2
28
.
4
WER
Figure 6.6:
Perplexities and WERs for uncommon word adaptation
6.5
Acoustic word similarity
In section 5.5, it was reasoned that acoustically similar words should not be clustered
to improve their distinctiveness.
Two adaptation models were proposed:
smooth
adaptation and threshold clustering, which both require the tuning of one or more
adaptation parameters.
These adaptations are performed analogue to the previous
section on the development set and the results are depicted in figure 6.7 and figure
6.8.
2
−
1
2
0
2
1
2
2
2
3
2
4
2
5
2
6
12 %
14 %
16 %
18 %
b (adaption parameter)
WER
a
=1
a
=0.4
a
=0.2
Figure 6.7:
WERs on the development set for acoustic word similarity clustering
with smooth adaptation
In the best case, smooth adaptation does not drastically change the results of the
model.
However, for higher values of
a
and lower values of
b
, which means that the
influence of the acoustic model is higher, the performance is significantly worse.
A
similar result can be drawn for the threshold approach, which has better results when
lower threshold values are used.
From these results, we can conclude that acoustical
similarity information is not useful for clustering in the class-based language model.
46
6.6.
Dependency-based model
no thres
0
1
2.5
5
7.5
10
12.5
15 %
20 %
12
.
26
12
.
4
12
.
06
12
.
23
12
.
43
12
.
37
21
.
25
21
.
44
threshold
WER
Figure 6.8:
WERs on the development set for acoustic word similarity clustering
with threshold adaptation
6.6
Dependency-based model
Chapter 4 discussed an alternative model for the neural network similarity vectors,
to achieve more syntactic similarity.
In this section, the classification based on these
dependency-based similarity vectors is compared with the classification based on
skip-gram similarity vectors.
First, the training corpus is tagged with the Alpino [
26
] dependency parser, after
which the word-context pairs are extracted as described in section 4.2.
With these
word-context pairs, a skip-gram neural network is trained with the same settings as
the skip-gram model in section 6.2.
The resulting word similarity vectors are used
to train a hierarchical clustering model with maximum linkage, denoted with dep,
analogue to the model in section 6.2, denoted with sg.
A second dependency-based
model is trained with POS-N clustering, denoted with dep POS-N, analogue to the
POS-N clustering in section 6.3, which is denoted with sg POS-N.
Baseline
sg
sg POS-N
dep
dep POS-N
220
240
260
280
232
279
243
254
244
232
232
232
232
232
Perplexity
Baseline
sg
sg POS-N
dep
dep POS-N
27 %
28 %
29 %
30 %
30
30
30
30
30
27
.
9
30
.
2
28
.
6
29
28
.
8
WER
Figure 6.9:
Perplexities and WERs for clustering with dependency-based word vectors
47
6.
Evaluation
We can see that the dependency-based model outperforms the skip-gram model,
which is again a strong indication that more syntactic clustering is beneficial,
a
result that was also observed when performing POS clustering.
If the clustering is
preceded with POS preclustering, skip-gram clustering is better than dependency-
based clustering.
One reason for this could be that the dependency-based clustering gives better
syntactic word vectors, which is beneficial for the clustering, but has a bigger problem
with data sparsity for uncommon words, as discussed in section 4.3.
When performing
POS preclustering, both models have more syntactic clustering, which diminishes
the advantage of the dependency-based clustering over the skip-gram model.
The
data sparsity disadvantage of the dependency-based clustering now outweighs the
syntactic advantage, leading to worse results.
If this assumption holds true, we could
conclude that data sparsity is a big concern in the dependency-based model, and
that it is important to have enough training data.
6.7
Final model:
integrating all information sources
For our final model, we select the best clustering technique and adaptations:
hier-
archical clustering, combined with POS tagging and uncommon word adaptation
with smooth clustering.
The word similarity vectors are generated with negative
sampling and hierarchical
softmax.
In both cases,
a model
is trained with and
without frequent word subsampling.
In the hierarchical clustering, both maximum
and average linkage are evaluated.
The clustering is limited to nouns, and the smooth
clustering parameter is held constant at 2
−
3
.
A sweep is performed over the amount of
classes,
for which the results are
compared on the development set and depicted in figure 6.10 for maximum linkage
and in figure 6.11 for average linkage.
3.160
10.000
17.800
31.600
56.200
100.000
no clust
12
12
.
5
13
classes
WER
negative - no subs
negative - subs
softmax - no subs
softmax - subs
Figure 6.10:
WERs on the development set for various word vector setups clustered
with maximum linkage hierarchical clustering
48
6.7.
Final model:
integrating all information sources
3.160
10.000
17.800
31.600
56.200
100.000
no clust
12
13
14
15
classes
WER
negative - no subs
negative - subs
softmax - no subs
softmax - subs
Figure 6.11:
WERs on the development set for various word vector setups clustered
with average linkage hierarchical clustering
These results show that negative subsampling is the best training algorithm for
the neural network, that maximum linkage is slightly better than average linkage and
that the performance declines with the introduction of frequent word subsampling.
However, these conclusions should be treated with care due to the possible dependency
between the smooth clustering parameter on one hand, and the the amount of classes,
the chosen word vector setup and chosen linkage technique on the other hand.
It could
be possible that better results are obtained when the smooth clustering parameter is
optimized together with the rest of the setup.
However, we do not expect this to
give significantly better results given the overall performance of the models.
A final
model
is selected and compared to the baseline.
This model
uses the
skip-gram neural network, with negative sampling training and no infrequent word
subsampling.
The words are preclustered in POS-sets and only words that are tagged
as noun or SPEC are clustered.
Uncommon word information is added in the form of
smooth adaptation and with parameter 2
−
3
.
The noun and SPEC set is clustered in
17
.
800 classes using hierarchical clustering with maximum linkage.
The results are
summarized in figure 6.12.
The final model performs much better than the original
maximum hierarchical
clustering from section 6.2,
which has no extra sources of
information.
However, the results of the baseline model are still slightly better than
that of the final model.
49
6.
Evaluation
Baseline
Maximum
Final
220
240
260
280
232
279
236
232
232
232
Perplexity
Baseline
Maximum
Final
27 %
28 %
29 %
30 %
30
30
30
27
.
9
30
.
2
28
.
1
WER
Figure 6.12:
Perplexities and WERs for hierarchical clustering with maximum linkage
without ’
Maximum
’ and with ’
F inal
’ optimization and extra information sources
6.8
Conclusion
From the experiments conducted in this chapter, several conclusions can be drawn.
The first conclusion is that incorporating acoustically similarity information between
words does not result in a better language model.
The results from POS clustering and the dependency-based model imply that,
when using information from similar histories, the similarity should preferably be of
a more syntactic nature.
This can be achieved by adding POS information, or by
using a dependency-based model, where the training data for the neural network is
preprocessed using syntactic relations.
The last approach faces problems with data
sparsity of infrequent words when the amount of training data is limited.
Uncommon words clustering is also beneficial to obtain a better classification.
This teaches us that the techniques to reduce data sparsity should focus on infrequent
words, while avoiding interference with more frequent words.
In the last section, a combination was made with the best performing techniques:
POS-clustering and uncommon words adaptation.
After an optimisation over the
amount of classes and the setup of the skip-gram neural network, the result of the
class-based model
was better than the initial
POS-N clustering,
but still
slightly
worse than the word-level language model.
50
Chapter 7
Conclusion and Further
Research
The goal of this thesis was to reduce the data sparsity problem in language models
for automatic speech recognition,
using a class-based language model
and neural
network word similarities.
A number of different models,
which investigated the
influence of various features in the language model, were proposed and compared.
In
this comparison, the WER of these models was taken as benchmark.
In the end, a
final model was proposed, combining the useful features from the different models.
7.1
Conclusion
In order to reduce data sparsity in N-gram language models, we used a class-based
N-gram model.
Classes were clustered based on neural network word vectors.
These
vectors were generated using the skip-gram neural
network,
which exists in two
training variants:
negative sampling and hierarchical softmax.
We proposed and compared two clustering techniques:
k-means and hierarchical
agglomerative clustering.
The advantage of k-means was the good time and memory
scaling.
Hierarchical agglomerative clustering, on the other hand, made it possible
to incorporate extra information.
To combine the good memory scaling of k-means,
with the extra information advantage of
hierarchical
clustering,
we introduced
a third hybrid method.
The best word error rate results were achieved using
hierarchical clustering with average linkage, closely followed by hierarchical clustering
with maximum linkage.
The hybrid method also scored well
when using a small
preclustering of 3 sets, but worse with a bigger preclustering of 10 sets.
Using the
hybrid method, the memory usage was greatly reduced.
Unfortunately, none of the
clustering algorithms managed to achieve better results than the baseline model.
To improve the models, we proposed three sources of extra information:
acous-
tic similarity information,
part of speech (POS) information and word frequency
information.
Two models to incorporate acoustic similarity were developed and
tested, but the evaluation results suggested that acoustic similarity is irrelevant to
the language model.
POS information was added to the language model
by only
51
7.
Conclusion and Further Research
clustering words with the same POS tag.
This proved to be a successful technique,
from which we concluded that more a syntactic classification is an important step
towards a better language model.
Next, we introduced word frequency information
in the language model.
Three different models were introduced:
smooth, threshold
and two-high threshold adaptation.
All three models improved on the word error
rate, which led us to conclude that it is important to focus the efforts against data
sparsity on infrequent words, and limit the interference with frequent words.
Another way to achieve more syntactic classes was found in adapting the neural
network.
By selecting more syntactic word relations in the training data of the neural
network,
the dependency-based model
was able to generate more syntactic word
vectors.
The disadvantage of this approach was an increased data sparsity for the word
vectors of infrequent words.
A comparison between the neural networks showed that
the dependency-based model performed better than the skip-gram network, which
confirmed the conclusion that a more syntactic classification improves the language
model.
However, when POS clustering was introduced, the dependency-based model
was slightly outperformed by the skip-gram model.
We assumed that this was due to
the fact that the POS clustering reduced the syntactic advantage of the dependency-
based model over the skip-gram model, and that the data sparsity disadvantage of
the dependency-based model became a dominant factor.
We concluded that it is
important to have enough training data when using the dependency-based model.
In the end,
we designed a model
comprising the best techniques:
hierarchical
clustering, POS clustering and smooth uncommon words adaptation.
A sweep was
made over the amount of classes, the linkage criterion and the training set-up for
the neural network, which resulted in a final model.
However, when this model was
compared to the baseline word-level N-gram model, it performed slightly worse.
We
can conclude that the usage of neural network word similarities to generate classes
for a class-based language model, is not a good technique to reduce data sparsity.
7.2
Further research
Bigger data sparsity in the dependency-based model in comparison to the skip-gram
model, has two causes:
the limited amount of training examples and the size of the
neural network.
Since the number of dependency relations between words are fixed,
the amount of training examples are limited.
It would be hard to increase the amount
of training examples without extra data.
The size of the neural network however, is
adaptable by changing the architecture of the neural network, for example by only
predicting the context word and not the dependency relation to the input word.
This
would scale the output layer back to the size of the vocabulary, at the cost of less
syntactic word embeddings.
The decline in syntactic information of the embeddings could be compensated
by changing the training objective of the neural
network to estimate the context
word and the dependency relation separately, using separate output neurons for the
dependency relation.
This would increase the size of the output layer with only the
number of possible dependency relations.
In the end, by changing the neural network
52
7.2.
Further research
architecture and training, it could be possible to find a better balance between data
sparsity and syntactic word embeddings.
The results of the uncommon words clustering taught us to focus on infrequent
words, while avoiding interference with more common words, when trying to reduce
data sparsity.
A more general
conclusion could be that these techniques should
focus their attention on probabilities that have a bad probability estimate due to
data sparsity, while only slightly adapting the more certain probability estimates.
If this last assumption holds true, the class-based language model would be a very
drastic measure to cope with data sparsity.
While it can take into account the
word frequency, it can not take into account the frequency of a whole N-gram.
This
means that it can only make a limited distinction between bad and good probability
estimates.
Other techniques are more discriminative in their data sparsity approach, such
as Kneser-Ney smoothing and Katz back-off.
In these approaches,
only a slight
adaptation is made for good estimates, in the form of a small subtraction
d
in formula
2.15 and formula 2.17.
The probabilities of infrequent N-grams on the other hand,
are heavily adapted to alleviate data sparsity.
These techniques do incorporate extra
information by backing-off or looking at the amount of different contexts the word
has appeared in.
However,
this information is only limited in comparison to the
information in the class-based approach.
It is possible to combine the advantages of both methods in several
ways:
In
a more class-based approach,
words that have a bad probability estimate can be
estimated using similar histories.
P
(
w|h
) =



C
(
w,h
)
−d
C
(
h
)
if
C
(
w, h
)
>
thres
α
(
h
)
P
(
w|c
w
)
P
(
c
w
|c
h
)
otherwise
(7.1)
To eliminate the last zero probabilities, back-off or Kneser-Ney adaptation can
be used.
In a more smooth approach, words with bad probability estimates can be predicted
based on the probability of similar words:
P
(
w|h
) =







C
(
w,h
)
−d
C
(
h
)
if
C
(
w, h
)
>
thres
α
(
h
)
P
w
2
f
sim
(
w,w
2
)

P
(
w
2
|h
)
P
w
2
f
sim
(
w,w
2
)

otherwise
(7.2)
or based on probabilities with similar histories:
P
(
w|h
) =







C
(
w,h
)
−d
C
(
h
)
if
C
(
w, h
)
>
thres
α
(
h
)
P
w
1
..
P
w
N−
1
f
sim
(
h
1
,w
1
)

..f
sim
(
h
N−
1
,w
N−
1
)

P
(
w|w
1
..w
N−
1
)
P
w
1
..
P
w
N−
1
f
sim
(
h
1
,w
2
)

..f
sim
(
h
N−
1
,w
N−
1
)

otherwise
(7.3)
53
7.
Conclusion and Further Research
with
h
i
the i
th
word in history
h
.
Other sources of information, such as POS or
word frequency information, could be added by modifying the function
f
based on
these features.
These language models would combine the advantage of class-based models, where
information is distributed over similar words, and the advantage of Kneser-Ney and
Katz back-off, in which only bad probability estimates are heavily adapted.
54
Bibliography
[1]
P. F. Brown, P. V. DeSouza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai.
Class-Based n-gram Models of Natural Language.
Computational
Linguistics,
18(1950):467–479, 1992.
[2]
K. Demuynck, A. Puurula, D. Van Compernolle, and P. Wambacq.
The ESAT
2008 system for N-Best Dutch speech recognition benchmark.
ASRU,
pages
339–344, 2009.
[3]
W. A. Gale.
Good-Turing Smoothing Without Tears.
Methods, 2(3):1–24, 1995.
[4]
W. A. Gale and K. W. Church. What’s wrong with adding one. In Corpus-Based
Research into Language, 1994.
[5]
Y.
Goldberg and O.
Levy.
word2vec Explained:
Deriving Mikolov et al.’s
Negative-Sampling Word-Embedding Method.
arXiv preprint arXiv:1402.3722,
(2):1–5, 2014.
[6]
I. Good.
The population frequencies of species and the estimation of population
parameters.
Biometrika, 40(3-4):237–264, 1953.
[7]
S. Haykin.
Neural Networks:
A Comprehensive Foundation, 1998.
[8]
D. Jurafsky and J. H. Martin.
Speech and Language Processing:
An Introduc-
tion to Natural
Language Processing, Computational
Linguistics, and Speech
Recognition, volume 21.
2000.
[9]
S. M. Katz.
Estimation of probabilities from sparse data for the language model
component of a speech recognizer.
In IEEE Transactions on Acoustics, Speech
and Signal
Processing, pages 400–401, 1987.
[10]
J. Kessens and D. van Leeuwen.
N-Best:
The Northern- and Southern-Dutch
benchmark evaluation of
speech recognition technology.
Proceedings of
the
Annual
Conference of
the International
Speech Communication Association,
INTERSPEECH, pages 1354–1357, 2007.
[11]
R. Kneser and H. Ney.
Improved backing-off for m-gram language modeling.
In
ICASSP, volume I, pages 181–184, 1995.
55
Bibliography
[12]
O. Levy and Y. Goldberg.
Dependency-Based Word Embeddings.
ACL, pages
302–308, 2014.
[13]
S. Martin, J. Liermann, and H. Ney.
Algorithms for Bigram and Trigram Word
Clustering.
Speech Commun., 24(1):19–37, 1998.
[14]
T. Mikolov, K. Chen, G. Corrado, and J. Dean.
Distributed Representations of
Words and Phrases and their Compositionality.
NIPS, pages 3111–3119, 2013.
[15]
T. Mikolov, G. Corrado, K. Chen, and J. Dean.
Efficient Estimation of Word
Representations in Vector Space.
ICLR, pages 1–12, 2013.
[16]
T. Mikolov, W.-t. Yih, and G. Zweig.
Linguistic regularities in continuous space
word representations.
NAACL-HLT, pages 746–751, 2013.
[17]
A.
Mnih and Y.
W.
Teh.
A Fast and Simple Algorithm for Training Neural
Probabilistic Language Models.
ICML, pages 1751–1758, 2012.
[18]
F.
Morin and Y.
Bengio.
Hierarchical
probabilistic neural
network language
model. Proceedings of the Tenth International Workshop on Artificial Intelligence
and Statistics, pages 246–252, 2005.
[19]
D. Müllner.
fastcluster:
Fast Hierarchical, Agglomerative Clustering Routines
for R and Python.
Journal
of Statistical
Software, 53(9):1–18, 2013.
[20]
J. Pelemans, K. Demuynck, and P. Wambacq.
A layered approach for dutch
large vocabulary continuous speech recognition.
In ICASSP, pages 4421–4424.
IEEE, 2012.
[21]
R.
Rehurek.
Parallelizing word2vec in Python.
http://radimrehurek.com/
2013/10/parallelizing-word2vec-in-python, 2013.
[22]
R. Rehurek and P. Sojka.
Software framework for topic modelling with large
corpora.
LREC Workshop on New Challenges for NLP Frameworks,
pages
46–50, 2010.
[23]
A. Stolcke.
SRILM – An extensible language modeling toolkit.
In ICSLP, pages
257–286, 2002.
[24]
J.
Uszkoreit and T.
Brants.
Distributed Word Clustering for Large Scale
Class-Based Language Modeling in Machine Translation.
ACL, pages 755–762,
2008.
[25]
A.
van den Bosch,
B.
Busser,
S.
Canisius,
and W.
Daelemans.
An efficient
memory-based morphosyntactic tagger and parser for Dutch.
Computational
Linguistics in the Netherlands 2006:
Selected papers from the seventeenth CLIN
Meeting, pages 191–206, 2007.
[26]
G. van Noord.
At Last Parsing Is Now Operational.
TALN, pages 20–42, 2006.
56
Bibliography
[27]
R.
a.
Wagner and M.
J.
Fischer.
The String-to-String Correction Problem.
Journal
of the ACM, 21:168–173, 1974.
[28]
S. Young.
A review of large-vocabulary continuous-speech recognition.
IEEE
Signal
Processing Magazine, 13:45–57, 1996.
57
