International Journal on Digital Libraries
https://doi.org/10.1007/s00799-018-0237-y
Toward meaningful notions of similarity in NLP embedding models
Ábel Elekes
1
· Adrian Englhardt
1
· Martin Schäler
1
· Klemens Böhm
1
Received: 15 September 2017 / Revised: 12 April 2018 / Accepted: 13 April 2018
© Springer-Verlag GmbH Germany, part of Springer Nature 2018
Abstract
Finding similar words with the help of word embedding models, such as Word2Vec or GloVe, computed on large-scale digital
libraries has yielded meaningful results in many cases. However, the underlying notion of similarity has remained ambiguous.
In this paper, we examine when exactly similarity values in word embedding models are meaningful. To do so, we analyze
the statistical distribution of similarity values systematically, conducting two series of experiments. The first one examines
how the distribution of similarity values depends on the different embedding model algorithms and parameters. The second
one starts by showing that intuitive similarity thresholds do not exist.
We then propose a method stating which similarity
values and thresholds actually are meaningful for a given embedding model. Based on these results, we calculate how these
thresholds, when taken into account during evaluation, change the evaluation scores of the models in similarity test sets. In
more abstract terms, our insights give way to a better understanding of the notion of similarity in embedding models and to
more reliable evaluations of such models.
Keywords
Word embedding models
·
Similarity values
·
Semantic similarity
1 Introduction
One important objective of so-called distributional models
[1,2] is to capture the semantic similarity of words, based on
their context in large corpora, such as the Wikipedia. If one
is able to quantify their similarity, there will be a good under-
standing of the actual meaning of a word, by knowing which
words are similar.
Adopting the taxonomy of Baroni et al.
[3], one can discern between count-based distributional mod-
els [4,5] and training-based,
predictive models,
also called
embedding models [6–12].
Embedding models use vectors
to represent words in a low-dimensional space to quantify
semantic similarities between words. All these models have
in common that two words are semantically similar if the vec-
tors representing them are close according to some distance
function.
Embedding models
have received renewed popularity
after Mikolov et al. [11,12] presented new neural-network-
based models. In comparison with count-based models, the
training of
such models
scales
very well
even to huge
corpora, while learning high-quality word vector representa-
tions. With this, embedding models have become key tools
B
Ábel Elekes
abel.elekes@kit.edu
1
Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany
in Natural Language Processing (NLP), showing impressive
results in various semantic tasks such as similarity detection
or analogical
reasoning [3].
Despite the limited linguistic
information distributional models contain, embedding mod-
els have proven to be successful not only in elementary tasks,
but also in complex ones such as part-of-speech (POS) tag-
ging [7], named entity recognition (NER) [13], dependency
parsing [14],
social
media sentiment
analysis [15],
image
annotation [16], and machine translation [17–19]. Apart from
the linguistic applications,
embedding models have been
used successfully in various other fields of computer sci-
ence,
such as bioinformatics,
most notably genomics [20],
recommender systems [20], and automated ontology enrich-
ment [22]. The latter two applications in particular are highly
relevant for digital libraries as well.
Textual
digital
libraries,
such as
Wikipedia,
Google
Books, or the ACM Digital Library offer great potential for
linguistic analysis. For instance, one may now be able to dis-
cover how language evolves over time based on the Google
Books historical
corpora,
or reveal
how language is used
in different
fields like natural
sciences compared to social
sciences based on the ACM Digital Library. Such analyses
ultimately target at understanding our society by means of
comprehensive empirical results.
123
Á. Elekes et al.
Word embedding models have value for subject access,
record recommendation,
and large-scale analysis of trends
in digital
libraries.
By focusing on the semantic contexts
of words,
they allow to understand relationships between
text segments in more accurate, less biased ways. However,
it
currently is not
clear
how those ‘relationships’
should
be interpreted,
how far
different
embedding models and
approaches are comparable, or how the training corpus and
parameter values influence their similarity values.
It currently is an open question whether embedding mod-
els in general are superior to traditional count-based models.
Various researchers suggest that they indeed are in various
similarity and analogy detection tasks [3,12]. But others have
argued that this superiority is only a result of better param-
eter settings [20–23]. However, these papers are only using
the similarity attribute of the models,
while the following
questions remain open: What do similarity values from those
models actually mean? For instance, are low values of sim-
ilarity comparable to each other? To illustrate, if Word A is
0.2—similar to Word B and 0.1—similar to Word C on a
[
−
1, 1] scale, should we say that A is more similar to B than
to C, or does it not make any difference at these low levels
of similarity? Are there ‘natural’ thresholds for similarity,
such that values above (beneath) it represent a definite simi-
larity (dissimilarity) of two words? For example, if A is more
than 0.5—similar to B, then are A and B always semantically
similar? How about the same questions with similarity lists,
i.e.,
lists of words most
similar to a certain words,
sorted
by similarity? For instance, can we say that the 100 words
most similar to an arbitrary word are always similar to this
one, or words not in the top 500 are always dissimilar? When
exactly is it meaningful to stick to the natural idea of taking
the top N most similar words for a certain word and deem
them similar? In this paper,
we study and answer all these
questions. These questions are not just academic in nature;
any study relying on comparisons of similarity values might
lack validity if these questions remain open.
Challenges
Several issues arise when studying similarity in
computer science: How does an evaluation test set look like,
what does it contain? How to create good baseline test sets,
how do they measure similarity, and how to evaluate a model
on them?
As for the first question there is a generally accepted sim-
ple structure how the similarity test sets should look like, and
any widely used test set such as WordSim353 [24] or MEN
[25] is formatted like this. These test sets contain word pairs
and similarity scores for every pair,
set by human annota-
tors. To show the difficulty of how to create such test sets,
think of the following linguistic challenge pointed out
by
Hill et al. [26] in this context: What is the definition of simi-
larity? Are cup and coffee similar words or only associated,
i.e., dissimilar? In general, does relatedness or associatedness
imply similarity or not? They argue that word pairs which are
only associated should have only moderately high similarity
scores. This is in opposition to test sets such as WordSim353
or MEN where this is not the case, i.e., associated pairs do
have very high scores. Batchkarov et al. [27] also address the
problem of creating good baseline test sets. They show that it
is challenging even for human annotators to assign similarity
scores to certain word pairs. For example, they show that the
similarity scores for the tiger-cat pair range from 5 to 9 on
a scale of ten in the WordSim353 test set. They also provide
example word pairs where the similarity scores differ sig-
nificantly when the pairs are contained in different test sets.
They argue that this is the result of the different notions of
similarity these test sets use.
Next, Avraham et al. [28] identify problems regarding the
evaluation of the models. They argue that the use of the same
rating scale for different types of relations and for unassoci-
ated pairs of words makes the evaluation biased. For example,
they say that it is meaningless to compare the similarity value
of cat-pet to winter-season, because they are unassociated,
and models which rank the wrong word pair higher should not
be punished. If cat-pet has a score of 0.7, and winter-season
has one of 0.8 in a similarity test set, an evaluation should not
punish a model which ranks cat-pet higher. They also find it
problematic how the conventional evaluation method mea-
sures the quality of a model [28]. It calculates the Spearman
correlation of the ranking by the annotator and the model
ranking,
without
considering the similarity values further.
To illustrate, such an evaluation penalizes a model that mis-
ranks two low-similarity, unassociated pairs (e.g., cat-door,
smart-tree) just as much as one that misranks two objectively
distinguishable pairs (e.g., singer-performer, singer-person).
Having said this,
the
concept
of
similarity remains
ambiguous, and understanding similarity values remains dif-
ficult as well, affecting several NLP tasks, especially when
it comes to evaluate embedding models on these tasks.
Contributions
To understand what similarity values in embed-
ding models mean,
we evaluate how different
parameter
settings (e.g., size of the corpus they are trained on, vocabu-
lary size) influence the similarity values of the models. We do
so by systematically training various models with different
settings and comparing the similarity-value distributions. In
addition, we consider two state-of-the-art embedding model
types which are not based on words, but on syllables and sen-
tences respectively, to generalize our findings. Our first core
contribution in this paper is that we find out that, except for a
few marginal cases, all distributions have a very similar bell
shape.
We prove with statistical tests that most of the nor-
malized distributions are almost identical even with the most
extreme parameter settings,
such as very large dictionaries
or small dimensionality.
123
Toward meaningful notions of similarity in NLP embedding models
One intention behind these experiments also is to confirm
that
the meaning of similarity values of two terms is not
sufficiently clear,
and to reveal that this also holds for the
relationship between model parameters and similarity values.
We show that indeed it is not always meaningful to compare
two word pairs by their similarity values.
Another core contribution of ours is the discovery that
meaningful similarity threshold values do indeed exist, and
we show that
they can be found.
We do so by calculat-
ing similarity-value and similarity-list aggregates based on
WordNet
[29]
similarity as the baseline and evaluate the
resulting similarity distributions of the models with statis-
tical tests. It turns out that these thresholds are not general
and should be calculated for every individual model using
the method we present
in this paper.
At
this point,
our
evaluation connects with the parameter
evaluation of
the
models just mentioned: The evaluation shows that altering
the parameters does not change our method; all similarity-
value distributions of the models are fundamentally simi-
lar.
Based on these results, we propose a similarity-threshold-
aware evaluation method of
word embedding models on
similarity tasks which does not compare the word pairs dur-
ing evaluation which fall
below the calculated threshold.
Using well-known benchmark test
sets,
we arrive at
two
insights. First, there are pairs in these sets that fall below the
threshold, i.e., that should not be part of the evaluation. Sec-
ond, excluding these pairs from the benchmark does change
the benchmark results to an extent that is noticeable.
This
ensures a more reliable comparison.
This is an important
step regarding the design of future word embedding models
as well as for the improvement of existing evaluation meth-
ods.
As a final
contribution,
we make all
the created mod-
els and evaluation scripts publicly available on our project
website.
1
To our knowledge, this is one of the biggest col-
lections of embedding models trained with systematically
different parameter settings. Hence, it is a valuable resource
for all researchers working in this area. Both the threshold-
calculation and the model-evaluation method are written in
Python.
This article is an extended version of an earlier publica-
tion [30], with the following extensions: Our evaluation now
covers more parameters of the word embedding models. It
also considers different
types of models,
the syllable and
the sentence-based embedding models in particular. Finally,
we also propose a new similarity-threshold-aware evaluation
method for word embedding models.
1
https://dbis.ipd.kit.edu/2542.php.
2 Fundamentals and notation
In the following, we first define word embedding models and
their parameters in general. We then introduce two relevant
models which we rely on in the paper. Finally, we introduce
other types of embedding models which assign vectors not
only to words, but to syllables, sentences or articles.
2.1 Background on word embedding models
Word embedding models “embed” words into a low-dimensi-
onal
space,
representing them as
dense vectors
of
real
numbers. It is possible to generate vectors for not only words,
but
Part-of-Speech tagged words,
phrases or sentences as
well (see Sect. 2.3).
A training algorithm generates word vectors based on an
input word
w
and its context words c. The context words are
the words surrounding
w
in a symmetric window. This means
that a large training corpus is scanned using a sliding window.
The model is trained in an unsupervised fashion by using a
possibly large and topically heterogeneous text corpus, e.g.,
the first one billion words of Wikipedia.
Vectors close to each other according to a distance function
represent
words that
are semantically related.
We use the
cosine similarity of two word vectors as the similarity score
of the respective words.
cosSim
(
wordvec
1
,
wordvec
2
)
=
wordvec
1
·
wordvec
2

wordvec
1
 · 
wordvec
2

∈
[
−
1
,
1
]
It ranges from
−
1 (unrelated words) to 1 (identical words).
Formally, a word embedding model is a function F which
takes a corpus C as input, such as a dump of the Wikipedia,
generates a dictionary D based on the corpus and associates
any word in the dictionary
w ∈
D with a d-dimensional
vector v
∈
R
d
.
The dimension size parameter (d) sets the
dimensionality of the vectors. It usually ranges between 50
and 1000 for word embedding models.
The training,
i.e.,
iteratively associating vectors with words in the dictionary,
is based on word-context pairs
w×
c
∈
D
×
D
2
×
win
extracted
from the corpus. There is a further parameter epoch_nr that
states how many times the training algorithm passes through
the corpus. If not stated otherwise, we will train models with
five iterations. win is the window size parameter, which deter-
mines the context of a word. For example, a window size of 5
means that the context of a word is any other word in its sen-
tence, and their distance is at most 5 words. However, there
are further parameters that affect the generation of the dictio-
nary. One is the minimum count parameter (min_cnt). When
creating the dictionary from the corpus, the model adds only
words to the dictionary which appear at least min_cnt times in
the corpus. An alternative is to set the dictionary size directly
123
Á. Elekes et al.
as a parameter (dict_size). This means that the model adds
only words to the dictionary which are in the dict_size most
frequent words of the corpus. In this paper, we rely on the
dict_size parameter, because we find it easier to handle in our
experiments. With this variant, the corpus does not influence
the size of the dictionary.
Having said this, we define word embedding models as:
F
(
C
,
epoch_nr
,
d
,
win
,
dict_size
)
∈
R
|
D
|
×
d
.
dict_size is not necessarily equal to the size of the dictionary
|
D
|
. For example, it is unequal when the number of distinct
words in the corpus is smaller than dict_size. F is not deter-
ministic, as it may use random values when initializing the
word vectors.
2.2 Realizations of word embedding models
In this paper, we work with two well-researched embedding
models,
Mikolov et
al.’s Word2Vec model
[12] and Pen-
nington et al.’s GloVe model [10].
These models learn the
vector representations differently.
Word2Vec models use a
neural-network-based learning algorithm. It learns by max-
imizing the probability of predicting either the word given
the context (Continuous Bag of Words model,
CBOW),
or
the context given the current word (Skip-Gram model, SG)
[11,12]. GloVe trains the word vectors by explicitly factoriz-
ing the log-count matrix of the underlying corpus, with regard
to word-context pairs [10]. Levy et al. [31] have shown that
the SG model implicitly factorizes a word-context pointwise
mutual information matrix. This means that the objectives of
the two models and sources of information they use are not
overly different,
and,
more important here,
that they share
the same parameter space.
See [32] for a further compari-
son.
To build the models, we use the gensim software package
[33] for the Word2Vec models and the GloVe toolkit
2
for the
GloVe models. More specifically, we use the gensim toolkit in
Python. It allows querying any word in the model dictionary
for its similarity with any other word.
This means that for
any word there is an indexed list containing every other word
in the dictionary, sorted by similarity. In this paper we use
the terms list index and position in the list as synonyms. The
similarity values are floating point numbers between
−
1 and
1, with 1 being the highest similarity. We will differentiate
between the similarity values of models and similarity lists.
In the first case, we are only concerned with the similarity
value of a word pair and not its position in those lists. In the
second case, our interest is the opposite one.
2
http://nlp.stanford.edu/projects/glove/.
2.3 Alternative embedding models
Recently, alternative approaches based on the idea of word
embeddings
have
been
presented
to
model
semantics.
Bojanowski et al.
[34] have refined the Word2Vec models
by additionally learning embeddings of subwords, instead of
only full word embeddings. A subword in this context is a
chain of characters, e.g., default 3–6 characters. In this way,
the model
unifies different
grammatical
forms or spelling
mistakes and is expected to learn good embeddings for rare
words. Additionally, it is also able to infer embeddings for
words that have not been present during training. The model
accomplishes this by averaging the vectors of the subwords
in the target word. The approach has been published together
with an efficient implementation fastText.
3
Since it is based
on Word2Vec, both Skip-Gram and Continuous Bag of Words
are available. The fastText models have been used in a vari-
ety of tasks, e.g., location prediction based on tweets [35] or
review rating prediction from a text [36].
While fastText
works on the character level,
the other
direction has been investigated as well: An approach called
Paragraph Vector learns an embedding for a sequence of
words [37].
Nowadays,
it
is often called Doc2vec since it
has been added to the gensim package.
Doc2vec is able to
learn low-dimensional representations for arbitrary lengths
of
text,
ranging from phrases up to multiple paragraphs.
Doc2vec is widely used for text classification [38,39] or to
compute document similarities [40].
During training,
each
sentence/paragraph is annotated with a unique ID. The first
type of model is called Distributed Memory Model (DM).
It is based on the idea of CBOW [37].
Given the sentence
or paragraph ID and a few words in the current window, the
model predicts the subsequent word. The second method is
called Distributed Bag of Words (DBOW) and ignores word
order [37]. The idea is similar to SG, and given the paragraph
ID the model tries to predict the words in a window.
3 An investigation of the influence of the
various parameters
In this section, we investigate how the different parameters
affect the similarity values of the models. We are particularly
interested in identifying parameters that change the stochas-
tic distribution of the similarity values significantly. At the
end of the section,
we investigate the similarity-value dis-
tributions produced by the alternative models introduced in
Sect.
2.3.
We show that their distributions tend to be very
similar to the ones of the word models, but not in every case.
These insights are relevant to understand word embedding
models. We also require such insights in the next section for
our threshold evaluation.
3
https://github.com/facebookresearch/fastText.
123
Toward meaningful notions of similarity in NLP embedding models
3.1 Investigation objectives
A core contribution of this paper is to find meaningful thresh-
olds both for similarity values and for similarity lists for a
given model. To this end, we evaluate how different models
and their parameters affect
the similarities.
We will
show
that
similarities in embedding models can differ
signifi-
cantly when trained with different
parameters.
Hence,
our
first hypothesis is as follows:
Hypothesis 1
It is not possible to find general value and list
thresholds that are reasonable for all embedding models, only
for specific ones.
We plan to confirm this hypothesis by showing that
the
similarity-value distributions have statistical characteristics
such as different mean values or different highest similarity
values which make uniform threshold values meaningless.
We present two examples of such models in the following.
Example 1
Think of two models, Model A with an average
similarity between two words of 0.0, and Model B with an
average of 0.1. This means that the similarity value is nega-
tive for roughly half of the pairs in Model A and for roughly
1% of the pairs in Model B. If one now assumed that a nega-
tive similarity value implied dissimilarity between the words
of the pair,
this assumption would have a highly different
meaning for the two models.
Example 2
Again think of two models. The highest similarity
score of a word pair is 0.9 in Model A and 0.6 in B. Saying
that a pair with a similarity above 0.7 is definitively similar
could be meaningful in Model A, but makes less sense in B.
This is because there is no word pair with such a similarity
value in this model.
Although the similarity-value distributions of the models can
significantly differ in certain characteristics, we hypothesize
that they are all similar in shape, with only their means and
standard deviations depending on the parameters.
Hypothesis 2
While the learning algorithms and parameters
influence the similarity-value distributions of the models,
these distributions are very similar in shape.
We plan to confirm this hypothesis as follows.
First,
we
normalize all
distributions,
so that
they have 0 mean and
1 standard deviation.
We then randomly draw 1000 values
from all distributions and pairwise compare the samples by
means of the two-sample Kolmogorov–Smirnov (K–S) test
[41] with 99% confidence. This test checks whether two sam-
ples are drawn from the same distribution.
For the overall understanding of the similarity values and
lists, it is important to know how the model selection and the
parameters affect the similarities. Our main contribution in
this section is that we conduct the evaluation systematically
for all the parameters and models already introduced.
3.2 Experiment setup
In this paper,
we work with Chelba et al.’s 1 Billion word
dataset [42] as training corpus,
one of the largest publicly
available language modeling benchmarks. It has shown to be
a good benchmark dataset for language modeling,
with its
great size,
large vocabulary and topical diversity [42].
The
dataset
is around 4 GB in size as a text
file and contains
almost 1 billion words in approximately 30 million English
sentences. The sentences are shuffled, and the data is split into
100 disjoint partitions. This means that one such partition is
1% of the overall data.
We train all our models using this
dataset as training corpus.
In the following,
for
every parameter,
we present
our
results in the same way.
In particular,
we graph results in
two figures. First, there are similarity-value distributions of
the models. For these plots, we randomly select 10,000 words
from the model dictionary and calculate the similarity values
of every other word to them. Then, we group the values in
0.01 intervals and count the number of values in each group.
Thus, the x -axis represents the similarity values from [
−
1,
1], the y-axis the share of the values per group.
The second figures contain the results from the similarity
lists experiments. In these experiments, we randomly select
10,000 words (
w
1
, w
2
, . . . , w
10000
)
from the dictionary of
the model. Their respective word vectors are (
v
1
, . . . , v
10000
).
For each of these words, we compute the most similar one
thousand words
w
i
,
1
, w
i
,
2
, . . . , w
i
,
1000
for i
∈
{
1
, . . . ,
10000
}
,
together with their respective similarity values, t
i
,
1
, t
i
,
2
, . . .,
t
i
,
1000
, where
t
i
,
j
=
cosSim

v
i
, v
i
,
j

,
i.e.,
t
i
,
j
is the similarity value of words
w
i
and
w
i
,
j
.
The
list
w
i
,
1
, w
i
,
2
, . . . , w
i
,
1000
is sorted by the similarity values
in descending order.
Because of this sorting for every i ,
it
holds that t
i
,
j
1
≥
t
i
,
j
2
, for any j
1
<
j
2
. We then calculate the
average similarity value for every list index
avg_sim
(
j
)
=
mean

t
.,
j

.
Finally,
we
plot
the
results
with the
x -axis
being the
list
indices
( j )
and the
y-axis
the
average
similarities
(avg_sim
(
j
)
)
.
Although the avg_sim
()
function is
only
defined for arguments that
are natural
numbers,
the plots
connect
the points to arrive at
a smooth curve,
for better
visibility.
At this point,
we are not trying to answer why different
parameters affect
the similarity values as they do;
we are
investigating how they affect the values. This means that we
123
Á. Elekes et al.
Fig. 1
Learning algorithms similarity-value distributions
Fig. 2
Learning algorithms similarity values by list indices
are not making qualitative statements, i.e., we are not con-
cerned how parameters affect the quality of the models on
different semantic tasks.
We are not making any statement
that any model is better or worse than the other one, but only
how and to which extent they are different. In other words,
we focus on the hypotheses from Sect. 3.1.
3.3 Model selection
The first parameter whose effect we investigate is the model
itself.
We consider
the three models already introduced,
namely Word2Vec SG,
Word2Vec CBOW and GloVe.
We
build all three models on the full 1 billion words dataset with
the same parameter settings. As we have noted in Sect. 2.2,
these models share the same parameter space. This means that
we can use the exact same parameter setting for the models.
The parameters we use are d
=
100
,
win
=
5
,
dict_size
=
100
,
000, the default settings for the Word2Vec models. These
values have shown to be a good baseline setting for different
semantic tasks [26,43].
Similarity values
Figure 1 shows that the approaches visually
differ much in their similarity values. The CBOW and GloVe
models are almost
identical,
although GloVe has slightly
higher values. But the SG algorithm generally yields higher
values than the other two, and only few pairs of words have
negative similarities.
This implies that,
while words in the
CBOW and GloVe model fill almost the entire space, the SG
model learns word vectors positioned at a high-density area
of the space, leaving the remainder of the space sparse. We
test Hypothesis 2 by comparing the normalized distributions
pairwise, cf. Sect. 3.1:
K _S_ p_value

sim_dist
i
,
sim_dist
j

>
0
.
01
for every i
,
j
∈
{
cbow
,
sg
,
glove
}
We conclude that the models are similar in their distributions.
Regarding Fig.
2,
although the GloVe model
generally
produces higher similarity values than CBOW, the values by
list position are smaller than with both Word2Vec models.
At the end of the top 1000 list, the values of the SG model
are the highest ones.
Result interpretation
Both results indicate that our hypothe-
ses hold. That is, the distributions of the similarity values are
indeed very similar, although at the same time they are vis-
ibly different in certain characteristics. This is important: It
indicates a certain robustness of embedding models and gen-
eralizability of empirical results. The differences also show
that we cannot set general thresholds which apply to every
model.
3.4 Dimensionality
When measuring similarity with the cosine distance,
the
dimensionality of the embedding model is a parameter that
strongly affects its similarity values. In this section, we train
every model
with the Word2Vec CBOW algorithm with
different
dimensionalities on the full
corpus,
with win
=
5
,
dict_size
=
100
,
000.
Similarity values
Figure 3 shows that the higher the dimen-
sionalities of the model are, the narrower are the similarity
distributions. We have expected this, as vector spaces with
lower dimensionality are denser when filled with 100,000
words than those with higher dimensionality. This leads to
closer words and higher similarity values. In contrast to the
visibly different distributions,
we again see that the distri-
butions are similar, as the K–S test does not distinguish the
normalized distributions, with 99% confidence.
Figure 4 is even more straightforward—the higher the
dimensionality,
the lower the similarity values in the simi-
larity lists are.
Result interpretation
The dimensionality parameter confirms
our hypotheses in a manner that we deem clearer than the
previous experiments. Namely, the models are fundamentally
very similar and at the same time different.
We cannot set
123
Toward meaningful notions of similarity in NLP embedding models
Fig. 3
Dimension size similarity-value distributions
Fig. 4
Dimension size similarity values by list indices
any general threshold values,
because average and highest
similarity values are very different. But the distributions only
differ in their standard deviations. This means that they are
fundamentally very similar.
3.5 Dictionary size
In this section,
we evaluate how the dictionary size of the
models affects their similarity values and lists. We train five
models with different
dictionary sizes with the Word2Vec
CBOW algorithm on the full corpus, with d
=
100
,
win
=
5.
Similarity values
Figure 5 shows that the dictionary size does
not affect the similarity-value distribution of the models up
to a certain size.
With very large dictionaries however,
the
numerous noise words (typos,
unmeaningful
words,
con-
traction,
etc.) have a very strong effect on the distribution.
The same effect is visible in the dimensionality experiment,
i.e., when considering many words in the dictionary, the 100
dimensional space is not large enough for the models to dis-
tribute them sufficiently. This leads to wider similarity-value
distributions and even to an asymmetric distribution with the
largest dictionary.
The K–S test confirms the similarity distribution of the
2 million word dictionary model to significantly differ from
Fig. 5
Dictionary size similarity-value distributions
Fig. 6
Dictionary size similarity values by list indices
the others, as
K _S_ p_value
(
sim_dist
2M
,
sim_dist
i
)
<
0
.
01
for every i
∈
{
5k
,
25k
,
100k
,
500k
}
.
Let us now look at the similarities of items with the same
position in the different similarity lists in Fig. 6. The smaller
dictionary models naturally have consistently lower similar-
ity values. This is because there are fewer words which are
close to each other.
Result interpretation
This is the only evaluation where one
distribution does not
have the bell
shape observable in all
other experiments.
This is a consequence of an unreason-
ably large dictionary. The models are not able to successfully
embed the words in the limited space. Apart from this, even
in a 500 thousand word dictionary the hypotheses hold,
as
the distributions are similar.
3.6 Corpus
Now we investigate how the size of the corpus affects sim-
ilarity values and lists.
We compare five different
models
which are trained on differently sized parts of the 1 billion
word benchmark dataset. Sampling is performed by retain-
ing different percentages of the 1 billion words data used for
123
Á. Elekes et al.
Fig. 7
Corpus size similarity-value distributions
Fig. 8
Corpus size similarity values by list indices
the training. Every other parameter of the models is identi-
cal. We train them with the Word2Vec CBOW model, with
d
=
100
,
win
=
5
,
dict_size
=
100
,
000
.
Similarity values
According to Fig. 7, the bigger the corpus,
the narrower the distribution is. We can see that using 25% of
the corpus is almost identical to using 50%, and very close to
using the entire corpus for training. We test the normalized
similarity distributions pairwise with the K–S test. Every p
value again is above 0.01.
This means that the models are
very similar.
Figure 8 shows that at the top 10 similar words there almost
is no difference between the models.
For
higher
indices,
models trained on smaller corpora generally have higher sim-
ilarity values, but the three models trained on bigger corpora
are almost identical.
Result interpretation
We conclude that
models trained on
more than 1 GB (which is 25% of the full corpus in this case)
of text data or approximately 250 million words have almost
identical similarity-value distributions. All distributions are
similar,
but
visibly different
at
the same time,
for smaller
corpus sizes in particular. This confirms our hypotheses.
3.7 Window size
In this section,
we train every model
with the Word2Vec
CBOW algorithm on the full
1 billion word corpus,
with
Fig. 9
Window size similarity-value distributions
Fig. 10
Window size similarity values by list indices
d
=
100
,
dict_size
=
100
,
000 and five different window
sizes.
Similarity values
Figure 9 shows that there is only a slight
difference of similarity values between models trained with
different window sizes. It is noteworthy that, when the win-
dow size is 1, the distribution has a higher mean. This implies
that the model has an area of higher density in the word vector
space. The distributions are very similar without even nor-
malizing them. The pairwise K–S test confirms this, as again
every p value is above 0.01. Consequently, the normalized
distributions are almost identical.
The similarities corresponding to different positions in the
similarity lists in Fig. 10 tell us that the differences between
the models are very small. Still we can see that the smaller
the window, the higher the similarity values are.
Result interpretation
These results are very similar to the
ones for dimensionality, with both figures consistently chang-
ing with the parameters, only at a smaller scale in this current
case. Only the smallest window size parameter, i.e., win
=
1,
interferes with the similarity distribution in an inconsistent
manner, but it also changes the mean of the distribution.
3.8 Optimization function
When training any model
with neural
networks,
one has
to choose an optimization function which approximates the
123
Toward meaningful notions of similarity in NLP embedding models
Fig. 11
Optimization function similarity-value distributions
Fig. 12
Optimization function similarity values by list indices
gradient. In our case, for word embedding models, more pre-
cisely for Word2Vec models, as they are the ones using neural
networks, there are two optimization functions used during
training.
First,
there is Negative Sampling (ns),
which we
have used in this paper
so far,
and,
second,
Hierarchical
Softmax (hs) [11,12].
For more details on the differences
between the functions we refer to [44]. So far we have used
Negative Sampling,
because it
is the one which is closely
related to matrix factorization,
as we have mentioned ear-
lier,
and therefore to the GloVe model
[31].
Next,
even
though the margin is small,
it
constantly outperforms the
Hierarchical Softmax function on word similarity tasks [3].
However,
in this section,
we are not
concerned with the
quality of the models,
but
their similarity values.
Hence,
we train models differing only in their optimization func-
tions and evaluate the similarity-value distributions of the
resulting models. We train every model with the Word2Vec
CBOW algorithm on the full
1 billion word corpus,
with
d
=
100
,
win
=
5
,
dict_size
=
100
,
000.
Similarity values
Figures 11 and 12 show that there are visi-
ble differences in the similarity-value distributions between
models trained with different optimization functions. Although
the K–S test confirms that the distributions are very similar
when normalized, it is noteworthy that the resulting p value
is only slightly higher than 0.01. This indicates a certain dif-
ference between the two distributions. Figure 12 shows that
Fig. 13
Iteration number similarity-value distributions
with Negative Sampling the model generally has higher sim-
ilarity values at the top of the similarity lists.
Result interpretation
The results show that changing the opti-
mization function affects the similarity-value distributions of
the models. Although the distributions are quite similar, we
can deduce a more significant structural difference from the
low K–S test score than the visible differences would suggest.
The similarity of the two distributions explains the small dif-
ference in the evaluation test set scores of models trained with
different optimization functions, which we have referred to
above.
3.9 Iteration number
The training of a word embedding model has several itera-
tions. In one iteration, the learning algorithm passes through
the entire training corpus. For every word in the corpus, the
algorithm updates the respective word vectors. So far in this
paper, we have trained the word embedding models with five
iterations, which is the default value in our model building
toolkit. In this section, we are interested in whether and how
the iteration number affects the similarity-value distributions
of the word embedding models. We train every model with
the Word2Vec CBOW algorithm on the full 1 billion word
corpus, with d
=
100
,
win
=
5
,
dict_size
=
100
,
000, with
five different iteration numbers.
Similarity values
We can see in Figs. 13 and 14 that the more
we iterate through the corpus,
the narrower the similarity-
value distributions and the lower the similarity values at the
top of the similarity lists become. However, the differences
are quite small. For example,
between seven and ten itera-
tions there is almost no visible difference. The distributions
are very similar without even normalizing them, and the K–
123
Á. Elekes et al.
Fig. 14
Iteration number similarity values by list indices
S test confirms that the normalized distributions are almost
identical.
Result interpretation
These results are almost
identical
to
the ones
for
the corpus
size,
with both figures
consis-
tently changing with the parameters.
We conclude that the
similarity-value distributions change only slightly with more
than five iterations. This confirms our original choice of the
iteration number.
3.10 Generalization with additional embedding
models
Now we turn to the alternative models. We evaluate the sim-
ilarity distributions produced by the fastText and Doc2vec
models.
To do so,
we train fastText embeddings (charvec)
on the Wikipedia dump.
4
The Wikipedia articles are shuffled
and trimmed to contain approximately 1 billion words. We
then aggregate these charvec vectors to word vectors (word-
vec).
For Doc2vec we learn two representations: First,
for
each sentence in the Wikipedia corpus, and second, for each
Wikipedia article.
We group the models by their learning
algorithms, i.e., CBOW-like models and SG-like models. All
models are trained with d
=
100
,
win
=
5. The distributions
are calculated using the full dictionary of the models.
CBOW similarity values
Figure 15 shows the similarity-value
distributions of the CBOW models.
We can see that
their
visual
appearances are very different.
It
is interesting that
this is not the case in the next section for the SG models.
Hence, we argue that the CBOW-like algorithms cause the
distortion in the additional
models,
not
the models them-
selves. Compared to the Word2Vec CBOW model, only the
charvec model distribution is similar according to the K–S
test, with 99% confidence, when normalized. In contrast, the
other three models are very different—all three of their K–S
test p values are smaller than 10
−
4
. It is also noteworthy that
the average similarity values are very different for the mod-
els. They are 0.0 for the charvec model, 0.19 for the wordvec
4
Available at http://download.wikimedia.org/enwiki/.
Fig. 15
CBOW models similarity-value distributions
Fig. 16
CBOW models similarity values by list indices
model, 0.37 for the article and 0.53 for the sentence model.
As explained before,
the higher this average value is,
the
more concentrated the vectors are in one part of the space.
At the top of the similarity lists, as seen in Fig. 16, every
model
is very similar,
except
for the charvec model.
The
charvec models top similarity values are decreasing right
away from the start.
This is a pattern we have not seen in
our evaluations yet.
Result interpretation
The results show that different mod-
els have different
similarity-value distributions.
Only the
charvec model is visually similar to the original Word2Vec
CBOW model. Even after normalization, all other models are
significantly different, even though with a different margin.
SG similarity values
Figures 17 and 18 show the results for the
SG models. We can see that, again, the distributions are visi-
bly different, and, except for the charvec model, they also are
not like their respective CBOW model distributions. Accord-
ing to the K–S test,
the charvec and wordvec models are
almost identical to the original SG model when normalized.
Both the article and sentence models also are similar to the
original SG model to some extent. Hence, their respective K–
S test p values are only slightly lower than 0.01. This means
that, although we have found significant evidence that the dis-
123
Toward meaningful notions of similarity in NLP embedding models
Fig. 17
SG models similarity-value distributions
Fig. 18
SG models similarity values by list indices
tributions are different, there are certain similarities between
these distributions and the original SG distribution. It is inter-
esting to note that these model distributions are not distorted
as they previously were in the CBOW versions. We can also
see similarity-value averages for these models different from
the ones for the respective CBOW models except, again, for
the charvec model.
The similarity values at
the top of the
similarity lists are very similar to the respective values of the
CBOW models.
Result interpretation
The results show that
the similarity-
value distributions
of
the models
highly depend on the
learning algorithm used, i.e., CBOW or SG. Only the charvec
model is similar to the respective CBOW model. However,
in contrast to the CBOW models, SG models are very similar
to the original Word2Vec SG model.
We conclude that the
hypotheses also hold for the SG models, as their similarity-
value distributions are visibly different, but at the same time
very similar when normalized.
3.11 Summarizing parameter effects
Our
evaluations
in this
section have confirmed the two
hypotheses.
We have shown that
different
algorithms and
parameter settings indeed affect
the value distributions of
embedding models significantly,
but at the same time they
have the same abstract shape. All value distributions of the
models are bell-shaped, except for one unrealistic setup of the
original model and for several models not based on words.
This remarkable robustness implies that one can now work
with one specific model and adjust the thresholds calculated
to other models later if necessary.
To our knowledge, such systematic experiments have not
been done for embedding models before. For systematic eval-
uations of the effect
of parameters on the quality of word
embedding models see Hill et al. [22], Altszyler et al. [45],
Chiu et al.
[43] and Lin et al.
[46].
These studies evaluate
how the corpus size, window size and dimensionality affect
the results of the models on similarity and analogy tasks. We
will show in the next section that all these evaluations suf-
fer from one thread of validity: They do not take the size of
the similarity values into consideration when comparing the
similarity of two word pairs.
4 Finding meaningful similarity values
In this section,
we answer the question when exactly sim-
ilarity values are meaningful
in word embedding models.
First,
we show that
intuitive similarity thresholds do not
exist. Then, we propose a general method to find meaningful
similarity-value thresholds for a given model and baseline
(e.g., WordNet) and examine the validity of this method with
various models.
4.1 Investigation objectives
Reviewing various approaches [24,25] have revealed that
their evaluations compare similarity values and list indices
without taking their size into account. This means that they
deem, say, two word pairs with similarity values 0.8 and 0.7
just
as different
as ones with values
−
0
.
2 and
−
0
.
1.
But
there is no examination of the distribution of the similarity
values of word vectors indicating that this is reasonable. In
fact, it might turn out that a more differentiated perspective
is required. From Sect. 3, we already know the distribution
characteristics of the similarity values of the word vectors,
for example their average and highest similarities.
But we
do not yet know how vector similarity corresponds to word
similarity, such as similarity values in WordNet.
4.2 Finding meaningful similarity thresholds
We now examine experimentally whether meaningful thresh-
olds
for
similarity values
exist.
Many approaches
using
similarity values or lists implicitly presume this, as they for
example only work with the top k most similar words. Our
findings indicate that respective results may be misleading.
123
Á. Elekes et al.
Fig. 19
LCH scores distribution
4.2.1 Experiment setting
Our procedure is similar to the one in Sect. 3.2. The main dif-
ference is that we compare the results to a baseline, WordNet
in this case. We conduct two series of experiments, one for
similarity values and one for lists. In both cases, we calculate
word-pair-similarity aggregates, one grouped by values, the
other one grouped by list indices, based on WordNet simi-
larity scores. We do so in order to understand to what extent
similarity values are meaningful in embedding models. We
use the Leacock and Chodorow (LCH) [47] similarity mea-
sure in WordNet
for the evaluation.
We have chosen this
measure because,
according to the taxonomy of [48],
it is
not corpus-based, but knowledge-based. This means that it
does not use any external resource or corpus,
but only the
WordNet
ontology itself [48].
It
also is a popular,
highly
researched measure and has proven to be a useful baseline
for semantic similarity [48–50].
The LCH measure scores
are on a [0, 3.64] scale, with a score of 3.64 corresponding
to identical words. We have calculated the similarity-value
distribution of the LCH measure just as we did in Sect. 3 for
the embedding models. Figure 19 shows the distribution. For
the sake of completeness, we compare the normalized LCH
distribution to the CBOW and SG model distributions with
the K–S test,
as we have done in Sect.
3.
We find that the
LCH distribution can be distinguished from the distributions
of the word embedding models with 99% confidence.
For more information on similarity measures in WordNet
see Meng et al. [51]. We have implemented our experiments
with WordNet
using the NLTK python toolkit
[52].
In all
our experiments in this section, the baseline similarity mea-
sure (LCH) is replaceable. This means that one simply can
rerun any experiment with a more specific, say, corpus-based
similarity measure, as well as with another model.
The model
we use in this section is trained with the
CBOW algorithm on the full
1 billion word corpus,
with
d
=
100
,
win
=
5
,
dict_size
=
100
,
000, the default model
and parameter settings in the gensim Word2Vec toolkit.
4.2.2 Similarity value and list experiments
For
the first
experiment,
we compute the similarity val-
ues of
every word to any other
word in the dictionary:
w
p
i
,
j
is
a word pair
containing words
w
i
and
w
j
for
i
,
j
∈
{
1
, . . . ,
100000
}
, t
i
,
j
is their similarity. We now group
these word pairs by their similarity value in 0.01 intervals:
G
−
1
.
0
,
G
−
0
.
99
, . . . ,
G
0
.
0
,
G
0
.
01
, . . . ,
G
1
.
0
are these groups.
To illustrate, G
0
.
05
contains all word pairs
w
p
i
,
j
for which
0
.
04
<
t
i
,
j
≤
0
.
05 holds.
Then,
we calculate the average
similarity with the LCH measure in each group:
avg_sim
(
G
k
)
=
average

LCH_dist

w
p
i
,
j

,
where
w
p
i
,
j
∈
G
k
.
In the second experiment, we create the full similarity lists for
every word in the dictionary,
w
i
,
1
, w
i
,
2
, . . . , w
i
,
100000
, i.e.,
for every i
∈
{
1
, . . . ,
100000
}
.
We create groups of word
pairs
(
G
1
, . . . ,
G
100000
)
. G
k
contains the pair

w
i
, w
i
,
k

for
every i
∈
{
1
, . . . ,
100
,
000
}
. We then calculate the average
similarity for every group with the LCH measure with the
same formula as above for the similarity-value groups.
For
both experiments,
if
a word is not
in the Word-
Net dictionary, we remove all word pairs including it from
the groups, in order to make the aggregation unbiased. We
observe that the standard deviations are relatively high in the
groups: In the similarity-value groups, it is between 0.25 and
0.55, in the similarity-list groups between 0.25 and 0.6. We
will return to this observation when discussing the outcomes
of the experiments.
In the similarity-value-distribution experiments, we eval-
uate the results only for values between
−
0
.
4 and 0.8. This
is because the small number of word pairs with similarity
values outside of this interval makes the data in these ranges
noisy. This is in line with our parameter-evaluation results.
Namely, we can see from the graphs in Sect. 3 that the vast
majority of word pairs has similarity values in this range for
the model used in this section.
To find meaningful threshold values, we check the plots of
the averages of the similarity-value distributions for patterns
that could imply meaningful values. We do so in two steps.
Our first step is an intuitive naïve inspection of the figures;
the second step is a statistical analysis of the graphs. We now
discuss these steps.
4.3 A naïve approach to find similarity thresholds
In this section, we describe a naïve visual approach to inspect
the similarity value and list
figures.
The naïve inspection
is important, because the statistics-based approaches to find
similarity thresholds described latter follow the same intu-
ition as described in this section.
123
Toward meaningful notions of similarity in NLP embedding models
Fig. 20
a, b LCH scores aggregates by similarity values
Fig. 21
a, b LCH score aggregates by list indices
When analyzing the results visually, we hope to find hori-
zontal segments in the result graph or other phenomena such
as breaks, i.e., flat segments of the graph followed by a steep
incline or decline, which might stand for certain properties
of the models.
A horizontal
segment,
for example,
would
mean that
there is no difference in similarity between the
values forming this line. To illustrate further, imagine that in
Fig. 21b there would be horizontal between list indices 800
and 1000.
Then,
we could interpret
this as follows:
There
is no general difference in similarity between a word being
the 800th or the 1000th most similar word to a given word.
Thus, it is meaningless to differentiate between words at these
similarities. The same would follow for the similarity-value
distribution if there was a horizontal segment there.
Other
phenomena such as a break in the figure would imply a gen-
eral change in similarity. For example, if there was a break,
we could interpret it as a threshold between relevant and irrel-
evant similarity values at first sight. However, as is observable
in Figs. 20 and 21, such a naïve approach does not yield any
useful result in our case. This is because there are no obvious
horizontal segments or breaks in the graphs.
4.4 Toward meaningful threshold values based on
unequal mean values
The previous step has not identified any patterns pointing to
intuitive threshold values for similarity. Hence, we now strive
for a statistically sound derivation of meaningful threshold
values, in contrast to a mere visual inspection.
4.4.1 Confidence-based threshold identification
The general idea is examining the results of our experiments
with statistical tests. We test the hypothesis that two popu-
lations have equal means, without assuming that they have
equal variance. Here, these populations are the LCH scores of
two groups of word pairs. Formally, such a group
(
LCH_G
k
)
is as follows: LCH_G
k
=

LCH_dist

w
p
i
,
j

: w
p
i
,
j
∈
G
k

,
where G
k
is either
a similarity value or
a similarity-list
group, as introduced in Sect. 4.2.2. We use Welch’s unequal
variances t test [53] for our experiments, a widely used two-
sample statistical
test
for this problem.
So the answers to
the research questions from the introduction are statistical in
nature, i.e., we will give answers with a certain confidence
such as 99%, based on Welch tests.
Our
tests
are
as
follows:
We
compare
two groups
(LCH_G
k
, LCH_G
l
)
, as introduced above, with the Welch
test.
The groups are obtained by similarity values (Experi-
ment 1) or by similarity-list indices (Experiment 2). The null
hypothesis in a Welch test is that the two groups have equal
means. One either rejects the null hypothesis at a confidence
level chosen a priori (99% in our case), or there is not enough
evidence to do so.
In case of a rejection,
we conclude that
there is a significant difference between the two groups in
terms of similarity. That is, the group with the higher LCH
mean contains significantly more similar word pairs.
4.4.2 Experimental results for similarity values
For the similarity-value-group evaluation,
we first
test
the
exemplary questions asked in the introduction.
We then
investigate generally to what extent similarity-value groups
are different.
Q1 Are low values of similarity comparable to each other?
Q2 If Words A and B have a higher similarity value than A
and C (say 0.2 for A and B,
0.1 for A and C),
is A more
similar to B than to C?
For Q2, we test the following null hypothesis: The aggregated
LCH scores have the same mean values for the word pairs
with a 0.10 and with a 0.20 similarity value.
The number
computed on our corpus is as follows:
welch_test_ p_value
(
LCH_G
0
.
10
,
LCH_G
0
.
20
)
=
5
.
19e
−
9
<
0
.
01
So we conclude with 99% confidence that the hypothesis is
false. We infer that the word pairs with 0.20 similarity values
tend to be more similar to each other than the pairs with 0.10
similarity values. In other words, to answer Q1, even at these
low levels of similarity, differences in value have a meaning.
We now turn to the systematic experiment concerning this
model
and generalize the findings in Sect.
4.5.
For every
group, we search for the next successive group, i.e., having a
higher index,
which significantly differs in its LCH scores
with 99% confidence (cf.
Fig.
22).
We explain the inter-
123
Á. Elekes et al.
Fig.
22
Groups with significant differences in LCH mean scores by
similarity values
pretation of the values with the following example: For the
−
0
.
30 similarity-value group (x -axis),
the next successive
group which significantly differs in similarity is the
−
0
.
17
similarity-value group (y-axis). Starting from the
−
0
.
18 (x -
axis) group, every successive group has a significantly higher
LCH score mean than the previous one. On the other hand,
there is a bend in the plot at
−
0
.
18.
It means that,
at low
values of similarity, i.e., below
−
0
.
18, there is no significant
evidence that a higher similarity-value group implies a higher
LCH score. Hence, we conclude that below the
−
0
.
18 sim-
ilarity value,
there is no significant difference between the
groups.
Another way to understand these values is as follows:
Somewhat
naturally,
we assume that
the
−
0
.
40 similarity
value group contains dissimilar word pairs. This is because
it
is the group with the pairs with the smallest
similarity
values.
For this group,
we calculate the next group having
a significantly higher LCH mean score,
this is the
−
0
.
18
similarity-value group. This means that between
−
0
.
40 and
−
0
.
18 there is no significant
difference in LCH scores
between the groups. Based on our assumption that the
−
0
.
40
group contains dissimilar word pairs, we conclude that the
word pairs with similarity values between
−
0
.
40 and
−
0
.
18
are dissimilar.
Because of the relatively high standard deviation in the
groups, we cannot conclude that all word pairs in these groups
are dissimilar, but we can say that the groups do not differ
significantly. For higher similarity values, i.e., above
−
0
.
18,
every group is significantly different, as we have seen. This
means that any increase in similarity, even if it is only 0.01,
implies a higher similarity of the word pairs. Again, we can-
not say this for every specific word pair, because of the high
deviation, but only in general terms, for the groups as a whole.
Overall, we conclude that the similarity-value groups are
significantly different from each other above
−
0
.
18 and not
different below this value. This also can be seen visually, as
there is a specific bend in Fig. 22 at
−
0
.
18 on the x
-axis.
4.4.3 Experimental results for similarity lists
We now investigate the same exemplary questions asked in
the introduction with similarity lists.
Q3 Does being in the top 100 list
of most
similar words
always imply similarity, or does not being in the top 500 list
always imply dissimilarity?
Q4 What are meaningful threshold values, and how to find
them?
We answer these questions with the following experiments.
Our experiments with similarity lists actually are the same
as just before, but with the word pairs being grouped by list
indices. Figure 22 shows that there is a long almost horizon-
tal noisy stripe of LCH averages. We are making the same
tests for the index groups (G
k
,
k
∈
{
1
, . . . ,
100
,
000
}
)
as
we have with the similarity-value groups,
again with 99%
confidence. For every index group, we search the next group
with higher index with a significantly different LCH simi-
larity mean using that test. The figure shows the following:
At the smallest indices, even small differences in the indices
imply significantly different mean score. But as the indices
increase, the bigger the differences have to be between groups
to yield a significant difference in the mean.
Figure 23a–c shows that there are certain indices which
generally identify the significant differences. These indices
correspond to groups
with particularly high LCH mean
scores,
and because of
this,
they are significantly differ-
ent from many lower index groups. The horizontal lines in
Fig. 23a identify them.
Just as we have done with the similarity values, we assume
that the last group of word pairs,
i.e.,
pairs consisting of a
word and its least similar word, are dissimilar. We test two
items:
•
Which is the last of these groups that is significantly dif-
ferent
from the very last
group regarding LCH score?
Formally, what is the highest index
(
i
)
so that, for every
j
>
i ,
welch_test_ p_value

LCH_G
100000
,
LCH_G
j

>
0
.
01
holds?
•
What is the first group that is not significantly different
from the last group? Formally, what is the lowest index
(
i
)
so that
welch_test_ p_value

LCH_G
100000
,
LCH_G
j

<
0
.
01
holds, for every j
<
i ?
The answers to these questions are the 31
,
584th group and
the 6094th group, respectively, for the specific model we are
working with in this section. Namely, the horizontal line in
123
Toward meaningful notions of similarity in NLP embedding models
Fig. 23
a, b, c Groups with significant differences in LCH mean scores
by similarity indices
Fig. 24 is the one separating the groups whose LCH mean
scores are significantly higher than the one of the last group
from the rest. We conclude that indices higher than 31,584 are
statistically not different from the last group. Based on our
assumption, our interpretation is that they contain dissimilar
word pairs. On the other side, all groups with indices below
6094 have a higher LCH mean than the last group. This means
that they all contain significantly more similar word pairs.
Again this does not
mean that
all
the word pairs in these
groups are dissimilar or similar,
respectively,
but
that
the
groups differ significantly.
4.4.4 Implications and external validity
The experimental
results indicate that
a confidence-based
comparison based on statistical tests identifies large ranges
of steady similarity values as well
as large ranges of list
positions where the similarity of word pairs is meaningful.
However, the results so far are specific to the model and text
corpus used. In the next section, we generalize our insights
with further models trained on different corpora to find mean-
ingful similarity values.
4.5 Generalization with additional corpora
The results
from the prior
subsection indicate that
our
approach to identify meaningful similarity values with a sta-
tistical test is promising. The results in Sects. 4.4.2 and 4.4.3
are already interesting for practitioners, as the corpus, embed-
ding model
(with these parameters),
and the baseline are
Fig. 24
Meaningful list indices
widely used. We now show that our approach yields mean-
ingful results with other corpora as well.
4.5.1 Rationale behind the experiments
With the model algorithm (e.g., SG or GloVe) and the param-
eters changing, the similarity values and lists change as well,
cf. Sect. 3. This means that one must adjust the specific num-
bers that identify ranges where similarity is meaningful for
any other model. To show that the procedure we propose is
generally relevant, we train two other models with different
underlying corpora, but with the same model and parameter
setting.
To make the results of the experiments compara-
ble, we use corpora of the same size as before. If the results
(i.e., the plots) will be highly similar to those from Sect. 4.4,
we will claim that our method to find meaningful similarity
thresholds or list sizes is valid in general.
4.5.2 Experimental results
The first dataset we train a model on is a Wikipedia dump (the
same which we have used in Sect. 3.10). The second model is
trained on a 5-gram corpus extracted from the Google Books
n-gram dataset [54]. We have extracted the 5-grams, shuffled
them,
and trimmed the data to have the same size as our
original 1 billion word dataset.
We note that working with
5-grams as the underlying corpus is slightly different from
working with full text corpora. This is because of the limited
size of the 5-grams, i.e., all the sentences considered by the
learning algorithm only have a length of 5. We conduct the
same experiments with the models trained on these corpora
as in Sect. 4.4 to achieve comparable results.
Figure 25 shows that the results are almost identical to
the ones in Sect. 4.4. The structure of the figures and even
the values are very similar. For all three models, the similar-
ity values which are not meaningful are between
−
0
.
4 and
approximately
−
0
.
2.
123
Á. Elekes et al.
Fig. 25
Significantly different groups by similarity value
Fig. 26
a, b Significantly different groups by list indices for the models
trained on 5-grams (a), Wikipedia (b)
As for the similarity lists, we again see that the figures are
very similar, but they naturally differ in the actual values. We
also test the two models regarding the same questions we have
asked earlier, namely: What is the last group which is signif-
icantly different in LCH similarity score from the last group
overall? What is the first group that is not significantly dif-
ferent from the last group? The results are 28,570 and 5889,
respectively, for the model trained on the Wikipedia corpus
and 35,402 and 6408, respectively, for the model trained on
the 5-grams. These numbers also are very much like the ones
calculated before (Fig. 26).
All this shows that our approach to derive those threshold
values is independent of the underlying corpus. The approach
is applicable on any kind of corpus, and only the model selec-
tion and its parameters influence the resulting numbers.
4.6 Robustness of evaluations methods
The results of Sect. 4 so far indicate that meaningful ranges
of similarity values exist. More specifically, for these values,
it is meaningful to compare two word pairs with different
similarity values and to conclude that higher values imply
greater semantic similarity. In contrast, the values outside of
these regions are either very noisy,
because of the lack of
word pairs with the respective values, or indistinguishable in
terms of similarity.
As the introduction has pointed out, evaluation methods
compare word-pair-similarities on the full scale of similar-
ity values and lists. Based on our results so far, we propose
that the comparison should only be done at certain ranges of
similarities. One can determine these ranges using the exper-
iments proposed in Sect. 4.4. In particular, we propose that
only values should be compared which significantly differ
in mean similarity scores,
cf.
Fig.
24.
For example,
when
evaluating the model in this section one should only com-
pare word-pair-similarity values when the values are above
−
0
.
18. It is also noteworthy that every 0.01 difference in this
range implies a significantly different similarity. For the list
indices, similar conclusions are feasible. For example, with
the model of this section,
we recommend to compare only
indices below approximately 31,500.
With other
models,
these values and indices could be
different, but the method how to calculate them and the impli-
cations are the same.
This means that
for any embedding
model we propose to calculate these values first, to improve
any evaluation.
We call
this method similarity-threshold-
aware evaluation.
5 Similarity-threshold-aware evaluation
method
In this section, we introduce two evaluation methods for word
embedding models.
The first
one is the baseline method,
which also is the commonly used approach. The second one
is our new similarity-threshold-aware evaluation method. We
investigate how our method affects results on different word-
similarity-test sets with the various models.
Generally,
the
objective of our method is to allow for a more reliable evalu-
ation of embedding models on test sets. It is important to note
that the objective is not to achieve the highest score possible.
5.1 Evaluation objective and setting
In this subsection, we describe test sets, embedding models
and evaluation methods we use.
5.1.1 Evaluation test sets and embedding models
We use six test
sets to evaluate word similarity.
Each test
set has the same format. They contain word pairs with sim-
ilarity scores,
assigned by human annotators.
The smallest
test set contains 65,
the biggest 3000 word pairs.
The test
sets are Finkelstein et al.’s WordSim353 [24] test set; Bruni
et al.’s MEN test set [25]; Hill et al.’s SimLex-999 test set
[26]; Rubinstein et al.’s RG-65 test set [55], Radinsky et al.’s
Mechanical Turk test set [56] and Luong et al.’s Rare Words
test set [57].
We train three different word embedding models (CBOW,
SG,
GloVe) with the same parameter settings (dict_size
=
100
,
000
,
d
=
100
,
win
=
5
)
on the full
1 billion word
corpus.
123
Toward meaningful notions of similarity in NLP embedding models
5.1.2 Evaluation methods and result interpretation
We evaluate the embedding models with two evaluation
methods.
Baseline method
To evaluate a model
with the baseline
method, we do the following for each similarity test set:
1.
We create two ranked lists of the word pairs both sorted
by similarity value. Both lists share the same word pairs.
a.
The first
list
is sorted according to the scores pro-
vided in the similarity test set, i.e., created by human
annotators.
b.
The second list is sorted according to the similarity
values computed using the embedding model.
2.
We calculate the Spearman’s correlation between both
rankings. This indicates how well the embedding model
similarities reflect the ground truth.
Using this method has two benefits. First, the values of the
scores do not have to be the same as we compare the ranks,
not
explicitly the values.
Usually,
human annotators use a
point system,
e.g.,
from 0 to 10,
where 10 indicates max-
imum similarity.
By contrast,
cosine similarity values are
distributed between [
−
1, 1], and a value of 1 indicates the
highest similarity. Second, the embedding model reflects the
similarity test set well if the rank of most word pairs in both
lists is very similar, but it does not necessarily have to be the
same. Spearman’s correlation expresses this well. The score
can interpreted as follows. A correlation of 1 states that the
ranked lists are identical. A value close to 1 means that the
embedding models reflects the similarity test set very well.
A value close to zero indicates that there is no connection
between the similarity values and the ground truth.
Similarity-threshold-aware method
It
is important
to note
that the objective of our similarity-threshold-aware evalua-
tion method is not to increase the value of the Spearman’s
correlation compared to the baseline method. The objective
is to return a more reliable correlation score. Therefore, the
score might be higher, lower, or even remain the same. The
explanation why our method results in a more reliable score
is justified based on statistics. The intuition is the following.
The difference of our and the baseline method is how we
compute the similarity threshold. Our new method removes
every word pair from both lists where the cosine distance
computed on the embedding models is below the threshold.
This is because differences in the similarity and the resulting
ranks for these word pairs are not reliable, as shown based on
the statistical tests in the prior section.
5
Hence, they might
change the correlation in an unpredictable way. If the order
of these word pairs accidentally is similar to their order in
the test set,
the originally computed correlation score,
i.e.,
the one computed with the baseline method, is too high. In
the opposite case,
the score is too low.
Finally,
in case the
order is randomized,
the score does not
change at
all,
but
the new result still is more reliable.
Consequently,
we can
perceive such word pairs which fall below the threshold as
noise, making the score less reliable. This is why they have
to be excluded from the evaluation.
5.1.3 Research questions
Generally, we expect observable,
but minor changes in the
correlation scores.
Nevertheless,
as this evaluation method
optimization affects all
embedding models,
and improve-
ments reported are often only small, even minor differences
are of
practical
relevance.
To quantify the effect
of
our
method, we focus on the following research questions.
Question 1 Do word pairs exist in the evaluation test sets
which fall below the calculated threshold?
The thresholds may be very low, as we have seen in Sect. 4.
Hence,
it may even turn out that the similarity test sets do
not feature such word pairs at all, or only some of them are
affected. If this was the case, our similarity-threshold-aware
evaluation method would not increase the reliability of the
evaluation results.
Question 2 Does our similarity-threshold-aware evaluation
method observably change the evaluation scores, compared
to the baseline method?
It
is not
obvious whether the scores will
change with our
method. Therefore, we aim at quantifying the effect as well its
direction, i.e., whether the scores tend to increase or decrease.
We conduct systematic experiments to answer this question.
5.2 Evaluation results
5.2.1 Model thresholds
First
we calculate the similarity-value thresholds
for
all
three models. The calculation itself is quite complicated, as
described in Sect. 4, but the output has a very simple structure,
i.e., a floating point number for every model. It represents the
similarity-value threshold. The actual numbers are given in
Table 1.
5
Note,
we find all these word pairs at the end of the model’s list as
they have low similarity values.
123
Á. Elekes et al.
Table 1
Model thresholds
Model
Threshold
CBOW
−
0
.
18
SG
0
.
20
GloVe
−
0
.
15
Table 2
CBOW word pair counts
rg65
ws353
rare w.
simlex999
mturk
men
Total
65
353
2034
999
287
3000
Comparable
64
349
2028
997
287
2983
Not comp. % 0.015
0.011
0.003
0.002
0
0.006
Table 3
CBOW evaluation results
rg65
ws353
rare w.
simlex999
mturk
men
Baseline
0.54
0.53
0.32
0.31
0.57
0.64
Ours
0.55
0.52
0.32
0.31
0.57
0.63
We see that the SG model has a significantly higher thresh-
old value than the other models. This is expected, as the SG
model generally has much higher similarity values on aver-
age, cf. Fig. 1.
5.2.2 Test set evaluation results
For every model, we create two tables. In the first tables, we
count for every test set how many word pairs fall below the
similarity-value threshold.
The first
row shows how many
word pairs exist in the full test set. The second row shows
how many are above the threshold,
and the third one how
many are below.
The second tables show the evaluation results for the mod-
els. In the first row, the numbers represent the results with the
baseline method, i.e., with all word pairs in the test sets. The
second row shows the results only for the word pairs which
are comparable, i.e., are above the threshold.
Tables 2 and 3 show that there are word pairs in the test
sets for the CBOW model which fall below the threshold.
However,
there are only a few of them.
Consequently,
the
new evaluation method changes the evaluation results only
slightly.
The tables for the SG model show that much more word
pairs fall below the threshold for this model,
compared to
the CBOW model. For most test sets, the evaluation results
change significantly. We also see that the evaluation scores
for different test sets are behaving rather inconsistently with
the different test sets. For instance, the score is significantly
better on the ws353 or men test sets, it is worse on the mechan-
ical turk test set, and it is the same on the rg65 (Tables 4, 5).
Table 4
SG word pair counts
rg65
ws353
rare w.
simlex999
mturk
men
Total
65
353
2034
999
287
3000
Comparable
59
318
1838
960
262
2672
Not comp. % 0.092
0.099
0.096
0.039
0.087
0.107
Table 5
SG evaluation results
rg65
ws353
rare w.
simlex999
mturk
men
Baseline
0.59
0.6
0.37
0.33
0.6
0.7
Ours
0.59
0.55
0.33
0.28
0.63
0.64
Table 6
GloVe word pair counts
rg65
ws353
rare w.
simlex999
mturk
men
Total
65
353
2034
999
287
3000
Comparable
62
350
2023
989
287
2991
Not comp. % 0.045
0.008
0.005
0.01
0
0.003
Table 7
GloVe evaluation results
rg65
ws353
rare w.
simlex999
mturk
men
Baseline
0.59
0.43
0.21
0.27
0.43
0.64
Ours
0.6
0.42
0.21
0.27
0.43
0.64
The tables for the GloVe model are very similar to the ones
for the CBOW model.
Very few word pairs fall below the
threshold. This leads to very small changes in the evaluation
scores (Tables 6, 7).
5.2.3 Evaluation results: summary
Having all these results,
we can answer the research ques-
tions presented previously.
Question 1 Do word pairs exist in the evaluation test sets
which fall below the calculated threshold?
Our results confirm that such word pairs exist in almost every
model and test set combination. As expected, their number is
model dependent and rather low. For the CBOW and GloVe
models, less than 1% of the word pairs usually fall below the
respective thresholds. For both models, the rg65 test set yields
a particularly high number (1.5 and 4.5% respectively), most
likely due to the small size of this test set. On the other hand,
many word pairs fall below the models threshold for the SG
model, the percentage ranging between 4 and 11%.
Question 2 Does our similarity-threshold-aware evaluation
method observably change the evaluation scores, compared
123
Toward meaningful notions of similarity in NLP embedding models
to the baseline method?
The answer to this question again is highly model dependent.
As hypothesized earlier, the change is observable, but not sig-
nificant, with the exception of the SG model. This is mainly
because of the sheer number of word pairs being incompa-
rable for the different
models.
We do not
see a consistent
increase or decrease in the evaluation scores.
This further
confirms that our method correctly removes the noise intro-
duced by word pairs falling below the similarity threshold.
We conclude that word pairs whose similarity is below the
model threshold should be excluded from the evaluation test
sets. Respective code that calculates the thresholds, removes
the incomparable word pairs from the test sets and outputs the
tables shown above for any model is available at our website.
6 Conclusions and future work
One set of methods that allow for in-depth analysis of nat-
ural
language data are word embedding models,
such as
Word2Vec or GloVe. Word embedding models quantify sim-
ilarities of words. This is used in a broad range of approaches.
However,
the notion of similarity and the meaning of sim-
ilarity values do remain ambiguous. In this paper, we have
studied when exactly such values are meaningful
in word
embedding models. To this end, we have designed and con-
ducted two series of experiments. With the first experiments,
we have shown how the distribution of
similarity values
changes when changing the embedding model
algorithms
or their parameters. As a result, we have seen that similarity
values highly depend on the algorithms and parameters, i.e.,
the same value can represent different grades of similarity in
different models. The second set of experiments has yielded
an evaluation method that finds meaningful similarity values
in embedding models. An important insight is that meaning-
ful intervals of similarity values do exist, and one can actually
find them for a specific embedding model. We have shown
that these insights are corpus-independent; they only depend
on the learning algorithms and the parameters evaluated
earlier. Finally, we have proposed a new similarity-threshold-
aware evaluation method of word embedding models, built
on top of the baseline method. We have compared the base-
line method and our similarity-threshold-aware evaluation
method with several models on various similarity tasks. We
have shown that our method indeed can affect the evaluation
results significantly. We have concluded that resulting scores
are more reliable for practical purposes.
Our conclusions strengthened our intention to generalize
our threshold-computation method in the future. We not only
aim at
generalizations within word embedding models or
NLP, but for any field of scientific research. We intend to find
use cases where different (not only similarity) scores can be
compared in order to find meaningful threshold values.
References
1.
Erk, K.: Vector space models of word meaning and phrase meaning:
a survey. Language and Linguistics Compass 6, 635–653 (2012)
2.
Clark,
S.:
Vector Space Models of Lexical
Meaning.
Handbook
of Contemporary Semantic Theory, pp. 493–522. Wiley, London
(2013)
3.
Baroni,
M.,
Dinu,
G.,
Kruszewski,
G.:
Don’t
count,
predict! A
systematic comparison of context-counting vs. context-predicting
semantic vectors. In: ACL, vol. 1 (2014)
4.
Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harsh-
man, R.: Indexing by latent semantic analysis. J. Am. Soc. Inf. Sci.
41, 391 (1990)
5.
Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent Dirichlet allocation. J.
Mach. Learn. Res. 3, 993–1022 (2003)
6.
Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C.: A neural proba-
bilistic language model. J. Mach. Learn. Res. 3, 1137–1155 (2003)
7.
Collobert,
R.,
Weston,
J.,
Bottou,
L.,
Karlen,
M.,
Kavukcuoglu,
K., Kuksa, P.: Natural language processing (almost) from scratch.
J. Mach. Learn. Res. 12, 2493–2537 (2011)
8.
Collobert,
R.,
Weston,
J.: A unified architecture for natural lan-
guage processing: deep neural networks with multitask learning.
In: Proceedings of the 25th International Conference on Machine
Learning (2008)
9.
Huang, E.H., Socher, R., Manning, C.D., Ng, A.Y.: Improving word
representations via global context and multiple word prototypes.
In: Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers, vol. 1 (2012)
10.
Pennington, J., Socher, R., Manning, C.D.: Glove: global vectors
for word representation. In: EMNLP (2014)
11.
Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation
of word representations in vector space. (2013). arXiv:1301.3781
12.
Mikolov,
T.,
Sutskever,
I.,
Chen,
K.,
Corrado,
G.S.,
Dean,
J.:
Distributed representations of words and phrases and their compo-
sitionality. In: Advances in Neural Information Processing Systems
(2013)
13.
Passos,
A.,
Kumar,
V.,
McCallum,
A.:
Lexicon infused phrase
embeddings for named entity resolution (2014). arXiv:1404.5367
14.
Komatsu, H., Tian, R., Okazaki, N., Inui, K.: Reducing lexical fea-
tures in parsing by word embeddings. In: Proceedings of the 29th
Pacific Asia Conference on Language, Information and Computa-
tion (2015)
15.
Wang,
W.Y.,
Yang,
D.:
That’s
so annoying!!!:
a lexical
and
frame-semantic embedding based data augmentation approach to
automatic categorization of
annoying behaviors using petpeeve
tweets.
In:
Proceedings of
the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP 2015), Lisbon,
Portugal (2015)
16.
Klein, B., Lev, G., Sadeh, G., Wolf, L.: Fisher vectors derived from
hybrid Gaussian–Laplacian mixture models for image annotation
(2014). arXiv:1411.7399
17.
Devlin,
J.,
Zbib,
R.,
Huang,
Z.,
Lamar,
T.,
Schwartz,
R.M.,
Makhoul, J.: Fast and robust neural network joint models for sta-
tistical machine translation. In: ACL, vol. 1 (2014)
18.
Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning
with neural networks. In: Advances in Neural Information Process-
ing Systems (2014)
19.
Liu, S., Yang, N., Li, M., Zhou, M.: A recursive recurrent neural
network for statistical machine translation. In: ACL, vol. 1 (2014)
123
Á. Elekes et al.
20.
Levy, O., Goldberg, Y., Dagan, I.: Improving distributional simi-
larity with lessons learned from word embeddings. Trans. Assoc.
Comput. Linguist. 3, 211–225 (2015)
21.
Lebret,
R.,
Collobert,
R.:
Rehabilitation of count-based models
for word vector representations.
In: International Conference on
Intelligent Text Processing and Computational Linguistics (2015)
22.
Hill,
F.,
Cho,
K.,
Jean,
S.,
Devin,
C.,
Bengio,
Y.: Not all neural
embeddings are born equal (2014). arXiv:1410.0718
23.
Schnabel,
T.,
Labutov,
I.,
Mimno,
D.,
Joachims,
T.:
Evaluation
methods for unsupervised word embeddings.
In: Proceedings of
EMNLP (2015)
24.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z.,
Wolfman, G.,
Ruppin,
E.: Placing search in context: the concept
revisited. In: Proceedings of the 10th International Conference on
World Wide Web (2001)
25.
Bruni,
E.,
Boleda,
G.,
Baroni,
M.,
Tran,
N.-K.:
Distributional
semantics in technicolor. In: Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics: Long Papers,
vol. 1 (2012)
26.
Hill, F., Reichart, R., Korhonen, A.: Simlex-999: evaluating seman-
tic models with (genuine) similarity estimation. Comput. Linguist.
41, 665–695 (2016)
27.
Batchkarov, M., Kober, T., Reffin, J., Weeds, J., Weir, D.: A cri-
tique of word similarity as a method for evaluating distributional
semantic models. In: Proceedings of the 1st Workshop on Evalu-
ating Vector Space Representations for NLP (2016)
28.
Avraham,
O.,
Goldberg,
Y.:
Improving reliability of word simi-
larity evaluation by redesigning annotation task and performance
measure (2016). arXiv:1611.03641
29.
Miller, G.A.: WordNet: a lexical database for English. Commun.
ACM 38, 39–41 (1995)
30.
Elekes, A., Schäler, M., Boehm, K.: On the various semantics of
similarity in word embedding models. In: 2017 ACM/IEEE Joint
Conference on Digital Libraries (JCDL) (2017)
31.
Levy, O., Goldberg, Y.: Neural word embedding as implicit matrix
factorization. In: Advances in Neural Information Processing Sys-
tems (2014)
32.
Shi,
T.,
Liu,
Z.:
Linking
GloVe
with
word2vec
(2014).
arXiv:1411.5595
33.
Rehurek,
R.,
Sojka,
P.: Software framework for topic modelling
with large corpora. In: Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks (2010)
34.
Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word
vectors with subword information (2016). arXiv:1607.04606
35.
Miura, Y., Taniguchi, M., Taniguchi,T., Ohkuma, T.: A simple scal-
able neural
networks based model
for geolocation prediction in
Twitter. In: WNUT 2016, vol. 9026924, p. 235 (2016)
36.
Seo,
S.,
Huang,
J.,
Yang,
H.,
Liu,
Y.:
Representation learning
of users and items for review rating prediction using attention-
based convolutional neural network.
In: 3rd International Work-
shop on Machine Learning Methods for Recommender Systems
(MLRec)(SDM’17) (2017)
37.
Le, Q., Mikolov, T.: Distributed representations of sentences and
documents.
In: Proceedings of the 31st International Conference
on Machine Learning (ICML-14) (2014)
38.
Badjatiya, P., Gupta, S., Gupta, M., Varma, V.: Deep learning for
hate speech detection in tweets. In: Proceedings of the 26th Inter-
national Conference on World Wide Web Companion (2017)
39.
Joulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for
efficient text classification (2016). arXiv:1607.01759
40.
Kusner,
M.,
Sun,
Y.,
Kolkin,
N.,
Weinberger,
K.:
From word
embeddings to document distances.
In: International Conference
on Machine Learning (2015)
41.
Massey Jr., F.J.: The Kolmogorov–Smirnov test for goodness of fit.
J. Am. Stat. Assoc. 46, 68–78 (1951)
42.
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P.,
Robinson, T.: One billion word benchmark for measuring progress
in statistical language modeling (2013). arXiv:1312.3005
43.
Chiu, B., Korhonen, A., Pyysalo, S.: Intrinsic evaluation of word
vectors fails to predict extrinsic performance. ACL 2016, 1 (2016)
44.
Rong,
X.:
word2vec
parameter
learning
explained
(2014).
arXiv:1411.2738
45.
Altszyler, E., Sigman, M., Slezak, D.F.: Comparative study of LSA
vs Word2vec embeddings in small corpora: a case study in dreams
database (2016). arXiv:1610.01520
46.
Lin,
C.-C.,
Ammar,
W.,
Dyer,
C.,
Levin,
L.: Unsupervised POS
induction with word embeddings (2015). arXiv:1503.06760
47.
Leacock, C., Chodorow, M.: Combining local context and WordNet
similarity for word sense identification.
WordNet: An Electronic
Lexical Database, vol. 49, pp. 265–283 (1998)
48.
Mihalcea,
R.,
Corley,
C.,
Strapparava,
C.:
Corpus-based and
knowledge-based measures of text semantic similarity. In: AAAI
(2006)
49.
Budanitsky, A., Hirst, G.: Evaluating wordnet-based measures of
lexical semantic relatedness. Comput. Linguist. 32, 13–47 (2006)
50.
Budanitsky,
A.,
Hirst,
G.:
Semantic distance in WordNet:
an
experimental, application-oriented evaluation of five measures. In:
Workshop on WordNet and Other Lexical Resources (2001)
51.
Meng, L., Huang, R., Gu, J.: A review of semantic similarity mea-
sures in wordnet. Int. Hybrid Inf. Technol. 6, 1–12 (2013)
52.
Bird,
S.: NLTK: the natural language toolkit.
In: Proceedings of
the COLING/ACL on Interactive Presentation Sessions (2006)
53.
Welch, B.L.: The generalization of Student’s’ problem when sev-
eral
different
population variances are involved.
Biometrika 34,
28–35 (1947)
54.
Michel, J.-B., Shen, Y.K., Aiden, A.P., Veres, A., Gray, M.K., Pick-
ett,
J.P.,
Hoiberg,
D.,
Clancy,
D.,
Norvig,
P.,
Orwant,
J.,
et
al.:
Quantitative analysis of culture using millions of digitized books.
Science 331, 176–182 (2011)
55.
Rubenstein, H., Goodenough,
J.B.: Contextual correlates of syn-
onymy. Commun. ACM 8, 627–633 (1965)
56.
Radinsky,
K.,
Agichtein,
E.,
Gabrilovich,
E.,
Markovitch,
S.:
A
word at a time: computing word relatedness using temporal seman-
tic analysis. In: Proceedings of the 20th International Conference
On World Wide Web (2011)
57.
Luong, T., Socher, R., Manning, C.D.: Better word representations
with recursive neural networks for morphology. In: CoNLL (2013)
123
