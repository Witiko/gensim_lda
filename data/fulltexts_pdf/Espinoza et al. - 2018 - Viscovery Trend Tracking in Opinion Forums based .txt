Viscovery:
Trend Tracking in Opinion Forums based on
Dynamic Topic Models
Ignacio Espinoza
a
, Marcelo Mendoza
a,b,∗
, Pablo Ortega
a
, Daniel Rivera
a
,
Fernanda Weiss
a
a
Universidad T´ecnica Federico Santa Mar´ıa, Santiago, Chile
b
Centro Cient´ıfico y Tecnol´
ogico de Valpara´ıso, CCTVal, Chile
Abstract
Opinions in forums and social networks are released by millions of people due to
the increasing number of users that use Web 2.0 platforms to opine about brands
and organizations.
For enterprises or government agencies it is almost impossi-
ble to track what people say producing a gap between user needs/expectations
and organizations actions.
To bridge this gap we create Viscovery,
a platform
for opinion summarization and trend tracking that is able to analyze a stream
of opinions recovered from forums.
To do this we use dynamic topic models, al-
lowing to uncover the hidden structure of topics behind opinions, characterizing
vocabulary dynamics.
We extend dynamic topic models for incremental learn-
ing,
a key aspect needed in Viscovery for model
updating in near-real
time.
In addition,
we include in Viscovery sentiment analysis,
allowing to separate
positive/negative words for a specific topic at different levels of
granularity.
Viscovery allows to visualize representative opinions and terms in each topic.
At a coarse level of granularity, the dynamic of the topics can be analyzed using
a 2D topic embedding,
suggesting longitudinal topic merging or segmentation.
In this paper we report our experience developing this platform, sharing lessons
learned and opportunities that arise from the use of sentiment analysis and topic
modeling in real world applications.
1.
Introduction
The emergence of the Web 2.0 has allowed that millions of users can send
posts and opinions about celebrities,
institutions,
organizations and brands.
As the volume of opinions in forums and blogs increases,
the need to develop
effective platforms for opinion search has become urgent.
In the stream of
opinions, trend tracking is a key building block of this kind of platforms, allowing
∗
Corresponding author
Email
addresses:
ignacio@novaviz.com (Ignacio Espinoza), marcelo.mendoza@usm.cl
(Marcelo Mendoza), pablo@novaviz.com (Pablo Ortega), daniel@novaviz.com (Daniel
Rivera), fernanda@novaviz.com (Fernanda Weiss)
Preprint submitted to Arxiv
May 2, 2018
arXiv:1805.00457v1 [cs.IR] 1 May 2018
to describe what users expect about institutions/organizations and how opinion
trends evolve over time.
Effective tools for opinion browsing need to incorporate opinion aggregation
functionalities, being relevant to obtain descriptions of each trend.
In addition,
the sentiment orientation of opinions w.r.t.
named entities lights up how users
act/react in front of
a given organization.
Sentiment analysis methods are
helpful in this task.
As the volume of opinions is huge, the need to develop effective aggregation
methods over opinions is the key building block of any opinion trend platform.
Opinion clustering is a way to aggregate opinions.
Using hard clustering al-
gorithms each opinion can be assigned to a single class.
However,
documents
achieve a best description by modeling its content with a mixture of
topics,
where each topic is defined as a probability distribution over words.
In this
way,
opinions belong to several
topics with different degrees of
membership.
This is the reason why documents are in general modeled using mixed member-
ship models,
and in particular Latent Dirichlet Allocation (LDA) models [2],
allowing to uncover the hidden structure of topics behind a corpus.
LDA has
made improvements in information retrieval
tasks [10]
and outperforms stan-
dard text clustering algorithms being the state-of-the-art method for document
aggregation.
In Chile,
the National
Agency of Consumers
1
centralize complaints about
brands and their products.
As it is almost impossible to follow each complaint,
consumers may be disappointed due to the slow response of
the Agency to
their needs.
To bridge this gap we created Viscovery,
a platform for opinion
aggregation and trend tracking that allow to browse a huge volume of opinions
in a few minutes.
The core of
Viscovery is based on Dynamic Topic Models
(DTM) [1],
an extension of
LDA [2]
capable of
model
a time sliced corpus,
being able to estimate dependencies between vocabularies across time slices.
To
create Viscovery, we had to develop an incremental learning component able to
update a model
with new opinions.
Our DTM update method achieves very
similar results to DTM batch fitting in terms of
topic coherence diminishing
computational costs.
In addition, we included in Viscovery sentiment analysis.
Sentiment analysis allows to distinguish between subjective/neutral
terms in
each distribution of words, enlightening how consumers opine about brands and
products.
To include sentiment analysis in DTM we explore a simple approach
based on aggregation,
using lexical
analysis at opinion level
and conducting
sentiment aggregations at topic and document level.
A third element included
in Viscovery is topic embedding.
Using a time sliced 2D topic embedding, topic
merging and topic segmentation are suggested.
Dynamics across topics are very
interesting for the analysis, and is a promising characteristic of Viscovery that
allow practitioners to understand how topics evolve.
Specific contributions of
the paper are:
1
Sernac:
Servicio Nacional del Consumidor, http://www.sernac.cl
2
• A scalable implementation of
DTM for online training updating model
parameters when new opinions come to the platform.
• A simple way to incorporate sentiment analysis into DTM,
allowing to
explore neutral/subjective words at different levels of aggregation.
• A topic visualization tool that works with a time sliced 2D topic embed-
ding, allowing to visualize how topics evolve over time.
The rest of the paper is organized as follows.
In Section 2 we review related
work on topic models and sentiment analysis.
Incremental learning on DTM is
presented in Section 3 and browsable sentiment analysis is discussed in Section
4.
Section 5 presents the architecture of Viscovery.
Implementation issues are
discussed in Section 6.
Viscovery data slices are presented in Section 7 and
finally we conclude in Section 8 giving conclusions and discussing future work.
2.
Related Work
Topic models.
Main efforts on topic models start with probabilistic Latent Se-
mantic Analysis [7]
(pLSA),
an aspect model
for text developed using topic
mixtures.
This approach decomposes a corpus of
documents across terms in-
troducing latent variables,
decoupling terms and documents with topic mix-
tures.
Model
fitting was conducted using the Expectation-Maximization algo-
rithm (EM) [3]
casted for matrix completion with incomplete data.
As the
term-space is a high dimensional
feature space,
pLSA needs a high amount of
data to perform well.
As in general, text data is sparse, pLSA tends to overfit
limiting generalization capabilities.
To tackle this problem, Blei et al.
[2] intro-
duced Dirichlet priors on vocabulary and document topic proportions.
Using
smoothing these models addressed the over-fitting limitations of
pLSA.
This
kind of models, known as Latent Dirichlet Allocation (LDA) were firstly fitted
using variational
EM (VEM),
an extension of the EM algorithm that success-
fully handle incomplete data with distributional
priors.
Later,
Griffiths and
Steyvers [6] explored Gibbs sampling for LDA model fitting, reducing the num-
ber of iterations until convergence.
Gibbs sampling is the standard method used
for LDA model
fitting until
today because its fast convergence does not affect
the quality of
the estimated models.
Dynamic Topic Models (DTM) [1]
was
introduced to deal with vocabulary dynamics.
DTM works over a corpus with
timestamps,
whilst model
fitting is conducted using time slices of
the corpus.
Temporal dependencies across vocabularies are modeled using Kalman filtering,
allowing to detect changes in descriptive words along different corpus slices.
The
inclusion of Kalman filtering in LDA for text dynamics involves additional com-
putational
costs in model
fitting,
slowing convergence.
Despite computational
costs involved in model fitting, DTM can successfully handle text dynamics.
Sentiment analysis and topic modeling.
A topic generative model for sentences
with polarity was proposed by Eguchi
and Lavrenko [4].
The model
distin-
guishes between neutral
words and sentiment words using a random binary
3
variable that controls the membership of each word to each one of the vocab-
ularies.
As documents can be generated from sentiment or topic words,
each
sentence achieves a polarity orientation calculated in terms of
the number of
sentiment words that contains.
Dirichlet smoothing was used on topic and sen-
timent word distributions to avoid over-fitting.
The performance of the model
in information retrieval
is tested inferring topic and sentiment orientation of
each query showing that the proposal is feasible.
Mei et al.
[12] proposed Topic
Sentiment Mixture (TSM), a sentiment topic model with a two tier mixture of
vocabularies to produce sentiment oriented sentences in a corpus.
A first tier
of
the model
is composed by neutral
term distributions (one per each topic)
and two additional
term distributions for positive and negative words.
Then,
each topic can be produced by a mixture of
these vocabularies defined from
document proportions.
The model
is non-parametric (no distributional
priors
were used) and model fitting was conducted using the EM algorithm.
TSM can
be considered as an extension of
pLSA to sentiment analysis being the main
difference the split conducted over the vocabulary to distinguish between fac-
tual/subjective sentences.
The Joint Sentiment Topic model
(JST) based on
LDA was proposed by Lin and He [11].
Term distributions were sampled over
a simplex over terms cross polarities,
then the generative model
drawn topic
proportions conditioned on each polarity.
In this way,
words can be drawn by
topics × polarities distributions,
producing words by the joint effect of topics
and polarities in the document.
As a consequence, sentiment coverages at doc-
ument level can be directly estimated by the model.
JST is able to successfully
address the sentiment classification task at document level.
An extension of
JST was proposed by Jo and Oh [9],
who introduced Aspect and Sentiment
Unification (ASUM). As JST, ASUM jointly models sentiments and topics, be-
ing topic proportions conditioned on polarities, with vocabularies at topic level
per each sentiment orientation.
However, ASUM models sentiment at sentence
level,
with words conditioned at a single topic per sentence.
Results on sen-
timent classification shows that ASUM outperforms JST and comes close to
supervised methods whilst ASUM does not require labels for model fitting.
The
state of the art shows that main efforts on sentiment topic modeling are focused
on static models,
discarding vocabulary dynamics.
As the core of Viscovery is
DTM, we will need to use a different approach to include sentiment analysis into
dynamic topic models.
We will show in Section 4 how we use sentiment analysis
at sentence level to conduct aggregation at different levels of granularities over
DTM.
3.
Incremental Learning for Dynamic Topic Models
3.1.
Dynamic Topic Models
A set of latent variables can be introduced to model the relationships between
terms and documents in a corpus.
Formally,
let d ∈ D = {d
1
, d
2
, . . . , d
N
} and
w ∈ W = {w
1
, w
2
, . . . , w
M
} be random variables representing documents and
terms,
respectively.
A set of random variables z ∈ Z = {z
1
, z
2
, . . . , z
k
} can be
4
introduced to model the joint probability of documents and terms, producing a
mixed membership model expressed as follows:
P (w|d) =
X
z∈Z
P (w|z) · P (z|d).
(1)
Using the Bayes rule to invert the conditional probability P (z|d), we obtain an
expression of the joint probability conditioned to the model parameters:
P (w, d) =
X
z∈Z
P (w|z) · P (d|z) · P (z).
(2)
The equation 2 is known as the generative formulation of the topic model of the
corpus.
Topic models based on Dirichlet allocation require two Dirichlet distribu-
tions.
A first one generates topic proportions for each document and a sec-
ond one generates terms conditioned on document topics proportions.
Specifi-
cally, a Dirichlet k-dimensional random variable θ takes values in a k-1 simplex
(0 ≤ θ
i
≤ 1,
P
k
i=1
θ
i
= 1), where its density function is defined by:
p(θ|α) =
Γ(
P
k
i=1
α
i
)
Q
k
i=1
Γ(α
i
)
θ
α
1
1
· . . . · θ
α
k
k
,
(3)
and {α
1
, . . . , α
k
} corresponds to the distributional
parameters,
α
i
> 0.
Then,
equation 2 is expanded using Dirichlet priors:
P (W, d) =
M
Y
n=1
P (w
n
|z
n
, β) · P (z
n
|θ
d
) · P (θ
d
|α).
(4)
In equation 4, θ
d
indicates the proportion of topics in d.
Then, z
n
is condi-
tioned on β and represents the sampling probability of w
n
on d.
Note that α
and β are the distributional parameters of the Dirichlet density functions.
Usu-
ally they are consigned as hyper-parameters to make a difference with model
parameters.
It is common to make an assumption of
density symmetry for
hyper-parameters,
that is α
1
= . . .
= α
k
= α and β
1
= . . .
= β
k
= β.
The
values α,
β control
the level
of smoothness/sharpness of the density functions
around the centroid of the simplex.
To model a time sliced corpus, Blei and Lafferty [1] introduced dynamic topic
models (DTM). DTM is based on the static Latent Dirichlet Allocation model
and use the mean parameterization of the multinomial topic distribution.
The
idea behind DTM is to use the mean parameterization of
the topics to intro-
duce mean chaining,
being possible to model time dependencies over time.
To
chain topics over time, DTM models the chain of mean parameters introducing
Gaussian noise, modeling uncertainty over time slices.
Let β
t,k
be the k-th topic
in the time slice t and let π be the mean parameter of the topic.
Note that the
i-th component of β
t,k
is given by β
i
= log

π
i
π
V

.
As π
i
represents the expected
value of
w
i
and π
V
is the expected value of
a random chosen word over the
5
whole vocabulary V ,
the fraction
π
i
π
V
is the odd of w
i
over V and then β
i
cor-
responds to the logit function for w
i
over V .
As is known, a zero variation over
V achieves a zero value in the logit function.
Positive or negative deviations of
w
i
in V achieves positive or negative values in [−1, +1], respectively.
Then, β
t,k
can be chained in a state space of parameters that evolves with Gaussian noise:
β
t,k
|β
t−1,k
∼ N (β
t−1,k
, σ
2
I).
(5)
Topic proportions are also chained in DTM,
using mean parameterization
over θ:
θ
t
|θ
t−1
∼ N (α
t−1
, δ
2
I).
(6)
Time chaining does not affect model
expressiveness.
In fact,
the decompo-
sition of the joint distribution of words and documents in a corpus remains the
same, except for the fact that both Dirichlet distributions (on topic proportions
and terms) are conditioned on the Dirichlet distributions of the previous time
slice:
P (W, d, t) =
M
Y
n=1
P (w
n
|z
n
, β
t
) · P (z
n
|θ
d,t
) · P (θ
d,t
|α).
(7)
Model
estimation has some drawbacks under these assumptions.
Posterior
inference (model
estimation of
parameters conditioned on observed variables)
is intractable due to the non conjugacy of Gaussians and multinomial distribu-
tions.
Blei
and Lafferty explored variational
methods for posterior inference,
discarding stochastic simulation (e.g.
Gibbs sampling) due to computational
difficulties inherent in the non conjugacy of Gaussians.
To retain the sequential
structure of topics over time,
DTM fits a dynamic model
with Gaussian vari-
ational
observations (
ˆ
β
k,1
, . . . ,
ˆ
β
k,t
, . . . ,
ˆ
β
k,T
),
fitting these parameters to min-
imize the Kullback-Leibler divergence between the resulting posterior and the
true posterior.
To mimic Gaussian variational observations, DTM uses Kalman
filtering, which enables the use of backward-forward calculations in a linear state
space model.
Analogously, topic proportions θ
t,d
are conditioned on free Dirich-
let parameters γ
t,d
and topic indexes z
t,d,n
are conditioned on free multinomial
parameters φ
t,d,n
:
q(β
k,1
, . . . , β
k,T
|
ˆ
β
k,1
, . . . ,
ˆ
β
k,T
),
(Kalman parameters)
(8)
q(θ
t,d
|γ
t,d
),
(Dirichlet parameters)
(9)
q(z
t,d,n
|φ
t,d,n
),
(Multinomial parameters)
(10)
Forward-backward calculations on Kalman parameters allows to estimate
posterior mean and variance parameters (m
t
and V
t
) in terms of Gaussian pa-
rameters (σ
2
) over topics.
As time sliced topics β
t
are conditioned on the
immediate past time sliced topic β
t−1
, and both are related by a Gaussian of σ
parameter, the variational state space model
ˆ
β
t
is conditioned on β
t
and both are
related by a Gaussian of ˆ
v
t
parameter.
Then, in forward calculations, posterior
mean and variance parameters (m
t
and V
t
) are calculated from σ and from the
variational parameters
ˆ
β
t
and ˆ
σ
t
.
In backward calculations the marginal mean
e
m
t−1
and variance
e
V
t−1
of β
t−1
depends on posterior mean m
t−1
and variance
6
V
t−1
,
σ and the one-step ahead marginal
mean e
m
t
and variance
e
V
t
.
Forward
calculations use initial
conditions m
0
and V
0
and backward calculations use
initial
conditions
e
m
T
= m
T
and
e
V
T
= V
T
.
The rest of the parameters are es-
timated using variational expectation maximization (VEM) as was proposed in
the original posterior inference algorithm of LDA [2].
3.2.
Variational
Inference Algorithm
To estimate model
parameters,
DTM works using VEM and Variational
Kalman filtering in a tandem.
The inference algorithm starts initializing Kalman
parameters using the LDA static VEM inference algorithm over the whole cor-
pus, discarding timestamps.
As an output of this process DTM obtains Kalman
variational
parameters (
ˆ
β
i
).
Forward calculations are conducted to estimate
posterior means and variances m
t
= E(β
t
|
ˆ
β
1:t
) and V
t
= E((β
t
− m)
2
|
ˆ
β
1:t
)
with initial
conditions m
0
= 0 and V
0
= σ
2
· e+03.
Backward recurrences
are used to estimate marginal
means and variances
e
m
t−1
= E(β
t−1
|
ˆ
β
1:T
) and
e
V
t−1
= E((β
t−1
− e
m
t−1
)
2
|
ˆ
β
1:T
) with initial conditions e
m
T
= m
T
and
e
V
T
= V
T
.
Likelihood bound variables ζ
t
=
P
w
e
e
m
tw
+0.5·
e
V
tw
are calculated for each topic
in each time slice and β
t,k,n
for each topic, term and time slice in the corpus.
After the initialization step, the inference algorithm runs the EM algorithm.
The E-step uses the static LDA VEM inference algorithm,
at document level,
in chronological order according to document timestamps.
Then, for each doc-
ument a free Dirichlet parameter γ
t,d
is obtained,
and for each word in each
document a multinomial parameter φ
t,d,n
is obtained.
The E-step iterates until
convergence following the LDA VEM convergence criteria.
Then,
the M-step
runs bounding topic likelihoods.
The process repeats the steps considered in
the initialization process conditioned on φ
t,d,n
model parameters.
Forward and
backward calculations are conducted reestimating variational Kalman parame-
ters iterating until
convergence following a topic likelihood criteria.
At global
level,
E-step and M-step alternates until
convergence,
following a criteria that
combines document and topic likelihoods.
3.3.
Incremental
learning on DTM
A key aspect of Viscovery relies on incremental learning.
As topics are used
as opinion containers,
the need to incorporate new opinions in a daily basis
is a key aspect to keep information updated.
To avoid the recalculation of
the entire model,
we extended DTM to allow incremental
learning,
updating
the model to be consistent with new opinions but avoiding the recalculation of
model parameters that depends on previous time slices.
When a new document batch is aggregated into Viscovery,
a set of unseen
words may appears.
Suppose that Q new words are appended by the new batch
to the model
and assume that the batch size (number of
documents in the
batch) is R.
Let W
new
= {w
M+1
, . . . , w
M+Q
} be the set of
new words and
let D
new
= {d
N+1
, . . . , d
N+R
} be the new batch.
We need to aggregate to the
model
a set of new parameters.
A first set of parameters is in dependence of
W
new
and previous slices 1 : T .
Topic parameters included into the model are
7
β
M+1,1:T
, . . . , β
M+Q,1:T
.
As new words were unobserved on previous slices, we
set these parameters using β
long-tail
, the value assigned by DTM to words in the
long-tail of the model.
In practice, by choosing at random any word in the tail
of
any topic,
β
long-tail
achieves only small
fluctuations (order 10
−12
).
Analo-
gously, we set
ˆ
β
M+1,1:T
, . . . ,
ˆ
β
M+Q,1:T
as
ˆ
β
long-tail
, the value assigned by DTM
to words in the long-tail
of the Kalman variational
parameters.
Then we fit a
static LDA over the new batch to obtain initial values for β
1:Q,T +1
parameters
(model
parameters for the new batch over the whole vocabulary).
Mean and
variance parameters (variational and marginal) are calculated using the forward-
backward procedure at one step (one step ahead for forward calculation and one
step behind for backward recursion).
To avoid unnecessary computation costs,
we discarded the recalculation of the entire chain of Kalman variational param-
eters,
constraining inference only to dependencies between batches in slices T
and T + 1.
The constrained forward-backward calculation produces estimations
for mean and variance in the new batch, and values for likelihood bounds ζ
T +1
and β
T +1,k,n
.
Now we follow the EM procedure.
Log likelihoods
of
topics
and docu-
ments modeled in previous batches are retrieved to be included in the global
likelihood criterion function used in the EM procedure.
The E-step is con-
ducted over the documents included in D
new
,
obtaining estimates for γ
T +1,d
and φ
T +1,d,n
, d ∈ D
new
, likelihood bounds for each document.
The E-step runs
until
convergence following the LDA VEM convergence criteria.
The M-step
runs bounding topic likelihoods.
The process repeats the following cycle at
word level
for each document in the new batch:
repeat:
topic bound est
→ batch model updating → new topic bound → check convergence.
As the M-step runs over the last chain of
DTM,
the convergence is very fast
and the overall
convergence is also very fast.
In the appendix we give more
details about how our incremental algorithm guarantees that the log likelihood
log p(d
1:T
) is bounded from below using the Jensen’s inequality.
4.
Browsable Sentiment Analysis
In this section we indicate how we produce a browsable sentiment analysis
view of
the data in Viscovery.
Sentiment analysis is a key aspect of
opinion
mining tools and in Viscovery is a salient aspect that helps users to distinguish
between subjective/neutral
information.
As a base service,
the Novaviz API
uses VADER [8] for sentiment sentence tagging.
VADER provides three senti-
ment scores at sentence level:
positive (sc
s
(⊕)), negative (sc
s
( )) and neutral
(sc
s
(
)) scores,
where sc
s
(⊕) + sc
s
( ) + sc
s
(
)
= 1.
We recover for each
sentence in our data these scores.
Document level.
A first level of aggregation considered in Viscovery is the docu-
ment level.
As opinions can be compounded by a number of sentences, sentiment
scores need to be aggregated at opinion level.
Let d be a document indexed in
Viscovery, and s ∈ d the sentences that compounds d, where |d| is the number
of sentences of d.
Sentiment scores at document level are obtained from:
8
sc
d
(∗) =
X
s∈d
sc
s
(∗)
|d|
,
with ∗ ∈ {⊕, , 
}
(11)
Note that sc
d
(⊕) + sc
d
( ) + sc
d
(
) = 1, as expected.
Topic level.
A second level of aggregation considered in Viscovery is the topic
level.
As opinions are aggregated into topics,
sentiment scores need to be ag-
gregated at topic level to indicate the level of polarity of each topic.
Let z be a
LDA latent variable, and P (d|z) the membership probability given by DTM and
defined in Equation 2.
Note that P (∗|z) =
P
d∈D
P (∗|d) · P (d|z).
For simplicity,
we denote P (∗|z) by sc
z
(∗).
Then, sentiment scores at topic level are obtained
from:
sc
z
(∗) = C
z
·
X
d∈D
sc
d
(∗) · P (d|z),
with ∗ ∈ {⊕, , 
}
(12)
where C
z
=
1
P
∗
sc
z
(∗)
.
Note that sc
z
(⊕) + sc
z
( ) + sc
z
(
) = 1, as expected.
Term level.
At a high level
of
granularity Viscovery browses terms.
To use
sentiment
analysis
at
term level,
we need to estimate P (∗|w),
denoted for
simplicity by sc
w
(∗).
As sc
w
(∗) can be expanded over latent variables by
P
z∈Z
sc
z
(∗) · P (z|w), using the Bayes rule on P (z|w) we obtain sc
w
(∗) as:
sc
w
(∗) =
X
z∈Z
sc
z
(∗) ·
P (w|z) · P (z)
P (w)
,
with ∗ ∈ {⊕, , 
}
(13)
Note that sc
w
(⊕) + sc
w
( ) + sc
w
(
) = 1, as expected.
Using topics as proxies.
Viscovery allows to browse opinions using topics as
proxies.
When a topic is picked in Viscovery,
the sentiment view of
the data
can be projected to documents or terms.
To show sentiment scores conditioned
on topics,
we reuse the scores defined in equations 11-13.
Sentiment scores at
document level conditioned on topics are defined by:
sc
d
(∗|z) =
X
w∈W
sc
w
(∗) · P (w|z)
!
· P (z|d),
with ∗ ∈ {⊕, , 
}
(14)
Analogously, sentiment scores at term level conditioned on topics are defined
by:
sc
w
(∗|z) =
X
d∈D
sc
d
(∗) · P (d|z)
!
· P (z|w),
with ∗ ∈ {⊕, , 
}
(15)
This simple way to aggregate scores from sentence sentiment scores allows
us to use sentiment analysis on DTM.
9
Figure 1:
Viscovery architectural
diagram.
Visualization components are connected to data
processing components using the Novaviz API.
5.
Viscovery:
Architecture and Design Principles
In this section we discuss how we integrate different algorithms to ingest
opinions into Viscovery.
We model
different algorithms as micro services to
develop a platform for trend tracking in opinion forums.
A micro service archi-
tecture organizes the platform as a set of weakly coupled services where each
service implements a set of
encapsulated procedures.
For example,
a micro
service in Viscovery corresponds to an indexer of opinionated tweets.
Services
in Viscovery are communicated using asynchronous protocols.
We developed
each service independently of the other.
Indeed each micro service has its own
database in order to be decoupled from other services.
To develop Viscovery we create a start-up named Novaviz.
The idea behind
Novaviz is to develop tools for text data management.
To accomplish this pur-
pose we develop the Novaviz API Gateway, a list of services and functionalities
implemented in Python,
requested by four components:
a) Data Ingester,
b)
Data Preprocessor, c) Data Processor, and d) Indexer.
For visualization we use
three libraries:
a) DFR browser, b) Kibana, and c) D3.
Visualization and pro-
cesses are connected through micro services defined in Novaviz API Gateway,
as is shown in the architectural diagram of Figure 1.
10
Data Ingester.
This component is in charge of
opinion recollection from het-
erogeneous sources as Twitter and web forums (e.g.
http://www.reclamos.cl).
It calls services from the Novaviz API. Among the services requested the most
important is web scrapping,
that allows Viscovery to retrieve opinions from
web page forums.
For storage, this component interacts with Redis
2
, an open
source (BSD licensed) in-memory data structure store, used as a cache database
to support this process.
Preprocessor.
This component normalizes the text.
It calls services from the
Novaviz API such as stop words removal,
caps normalization and punctuation
removal.
It allows Viscovery to create a vocabulary of
keywords to describe
opinions by content.
For storage this component interacts with MongoDB
3
, a
noSQL database for document storage and retrieval.
Processor.
This component is in charge of
text analysis and is the core com-
ponent of Viscovery.
It calls services from the Novaviz API as Dynamic Topic
Models and Sentiment Analysis.
For Dynamic Topic Models (DTM),
the API
wraps Gensim
4
.
Gensim is an implementation of
topic modeling written in
Python [13].
It includes implementations of LDA, LSI and DTM. For sentiment
analysis, the API wraps Vader
5
, a rule-based model for sentiment analysis that
uses a lexicon of
English words [8].
As the preprocessor component,
the pro-
cessor interacts for storage with MongoDB,
allowing to register each view of
the data (e.g.
topic model view) as a document view of each opinion, with the
attributes leveraged by the respective view.
For instance,
from the sentiment
view of
an opinion,
each document in MongoDB stores neutral,
positive and
negative scores at sentence level.
Weights for topic membership are stored in
the topic model view of each opinion.
Then, documents in MongoDB will ingest
the indexer, the component that provides data for opinion search and browsing.
Indexer.
This component is in charge of
opinion indexing.
For each view of
the data, we create an index allowing search and browsing at different levels of
granularity.
As opinions are clustered using topic models, browsing is conducted
using topics as opinion aggregation containers.
For each topic,
each opinion
register its membership score, which indicates the degree of membership of each
opinion to the topic.
As each topic is a probability distribution over words,
we store the weights of
each word per topic.
As browsing is conducted over
topics, the use of words to describe each topic is a key element of Viscovery.
To
integrate the sentiment view of
the data,
we index opinions and their related
sentiment weights for search and browsing.
To ingest these indexes, we recover
the document views created by the processor in the previous step,
processing
and indexing them into Elasticsearch
6
.
Elasticsearch is a distributed, RESTful
2
https://redis.io/
3
https://www.mongodb.com/
4
http://radimrehurek.com/gensim/
5
https://github.com/cjhutto/vaderSentiment
6
https://www.elastic.co/
11
search and analytics engine capable of support searches over unstructured data
implementing fast and efficient data access operations using inverted indexes.
We use Elasticsearch indexes to support all the search and browsing operations
in Viscovery.
DFR browser.
To visualize opinion trends we started using DFR browser
7
,
a
visualization tool
that works over topic models to integrate data views into a
single, coherent, and searchable visualization of the data.
As the code of DFR
browser is available, we started working over DFR browser to cast this tool to
our needs and requirements.
DFR allows to search over topics, the basic search-
able element in the visualization,
and to disaggregate the information at topic
level into documents and words by topic.
Kibana.
Kibana is part of
the suite provided by Elastic,
named The Open
Source Elastic Stack.
The purpose of
Kibana is to hand Elaticsearch visual-
izations.
D3.js.
Another tool
that we use for data visualizations is D3.js
8
.
D3.js is a
JavaScript library for data visualizations compatible with HTML,
SVG,
and
CSS. D3 follows a data-driven approach for data manipulation, using DOM as
a standard for document representation.
6.
Implementation Issues
6.1.
Novaviz API
The Novaviz API includes a list of services and functionalities.
As the archi-
tecture of Viscovery is micro service oriented,
the Novaviz API contains a list
of reusable and generic services.
Our API includes seven services:
Scrapper.
This
service extracts
and recovers
data from heterogeneous
data
sources as Twitter or opinion web forums (e.g.
reclamos.cl).
In the case of
Twitter, it takes as seed a hashtag using the public API and producing a .json
file compounded by the list of
tweets that contains the hashtag.
In the case
of
reclamos.cl
we scrap the html
source code of
the forum recovering a semi-
structured view of the forum in a .json file.
The attributes included in the file
are creation date,
the complaint content (unstructured),
the url
(a permalink
created by reclamos.cl
for each opinion),
and the title of
the complaint.
It is
implemented using Scrapy
9
, a scrapper implemented in Python.
The scrapper
is called by the ingester component of Viscovery.
7
https://agoldst.github.io/dfr-browser/
8
https://d3js.org/
9
https://scrapy.org/
12
Corpus constructor.
It takes scrapper outputs in .json format preprocessing the
content to normalize the text.
It starts tokenizing the text.
Is in this service
that caps,
stop words,
accents,
punctuation and symbols are processed.
We
include a rule-based word removal
by frequency.
By default,
words with one
occurrence in the data are removed.
In addition,
a second rule-based word
removal
is included,
removing words by length.
Words with less than two
chars are removed from the vocabulary.
The constructor is language-flagged.
Novaviz considers two languages,
English and Spanish.
By default,
the con-
structor is set to English.
The stopword list is customizable.
In addition,
the
basis for time slicing can be defined here using a parameter with values in
{daily, weekly, monthly, yearly}.
Output files produced by the corpus cre-
ator are foo.dict (dictionary), foo.lda-c (a row oriented file with one doc per row
and entries indicating word occurrences), sliced.json (docIDs and timestamps).
These files are used for LDA model fitting.
The corpus constructor is called by
the preprocessor component of Viscovery.
LDA fitting.
It takes foo.dict (dictionary) and foo.lda-c for LDA model fitting.
It needs the number of topics as a parameter (five topics by default).
LDA fitting
wraps the Gensim implementation of LDA that is based on Gibbs sampling [6].
Output files produced by LDA fitting are stored in a directory that contains
topic-word.json (truncated to the top-30 words per topic), doc-topic.json (topic
proportions), frequency.txt (a list of words with their occurrences of the corpus),
and foo which corresponds to the model file, a coded view of the fitted model.
LDA fitting is called by the processor component of Viscovery.
DTM fitting.
Analogously to LDA fitting,
this service wraps the Gensim im-
plementation of DTM. It takes foo.dictionary, foo.lda-c and sliced.json for DTM
fitting.
Output files produced by DTM are topic-word.json with timestamps
(one timestamp per time slice for each word in the dictionary),
doc-topic.json
(topic proportions), frequency.txt, and foo which corresponds to the coded view
of the DTM model.
DTM fitting is called by the processor component of Vis-
covery.
Sentiment Analysis.
It takes the .json file produced by the Scrapper and con-
ducts sentiment analysis using VADER. It works at three different levels of gran-
ularity,
sentences,
documents and topics.
Output files produced by Sentiment
Analysis are stored in .json files (with pairwise entries ID-sentiment score).
A
detailed discussion about how sentiment scores are calculated at different levels
of granularity is provided in Section 4.
NER.
It takes the .json file produced by the Scrapper service,
and conducts
named entity recognition using NLTK
10
(Natural Language ToolKit, a Python
implementation of NLP basic tools).
Output files produced by NER are stored
in .json files (with pairwise entries wordID-NER tag).
10
http://www.nltk.org/
13
POS.
It takes the .json file produced by the Scrapper service,
and conducts
part-of-speech using NLTK.
Output files produced by POS are stored in .json
files (with pairwise entries wordID-POS tag).
Topic scaling.
It takes as input the foo model
file retrieved from LDA fitting.
In addition it takes the foo.dict (dictionary) and foo.lda-c from corpus construc-
tor to recover document lengths.
Topic scaling wraps Principal
Coordinate
Analysis (PCoA) using the implementation provided in Scikit-bio
11
.
Dimen-
sionality reduction is conducted using PCoA towards a 2D embedding based on
the Jensen-Shannon divergence between topics.
The output file produced by
topic scaling is a .json file with paiwise entries topicID - hx, yi.
Topic scaling is
called by DFR browser for topic visualization.
6.2.
Elastic Indexes
For data storage we use Elastisearch indexes.
Elasticsearch provides services
for data indexing and retrieval.
Elastic indexes are key elements for opinion
browsing, allowing to browse opinions at different levels of granularity.
For each
level of granularity we created a specific index in Elastic:
Opinions index.
This index retrieves opinions using a docID as a search key.
Each opinion is indexed at full content, allowing fast retrieval of opinions when
documents are picked in Viscovery.
Topic-word index.
This index retrieves top-K words per topic, using topicID as
a search key.
For each word the index stores the membership level for the topic.
Topic-document index.
This index retrieves top-k documents per topic,
using
topicID as a search key.
For each document the index stores the membership
level for the topic.
Term frequency index.
This index retrieves the frequency of
each term using
termID as a search key.
Sentiment-document index.
This index retrieves sentiment scores at document
level using docID as a search key.
Sentiment-topic index.
This index retrieves sentiment scores at topic level using
topicID as a search key.
Sentiment-sentence index.
This index retrieves sentiment
scores at
sentence
level using sentenceID as a search key.
11
http://scikit-bio.org/
14
6.3.
DFR cast
DFR (Data For Research)
12
is a visualization tool
that produces global
data views avoiding unnecessary accesses to documents at finer granularity lev-
els.
It produces a global
first data view comprising document contents using
topics as containers and words as topic descriptors.
Topics can be picked in the
global data view showing the most relevant words of the selected topic as a list.
The temporal
evolution of
the topic is showed in the topic view.
Lengthwise
selection on the timeline of
the topic exhibits data responsiveness,
updating
top-documents at topic level.
Along with top document at topic level, a list of
top-50 words is showed.
The document list includes three attributes per document:
the title, the de-
gree of membership of the document to the selected document, and the number
of tokens that compound it.
These lists include the top-20 most salient docu-
ments per topic in terms of degree of membership.
The topic view provided by
DFR is shown in Figure 2.
Figure 2:
DFR topic-view.
We extended the DFR topic view to include sentiment analysis and a 2D
topic embedding global view between the timeline and the list of top documents.
To en-chase the topic embedding we modified the DRF topic view.
By default,
DFR does not include sentiment visualizations.
Then two files, sentiment scores
at term and document levels were included to allow sentiment visualizations.
These files were used to indicate the polarity of topics,
documents and words.
12
http://agoldst.github.io/dfr-browser/demo
15
At topic level, each topic was colored according to its polarity, using a white/red
color palette (negative scores were represented in red).
In addition we included
a button in the top words list to change the length of each word bar according
to objective/subjective scores.
A screen shoot of our sentiment DFR extension
is shown in Figure 3.
Figure 3:
Sentiment DFR topic-view.
7.
Data slices and preliminary results
Incremental
learning testing
We evaluate our incremental version of DTM to measure speed up and model
quality in terms of topic coherence [14].
We expect to reduce the computational
time involved in model fitting avoiding a retrogress in terms of topic coherence.
To test this aspect of our proposal, we run ten trials of model fitting for a corpus,
with and without incremental
learning on the last time slice.
We used as test
data a curated dataset to evaluate topic coherence provided by Greene & Cross
[5] that comprises news about the political agenda of the European Parliament.
The dataset is divided into four time slices and is compounded by 1324 news
articles classified into a manually-specified number of topics, helping to evaluate
topic coherence.
Mean and variance of
coherence and average computational
times involved in both algorithms are reported in Table 1.
16
Algorithm
Mean Coh.
Var Coh.
Avg Comp.
Time
DTM
-1.5747
0.0047
2:06:56
DTM + Seq.
Upd.
-1.5373
0.0063
1:47:37 + 0:11:50
Table 1:
Topic coherence and computational
times in DTM and DTM+seq update.
Mean
and variance over ten runs per algorithm are reported.
As expected, the mean computational time involved in DTM + Seq.
Upd.
is
less than the time registered by DTM, with only 11 minutes spent in the fourth
slice.
This result indicate that the most expensive step of the algorithm is the
Kalman variational inference and as our proposal constraints this step two the
last slice,
it reduces the cost involved.
As the data used for this experiment
is small
(we used this dataset to estimate topic coherence) the difference be-
tween both algorithms in terms computational time is small.
In Reclamos.cl, a
big data set with more than 200,000 complains that we indexed in Viscovery,
the difference between both algorithms is high.
If we use DTM over the whole
dataset,
model
fitting takes 14.7 hours.
On the other hand,
using sequential
update over the last time slice it takes 1.91 hours.
Note that without sequential
update, we will need to retrain the whole model for each new slice and our pro-
posal avoids this with a speed up of almost 8x.
Surprisingly the time reduction
does not affect the quality of the model in terms of topic coherence as is shown
in Table 1.
In fact, our proposal achieves a slight improvement over DTM at a
cost of a higher variance between the different trials.
7.1.
Data slices over Reclamos.cl
We are developing Viscovery implementing new functionalities.
In fact, the
current version of Viscovery implements browsable sentiment analysis at topic
and word levels.
Currently we are working on the implementation of sentiment
analysis at document level,
according to the proposal
introduced in Section 4.
Viscovery allows to browse opinions using topics as proxies of
opinions.
We
indexed into Viscovery 12 years of data from Reclamos.cl, a Chilean forum for
complaints abouts companies,
marks and institutions.
The dataset contains
201,969 different complaints.
Retail,
government,
banks and universities are
among the most frequent subjects of opinions.
The size of the vocabulary after stopword removal is 86,723 terms.
We used
18 hours to create the corpus of
reclamos using Viscovery.
Reclamos.cl
is a
very active site in Chile,
reporting an overall
of
90,128 persons contacted by
companies after complaint publication (a significant number in proportion to
the Chilean population).
Running DTM using years as timestamps, we achieved 12 time slices of the
data.
We used the default value for the number of
topics,
set as 10 for this
example.
Data slices for this corpus are shown in Figure 4.
As Figure 4 shows,
the a) Corpus view (overview) has four alternatives for corpus deployment:
grid,
scaled,
list and stacked.
We show topics using lists as data view.
The
list of
topics include topics proportions over time,
top words per topic and
17
Figure 4:
Viscovery data views.
Three data slices are deployed from the a) Corpus view (list of
topics and temporal proportions) after topic selection:
b) Topic view, which includes top words
per topic, topic proportions on time and topic embedding, c) Document view (membership of
the document to the given topic), and d) Word view across topics, showing the ranking of the
word in each topic where the word is prominent.
the proportion of
the topic in the corpus.
When a topic is selected (we click
topic 1 for this example),
the b) Topic view is deployed.
A topic view shows
the list of top words for the topic,
sorted in decreasing order according to the
18
proportion on the topic.
If the polarity bottom is pressed the bars are modified
according to the sentiment weight of
the word in the topic.
For this version
of
Viscovery,
the bar size is proportional
to the sum of
positive and negative
scores.
Currently we are implementing an extension that produces two bars
per term,
one per polarity.
Topic embeddings are also shown in this view,
illustrating the correlation of
topic (distances in the topic embedding).
The
polarity orientation of
each topic is shown using a color bar,
where negative-
biased topics are indicated with red shades.
A list of top-documents per topic
is shown below the embedding (omitted in this figure) and the user can select
a specific opinion.
In this case, Viscovery deploys the c) Document view, where
the top words of the document in the given topic are shown.
The subject and
the date of the complaint is shown at the top of the view.
Finally,
Viscovery
provides a d) Term view across topics,
showing how relevant is a given word
across topics.
In the example, we show the word view using the term ’company’,
and as expected, this word is used in many topics of reclamos.cl, with different
levels of membership.
8.
Conclusions
We present Viscovery, a tool for opinion browsing and trend tracking.
Key
elements of
Viscovery are Dynamic Topic Models (DTM) and our extension
of
DTM for sequential
updating.
We include sentiment analysis in Viscovery
starting from sentiment scores at sentence level and then, conducting aggrega-
tion across topics and documents.
This approach is simple and effective.
For
visualization we use DFR browser, extending DFR to include topic embeddings
and sentiment analysis.
Currently we are extending Viscovery to include more functionalities.
Among
these functionalities we are working on the sentiment-document view, topic evo-
lution tracking view and opinion search module.
We are implementing these
modules using Kibana and D3, two visual components considered in Viscovery
not included in the current version.
In addition, we are using Viscovery to index
more sources, as opinions retrieved from Twitter and Reddit.
Acknowledgment
This work was supported by the Fondef VIU 15E0085 project of the National
Agency of Science and Technology, Conicyt, Chile.
References
[1]
David Blei and John Lafferty. 2006.
Dynamic topic models. In Proceedings
of
the 23rd International
Conference on Machine Learning,
ICML.
113–
120.
[2]
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet Allo-
cation.
Journal
of Machine Learning Research 3, 4-5 (2003), 993–1022.
19
[3]
Arthur Dempster,
Nan Laird,
and Donald Rubin.
1977.
Maximum likeli-
hood from incomplete data via the EM algorithm.
Journal
of
the Royal
Statistics Society 39 (1977), 1–38.
[4]
Koji Eguchi and Victor Lavrenko. 2006.
Sentiment retrieval using genera-
tive models. In Proceedings of the 6th Conference on Empirical
Methods in
Natural
Language Processing, EMNLP. 345–354.
[5]
Derek Greene and James P.
Cross.
2016.
Exploring the Political
Agenda
of the European Parliament Using a Dynamic Topic Modeling Approach.
CoRR abs/1607.03055 (2016).
[6]
Thomas Griffiths and Mark Steyvers. 2004.
Finding scientific topics.
Pro-
ceedings of the National
Academy of Sciences, PNAS 101, 1 (2004), 5228–
5235.
[7]
Thomas Hofmann. 2001.
Unsupervised learning by probabilistic latent se-
mantic analysis.
Machine Learning 42, 2 (2001), 177–196.
[8]
C.J.
Hutto and Eric Gilbert.
2014.
VADER:
A Parsimonious Rule-based
Model
for Sentiment Analysis of Social
Media Text.
In Proceedings of the
Eighth International
Conference on Weblogs and Social
Media, ICWSM.
[9]
Yohan Jo and Alice Oh.
2011.
Aspect and sentiment unification model
for online review analysis.
In Proceedings of
the 4th ACM International
Conference on Web Search and Data Mining, WSDM. 815–824.
[10]
Oren Kurland and Lillian Lee. 2009. Clusters, language models, and ad hoc
information retrieval.
ACM Transactions on Information Systems,
TOIS
27, 3 (2009).
[11]
Chenghua Lin and Yulan He.
2009.
Joint sentiment/topic model
for sen-
timent analysis. In Proceedings of the 18th ACM International
Conference
on Information and Knowledge Management, CIKM. 375–384.
[12]
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai.
2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In
Proceedings of the 16th ACM International
World Wide Web Conference,
WWW. 171–180.
[13]
Radim
ˇ
Reh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Mod-
eling with Large Corpora. In Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks. 45–50.
[14]
Hanna Wallach,
Iain Murray,
Ruslan Salakhutdinov,
and David Mimno.
2009.
Evaluation methods for topic models.
In Proceedings of
the 26th
International
Conference on Machine Learning, ICML. 1105–1112.
20
Appendix.
Lower bound of the likelihood for the incremental
algo-
rithm
In this section we give details of the incremental algorithm that maximizes
the lower bound of
the likelihood on log p(d
1:T
).
This section is an extension
of
the appendix provided in Blei
and Lafferty [1]
where the lower bound is
calculated for the static algorithm.
For the incremental version of the inference
algorithm,
we only need to calculate the terms for the last time slice T .
The
first term of the lower bound is:
E
q
log
p
(β
T
|β
T −1
) = −
V
2
(log σ
2
+ log(2σ
2
)
−
1
2σ
2
E
q
(β
T
− β
T −1
)
T
(β
T
− β
T −1
)
= −
V
2
(log σ
2
+ log 2π) −
1
2σ
||
˜
m
T
−
˜
m
T −1
||
2
−
1
σ
2
T r(
˜
V
T
) +
1
2σ
2
(T r(
˜
V
0
) − T r(
˜
V
T
))
(16)
The second term is:
E
q
log
p
(d
T
|β
T
) =
P
w
n
tw
E
q
(β
w
− log
P
w
exp (β
w
))
≥
P
w
n
w
˜
m
w
− n
w
ζ
−1
T
P
w
exp( ˜
m
T
+
˜
V
w
/2)
+n
T
− n
T
log ζ
−1
T
(17)
where n
T
=
P
w
n
w
.
The third term is the entropy H(q) =
1
2
P
w
log
˜
V
w
+
V
2
log 2π.
The term
V
2
log 2π is canceled in term 1 and the entropy.
In term 2
n
T
ζ
−1
T
P
w
exp( ˜
m
T
+
˜
V
w
/2) = n
T
ζ
−1
T
ζ
T
= n
T
(18)
The new term −n
T
is canceled with the corresponding n
T
.
Then, the bound
can be obtained as
= −
V
2
(log σ
2
) −
1
2σ
||
˜
m
T
−
˜
m
T −1
||
2
−
1
σ
2
T r(
˜
V
T
)
+
1
2σ
2
(T r(
˜
V
0
) − T r(
˜
V
T
)) +
P
w
n
w
˜
m
w
− n
T
log
˜
ζ
T
+
1
2
P
w
log
˜
V
w
(19)
21
