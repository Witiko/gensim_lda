Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 271–276,
Gothenburg, Sweden, 23-24 May 2017.
c
2017 Link
¨
oping University Electronic Press
Word vectors, reuse, and replicability:
Towards a community repository of large-text resources
Murhaf Fares, Andrey Kutuzov, Stephan Oepen, Erik Velldal
Language Technology Group
Department of Informatics, University of Oslo
{murhaff
|
andreku
|
oe
|
erikve}@ifi.uio.no
Abstract
This paper describes an emerging shared
repository of large-text resources for creat-
ing word vectors, including pre-processed
corpora and pre-trained vectors for a range
of frameworks and configurations.
This
will facilitate reuse, rapid experimentation,
and replicability of results.
1
Introduction
Word embeddings provide the starting point for
much current work in NLP, not least because they
often act
as the input
representations for neural
network models.
In addition to be being time-
consuming to train, it can be difficult to compare
results given the effects of different pre-processing
choices and non-determinism in the training algo-
rithms. This paper describes an initiative to create
a shared repository of large-text resources for word
vectors, including pre-processed corpora and pre-
trained vector models for a range of frameworks
and configurations.
1
This will facilitate rapid ex-
perimentation and replicability. The repository is
available for public access at the following address:

http://vectors.nlpl.eu
To demonstrate the impact
of
different
pre-
processing choices and parameterizations, we pro-
vide indicative empirical results for a first set of
embeddings made available in the repository (Sec-
tion 3). Using an interactive web application, users
are also able to explore and compare different pre-
trained models on-line (Section 4).
2
Motivation and background
Over the last few years, the field of NLP at large has
seen a huge revival of interest for distributional se-
mantic representations in the form of word vectors
1
Our repository in several respects complements and up-
dates the collection of Wikipedia-derived corpora and pre-
trained word embeddings published by Al-Rfou et al. (2013).
(
Baroni et al., 2014). In particular, the use of dense
vectors embedded in low-dimensional spaces, so-
called word embeddings, have proved popular. As
recent studies have shown that beyond their tra-
ditional use for encoding word-to-word semantic
similarity, word vectors can also encode other re-
lational or ‘analogical’ similarities that can be re-
trieved by simple vector arithmetic, these models
have found many new use cases. More importantly,
however, the interest in word embeddings coincides
in several ways with the revived interest in neural
network architectures: Word embeddings are now
standardly used for providing the input layer for
neural models in NLP, where their low dimension-
ality is key. Some of the most popular frameworks
for training embeddings are also themselves based
on neural nets, like the neural language models un-
derlying the word2vec-like algorithms
(
Mikolov,
Sutskever, et al., 2013).
These models
are sometimes
referred to as
‘prediction-based’, and contrasted with traditional
‘count-based’ models based on representing co-
occurrence counts,
as the vector values of these
embeddings are optimized for predicting neighbor-
ing words. In practice, this distinction is not clear-
cut,
and a more useful
distinction can be made
between explicit representations,
where each di-
mension of a high-dimensional and sparse vector
directly corresponds to a contextual feature,
and
embeddings in the sense of dimensionality-reduced
continuous representations. Of course, research on
vectorial representations of distributional semantics
dates back several decades, including dimension-
ality reduced variants like Latent Semantic Analy-
sis (LSA) based on Singular Value Decomposition
(
Landauer & Dumais,
1997),
Probabilistic LSA
based on a version of the EM algorithm
(
Hofmann,
1999), Random Indexing based on random projec-
tions
(
Kanerva et al., 2000; Karlgren & Sahlgren,
2001; Widdows & Ferraro, 2009; Velldal, 2011),
Locality Sensitive Hashing (LSH)
(
Ravichandran
271
et al., 2005; Durme & Lall, 2010), and others.
It it is important to note that although the practi-
cal examples, experimental results and discussion
in this paper will be concerned with embeddings
generated with neural skipgram models and Global
Vectors (GloVe)
(
Pennington et al., 2014), most of
the issues will apply to word vectors more gener-
ally.
The repository itself is also intended to host
word vectors across all paradigms.
Deep learning and neural network architectures
are now increasingly replacing other
modeling
frameworks for a range of core different NLP tasks,
ranging from tagging
(
Plank et al., 2016) and pars-
ing
(
Dyer et al., 2015; Straka et al., 2015) to named
entity recognition
(
Lample et al., 2016) and sen-
timent analysis
(
Socher et al., 2013; Kim, 2014).
Word embeddings typically provide the standard in-
put representations to these models, replacing tradi-
tional feature engineering. Training is usually done
on large amounts of unlabeled text, and all stages in-
volved in the data preparation can potentially affect
the resulting embeddings; content extraction from
mark-up, sentence segmentation, tokenization, nor-
malization, and so on.
Just as important, the pro-
cess for generating embeddings in with the afore-
mentioned algorithms is non-deterministic: For the
same set of hyperparameters and the same input
data, different embeddings can (and most probably
will) be produced. In sum these factors pose serious
challenges for replicability for any work relying on
word embeddings (Hellrich & Hahn, 2016).
The availability of pre-trained models is impor-
tant in this respect.
It can ensure that the same
embeddings are re-used across studies, so that ef-
fects of other factors can be isolated and tested. Pre-
trained models are currently available for a range of
different algorithms: Continuous Skipgram, Con-
tinuous Bag-of-Words, GloVe, fastText
(
Bojanowski
et al., 2016) and others. However, even when com-
paring the results for different pre-trained embed-
dings for a given data set,
it can still be hard to
know whether an observed difference is due to the
embedding algorithm or text pre-processing.
Moreover, the available choices for pre-trained
vectors are very limited in terms of data sets and
pre-processing. Typically only embeddings trained
on full-forms are available.
However,
given the
convenience of using pre-trained vectors – train-
ing your own can often take several days and be
computationally demanding – many studies use em-
beddings not ideally suited for the particular task.
For many semantically oriented tasks for example,
embeddings trained on PoS-disambiguated lemmas
would make more sense than using full-forms.
Given the considerations above, we find it im-
portant to establish a shared repository where it
is possible to share training data, pre-processing
pipelines, and pre-trained embeddings. Whenever
possible, the training texts should be made avail-
able with various levels of pre-processing (e.g.,
lemmatized and PoS-tagged).
In cases where li-
censing does not permit this, standardized pipelines
for pre-processing should still be shared.
In addi-
tion,
a selection of sets of pre-trained word vec-
tors should be made available for a few different
parameterizations across different modeling frame-
works, trained on data with varying degrees of pre-
processing.
This will facilitate reuse and rapid experimen-
tation.
Should you still
need to train you own
vectors,
you can use a standardized pipeline for
pre-processing the training data. Most importantly,
such a repository will help to ensure the replicabil-
ity of results.
3
On the effects of corpus preparation
Levy et al. (2015) show that careful optimization
of hyperparameters is often a more important fac-
tor for performance than the choice of embedding
algorithm itself. The explicit specification of these
hyperparameters is therefore essential to achieving
a nuanced comparison between different word em-
bedding approaches as well as replicability – inas-
much as replicating word embeddings is possible.
As discussed in Section 2, the space of parameters
associated with text pre-processing prior to train-
ing is also an important factor.
To the best of our
knowledge, however, there has been little research
on the effect of corpus preparation on the training
and performance of word embeddings.
In addition to the choice of training corpus itself,
e.g. Wikipedia or Gigaword
(
Parker et al., 2011),
there are many pre-processing steps involved in
creating word embeddings.
These steps include,
but
are not
limited to,
defining the basic token
unit (full-form vs. lemma vs. PoS-disambiguated
lemma), stop-word removal, downcasing, number
normalization, phrase identification, named entities
recognition and more. Other pre-processing steps
depend on the nature of the training corpus;
for
example in training embeddings on text extracted
from Wikipedia, the actual training corpus depends
272
on the content-extraction tools used to interpret
Wiki markup. Moreover, in most cases the choice
of the particular tool to use for steps like tokeniza-
tion and sentence splitting will also make a differ-
ence. One of the important considerations we take
in creating a shared repository of word embeddings
is to spell out such choices.
A pilot study
To empirically demonstrate the im-
pact of text pre-processing on word embeddings,
we here present a pilot experiment, presenting in-
trinsic evaluation of a suite of embeddings trained
for different choices of training corpora and pre-
processing.
We trained twelve word embedding models on
texts extracted from the English Wikipedia dump
from September 2016 (about 2 billion word tokens)
and Gigaword Fifth Edition (about 4.8 billion word
tokens). We extracted the content from Wikipedia
using WikiExtractor.
2
Further, we sentence-split,
tokenized, and lemmatized the text in Wikipedia
and Gigaword using Stanford CoreNLP Toolkit
3.6.0
(
Manning et al.,
2014).
We also removed
all stop-words using the stop list defined in NLTK
(
Bird et al., 2009). In terms of pre-processing, the
models differ in whether they were trained on full-
forms or lemmas. Additionally, the models differ in
the training text: Wikipedia (words with frequency
less than 80 were ignored), Gigaword (frequency
threshold 100) and Wikipedia concatenated with
Gigaword (frequency threshold 200).
All the cor-
pora were shuffled prior to training.
The combination of the token choices and the
training corpora leads to six different configura-
tions.
To eliminate the possibility of the effect
of text pre-processing being approach-specific, we
trained embeddings using both GloVe
(
Pennington
et al., 2014) and Continuous Skipgram
(
Mikolov,
Chen, et al., 2013) with negative sampling (SGNS).
In terms of hyperparameters,
we aligned GloVe
and SGNS hyperparameters as much as possible:
in both approaches we set the dimensionality to
300 and the symmetric context window size to 5.
The SGNS models were trained using the Gensim
implementation
(
ˇ
Reh˚
u
ˇ
rek & Sojka, 2010), using
identical seed for all models;
the GloVe models
were trained with the reference implementation
published by the authors.
We then evaluated the resulting models on two
standard test datasets: SimLex-999 semantic sim-
2
https://github.com/attardi/
wikiextractor
ilarity dataset
(
Hill et al.,
2015) and the Google
Analogies Dataset
(
Mikolov,
Chen, et al.,
2013).
The former contains human judgments on which
word pairs are more or less semantically similar
than the others (for example ‘sea’ and ‘ocean’
are more similar than ‘bread’ and ‘cheese’).
The
task for the model here is to generate similarity
values most closely correlating with those in the
dataset. We follow the standard approach of evalu-
ating performance towards SimLex-999 by comput-
ing Spearman rank-order correlation coefficients
(
Spearman,
1904),
comparing the judgments on
word pair similarities according to a given embed-
ding model and the gold data.
The Google Analogies Dataset contains question
pairs with proportional analogies:
a : a
∗
:: b : b
∗
.
For example, ‘Oslo’ is to ‘Norway’ as ‘Stockholm’
is to ‘Sweden’. The task for the model is, given the
vectors (
a
,
a
∗,
b
),
to generate a vector,
for which
the closest vector in the model is
b
∗
.
As a rule,
the models solve this using the 3CosAdd approach
(Levy & Goldberg, 2014): b
∗ =
a
∗ +
b
−
a.
Results for the Google analogies test are stan-
dardly reported for two distinct sets of analogies in
the data: 5 sections of ‘semantic’ relations (8,869 in
total) and 9 sections of ‘syntactic’ relations (10,675
in total). The semantic relations are similar to the
example with the capitals, while the syntactic part
features analogies like ‘walk’ is to ‘walks’ as ‘run’
is to ‘runs’.
Measuring the effect of choices like
using lemmas or full-forms only makes sense for
the semantic tests,
so we will
not
focus on the
morphological and derivational analogies in our
experiments.
3
Results and discussion
Table 1 presents the re-
sults of evaluating our trained models on the bench-
mark datasets described above, showing how the
results depend both on linguistic pre-processing
and on the embeddings algorithm used. In Table 1,
‘wiki’,
‘giga’ and ‘comb’ denotes our 3 training
corpora. The GloVe embeddings were trained with
the default parameters except for the initial learn-
ing rate (0.02), number of iterations (100) and the
window and vector size (cf.
Section 3),
‘SGNS’
denotes Continuous Skipgram embeddings using
3
It is worth noting that some of the sections standardly
regarded as ‘syntactic’ could well be argued to contain se-
mantic relationships, like the ‘nationality–adjective’ section,
but for comparability of results we here adhere to the stan-
dard split, where the semantic part include the sections titled
‘capital-common-countries’, ‘capital-world’, ‘currency’, ‘city-
in-state’, and ‘family’.
273
Model
SimLex
Analogy
GloVe wiki lemmas
36.13
83.08
GloVe wiki forms
31.27
81.80
GloVe giga lemmas
37.74
73.37
GloVe giga forms
32.36
72.20
GloVe comb lemmas
39.96
78.90
GloVe comb forms
34.81
77.46
SGNS wiki lemmas
40.19
78.86
SGNS wiki forms
35.54
77.60
SGNS giga lemmas
41.90
67.47
SGNS giga forms
37.96
66.84
SGNS comb lemmas
42.58
72.62
SGNS comb forms
38.21
72.54
Table 1: Results for SimLex-999 and the semantic
sections of the Google Analogies Dataset.
10 negative samples.
Analysis of the evaluation results shows several
important issues.
First, while our SGNS models
perform slightly better for the semantic similarity
task, our GloVe models are more efficient in the
semantic analogy test.
The latter observation is
perhaps not so surprising given that analogical in-
ference was one of the primary aims of its authors.
Second, we see that more data is not necessar-
ily better.
For the benchmarking against SimLex-
999, we do see that more data consistently leads
to higher scores.
For the semantic analogies task,
however, the Gigaword corpus consistently results
in models performing worse than Wikipedia, de-
spite the fact that it is 2.5 times larger. Combining
Gigaword and Wikipedia still yields lower scores
than for Wikipedia alone.
Moreover, with an ac-
curacy of 83.08 for the semantic analogies,
the
GloVe model trained on the lemmatized version of
Wikipedia outperforms the GloVe model trained on
42 billion tokens of web data from the Common
Crawl reported in
(
Pennington et al., 2014), which
at an accuracy of 81.9 was the best result previously
reported for this task.
Finally, for both the semantic analogy task and
the similarity task,
we observe that
the models
trained on the lemmatized corpora are consistently
better than the full-form models. In the future we
plan to also evaluate our models on more balanced
analogy datasets like that of Gladkova et al. (2016).
4
Infrastructure: Embeddings on-line
To achieve our goals of increased re-use and replica-
bility, we are providing a public repository of texts,
tools, and ready-to-use embeddings in the context
of the Nordic e-Infrastructure Collaboration
4
and
with support from national supercomputing centers.
A comprehensive collection of resources for En-
glish and Norwegian
5
is available for download as
well as for direct access by supercomputer users,
combined with emerging documentation on the
complete process of their creation, ‘getting started’
guides for end users, as well as links to indicative
empirical results using these models.
We invite
feedback by academic peers on the repository al-
ready in this early stage of implementation and will
welcome contributions by others.
In ongoing work, we are extracting even larger
text corpora from web-crawled data and collabo-
rating with other Nordic research centers (notably
the University of Turku) to provide resources for
additional languages. As the underlying supercom-
puting infrastructure is in principle open to all (non-
commercial) researchers in the Nordic region, we
hope that this repository will grow and develop into
a community-maintained resource that greatly re-
duces the technological barrier to using very large-
scale word vectors. The exact procedures for com-
munity contributions have yet to be determined, but
we anticipate a very lightweight governing scheme.
We intend to ‘snapshot’ versions of the repository at
least once a year and publish these releases through
the Norwegian Data Archive, to ensure long-term
accessibility and citability.
The repository also provides the WebVectors
web-service featuring pre-trained vectors for En-
glish and Norwegian.
6
Serving as an interactive
explorer for the models, it allows users to retrieve
nearest semantic associates, calculate similarities,
apply algebraic operations to word vectors and per-
form analogical inference. It also features visualiza-
tions for semantic relations between words in the
underlying models. This web service is thoroughly
described by Kutuzov & Kuzmenko (2017).
4
https://neic.nordforsk.org/
5
While intended to continually grow,
in mid-2017 the
repository already makes available the pre-trained English
word embedding models produced by word2vec, fastText and
GloVe.
For these frameworks and for varying levels of text
pre-processing,
it contains models based on the Gigaword
Corpus, the British National Corpus and an English Wikipedia
dump from February 2017; we plan to regularly update the
Wikipedia-derived corpora and models, and also evaluate al-
ternative text extraction frameworks for Wiki markup,
e.g.
Wikipedia Corpus Builder by Solberg (2012).
Additionally,
there are corresponding models trained on the Norwegian
News Corpus (Hofland, 2000).
6
http://vectors.nlpl.eu/explore/
embeddings/
274
Acknowledgments
This initiative is part of the Nordic Language Pro-
cessing Laboratory (
http://www.nlpl.eu
),
an emerging distributed research environment sup-
ported by the Nordic e-Infrastructure Collaboration,
the universities of Copenhagen,
Helsinki,
Oslo,
Turku, and Uppsala, as well as the Finnish and Nor-
wegian national
e-infrastructure providers,
CSC
and Sigma2. We are grateful to all Nordic taxpay-
ers.
References
Al-Rfou,
R.,
Perozzi,
B.,
& Skiena,
S.
(2013).
Polyglot.
Distributed word representations for multilingual NLP.
In
Proceedings of the 17th Conference on Natural Language
Learning (p. 183 – 192).
Sofia, Bulgaria.
Baroni, M., Dinu, G., & Kruszewski, G.
(2014).
Don’t count,
predict! A systematic comparison of context-counting vs.
context-predicting semantic vectors.
In Proceedings of
the 52nd Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) (p. 238 – 247).
Baltimore, Maryland: Association for Computational Lin-
guistics.
Bird, S., Klein, E., & Loper, E.
(2009).
Natural language
processing with Python.
Beijing: O’Reilly.
Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T.
(2016).
Enriching word vectors with subword information.
arXiv
preprint arXiv:1607.04606.
Durme, B. V., & Lall, A.
(2010).
Online generation of local-
ity sensitive hash signatures.
In Proceedings of the 48th
Meeting of the Association for Computational Linguistics
(p. 231 – 235).
Uppsala, Sweden.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith,
N.
(2015).
Transition-based dependency parsing with
stack long short-term memory.
In Proceedings of the 53rd
Meeting of the Association for Computational Linguistics
and of the 7th International Joint Conference on Natural
Language Processing.
Bejing, China.
Gladkova, A., Drozd, A., & Matsuoka, S.
(2016).
Analogy-
based Detection of Morphological and Semantic Relations
with Word Embeddings: What Works and What Doesn’t.
In Proceedings of the NAACL Student Research Workshop
(p. 8 – 15).
San Diego, California: Association for Compu-
tational Linguistics.
Hellrich,
J.,
& Hahn,
U.
(2016).
Bad
Com-
pany—Neighborhoods in Neural Embedding Spaces Con-
sidered Harmful.
In Proceedings of COLING 2016,
the
26th International Conference on Computational Linguis-
tics: Technical Papers (p. 2785 – 2796).
Osaka, Japan: The
COLING 2016 Organizing Committee.
Hill, F., Reichart, R., & Korhonen, A.
(2015).
SimLex-999:
Evaluating Semantic Models With (Genuine) Similarity
Estimation.
Computational Linguistics, 41(4), 665 – 695.
Hofland, K.
(2000).
A self-expanding corpus based on news-
papers on the Web.
In Proceedings of the Second Interna-
tional Conference on Language Resources and Evaluation
(LREC-2000).
Hofmann, T.
(1999).
Probabilistic latent semantic analysis.
In
Proceedings of the fifteenth conference on uncertainty in
artificial intelligence (p. 289 – 296).
Stockholm, Sweden.
Kanerva, P., Kristoferson, J., & Holst, A.
(2000).
Random
indexing of text samples for latent semantic analysis.
In
Proceedings of the 22nd annual conference of the cognitive
science society (p. 1036).
PA, USA.
Karlgren, J., & Sahlgren, M.
(2001).
From words to under-
standing.
In Y.
Uesaka,
P.
Kanerva,
& H.
Asoh (Eds.),
Foundations of real-world intelligence (p. 294 – 308).
Stan-
ford, CA, USA: CSLI Publications.
Kim,
Y.
(2014).
Convolutional
neural
networks for sen-
tence classification. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing
(p. 1746 – 1751).
Doha, Qatar.
Kutuzov,
A.,
& Kuzmenko,
E.
(2017).
Building Web-
Interfaces for Vector Semantic Models with the WebVectors
Toolkit.
In Proceedings of the Demonstrations at the 15th
Conference of the European Chapter of the Association for
Computational Linguistics.
Association for Computational
Linguistics.
Lample,
G.,
Ballesteros,
M.,
Subramanian,
S.,
Kawakami,
K., & Dyer, C.
(2016).
Neural architectures for named
entity recognition.
In Proceedings of the 2016 Meeting of
the North American Chapter of the Association for Com-
putational Linguistics and Human Language Technology
Conference (p. 260 – 270).
San Diego, CA, USA.
Landauer,
T.
K.,
& Dumais,
S.
T.
(1997).
A solution to
Plato’s problem: The Latent Semantic Analysis theory of
the acquisition, induction, and representation of knowledge.
Psychological Review(104), 211 – 240.
Levy, O., & Goldberg, Y.
(2014).
Linguistic Regularities in
Sparse and Explicit Word Representations.
In Proceedings
of the Eighteenth Conference on Computational Natural
Language Learning (p. 171 – 180).
Ann Arbor, Michigan:
Association for Computational Linguistics.
Levy, O., Goldberg, Y., & Dagan, I.
(2015).
Improving Dis-
tributional Similarity with Lessons Learned from Word
Embeddings.
Transactions of the Association of Computa-
tional Linguistics, 3, 211 – 225.
Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard,
S. J., & McClosky,
D.
(2014).
The Stanford CoreNLP
Natural Language Processing Toolkit.
In Association for
Computational Linguistics (ACL) System Demonstrations
(p. 55 – 60).
Mikolov, T., Chen, K., Corrado, G., & Dean, J.
(2013).
Effi-
cient Estimation of Word Representations in Vector Space.
275
arXiv preprint arXiv:1301.3781.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J.
(2013).
Distributed Representations of Words and Phrases
and their Compositionality.
Advances in Neural Informa-
tion Processing Systems 26, 3111 – 3119.
Parker, R., Graff, D., Kong, J., Chen, K., & Maeda, K.
(2011).
English Gigaword Fifth Edition LDC2011T07 (Tech. Rep.).
Technical Report. Linguistic Data Consortium, Philadel-
phia.
Pennington, J., Socher, R., & Manning, C.
(2014).
GloVe:
Global Vectors for Word Representation.
In Proceedings
of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP) (p. 1532 – 1543).
Doha,
Qatar: Association for Computational Linguistics.
Plank, B., Søgaard, A., & Goldberg, Y.
(2016).
Multilingual
part-of-speech tagging with bidirectional long short-term
memory models and auxiliary loss.
In Proceedings of
the 54th Meeting of
the Association for Computational
Linguistics.
Berlin, Germany.
Ravichandran, D., Pantel, P., & Hovy, E.
(2005).
Randomized
algorithms and NLP: using locality sensitive hash function
for high speed noun clustering.
In Proceedings of the 43rd
Meeting of the Association for Computational Linguistics
(p. 622 – 629).
Ann Arbor, MI, USA.
ˇ
Reh˚
u
ˇ
rek, R., & Sojka, P.
(2010).
Software Framework for
Topic Modelling with Large Corpora.
In Proceedings of
the LREC 2010 Workshop on New Challenges for NLP
Frameworks (pp. 45–50).
Valletta, Malta: ELRA.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.,
Ng, A., & Potts, C.
(2013).
Recursive deep models for
semantic compositionality over a sentiment treebank.
In
Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing (p. 1631 – 1642).
Seattle,
WA, USA.
Solberg, L. J.
(2012).
A corpus builder for Wikipedia.
Unpub-
lished master’s thesis, University of Oslo, Norway.
Spearman,
C.
(1904).
The Proof and Measurement of As-
sociation between Two Things.
The American Journal of
Psychology, 15(1), 72 – 101.
Straka, M., Haji
ˇ
c, J., Straková, J., & Haji
ˇ
c jr., J.
(2015).
Pars-
ing universal dependency treebanks using neural networks
and search-based oracle.
In Proceedings of the 14th Inter-
national Workshop on Treebanks and Linguistic Theories.
Warsaw, Poland.
Velldal,
E.
(2011).
Random indexing re-hashed.
In Pro-
ceedings of the 18th Nordic Conference of Computational
Linguistics (p. 224 – 229).
Riga, Latvia.
Widdows,
D.,
& Ferraro,
K.
(2009).
Semantic vectors:
a
scalable open source package and online technology man-
agement application.
In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evaluation.
Marrakech, Morocco.
276
