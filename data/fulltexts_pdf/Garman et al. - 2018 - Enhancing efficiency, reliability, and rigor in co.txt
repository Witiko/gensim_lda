Competency-based Education. 2018;e01164. 
wileyonlinelibrary.com/journal/cbe2 
 
|
  
1 of 4
https://doi.org/10.1002/cbe2.1164
© 2018 Western Governors University
1 | INTRODUC TION
Competency modeling is frequently used in higher education and 
workplace settings to inform a variety of learning and performance 
improvement programs (Campion et al., 2011). However, the ap‐
proaches commonly used for tasks such as organizing and cross‐
walking competency models often require considerable time and 
effort on the part of subject experts, and are vulnerable to the bi‐
ases of the people completing these tasks (Lucia & Lepsinger, 1999). 
As the use of competency‐based education (CBE) and assessment 
becomes more widespread, more efficient and robust approaches 
to development and analytic tasks are also likely to become more 
important.
A potential approach to creating greater efficiency in com‐
petency 
analysis 
tasks 
is 
through 
automated 
natural 
language 
processing (NLP). NLP involves the use of software‐based systems 
to 
quantitatively 
analyze 
and 
organize 
text 
information 
(Bates, 
1995). Use of NLP has expanded rapidly in recent years, and schol‐
arly work has begun to emerge in areas such as organizational studies 
(Goldenstein, Poschmann, & Handschke, 2015), as well as practi‐
cal applications such as team formation (Montelisciani, Gabelloni, 
Giacomo Tazzini, & Gualtiero Fantoni, 2014) and diversity and in‐
clusion (Morgan, Dunleavy, & DeVries, 2016). An important enabler 
of this work has been the increasing availability and sophistication 
of open source applications capable of running NLP analytics, such 
as NLTK and Gensim, which can be operated using the open source 
programming language, Python (Coehlo & Richert, 2015).
One NLP technique in particular—assessment of document or 
text similarity—may be particularly relevant to competency mod‐
eling. The approach involves first combining a set of texts into a 
Received: 24 February 2018 
|
  Revised: 7 April 2018 
|
  Accepted: 8 May 2018
DOI: 10.1002/cbe2.1164
O R I G I N A L R E S E A R C H
Enhancing efficiency, reliability, and rigor in competency model 
analysis using natural language processing
Andrew N. Garman
1
 | Melanie P. Standish
2
 | Dae Hyun Kim
3
1
Health Systems Management, Rush 
University, Chicago, Illinois
2
Industrial/Organizational Psychology, 
Illinois Institute of Technology, Chicago, 
Illinois
3
Department of Health Administration, 
University of Alabama Birmingham, 
Birmingham, Alabama
Correspondence
Andrew N. Garman, Health Systems 
Management, Rush University, 1700 W. Van 
Buren St., Ste 126B, Chicago, IL 60612.
Email: Andy_N_Garman@rush.edu
Background: Competency modeling is frequently used in higher education and 
workplace settings to inform a variety of learning and performance improvement 
programs. However, approaches commonly taken to modeling tasks can be very 
labor‐intensive, and are vulnerable to perceptual and experience biases of raters.
Aims: The present study assesses the potential for natural language processing (NLP) 
to support competency‐related tasks, by developing a baseline comparison of results 
generated by NLP to results generated by human raters.
Methods: Two raters separately conducted cross‐walks for leadership competency 
models of graduate healthcare management programs from eight universities against 
a newly validated competency model from the National Center for Healthcare 
Leadership containing 28 competencies, to create 224 cross‐walked pairs of “best 
matches”.
Results: Results indicated that the NLP model performed at least as accurately as 
human raters, who required a total of 16 work hours to complete, versus the NLP 
calculations which were nearly instantaneous.
Conclusion: Based on these findings, we conclude that NLP has substantial promise 
as a high‐efficiency adjunct to human evaluations in competency cross‐walks.
K E Y W O R D S
competency models, leadership, natural language processing, validation
2 of 4  
|
    
GARMAN 
et
Al
.
corpus, and then assigning numerical values to words and/or phrases 
based on their relative frequency and uniqueness within the corpus. 
This calculation then allows the individual texts to be compared 
based on a formula that quantifies the number of words they share in 
common, adjusting for the relative frequency or infrequency of the 
words within the corpus. A cosine similarity statistic—or “cosine” for 
short—is frequently used for this purpose, which will vary between 
zero and one, with one indicating identical texts (Singhal, 2001).
Cosine approaches have been shown to perform relatively well 
for many applications (Fang, Tao, & Zhai, 2004); however, they can 
run into trouble when words are misused or misspelled, sentence 
structures are dissimilar, or creative grammar is being deployed—
such as in the analysis of Facebook posts or Twitter feeds. The be‐
havioral statements comprising competency models do not tend to 
suffer from these limitations, and thus appear to be a particularly 
appropriate task for NLP.
The present study seeks to assess the potential for NLP as a 
resource in competency cross‐walking by comparing its perfor‐
mance on a real‐world task to the performance of human raters. The 
specific task that will be assessed involves cross‐walking the com‐
petency models currently being used by a variety of graduate health‐
care management programs in the United States to a research‐based 
referent model. The task represents a real‐world problem: accred‐
ited healthcare management programs are required to use a compe‐
tency model to inform their curriculum and to post their competency 
model on their public websites; however, there is not currently a 
universally recognized model they are required to adopt. A referent 
model that cross‐walks successfully across existing programs could 
help them to identify a common standard across their programs. 
For the purposes of this research project, the referent model being 
tested is from nonprofit National Center for Healthcare Leadership 
(NCHL), version 3.0, a model version that had recently been vali‐
dated at the time of this study and was not yet in use by the field.
2 | METHODS
The first task involved identifying a pool of accredited graduate 
programs to serve as our test cases. Eight graduate programs were 
identified through searches of the online database of their accred‐
iting organization, the Commission on Accreditation of Healthcare 
Management Education (CAHME; www.cahme.org). Next, any grad‐
uate programs that were using prior versions of the NCHL compe‐
tency model were eliminated, replaced with new randomly drawn 
programs until a total of eight was reached. The eight models were 
then transferred into Excel spreadsheets, for comparison to the ref‐
erent model (NCHL v. 3.0). Two independent raters then iterated 
through the NCHL model eight times, once for each graduate pro‐
gram model, to (a) select the competency they felt best fit each of the 
28 competencies in the NCHL model (224 comparisons in total), and 
then (b) evaluate the strength of the “fit” for each match on a four‐
point scale (no confidence, low confidence, medium confidence, high 
confidence). Both raters were familiar with the NCHL model through 
prior graduate work in their respective disciplines; one of the raters 
had additional training in competency analysis through her graduate 
program in industrial psychology, and was thus considered a more 
experienced coder. Once both sets of codings were complete, the 
raters met to compare their separate results, and developed a con‐
sensus document of matched competency pairings. Raters were also 
asked to track the amount of time it took them to complete each of 
these tasks.
The NLP analysis was conducted using an internally developed 
program (CrossBot, v1.1: Garman & Lindsey, 2017) in conjunction 
with the open source NLP application Gensim (Rehurek & Sojka, 
2010). The program first creates a matrix of cosines for any two 
competency models being compared, and then identifies which 
comparison yielded the largest cosine value (i.e., showed the great‐
est similarity), which is labeled as the best fit. The NLP analysis was 
then compared to those of the two raters as well as their consensus 
estimate.
3 | RESULTS
The first analysis compared human ratings of confidence to the NLP 
cosines. For the NLP analysis, the average cosine similarities for 
best‐fit pairings was 0.19 (SD = 0.16). For the human raters, pooled 
assessments of pairings were 47% in the “high confidence” category, 
33% in the “medium confidence” category, and 20% in the “low” and 
“no confidence” categories. The latter two categories were com‐
bined due to their relatively low numbers. As human confidence in 
pairings increased, so did the cosines, with an average of 0.12 for the 
“no” and “low confidence” categories, 0.14 for the “medium confi‐
dence” category, and 0.26 for the “high confidence” category.
Our 
second 
analysis 
examined 
inter‐rater 
agreement, 
using 
Cohen’s kappa (Carletta, 1996). The kappa formula provides a mea‐
sure of the level of agreement between two raters, which is corrected 
for the probability that agreement could have happened by chance 
alone. Kappa can vary between zero (no agreement) and one (perfect 
agreement). For this study, the kappa between the two human raters 
was 0.26. Kappa statistics comparing NLP to human raters was 0.28 
for the less experienced rater, 0.31 for the more experienced rater, 
and 0.32 for the consensus ratings. Magnitudes suggested that NLP 
performed comparably to the human raters. We also assessed how 
effectively cosine magnitude agreed with the consensus human pair‐
ings at various threshold levels, as shown in Figure 1. As this figure 
illustrates, the NLP approach reaches majority agreement at cosines 
of 0.25 and above. At 0.3 and above, the NLP approach reaches ap‐
proximately 2/3 agreement at 0.3 and above; however at that level, 
the number of pairings also declines steeply, indicating a clear trade‐
off between sensitivity and specificity.
The final analysis examined relative efficiency, as measured by 
total human effort involved in the cross‐walking processes. For this 
step, the two raters were asked to track the total time they spent 
on comparing and coding competency statements individually, as 
well as the time required to complete the consensus discussion. The 
    
|
 3 of 4
GARMAN 
et
Al
.
combined total time was estimated to be 16 total hours, or roughly 
two business days. In contrast, the NLP analysis was essentially in‐
stantaneous, and postprocessing (copying/pasting results into an 
analyzable file) required approximately ten minutes. The preproduc‐
tion time necessary to prepare competency models for analysis was 
not tracked separately, and could represent an additional time cost 
to use of the NLP approach. In most cases, the source models could 
be readily copied and pasted into the required format in a matter of 
a few minutes, with an estimated total time of not more than 80 min 
across the eight models, suggesting a potential total time savings of 
14–15 hr for a project of this scope.
4 | DISCUSSION
Findings from this study suggest that successful application of 
NLP to competency‐related tasks could have substantial implica‐
tions for both academicians and practitioners. For academicians, 
making judgments about similarity and relatedness of concepts 
has always been one of the more difficult aspects of not only com‐
petency modeling, but also organizational research of many other 
kinds. NLP approaches could provide a pathway to more efficient 
and reliable approaches to these types of classification tasks, and 
studies such as the ones proposed can begin to provide greater 
exposure of these concepts to industrial psychologists working in 
academic settings. Additionally, this project can provide academi‐
cians with greater exposure to the important open source tools 
that are available to support NLP analyses. For practitioners, suc‐
cessful results could provide a much more efficient means for 
completing competency modeling and cross‐walking tasks, and 
may be less susceptible to human biases and errors. Even in cases 
where the approach yields relatively few cosines at levels sug‐
gesting high confidence, its use may still speed up human efforts 
to complete these tasks by prioritizing “most likely” matches for 
human review. This would eliminate the need to review all possible 
matches, by automatically eliminating the ones that show little or 
no concordance. Lastly, it is worth noting that all of the preproc‐
essing and analytics functions used in this study were developed 
using readily accessible, open source applications requiring rela‐
tively little additional programming.
There are numerous ways in which future studies could use‐
fully build on the results presented here. One area that seems 
particularly fruitful involves developing additional preprocessing 
steps that enhance the breadth of potential applications. For ex‐
ample, the approach used in the present study did not attempt 
to classify the relative level of cognitive complexity represented 
by the competencies. Programming a preprocessing step to rec‐
ognize taxonomies such as those originally pioneered by Bloom 
(1965), may allow NLP applications to facilitate the matching of 
individual learning objectives to curricular competency frame‐
works. Another useful expansion could relate to the pursuit of 
transdisciplinary 
competencies. 
Community 
and 
governmental 
organizations are increasingly recognizing that complex systems 
challenges, such as community resilience, require the synthesis 
of multiple professional skillsets into collaborative teams (Acosta, 
Chandra, & Madrigano, 2017). The use of profession‐specific lan‐
guages for similar concepts presents challenges to these efforts; 
computer‐assisted identification of definitional equivalency could 
help address some of these challenges more efficiently.
The field of natural language processing is evolving rapidly, 
aided in part by the accessibility of open source tools and re‐
sources that can assist users in their application. For readers who 
may be interested in setting up their own NLP analysis, tutorials 
on the Python programming language are readily accessible on the 
web (e.g., https://docs.python.org/3/tutorial/), and most of the 
popular open source platforms also offer useful tutorials on how 
to get started. Gensim tutorials are available on the program au‐
thor’s website (https://radimrehurek.com/gensim/tutorial.html).
ORCID
Andrew N. Garman 
http://orcid.org/0000‐0001‐6733‐4951 
FIGURE 1 Percent agreement with 
human raters and proportions of pairings 
at different cosine cutoffs
68%
54%
49%
45%
44%
17%
31%
39%
54%
67%
0%
10%
20%
30%
40%
50%
60%
70%
80%
0.3
0.25
0.2
0.15
0.1
Percent agreement with 
human raters
Proportion of pairs above the 
Cosine cutoff
4 of 4  
|
    
GARMAN 
et
Al
.
REFERENCES
Acosta, J., Chandra, A., & Madrigano, J. (2017). An agenda to advance 
integrative resilience research and practice: Key themes from a re‐
silience roundtable. White paper, RAND Corporation.
Bates, M. (1995). Models of natural language understanding. Proceedings 
of the National Academy of Science, 92, 9977–9982. https://doi.
org/10.1073/pnas.92.22.9977
Bloom, B. S. (1965). Taxonomy of educational objectives: The classification 
of educational goals. New York: David McKay Company, Inc.
Campion, M. A., Fink, A. A., Ruggeberg, B. J., Carr, L., Phillips, G. M., & 
Odman, R. B. (2011). Doing competencies well: Best practices in 
competency modeling. Personnel Psychology, 64, 225–262. https://
doi.org/10.1111/j.1744‐6570.2010.01207.x
Carletta, J. (1996). Assessing agreement on classification tasks: The 
kappa statistic. Computational linguistics, 22, 249–254.
Coehlo, L. P., & Richert, W. (2015). Building machine learning systems with 
Python, 2nd ed. Birmingham, UK: Packt Publishing.
Fang, H., Tao, T., & Zhai, C. X. (2004). A formal study of information re‐
trieval heuristics. In K. Järvelin, M. Sanderson, P. Bruza & J. Allan 
(Eds.), Proceedings of the 27th ACM SIGIR conference on research 
and development in information retrieval (SIGIR’04) (pp. 49–56). 
ACM.
Garman, A. N., & Lindsey, Z. (2017). CrossBot v1.1 – a Gensim‐enabled 
Python program to automate competency crosswalks. Working 
paper, Rush University HSM Leadership Center, Chicago, IL.
Goldenstein, J., Poschmann, P., & Handschke, S. G. M. (2015). Linguistic 
analysis: The study of textual data in management and organi‐
zation studies with natural language processing. Proceedings of 
the Academy of Management, 1, 10882. https://doi.org/10.5465/
ambpp.2015.10882abstract
Lucia, A. D., & Lepsinger, R. (1999). The art and science of competency mod-
els: Pinpointing critical success factors in organizations. San Francisco, 
CA: Jossey‐Bass.
Montelisciani, G., Gabelloni, D., Giacomo Tazzini, G., & Gualtiero Fantoni, 
G. (2014). Skills and wills: The keys to identify the right team in 
collaborative innovation platforms. Technology Analysis & Strategic 
Management, 6, 687–702. https://doi.org/10.1080/09537325.201
4.923095
Morgan, W. B., Dunleavy, E., & DeVries, P. D. (2016). Using big data to 
create diversity and inclusion in organizations. In S. Tonidandel, 
E. B. King, & J. M. Cortina (Eds.), Big data at work: The data science 
revolution and organizational psychology (pp. 310–335). New York: 
Routledge.
Rehurek, R., & Sojka, P. (2010). Software framework for topic modeling with 
large corpora. Proceedings of the LREC workshop New challenges for 
NLP frameworks. Valletta: University of Malta.
Singhal, 
A. 
(2001). 
Modern 
information 
retrieval: 
A 
brief 
overview. 
Bulletin of the IEEE Computer Society Technical Committee on Data 
Engineering, 24(4), 35–43.
How to cite this article: Garman AN, Standish MP, Kim DH. 
Enhancing efficiency, reliability, and rigor in competency 
model analysis using natural language processing. 
Competency-based Education. 2018;e01164. https://doi.
org/10.1002/cbe2.1164
AUTHOR’S BIOGRAPHY
Andrew N. Garman is professor of health systems management, 
Rush University, Chicago, IL, and Chief Executive Officer of 
the National Center for Healthcare Leadership (www.nchl.org). 
Established in 2001, the nonprofit NCHL supports health sec‐
tor organizations in developing leadership capacity by providing 
research and evidence‐based resources supporting high‐quality, 
mission‐based development programs.
Melanie P. Standish is a PhD student in industrial/organizational 
psychology at Illinois Tech, Chicago, IL. During this research 
work, Melanie worked as Project Coordinator for Leadership 
Competencies 
Research 
at 
Rush 
University, 
and 
previously 
as Operations Assistant for the Illinois Tech Global Leaders 
program.
Dae Hyun Kim is a PhD student in Health Services Administration 
at University of Alabama Birmingham, AL. Prior to this role, 
Daniel worked as a research assistant in the School of Public 
Health at the University of Michigan, Ann Arbor, MI.
