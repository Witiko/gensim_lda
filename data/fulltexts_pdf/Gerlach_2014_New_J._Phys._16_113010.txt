New Journal of Physics
PAPER • OPEN ACCESS
Scaling laws and fluctuations in the statistics of
word frequencies
To cite this article: Martin Gerlach and Eduardo G Altmann 2014 New J. Phys. 16 113010
View the article online for updates and enhancements.
Related content
Generalized entropies and the similarity of
texts
Eduardo G Altmann, Laércio Dias and
Martin Gerlach
-
Authorship recognition via fluctuation
analysis of network topology and word
intermittency
Diego R Amancio
-
Scaling laws and model of words
organization in spoken and written
language
Chunhua Bian, Ruokuang Lin, Xiaoyu
Zhang et al.
-
Recent citations
Zipf and Heaps laws from dependency
structures in component systems
Andrea Mazzolini et al
-
Unifying models of dialect spread and
extinction using surface tension dynamics
James Burridge
-
Generalized entropies and the similarity of
texts
Eduardo G Altmann et al
-
This content was downloaded from IP address 147.251.55.69 on 26/10/2018 at 22:12
Scaling laws and ﬂuctuations in the statistics of word
frequencies
Martin Gerlach and Eduardo G Altmann
Max Planck Intsitute for the Physics of Complex Systems,
D-01187 Dresden,
Germany
E-mail: gerlach@pks.mpg.de and edugalt@pks.mpg.de
Received 11 July 2014,
revised 10 September 2014
Accepted for publication 19 September 2014
Published 4 November 2014
New Journal
of Physics 16 (2014) 113010
doi:10.1088/1367-2630/16/11/113010
Abstract
In this paper,
we combine statistical
analysis of written texts and simple sto-
chastic models to explain the appearance of scaling laws in the statistics of word
frequencies.
The average vocabulary of
an ensemble of
ﬁxed-length texts is
known to scale sublinearly with the total number of words (Heaps’ law).
Ana-
lyzing the ﬂuctuations around this average in three large databases (Google-
ngram, English Wikipedia, and a collection of scientiﬁc articles), we ﬁnd that the
standard deviation scales linearly with the average (Taylorʼs law), in contrast to
the prediction of
decaying ﬂuctuations obtained using simple sampling argu-
ments. We explain both scaling laws (Heaps’ and Taylor) by modeling the usage
of
words using a Poisson process with a fat-tailed distribution of
word fre-
quencies (Zipfʼs law) and topic-dependent frequencies of individual words (as in
topic models). Considering topical variations lead to quenched averages, turn the
vocabulary
size
a
non-self-averaging
quantity,
and
explain
the
empirical
observations.
For the numerous practical applications relying on estimations of
vocabulary size,
our results show that uncertainties remain large even for long
texts.
We show how to account
for
these uncertainties in measurements of
lexical richness of texts with different lengths.
Keywords:
scaling laws,
stochastic processes,
statistical
ﬂuctuations,
natural
language
Content from this work may be used under the terms of the Creative Commons Attribution 3.0 licence.
Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal
citation and DOI.
New Journal
of Physics 16 (2014) 113010
1367-2630/14/113010+18$33.00
© 2014 IOP Publishing Ltd and Deutsche Physikalische Gesellschaft
1. Introduction
Fat-tailed distributions [1–3],
allometric scaling [4,
5],
and ﬂuctuation scaling [6–8] are the
most prominent examples of scaling laws appearing in complex systems. Statistics of words in
written texts provide some of
the best
studied examples:
the fat-tailed distribution of
word
frequencies (Zipfʼs law) [9] and the sublinear growth (as in allometric scalings) of the number
of distinct words as a function of database size (Heaps’ law) [10, 11]. The connection between
these two scalings
has
been known at
least
since Mandelbrot
[12]
and has
been further
investigated in recent
years
[13–15],
especially for
large databases
[16],
ﬁnite text
sizes
[17, 18], and more general distributions [19, 20]. In this paper, we report the existence of a third
type of
scaling in the statistics
of
words:
ﬂuctuation scaling.
This
scaling appears
when
investigating the ﬂuctuations around the Heaps’ law,
i.e.,
the variance of the vocabulary over
different texts of the same size scales with the average. We show that this scaling results from
topical
aspects of
written text
that
are ignored in the usual
connection between Zipfʼs and
Heaps’ law.
The importance of looking at the ﬂuctuations around Heaps’ law is that this law is used in
different
applications [21],
e.g.,
(i)
to optimize the memory allocation in inverse indexing
algorithms [22]; (ii) to estimate the vocabulary of a language [23, 24]; and (iii) to compare the
vocabulary
richness
of
documents
with
different
lengths
[25–27].
Beyond
linguistic
applications,
scalings of the number of unique items as a function of database size similar to
Heaps’ law have been observed in other domains, e.g. the species-area relationship in ecology
[28, 29], collaborative tagging [30], network growth [31], and in the statistics of chess moves
[32].
These scaling laws
have
been analyzed from the general
viewpoint
of
innovation
dynamics
[33]
and sampling problems
[34
].
Our
results
allow for
the
quantiﬁcation of
uncertainties in the estimation of these scaling laws and lead to a rethinking of the statistical
signiﬁcance of previous ﬁndings.
We use as databases three different
collections of
texts:
(i)
all
articles of
the English
Wikipedia [35],
(ii) all
articles published in the journal
PlosOne [36],
and (iii) the Google-
ngram database [23],
a collection of books published in 1520–2008 (each year is treated as a
separate document).
See appendix A for details on the data.
The manuscript is divided as follows. Section 2 reports our empirical ﬁndings with focus
on the deviations from a Poisson null
model.
Section 3 shows how these deviations can be
explained by including topicality,
which plays the role of a quenched disorder and leads to a
non-self averaging process.
The consequences of our ﬁndings to applications,
e.g.
vocabulary
richness,
are discussed in section 4.
Finally,
section 5 summarizes our main results.
2. Empirical scaling laws
The most-prominent scaling in language is Zipfʼs law [9], which states that the frequency, F, of
the rth most frequent word (i.e.,
the fraction of times it occurs in the database) scales as
∝
≫
α
−
F
r
r
for
1.
(1)
r
Another
well-studied scaling in language concerns the vocabulary growth and is known as
Heaps’ law [10, 11]. It states that the number of different words, N, scales sublinearly with the
total number of words,
M,
i.e.
2
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
∝
≫
λ
N M
M
M
(
)
for
1,
(2)
with
λ
<
<
0
1
. As a third case, we consider here the problem of the vocabulary growth for an
ensemble of texts, and study the scaling of ﬂuctuations by looking at the relation between the
standard deviation,

σ
=
M
N M
(
)
[
(
)
]
,
and the mean value,

μ
=
M
N M
(
)
[
(
)
]
,
computed
over the ensemble of texts with the same textlength M.
In other systems,
Taylorʼs law [6]
σ
μ
μ
∝
≫
β
M
M
M
(
)
(
)
for
(
)
1
(3)
with
β
⩽
⩽
1 2
1
is typically observed [8].
The connection between scalings (1)
and (2)
(Zipfʼs and Heaps’
law)
can be revealed
assuming the usage of each word r is governed by an independent Poisson process with a given
frequency F
r
.
In this description,
the number
of
different
words,
N,
becomes a stochastic
variable for which we can calculate the expectation value
 N M
[
(
)
]
and the variance
 N M
[
(
)
]
over the realizations of the Poisson process (see appendix B for details)

∑
μ
≡
=
−
−
N M
M
[
(
)
]
(
)
1
e
,
(4)
r
MF
r
⎡
⎣
⎤
⎦



∑
σ
≡
≡
−
=
−
−
−
N M
M
N M
N M
[
(
)
]
(
)
(
)
[
(
)
]
e
e
.
(5)
r
MF
MF
2
2
2
2
r
r
Assuming Zipfʼs law (1), for
≫
M
1
we recover Heaps’ law (2), i.e.,

∝
λ
N M
M
[
(
)
]
, with a
simple relation between the scaling exponents
α
λ
=
−1
[37] and Taylorʼs law (3) with
β = 1 2
.
In ﬁgure 1,
we show empirical
data of
real
texts for
the scaling relations (1)–(3)
and
compare them with predictions from the Poisson null model in equations (4), (5). The Poisson
null
model
correctly elucidates the connection between the scaling exponents in Zipfʼs and
Heaps’ law, but it suffers from two severe drawbacks. First, it is of limited use for a quantitative
prediction of the vocabulary size for individual
articles as it
systematically overestimates its
magnitude,
see ﬁgures 1(b),
(e) and (h).
Second,
it
dramatically underestimates the expected
ﬂuctuations of the vocabulary size, yielding a qualitatively different behavior in the ﬂuctuation
scaling:
whereas the Poisson null
model
yields an exponent
β ≈ 1 2
expected from central-
limit-theorem-like convergence [8], the three empirical data (ﬁgures 1(c), (f) and (i)) exhibit a
scaling with
β ≈ 1
. This implies that relative ﬂuctuations of N around its mean value μ for ﬁxed
M do not decrease with larger text size (the vocabulary growth,
N(M), is a non-self-averaging
quantity)
and remain of
the order
of
the expected value.
Indeed,
we ﬁnd that
in all
three
databases
σ
μ
≈
M
M
(
)
0.1 (
).
(6)
Instead of looking at a single value (N, M) for each document, as described previously, an
alternative approach is to count the number of different words,
N,
in the ﬁrst M words of the
document. This leads to a curve N(M) for
=
…
M
M
1,
2,
,
max
, where
M
max
is the length of the
document.
This alternative approach was employed in ﬁgures 1(e) and (f) and leads to results
equivalent to the ones obtained using single values (N,
M),
i.e.,
the
μ M
(
)
and
σ M
(
)
obtained
over different
texts lead to identical
Heaps’ and Taylorʼs laws.
In ﬁgure 1(f),
we show that
anomalous ﬂuctuation scaling in the vocabulary growth is preserved if shufﬂing the word order
of individual texts. This illustrates that in contrast to usual explanations of ﬂuctuation scaling in
terms of
long-range correlations in time-series [8],
here,
the observed deviations from the
Poisson null model are mainly due to ﬂuctuations across different texts.
3
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
Figure 1.
Scaling of Zipfʼs law (1),
Heaps’ law (2),
and ﬂuctuation scaling (3).
Each
row corresponds to one of the three databases used in our work. (a,d,g) Zipfʼs law: rank-
frequency distribution F
r
considering the full database (the double power-law nature of
the curves is apparent [19]). (b,e,h) Heaps’ law: the number of different words, N, as a
function of
textlength,
M,
for
each individual
article in the corresponding database
(black dots).
(c,f,i) Fluctuation scaling: standard deviation,
σ M
(
)
,
as a function of the
mean,
μ M
(
)
, for the vocabulary N(M) conditioned on the textlength M. Poisson (blue-
solid)
shows
the expectation from the Poisson null
model,
equations
(4)
and (5),
assuming the empirical
rank-frequency distribution from (a,d,g),
respectively.
(Data:
μ σ
,
) (yellow-solid) shows the mean,
μ M
(
)
, and standard deviation,
σ M
(
)
, of the data
N(M) within a running window in M (see appendix A for the details on the procedure).
Additionally, (e,f) show the results (Data:
μ σ
,
) obtained after shufﬂing the word order
for each individual article (thin green-solid). The fact that this curve is indistinguishable
from the original curve shows that the results are not due to temporal correlations within
the
text.
For
comparison,
we
show in (c,f,i)
the
scalings
σ
μ
∝
M
M
(
)
(
)
1 2
and
σ
μ
∝
M
M
(
)
(
)
(dashed).
4
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
In the following,
we argue that
these observations can be accounted for by considering
topical aspects of written language,
i.e.,
instead of treating word frequencies as ﬁxed,
we will
consider them to be topic dependent (
↦
F
F (topic)
r
r
).
3. Topicality in vocabulary growth
3.1. Topicality
The frequency of an individual word varies signiﬁcantly across different texts, meaning that its
usage cannot be described alone by a single global frequency [38–40]. For example,
consider
the usage of the (topical) word ‘network’ in all articles published in the journal PlosOne. It has
an overall
rank
=
r
*
428
and a global
frequency,
≈
×
=
−
F
2.9
10
r
*
428
4
,
see ﬁgure 2(a).
The
local
frequency obtained from each article separately varies over more than one decade,
see
ﬁgure 2(b).
Note that,
although in this case the local
rank-ordering differs from document
to
document, the index r still refers to the globally determined rank and is used as a unique label
for each word.
One popular approach to account for the heterogeneity in the usage of single words is topic
models [41]. The basic idea is that the variability across different documents can be explained
by the existence of (a smaller number of) topics.
In the framework of a generative model,
it
assumes
(i)
that
individual
documents
are composed of
a mixture of
topics
(indexed by
=
t
T
1,..,
),
with each topic
represented in an individual
document
by the
probabilities
=
P
t
(topic
)
doc
and (ii) that the frequency of each word is topic dependent,
i.e.,
=
F
t
(topic
)
r
,
which leads to a different
effective frequency in each document,
=
∑
=
F
P
t
F t
( )
( )
r
t
T
r
,doc
1
doc
.
One particularly popular variant
of topic models is Latent
Dirichlet
Allocation (LDA) [42],
which assumes that
the topic composition
P
(topic)
doc
of
each document
is drawn from a
0
Figure 2.
Variation of frequencies due to topicality in the PlosOne database.
(a) Rank-
frequency distribution considering the complete database. The word ‘network’ (dotted line)
has
≈
×
=
−
F
2.9
10
*
r
428
4
.
(b) Distribution
P F
(
)
*
r
of the local
frequency
F
*
r
obtained
from each article separately for the word ‘network’ with the global
frequency from (a)
(dotted).
(c)
Topic-dependent
frequencies
F
(topic)
*
r
inferred from LDA with T = 20
topics for the word ‘network’ with global frequency from (a) as comparison (dotted). (d)
One realization for the topic composition of a single document,
P
(topics)
doc
, drawn from a
Dirichlet
distribution.
For
this
realization,
the
effective
frequency
is
=
∑
≈
×
=
−
F
P
t
F t
( )
( )
2.0
10
r
t
T
r
,doc
1
doc
4
and is shown in (b) (solid).
5
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
Dirichlet
distribution,
P
Dir
,
such that
only a
few topics
contribute
substantially to each
document.
Given a database of
documents,
LDA infers
the topic-dependent
frequencies,
F (topic)
r
, from numerical maximization of the posterior likelihood of the generative model [43].
As
an illustration,
in ﬁgure 2(c),
we show
F
(topic)
r
*
obtained using LDA for
the word
‘network’ in the PlosOne database. As expected from a meaningful topic model, we see that the
conditional frequencies vary over many orders of magnitude, and that the global frequency
F
r
*
is governed by few topics. The advantage of LDA is that, instead of measuring the distribution
of frequencies of each individual
word (or two-point
distributions for assessing correlations)
over different documents, it estimates the frequency of individual words for a ﬁnite (and small)
number of topics.
In combination with the generative model (e.g.,
drawing
P
(topic)
doc
from a
Dirichlet
distribution),
this
not
only yields
a more compact
description of
topicality by
dramatically reducing the number of parameters,
but
also allows for an easy extrapolation to
unseen texts from a small training sample [42].
3.2. General
treatment
In this section,
we show how topicality can be included in the analysis of
the vocabulary
growth. The simplest approach is to consider again that the usage of each word is governed by
Poisson processes,
but
this time to consider that
frequencies are not
ﬁxed but
are themselves
random variables that vary across texts.
In this setting, the random variable representing the vocabulary size, N, for a text of length
M can be written as
⎡
⎣
⎤
⎦
∑
=
(
)
N M
I
n
M F
(
)
,
,
(7)
r
r
r
in which n
r
is the integer number of times the word r occurs in a Poisson process of length M
with frequency F
r
and
I
x
[
]
is an indicator-type function, i.e.,
=
=
I
x
[
0]
0
and
⩾
=
I
x
[
1]
1
.
The calculation of
the expectation value now consists
of
two parts:
(i)
the average over
realizations i
of
the Poisson processes
n
M F
(
,
)
r
i
r
j
( )
( )
for
a given realization j
of
the set
of
frequencies
F
r
j
( )
and (ii) the average overall possible realizations j of the sets of frequencies
F
r
j
( )
(which vary due to topicality).
In this framework,
expectation values correspond to quenched
averages (denoted by subscript q)
⎡
⎣
⎤
⎦

∑
∑
=
=
=
−
−
(
)
N M
N M
I
n
M F
[
(
)
]
(
)
,
1
e
,
(8)
q
i j
i j
r
r
i
r
j
i j
r
MF
j
( ,
)
,
( )
( )
,
r
j
( )
where we used
⎡
⎣
⎤
⎦
=
−
=
=
−
−
(
)
(
)
I
n
M F
P n
M F
,
1
0;
,
1
e
.
(9)
r
i
r
j
i
r
r
j
MF
( )
( )
( )
r
j
( )
The last equation corresponds to the probability of word r not occurring for a Poisson process of
duration M with frequency
F
r
j
( )
, as in equation (4). For simplicity,
hereafter
… ≡ …
j
(the
average over realizations of sets of frequencies
F
r
j
( )
).
Using the inequality between arithmetic and geometric mean
=
⩽
=
x
x
e
e
,
(10)
x
x
ln
geometric
arithmetic
ln
6
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
we obtain that


∑
∑
=
−
⩽
−
≡
−
−
N M
N M
[
(
)
]
1
e
1
e
[
(
)
]
.
(11)
q
r
MF
r
M F
a
r
r
The right-hand side corresponds to the result of the Poisson null model (with ﬁxed
= 〈
〉
F
F
r
r
),
see equation (4), and can be interpreted as an annealed average (denoted by subscript a). This
implies that the heterogeneous dissemination of words across different texts leads to a reduction
of the expected size of the vocabulary, in agreement with the ﬁrst deviation of the Poisson null
model reported in ﬁgures 1(b),
(e) and (h).
For the quenched variance,
we obtain (see appendix C)
⎡
⎣
⎤
⎦



≡
−
N M
N M
N M
[
(
)
]
(
)
[
(
)
]
(12)
q
q
q
2
2
⎡
⎣
⎤
⎦
∑
∑∑
=
−
+
−
−
′≠
−
−
′
e
e
e
e
Cov
,
(13)
r
MF
MF
r
r
r
MF
MF
2
r
r
r
r
where
≡ 〈
〉 − 〈
〉〈
〉
−
−
−
−
−
−
′
′
′
Cov[e
,
e
]
e
e
e
e
MF
MF
MF
MF
MF
MF
r
r
r
r
r
r
. Comparing to the Poisson case in
equation (5),
we see that
the quenched average yields
an additional
term containing the
correlations of different words. In general, this term does not vanish and is responsible for the
anomalous ﬂuctuation scaling with
β = 1
observed in real text, explaining the second deviation
from the Poisson null model reported in ﬁgures 1(c),
(f) and (i).
3.3. Specific ensembles
In this section, we compute the general results from equation (8), (13) for particular ensembles
of frequencies
F
r
j
( )
and compare them to the empirical
results.
In the absence of a generally
accepted
parametric
formulation
of
such
an
ensemble,
we
propose
two
nonparametric
approaches explained in the following.
In the ﬁrst
approach,
we construct
the ensemble
F
r
j
( )
directly from the collection of
documents, i.e., the frequency
F
r
j
( )
corresponds to the frequency of word r in document j, such
that
∑
=
−
=
−
D
e
1
e
,
(14)
MF
j
D
MF
1
r
r
j
( )
where D is the number of documents in the data,
see ﬁgure 2(b).
In the second approach,
we construct
the ensemble from the LDA topic model
[42],
in
which
=
=
F
F
j
(topic
)
r
j
r
( )
corresponds to the frequency of word r conditional
on the topic
=
…
j
T
1,
,
, see ﬁgures 2(c) and (d). In this particular formulation, each document is assumed
to consist of a composition of topics,
P
(topic)
doc
, which is drawn from a Dirichlet distribution,
such that we get for the quenched average
∫
θ
θ α
=
θ
−
−
P
e
d
(
)e
,
(15)
MF
MF
Dir
(
)
r
r
in which
θ
θ
θ
=
…
(
,
,
)
T
1
are the probabilities of each topic,
θ
θ
=
∑
=
=
F
F
j
(
)
(topic
)
r
j
T
j
r
1
, and
the
integral
is
over
a
T-dimensional
Dirichlet-distribution
θ α
P
(
|
)
Dir
with
concentration
parameter α.
We infer the
F (topic)
r
using Gensim [43] for LDA with T = 100 topics.
The results from both approaches are compared to the PlosOne database in ﬁgure 3.
Figure 3(a) shows that both methods lead to a reduction in the mean number of different words.
7
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
Whereas the direct ensemble, equation (14), almost perfectly matches the curve of the data, the
LDA-ensemble,
equation (15),
still
overestimates the mean number of different
words in the
data. This is not surprising because, due to the fewer number of topics (when compared to the
number of documents),
it
constitutes a much more coarse-grained description than the direct
ensemble. Additionally, the LDA-ensemble relies on a number of ad-hoc assumptions, e.g., the
Dirichlet-distribution in equation (15) or the particular choice of parameters in the inference
algorithm,
which were not optimized here.
More importantly,
both methods correctly account
for the anomalous ﬂuctuation scaling with
β = 1
observed in the real data, see ﬁgure 3(b), and
even yield a similar
proportionality factor
in the quantitative agreement
with the data.
The
comparison of the individual contributions to the ﬂuctuations, equation (13), shown in the inset
of ﬁgure 3(b) shows that
the anomalous ﬂuctuation scaling is due to correlations in the co-
occurrence of different words (contained in the term
−
−
′
Cov[e
,
e
]
MF
MF
r
r
).
4. Applications
4.1. Adding texts
In thermodynamic terms, Heaps’ law (as other allometric scalings) implies that the vocabulary
size
is
neither
extensive
nor
intensive
(
<
<
N M
N
M
N M
(
)
(2
)
2
(
)
,
also for
→ ∞
M
).
Although this can be seen as a direct consequence of Zipfʼs law, our results show that Heaps’
law depends also sensitively on the ﬂuctuations of
the frequency of
speciﬁc words across
different documents. To illustrate this, consider the problem of doubling the size of a text of size
M.
This can be done either
by simply extending the size of
the same text
up to size
M
2
Figure 3.
Vocabulary growth for speciﬁc topic models. (a) Average vocabulary growth
and (b) ﬂuctuation scaling in the PlosOne database (Data) and in the calculations from
equations (8),
(13)
for
the two topic models based on the measured frequencies in
individual articles (Real Freq) and on LDA (LDA Freq), compare equations (14), (15).
For
comparison,
we
show the
results
from the
Poisson
null
model
(Poisson),
equations (4),
(5),
which do not
consider
topicality.
The inset
in (b) (same scale as
main ﬁgure)
shows the individual
contributions to the ﬂuctuations in equation (13):
∑
〈
〉 − 〈
〉
−
−
e
e
r
MF
MF
2
r
r
(dotted) and
∑ ∑
′≠
−
−
′
Cov[e
, e
]
r
r
r
MF
MF
r
r
(solid),
illustrating that
correlations between different
words lead to anomalous ﬂuctuation scaling.
The solid
lines for LDA-Freq and Real
Freq in (b) show the calculations of the corresponding
topic
models
replacing
the
Poisson
by
multinomial
usage
in
the
derivation
of
equations (8),
(13) to avoid ﬁnite-size effects for
μ
<
M
(
)
100
.
8
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
(denoted by
′
=
M
M
2 ·
) or by concatenating another text of size M (denoted by
′
=
×
M
M
2
).
The
Poisson model
(ﬁxed frequency or
annealed average)
predicts
the
same
expected
vocabulary for both procedures


∑
=
×
=
−
−
N
M
N
M
[
(2 ·
)
]
[
(2
)
]
1
e
.
(16)
a
a
r
M F
2
r
Taking ﬂuctuations
of
individual
frequencies
across
documents
(quenched average)
into
account yields (see appendix D for details)


∑
∑
=
−
×
=
−
−
−
N
M
N
M
[
(2 ·
)
]
1
e
and
[
(2
)
]
1
e
.
(17)
q
r
MF
q
r
MF
2
2
r
r
Using equation (10) and the fact that
〈
〉 ⩾
x
x
2
2
,
we obtain the following general result




⩽
×
⩽
×
=
N
M
N
M
N
M
N
M
[
(2 ·
)
]
[
(2
)
]
[
(2
)
]
[
(2 ·
)
]
.
(18)
q
q
a
a
This is consistent with the intuition that the concatenation of different texts (e.g.,
on different
topics) leads to larger vocabulary than a single longer text. The preceding calculations remain
true if the text is extended by a factor k (instead of 2),
even for
→ ∞
k
.
The ﬂuctuations
around the mean show a more interesting behavior,
as
revealed by
repeating the preceding calculations for the variance.
We consider the case of k texts each of
length M,
such that
′ =
×
M
k
M
,
and focus on the terms containing correlations between
different words shown to be responsible for the anomalous ﬂuctuation scaling (see appendix D
for details):

∑
×
∼
−
′
−
−
−
−
′
′
N k
M
[
(
)
]
e
e
e
e
.
(19)
q
r r
MF
MF
k
MF
k
MF
k
,
r
r
r
r
The individual terms can be written as
⎡
⎣
⎢
⎢
⎤
⎦
⎥
⎥
∑
∑
=
′
−
−
−
…
′
e
e
e
,
(20)
r r
MF
MF
k
r
MkF
j
j
,
¯
2
,
,
r
r
r
k
k
(
)
1
⎡
⎣
⎢
⎢
⎢
⎤
⎦
⎥
⎥
⎥
∑
∑
=
′
−
−
−
…
′
e
e
e
,
(21)
r r
MF
k
MF
k
r
MkF
j
j
,
¯
,
,
2
r
r
r
k
k
(
)
1
in which
〈
〉
…
·
j
j
,
,
k
1
denotes the averaging over the realizations
…
j
j
(
,
,
)
k
1
of frequencies
F
r
j
(
)
i
in
each single text
=
…
i
k
1,
,
and
=
∑
=
F
F
¯
r
k
k
i
k
r
j
(
)
1
1
(
)
i
is the k-sample average frequency based on
the realizations
…
j
j
(
,
,
)
k
1
.
In the limit
→ ∞
k
:
→
F
F
¯
r
k
r
(
)
such that
∑
−
→
′
−
−
−
−
′
′
e
e
e
e
0
(22)
r r
MF
MF
k
MF
k
MF
k
,
r
r
r
r
for
→ ∞
k
.
This implies that,
for
≫
k
1
,
(adding many different texts) the ﬂuctuations in the
vocabulary across documents (and therefore the correlations between different
words) vanish
and normal
ﬂuctuation scaling (
β = 1 2
) is recovered.
This prediction can be tested in data.
Starting from a collection of
documents,
we create a new collection by concatenating k
randomly selected documents
(each document
is
used once).
We then compute for
each
9
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
concatenated document
the number
of
distinct
words
N up to size M for
increasing M,
 N M
[
(
) ],
and
 N M
[
(
) ]
. We observe a transition of the exponent β in the ﬂuctuation scaling,
equation (3),
from
β
β
≈
→ ≈
1
1 2
.
4.2. Vocabulary richness
When measuring vocabulary richness, we want a measure that is robust to different text sizes.
The traditional approach is to use Herdanʼs C, i.e.,
=
C
N
M
log
log
[25–27]. Although quite
effective for rough estimations, this approach has several problems. An obvious problem is that
it
does not
incorporate any deviations from the original
Heaps’ law (e.g.,
the double scaling
regime [19]). More seriously, it does not provide any estimation of the statistical signiﬁcance or
expected ﬂuctuations of the measure.
For instance,
if two values are measured for different
texts, one cannot determine whether one is signiﬁcantly larger than the other. Our approach is to
compare observations with the ﬂuctuations expected from models in the spirit of section 3.2.
The computation of
statistical
signiﬁcance requires an estimation of
the probability of
ﬁnding N different words in a text of length M,
P N M
(
|
)
, which can be obtained from a given
generative model
(e.g.,
as presented in section 3).
For a text with
N
M
(
*
,
*
)
,
we compute the
percentile
>
P N
N M
(
*
|
*
)
, which allows for a ranking of texts with different sizes such that the
smaller
the percentile,
the richer
the vocabulary.
An estimation of
the signiﬁcance of
the
difference in the vocabulary can then be obtained by comparison of the different percentile.
For the sake of simplicity,
we illustrate this general approach by approximating
P N M
(
|
)
using a
Gaussian distribution.
In this
case,
the
percentile
are
determined by the
mean,

μ
=
M
N M
(
)
[
(
) ]
,
and the variance,

σ
=
M
N M
(
)
[
(
) ]
,
in terms of the z-score
μ
σ
=
−
z
N
M
M
(
)
(
)
,
(23)
N M
(
,
)
which shows how much the measured value (N, M) deviates from the expected value
μ M
(
)
in
units of standard deviations (
z
N M
(
,
)
follows a standard normal distribution:

∼
z
(0,
1)
d
). If we
consider our quantitative result
on ﬂuctuation scaling in the vocabulary in equation (6),
i.e.,
σ
μ
≈
M
M
(
)
0.1 (
)
,
we can calculate the z-score of the observation (N,
M) as
⎛
⎝
⎜
⎞
⎠
⎟
μ
μ
μ
≈
−
=
−
z
N
M
M
N
M
(
)
0.1 (
)
10
(
)
1 ,
(24)
N M
(
,
)
in which we need to include the expected vocabulary growth,
μ M
(
)
,
from a given generative
model (e.g., Heaps’ law with two scalings [19]). We can now: (i) for a single text (N, M), assign
a value of lexical richness, the z-score
z
N M
(
,
)
, considering deviations from the pure Heaps’ law
that should be included in
μ M
(
)
; (ii) given two texts
N M
(
,
)
1
1
and
N
M
(
,
)
2
2
,
compare directly
the respective z-scores
z
N M
(
,
)
1
1
and
z
N M
(
,
)
2
2
to assess which text
has a higher lexical
richness
independent of the difference in the textlengths; and (iii) estimate the statistical signiﬁcance of
the
difference
in vocabulary by considering
Δ
=
−
z
z
z
:
N M
N M
(
,
)
(
,
)
1
1
2
2
,
which is
distributed
according to

Δ ∼
z
(0,
2)
d
because

∼
z
(0,
1)
d
. Point (iii) implies that the difference in the
vocabulary richness
of
two texts
is
statistically signiﬁcant
on a 95%-conﬁdence level
if
Δ
>
z
|
|
2.77
, i.e., in this case there is at most a 5% chance that the observed difference originates
from topic ﬂuctuations.
As a general
rule,
for
two texts of
approximately the same length
(
μ
≈
N M
M
(
)
(
)
), the relative difference in the vocabulary must be larger than
27.7%
to be sure
on a 95%-conﬁdence level that the difference is not due to expected topic ﬂuctuations.
10
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
We illustrate this approach for the vocabulary richness of Wikipedia articles.
As a proxy
for the true vocabulary richness, we measure how much the vocabulary of each article,
N(M),
exceeds the average vocabulary
N
M
(
)
avg
with the same textlength M empirically determined
from all articles in the Wikipedia. In practice however, when assessing the vocabulary richness
of a single article, information of
N
M
(
)
avg
from an ensemble of texts is usually not available and
measures such as the ones described previously are needed.
In ﬁgure 4,
we compare the
accuracy of measures of vocabulary richness according to Herdanʼs C,
ﬁgure 4(a),
and the z-
score, ﬁgures 4(b) and (c). For the latter, we use equation (24) and calculate
μ M
(
)
from Poisson
word usage by ﬁxing Zipfʼs law and assuming Gamma-distributed word frequencies across
documents,
see appendix E for details.
We see in ﬁgure 4(a) that Herdanʼs C shows a strong
bias towards assigning high values of C to shorter texts: following a line with constant C, we
observe for
≳
M
10
articles with a vocabulary below average,
whereas for
>
M
1000
articles
with a vocabulary above average.
A similar (weaker) bias is observed in ﬁgure 4(b) for the
calculation of the z-score for the case in which we consider deviations from the pure Heaps’ law
but
treat
frequencies
of
individual
words
as
ﬁxed,
i.e.,
ignoring topicality.
The
z-score
calculations including topicality in ﬁgure 4(c) show that
we obtain a measure of vocabulary
richness which is approximately unbiased with respect
to the textlength M (contour lines are
roughly horizontal). Furthermore, in contrast to the two other measures, we correctly assign the
highest z-score to the article with the highest ratio
N M N
M
(
)
(
)
avg
. Altogether, this implies that
it is not only important to consider deviations from the pure Heaps’ law, but that it is crucial to
consider topicality in the form of a quenched average.
Figure 4.
Measures of vocabulary richness.
For 5000 randomly selected articles from
the Wikipedia database (black dots),
we compute the ratio between the number
of
different words N(M) and the average number of different words
N
M
(
)
avg
(empirically
determined from all articles with the same textlength M). We compare the predictions of
different
measures of
vocabulary richness (solid lines):
(a)
Herdanʼs C and (b+c)
z-
score, equation (24), in which we calculate the expected null model,
μ M
(
)
, according to
equation (E.5) with parameters
γ = 1.77
,
=
r
˜
7830
[19], and
→ ∞
a
(in b) or a = 0.08
(in c). The solid lines are contours corresponding to values of N(M) that yield the same
measure of vocabulary richness varying from rich (red:
C = 0.98 and z = 4) to poor
(purple:
C = 0.8 and
= −
z
4
)
vocabulary.
The article with the richest
vocabulary
according to each measure is marked by × (red).
11
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
5. Discussion
In summary, we used large text databases to investigate the scaling between vocabulary size N
(number of different
words) and database size M.
Besides the usual
analysis of the average
vocabulary size (Heaps’ law), we measured the standard deviation across different texts with the
same length M. We found that the relative ﬂuctuations (standard deviation divided by the mean)
do not decay with M in contrast to simple sampling processes.
We explained this observation
using a simple stochastic process (Poisson usage of words) in which we account
for topical
aspects of written text,
i.e.,
the frequency of an individual word is not treated as ﬁxed across
different documents. This heterogeneous dissemination of words across different texts leads to a
reduction of the expected size of the vocabulary and to an increase in the variance.
We have
further shown the implications of these ﬁndings by proposing a practical measure of vocabulary
richness that allows for a comparison of the vocabulary of texts with different lengths, including
the quantiﬁcation of statistical signiﬁcance.
Our ﬁnding of anomalous ﬂuctuation scaling implies that
the vocabulary is a non-self-
averaging quantity,
meaning that
the vocabulary of a single text
is not
representative of the
whole ensemble.
Here,
we emphasized that
topicality can be responsible for
this
effect.
Although the existence of different
topics is obvious for a collection of articles as broad in
content
as the Wikipedia,
our analysis shows that
we can apply the same reasoning for the
Google-ngram data, in which case the frequency variation is measured at different times. This
offers a new perspective on language change [44]:
the difference in the vocabulary from
different years can be seen as a shift in the topical content over time. Similarly, other systematic
ﬂuctuations (e.g.,
across different
authors or in the parameters of the Zipfʼs law) can play a
similar role as topicality.
Beyond linguistic applications,
allometric scaling [4,
5]
and other
sublinear
scalings
similar to Heaps’ law [28–33] have been observed in different
complex systems.
Our results
show the importance of studying ﬂuctuations around these scalings and provide a theoretical
framework for the analysis.
Acknowledgements
We thank Diego Rybski for insightful discussion on ﬂuctuation scaling.
Appendix A. Data
The Wikipedia database consists of the plain text of all
3,
743,
306
articles from a snapshot of
the complete English Wikipedia [35].
The PlosOne database consists of all
76,
723
articles
published in the journal PlosOne, which were accessible at the time of the data collection [36].
The Google-ngram database is a collection of printed books counting the number of times a
word appears
in a given year
∈
t
[1520–2008]
[23].
We treat
the collection of
all
books
published in the the same year as a single document, yielding 393 observations for different t.
We apply the same ﬁltering for each database:
(i) we decapitalize each word (e.g.,
‘the’
and ‘The’ are counted as the same word) and (ii) we restrict
ourselves to words consisting
uniquely of
letters
present
in the alphabet
of
the English language.
This
is
meant
as
a
12
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
conservative approach to minimize the inﬂuence of foreign words,
numbers (e.g.
prices),
or
scanning problems present in the raw data (for details on the preprocessing see [19]).
Due to peculiarities of the individual databases the data (Data:
μ σ
,
) in ﬁgure 1,
i.e.,
the
calculation of the curves
μ M
(
)
and
σ M
(
)
conditioned on the textlength M, is constructed in a
slightly different way in each case. In the Wikipedia data, we order all datapoints N(M) (of the
full article) according to textlength M and consider 1000 consecutive datapoints (in M),
from
which we calculate the average value of the textlength M, and the conditional mean,
μ M
(
)
, and
variance,
σ M
(
)
, of the vocabulary N. In the PlosOne data, the length of all articles is much more
concentrated,
which is why we consider the full trajectory N(M) with
=
…
M
M
1,
2,
,
max
for
each individual
article.
For an arbitrary value of M,
we calculate
μ M
(
)
and
σ M
(
)
from the
ensemble of all articles with vocabulary N at the particular textlength M. In the Google-ngram
data, we impose a logarithmic binning in M such that we can calculate
μ M
(
)
and
σ M
(
)
from a
ﬁnite number of samples in each bin.
Appendix B. Poisson null model
The number of different words in each realization of the Poisson process is given by
⎡
⎣
⎤
⎦
∑
=
(
)
N M
I
n
M F
(
)
,
,
(B.1)
r
r
r
in which n
r
is the integer number of times the word r occurs in a Poisson process of length M
with frequency F
r
and
I
x
[
]
is an indicator-type function, i.e.,
=
=
I
x
[
0]
0
and
⩾
=
I
x
[
1]
1
.
Averaging
over
realizations
of
the
Poisson
process
requires
the
calculation
of

≡ 〈
〉 =
−
−
I
n
M F
I
n
M
[
[
(
,
) ] ]
[
(
) ]
1
e
r
r
r
MF
r
,
which is the probability that the word with rank
r appears at least once in a text of length M.
Considering all words,
we obtain

∑
∑
=
=
−
−
[
]
N M
I
n
M
[
(
)
]
(
)
1
e
,
(B.2)
r
r
r
MF
r
⎡
⎣
⎤
⎦



≡
−
N M
N M
N M
[
(
)
]
(
)
[
(
)
]
(B.3)
2
2
∑
∑
=
−
′
′
′
′
[
]
[
]
[
]
[
]
I
n
I
n
I
n
I
n
(B.4)
r r
r
r
r r
r
r
,
,
∑
∑
∑
=
+
−
′
′
′
′
≠ ′
[
]
[
]
[
]
[
]
[
]
I
n
I
n
I
n
I
n
I
n
(B.5)
r
r
r r
r
r
r r
r
r
2
,
,
r
r
∑
∑
∑
=
+
−
′
′
′
′
≠ ′
[
]
[
]
[
]
[
]
[
]
I
n
I
n
I
n
I
n
I
n
(B.6)
r
r
r r
r
r
r r
r
r
,
,
r
r
∑
=
−
−
−
e
e
(B.7)
r
MF
MF
2
r
r
where we used that
=
I x
I
x
[
]
[
]
2
and that
Poisson processes of
different
words (
≠ ′
r
r
)
are
independent of each other.
13
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
Appendix C. Calculation E
q
N(M )
2
h
i
⎡
⎣
⎤
⎦

=
N M
N M
N M
(
)
(
)
(
)
(C.1)
q
i j
i j
i j
2
( ,
)
( ,
)
,
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
∑
=
′
′
′
(
)
(
)
I
n
M F
I
n
M F
,
,
(C.2)
r r
r
i
r
j
r
i
r
j
i j
,
( )
( )
( )
( )
,
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
∑
∑∑
=
+
′≠
′
′
(
)
(
)
(
)
I
n
M F
I
n
M F
I
n
M F
,
,
,
(C.3)
r
r
i
r
j
i j
r
r
r
r
i
r
j
r
i
r
j
i j
( )
( )
2
,
( )
( )
( )
( )
,
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
∑
∑ ∑
=
+
′≠
′
′
(
)
(
)
(
)
I
n
M F
I
n
M F
I
n
M F
,
,
,
(C.4)
r
r
i
r
j
i j
r
r
r
r
i
r
j
i
r
i
r
j
i
j
( )
( )
,
( )
( )
( )
( )
∑
∑ ∑
=
−
+
−
−
−
′≠
−
−
′
(
)(
)
1
e
1
e
1
e
(C.5)
r
MF
r
r
r
MF
MF
r
r
r
where we used
=
I x
I
x
[
]
[
]
2
,
equation (9),
and that
two Poisson process of
different
words
(
≠ ′
r
r
) with a given set of frequencies
F
r
j
( )
are independent of each other.
Appendix D. Adding texts
In this section, we show the calculation for the quenched averages of the mean and the variance
of the vocabulary growth when considering a text
of length
′
M
from the concatenation of k
different
texts of length M
i
with
′ =
∑
=
M
M
i
k
i
1
.
We will
ﬁrst
focus on the case k = 2,
i.e.,
′ =
+
M
M
M
1
2
,
from which we can easily generalize to arbitrary k.
We consider the vocabulary growth,
′
N M
(
)
, as a random variable in which we concatenate
two independent
realizations of the stochastic process introduced in section 3.2 indicated by
subscript (1) and (2) respectively:
⎡
⎣
⎤
⎦
∑
′ =
+
=
+
(
)
(
)
(
)
N M
M
M
I
n
M F
n
M F
,
,
(D.1)
r
r
r
r
r
1
2
(1)
1
(1)
(2)
2
(2)
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
⎡
⎣
⎤
⎦
∑
=
+
−
(
)
(
)
(
)
(
)
I
n
M F
I
n
M F
I
n
M F
I
n
M F
,
,
,
,
(D.2)
r
r
r
r
r
r
r
r
r
(1)
1
(1)
(2)
2
(2)
(1)
1
(1)
(2)
2
(2)
14
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
in which the word r is counted as part
of the vocabulary if it
appears in either
of the two
concatenated realizations of the stochastic process.
In the same spirit as in section 3.2,
taking
expectation values requires averaging over all realizations of the Poisson process (
i
i
,
1
2
) given
the frequencies
F
F
,
r
j
r
j
(
)
(
)
1
2
as well as averaging over all realizations of those frequencies (
j
j
,
1
2
),
which we denote by
〈
〉
·
i
i
j
j
,
,
,
1
2
1
2
. For the individual terms appearing in
′ =
+
N M
M
M
(
)
1
2
,
we
obtain
⎜
⎟
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥
=
−
−
(
)
I
n
M F
,
1
e
(D.3)
(
)
r
i
r
j
i
i
j
j
M F
j
(
)
1
,
,
,
r
j
1
1
1
2
1
2
1
1
1
⎜
⎟
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥
=
−
−
(
)
I
n
M F
,
1
e
,
(D.4)
(
)
r
i
r
j
i
i
j
j
M F
j
(
)
2
,
,
,
r
j
2
2
1
2
1
2
2
2
2
⎜
⎟
⎜
⎟
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
⎛
⎝
⎜
⎜
⎞
⎠
⎟
⎟
=
−
−
′
′
−
−
′
(
)
(
)
I
n
M F
I
n
M F
,
,
1
e
1
e
,
(D.5)
(
)
(
)
r
i
r
j
r
i
r
j
i
i
j
j
M F
j
M F
j
(
)
1
(
)
2
,
,
,
r
j
r
j
1
1
2
2
1
2
1
2
1
1
1
2
2
2
in which we
can separate
the
average
over
i
j
(
,
)
1
1
and
i
j
(
,
)
2
2
,
assuming that
the
two
concatenated realizations
i
j
(
,
)
1
1
and
i
j
(
,
)
2
2
of the original stochastic process are independent.
For the calculation of the expectation of
′ =
+
N M
M
M
(
)
1
2
2
,
we get
higher order terms for
≠ ′
r
r
:
⎜
⎟
⎜
⎟
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥
⎛
⎝
⎜
⎞
⎠
⎟
⎛
⎝
⎜
⎞
⎠
⎟
=
−
−
′
′
−
−
′
(
)
(
)
I
n
M F
I
n
M F
,
,
1
e
1
e
.
(D.6)
(
)
(
)
r
i
r
j
r
i
r
j
i
i
j
j
M F
M F
j
(
)
1
(
)
1
,
,
,
r
j
r
j
1
1
1
1
1
2
1
2
1
1
1
1
1
From this,
we can evaluate the mean and variance
⎡
⎣
⎤
⎦

∑
′ =
+
=
−
−
−
(
)
N M
M
M
1
e
e
(D.7)
q
r
M F
M F
1
2
r
r
1
2
⎡
⎣
⎤
⎦

∑
∑
′ =
+
=
−
+
−
−
−
−
−
′
−
−
−
−
−
−
−
−
′
′
′
′
(
)
N M
M
M
e
e
e
e
e
e
e
e
e
e
e
e
.
(D.8)
q
r
M F
M F
M F
M F
r r
M F
M F
M F
M F
M F
M F
M F
M F
1
2
2
2
,
r
r
r
r
r
r
r
r
r
r
r
r
1
2
1
2
1
1
2
2
1
1
2
2
Generalizing to the concatenation of an arbitrary number of k texts can be treated in the
very same way; however,
we will only state the result for the case of adding k texts of equal
length M such that
′ =
×
M
k
M
:
15
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann

∑
′ =
×
=
−
−
N M
k
M
[
(
)
]
1
e
(D.9)
q
r
MF
k
r

∑
∑
′ =
×
=
−
+
−
−
−
′
−
−
−
−
′
′
N M
k
M
[
(
)
]
e
e
e
e
e
e
.
(D.10)
q
r
MF
k
MF
k
r r
MF
MF
k
MF
k
MF
k
2
,
r
r
r
r
r
r
Appendix E. Vocabulary growth for Gamma-distributed frequency and a double
power-law
Assuming a gamma-distribution for the distribution of the frequency of single words across
different texts [38]
Γ
=
=
Γ
−
−
−
(
)
P F
x
a b
a
b
x
;
,
1
(
)
e
(E.1)
r
a
a
x b
1
we can calculate the quenched average
∫
=
=
=
+
Γ
−
−
−
(
)
xP F
x
a b
bM
e
d
;
,
e
(1
)
.
(E.2)
MF
r
Mx
a
r
If we assume that
the distribution of frequencies for all
words is given by the same shape-
parameter a (e.g.,
a = 1 corresponds to an exponential
distribution) and ﬁx the mean of the
distribution,
given by
=
F
ab
r
we get
=
+
−
−
M F
a
e
(1
)
MF
r
a
r
.
Assuming a double
power-law for
the average rank-frequency distribution [19]
with parameters
γ and
r
˜
,
i.e.,
=
−
F
Cr
r
1
for
⩽
r
r
˜
and
=
γ
γ
−
−
F
Cr
r
˜
r
1
for
>
r
r
˜
, where
γ
=
C
C r
(
˜
,
)
is the normalization
constant
determined by imposing
∑
=
F
1
r
r
,
we
can calculate
the
vocabulary growth
according to equation (4) analytically in the continuum approximation by substituting
=
x
F
:
r

∑
=
−
+
−
(
)
N M
M F
a
[
(
)
]
1
1
(E.3)
q
r
r
a
∫
= −
−
+
−
[
]
x
r
x
Mx a
d
d
d
1
(1
)
(E.4)
a
0
1
which can be expressed in terms
of
the ordinary hypergeometric function
=
H
F
:
2
1
[45]
yielding
⎜
⎟
⎜
⎟
⎜
⎟
⎜
⎟
⎡
⎣
⎢
⎛
⎝
⎜
⎞
⎠
⎟
⎤
⎦
⎥
⎛
⎝
⎞
⎠
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥
⎛
⎝
⎞
⎠
⎡
⎣
⎢
⎛
⎝
⎞
⎠
⎤
⎦
⎥

γ
γ
Γ
Γ
Γ
Γ
=
−
+
−
−
−
−
−
+
+
+
+
−
−
+
+
+
+
+
−
−
−
−
N M
r
C
r
H a
CM
ar
C
M
a
a
a
a
H
a
a
M
r
CM
ar
a
a
a
H
a
ar
CM
[
(
)
]
˜
˜
,
1
,
1
1
,
˜
1
1
(1
)
(2
)
1,
1,
2
,
1
˜
1
˜
(1
)
(2
)
1,
1,
2
,
˜
1 ,
(E.5)
q
a
a
where the vocabulary growth

N M
[
(
)
]
q
is parametrized by γ,
r
˜
,
and a.
16
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
In
the
limit
→ ∞
a
,
the
Gamma
distribution
=
Γ
P F
x
a b
(
;
,
)
r
with
given
mean
=
=
F
ab
const.
r
converges to a Gaussian with
σ
=
F
a
r
2
2
.
For
→ ∞
a
,
σ
→ 0
2
and
we recover the Poisson null model, equations (4), (5), in which the individual frequencies F
r
are
ﬁxed (annealed average).
References
[1]
Clauset A,
Shalizi C R and Newman M E J 2009 SIAM Review 51 661
[2]
Mitzenmacher M 2004 Internet Math.
1 226–51
[3]
Newman M E J 2005 Contemp.
Phys.
46 323–51
[4]
West G B 1997 Science 276 122–6
[5]
Bettencourt L M A,
Lobo J,
Helbing D,
Kühnert C and West G B 2007 Proc.
Natl Acad.
Sci.
104 7301–6
[6]
Taylor L R 1961 Nature 189 732–5
[7]
de Menezes M and Barabási A L 2004 Phys.
Rev.
Lett.
92 028701
[8]
Eisler Z,
Bartos I and Kertész J 2008 Adv.
Phys.
57 89–142
[9]
Zipf G K 1949 Human Behaviour and the Principle of Least Effort (Cambridge, MA: Addison-Wesley Press)
[10]
Herdan G 1958 Biometrika 45 222–8
[11]
Heaps H S 1978 Information Retrieval (New York: Academic)
[12]
Mandelbrot
B 1961 On the Theory of Word Frequencies and on Related Markovian Models of Discourse
Structure of Language and Its Mathematical Aspects: Proceedings of Symposia in Applied Mathematics
Vol XII (Providence,
RI: American Mathematical Society)
[13]
van Leijenhorst D C and van der Weide T P 2005 Inf.
Sci.
170 263–72
[14]
Zanette D and Montemurro M 2005 J.
Quant.
Linguist.
12 29–40
[15]
Serrano M A,
Flammini A and Menczer F 2009 PloS One 4 e5372
[16]
Williams H E and Zobel J 2005 Int.
J.
Dig.
Lib.
5 99–105
[17]
Bernhardsson S,
Correa da Rocha L E and Minnhagen P 2009 New J.
Phys.
11 123015
[18]
Lü L,
Zhang Z and Zhou T 2010 PloS One 5 e14139
[19]
Gerlach M and Altmann E G 2013 Phys.
Rev.
X 3 021006
[20]
Font-Clos F,
Boleda G and Corral A 2013 New J.
Phys.
15 093033
[21]
Manning C D,
Raghavan P and Schuetze H 2008 Introduction to Information Retrieval
(New York:
Cambridge University Press)
[22]
Baeza-Yates R and Navarro G 2000 J.
Am.
Soc.
Inf.
Sci.
51 69–82
[23]
Michel J B et al 2011 Science 331 176–82
[24]
Klein W 2013 Von Reichtum und Armut des Deutschen Wortschatzes Reichtum und Armut der Deutschen
Sprache (Berlin: De Gruyter) pp 15–56
[25]
Wimmer G and Altmann G 1999 J.
Quant.
Linguist.
6 1–9
[26]
Baayen R H 2001 Word Frequency Distributions (Dordrecht,
Netherlands: Kluwer Academic Publishers)
[27]
Yasseri T,
Kornai A and Kertész J 2012 PLoS One 7 e48386
[28]
Brainerd B 1982 J Appl.
Probab.
19 785–93
[29]
García Martín H and Goldenfeld N 2006 Proc.
Natl Acad.
Sci.
103 10310–5
[30]
Cattuto C,
Barrat A,
Baldassarri A,
Schehr G and Loreto V 2009 Proc.
Natl Acad.
Sci.
106 10511–5
[31]
Krapivsky P L and Redner S 2013 J.
Stat.
Mech.
2013 P06002
[32]
Perotti J I,
Jo H H,
Schaigorodsky A L and Billoni O V 2013 EPL (Europhysics Letters) 104 48005
[33]
Tria F,
Loreto V,
Servedio V D P and Strogatz S H 2014 Sci.
Rep.
4 5890
[34]
Gnedin A,
Hansen B and Pitman J 2007 Probab.
Surv.
4 146–71
[35]
Wikimedia.
Dump
of
the
English
Wikipedia
from 02/06/2012
(http://dumps.wikimedia.org/enwiki/);
accessed 26/06/2012
[36]
PlosApi.
All articles published in the journal plosone (http://api.plos.org/); accessed 17/10/2013
17
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
[37]
Eliazar I 2011 Physica A 390 3189–203
[38]
Church K W and Gale W A 1995 Nat.
Lang.
Eng.
1 163–190
[39]
Montemurro M and Zanette D 2010 Adv.
Complex Syst.
13 135
[40]
Altmann E G,
Pierrehumbert J B and Motter A E 2011 PloS One 6 e19009
[41]
Blei D M 2012 Commun.
ACM 55 77
[42]
Blei D M,
Ng A Y and Jordan M I 2003 J.
Mach.
Learn.
Res.
3 993–1022
[43]
Řehůřek R and Sojka P 2010 Software Framework for Topic Modelling with Large Corpora Proc.
LREC
2010 Workshop on New Challenges for NLP Frameworks (Valletta,
Malta: ELRA) pp 45–50
[44]
Baronchelli A,
Loreto V and Tria F 2012 Adv.
Complex Syst.
15 1203002–1
[45]
Abramowitz M and Stegun I
A (ed)
1972 Handbook of
Mathematical
Functions
(New York:
Dover
Publications)
18
New J. Phys. 16 (2014) 113010
M Gerlach and E G Altmann
