Assessing author self-citation as a mechanism of relevant
knowledge diffusion
Ramiro H.
Ga
´
lvez
1
Received: 19 October 2016 / Published online: 8 March 2017
Ó Akade
´
miai Kiado
´
,
Budapest,
Hungary 2017
Abstract
Author self-citation is a practice that has been historically surrounded by con-
troversy.
Although the prevalence of self-citations in different
scientific fields has been
thoroughly analysed,
there is a lack of large scale quantitative research focusing on its
usefulness at guiding readers in finding new relevant scientific knowledge. In this work we
empirically address this issue.
Using as our main corpus the entire set of PLOS journals
research articles,
we train a topic discovery model able to capture semantic dissimilarity
between pairs of articles. By dividing pairs of articles involved in intra-PLOS citations into
self-citations (articles linked by a cite which share at
least
one author)
and non-self-
citations (articles linked by a cite which share no author),
we observe the distribution of
semantic dissimilarity between citing and cited papers in both groups.
We find that
the
typical semantic distance between articles involved in self-citations is significantly smaller
than the observed one for articles involved in non-self-citations. Additionally, we find that
our results are not driven by the fact that authors tend to specialize in particular areas of
research,
make use of specific research methodologies or simply have particular styles of
writing.
Overall,
assuming shared content as an indicator of relevance and pertinence of
citations,
our results indicate that self-citations are,
in general,
useful as a mechanism of
knowledge diffusion.
Keywords Author self-citation  Latent Dirichlet allocation  Semantic dissimilarity 
Knowledge diffusion
& Ramiro H.
Ga
´
lvez
rgalvez@dc.uba.ar
1
Departamento de Computacio
´
n,
FCEyN,
Universidad de Buenos Aires, Buenos Aires, Argentina
123
Scientometrics (2017) 111:1801–1812
DOI 10.1007/s11192-017-2330-1
Introduction
Studying how knowledge flows between actors is a topic of great concern across different
fields of research and policy making (see Fagerberg et al.
2010; Rogers 2003). When the
focus is placed on scientific knowledge,
the interest is usually centred on its flow within
and between scientific areas through citation activities to scientific publications, as well as
on its impact in scientific innovation (Yu et al.
2010; Estabrooks et al.
2008; Park et al.
2005).
Apart from playing a key role in knowledge diffusion,
how scholarly articles cite
each other also matters a lot when assessing the performance of researchers, departments,
universities,
and even national
level
research policies (see Bornmann and Daniel
2008).
For
these reasons,
describing and understanding the citing behavior
of
research actors
stands as a key task in comprehending how scientific knowledge is built
(Knorr-Cetina
1981; Merton 1973).
This work focuses on studying one particular practice related to citing behavior: author
self-citation.
Author self-citation occurs when authors of a given article cite their own
previous work.
The importance of this practice is by no means negligible.
Based on a
sample of
46,849 articles,
Aksnes (2003)
calculates the mean share of
received self-
citations in the order of 21% and estimates that as much as 36% of all citations received by
an article in the first three years after its publication are self-citations.
For this reason its
impact
on authors’
bibliometric indexes is also considered to be significant.
Schreiber
(2007), for example, analyses the impact of removing all self-citations from the calculation
of the H-index (Hirsch 2005),
finding that for prominent researchers it may imply a 20%
reduction in its value and for less established or younger ones a reduction of as much as
40%.
1
Although the prevalence of
self-citations in different
scientific fields has been thor-
oughly analysed (e.g.,
Kulkarni et al.
2011; Hudson 2007; Aksnes 2003; Motamed et al.
2002),
there is a lack of consensus regarding its usefulness at guiding readers in finding
additional relevant knowledge. On the one hand, authors who support this practice tend to
argue that
it
is
useful
at
guiding readers
into important
relevant
research,
helpful
in
avoiding repetition of information by connecting the citing articles to authors’ relevant
previous work,
or even unavoidable when an article focuses on amending or expanding
authors’ previous work (Tagliacozzo 1977).
On the other hand,
given how central
bib-
liometric measures are to researchers careers (Ball 2005) and given the perception that the
share of self-citations relative to all
citations is excessive (MacRoberts and MacRoberts
1989), the practice of author self-citation is also viewed as an undesirable consequence of a
strategic mechanism for artificially inflating the impact
of authors’ own work (Bartneck
and Kokkelmans 2011; Engqvist and Frommen 2008; Schreiber 2007; Hyland 2003) and
even as an act of egotism (Lawani 1982) or self-advertising (Seglen 1992). As an example
of the particularities and consequences attributed to this practice,
Maliniak et al.
(2013)
point towards the existence of a gender citation gap in the international relations literature,
which they partially attribute to the fact that women tend to self-cite themselves less than
men in this field.
Such is the bad reputation of self-citations that most popular scientific
citation indexing services offer options or explicit instructions on how not to include them
in their query results. Moreover, this has translated also into recommendations on how to
deflate their importance on bibliometric indexes (e.g.,
Bartneck and Kokkelmans 2011;
Schreiber 2007).
1
Nevertheless, it should also be noted that there is evidence suggesting that the exclusion of self-citations
has a small or null effect in evaluative bibliometrics at the macro level (Gla
¨
nzel and Thijs 2004).
1802
Scientometrics (2017) 111:1801–1812
123
Even when positive and negative opinions regarding this practice might
hold some
truth,
there is a lack of large scale quantitative research studying up to what degree self-
citations are effectively useful
at
guiding readers towards relevant
research or
are just
strategic citations with no added value for readers seeking further pertinent knowledge. In
this paper we empirically address this issue.
Using as our main corpus all of the research articles published up to October 2015 in the
Public Library of Science (PLOS) journals, we train a topic discovery model from which
we estimate a metric that
captures semantic dissimilarity between articles.
Having esti-
mated this metric,
we divide pairs of articles involved in intra-PLOS citations (a citation
from one PLOS research article to another PLOS research article) into two groups,
self-
citations and non-self-citations.
We then analyse the distribution of the semantic dissim-
ilarity metric between pairs of citing and cited articles in both groups.
Our results place
author
self-citations in a pretty good spot.
We find that
the typical
semantic distance
between articles involved in self-citations is significantly smaller than the one observed for
pair of articles involved in non-self-citations. We also check that our results are not driven
by the very fact that authors tend to focus on particular research areas, make use of specific
methodologies or simply have particular styles of writing. We find that papers which share
authors but are not linked by a cite tend to have a larger semantic dissimilarity than papers
involved in non-self-citations. This suggests that just sharing authors is not the sole driver
of semantic similarity. Taking these results into account, and assuming that shared content
is an indicator of usefulness of citations, the argument that author self-citations are purely
strategic and not useful in guiding readers at deepening their knowledge on topics covered
by the citing article does not seem to hold.
The rest
of
this
article is
structured as
follows:
‘‘Materials
and methods’’
section
describes the data and presents the methodology used; ‘‘Results’’ section presents our main
results; ‘‘Discussion and conclusions’’ section concludes.
Materials and methods
Data
In order to analyse a large and detailed corpus of peer-reviewed research articles covering
several fields of research, we chose as our main corpus all research articles published in the
PLOS journals up to October
2015 (n = 125,269).
The Public Library of Science is a
scientific publishing project aimed at creating a library of open access journals, by the time
of
data collection it
published seven journals:
PLOS Biology,
PLOS Computational
Biology, PLOS Genetics, PLOS Medicine, PLOS Neglected Tropical Diseases, PLOS ONE,
and PLOS Pathogens. One key advantage for this study is that, by using the PLOS API and
its associated R library (Chamberlain et al. 2015), we were able to download full-text data
for every research article published in all seven PLOS journals.
To get
a clearer picture of our corpus,
Table 1 details the total
number of published
articles per PLOS journal.
As seen in Table 1,
the vast majority of articles we analyse were published in PLOS
ONE. PLOS ONE is a peer-reviewed open access scientific journal which covers primary
research from any discipline within science and medicine. An important feature of PLOS
ONE is that,
according to this journal,
articles are not
excluded on the basis of lack of
perceived importance or
adherence to a scientific field.
This
translates
into a large
Scientometrics (2017) 111:1801–1812
1803
123
acceptance rate for articles submitted to PLOS ONE. By contrast, the rest of the analysed
journals are narrower in scope and are known for being far more selective in terms of
acceptance rate.
Apart from the full-text data,
we also used the PLOS API to download metadata and
citation data on all
articles for
which we retrieved full-text
information.
Although the
PLOS API provides a wide range of metadata fields, in this paper we only used the id field,
the authors field, the publication date field and the subject field. The id field is unique to
every article and is equal to its Digital Object Identifier (DOI).
The author field lists the
names of every author in each article. The publication date field indicates the day in which
the article was made available online.
The subject
field holds information on subjects
covered by the article.
2
Regarding the subject field, we only analysed top tier PLOS subject
areas (Biology and life sciences,
Computer and information sciences,
Earth sciences,
Ecology and environmental
sciences,
Engineering and technology,
Medicine and health
sciences,
People and places,
Physical
sciences,
Research and analysis methods,
Science
policy, and Social sciences), where one article may belong to more than one top tier subject
area.
For example,
an article may belong to both Earth sciences and Ecology and envi-
ronmental sciences subject areas.
Table 2 details the total number of published articles per PLOS subject area.
From Table 2 it can be seen that the distribution of articles across PLOS subject areas is
far from uniform. In particular, a large proportion of our corpus covers subjects related to
biology, life sciences, medicine and health sciences; whereas areas such as computer and
information sciences or
social
sciences are under-represented in relative terms.
Never-
theless, it should be mentioned that even when some subject areas are under-represented in
relative terms,
the absolute number of articles considered in our sample for any of them
generally surpasses by far the number of articles analysed in many previous scientometrics
studies focusing on self-citations (e.g.,
Hyland 2003; Snyder and Bonzi 1998).
With respect
to citation data,
for each of the 125,269 research articles we retrieved
which articles they cite. As the Uniform Resource Identifier of cited articles is equal to the
DOI
for
the case of
PLOS articles,
building up the intra-PLOS citation network is
straightforward.
2
The detailed construction of this last field is described in Public Library of Science (2015).
Table 1
Number of articles per
PLOS journal
Journal
n
Share (%)
PLOS One
110,394
88.13
PLOS Genetics
3638
2.90
PLOS Pathogens
2938
2.35
PLOS Neglected Tropical Diseases
2810
2.24
PLOS Computational Biology
2711
2.16
PLOS Biology
1753
1.40
PLOS Medicine
1025
0.82
Whole corpus
125,269
100
1804
Scientometrics (2017) 111:1801–1812
123
Measuring semantic dissimilarity between articles
With the objective of automatically identifying latent
semantic topics in our corpus of
articles, we used the latent Dirichlet allocation (LDA) generative model (Blei et al. 2003).
As Hu (2009)
states,
LDA assumes that
words carry strong semantic information,
and
documents discussing similar topics will use a similar group of words.
Latent topics are
thus discovered by identifying groups of words in the corpus that frequently occur together
within documents.
In this way,
documents are modeled as a random mixture over latent
topics,
where each topic is
characterized by its
own particular
distribution over
the
vocabulary of words. To train the LDA model we used the Gensim’s implementation of the
LDA training algorithm (R
ˇ
ehu
˚
r
ˇ
ek and Sojka 2010).
At training,
we left all parameters in
their default values with the only exception being the numbers of topics, which we set at
1500.
The LDA training algorithm takes tokenized text as input, so, to transform raw text into
a series of tokens,
we ran a series of pre-processing steps.
First,
for each article we kept
only the main text (this means that we removed all references to other articles, all tables, all
table captions,
all
figure captions,
all
section titles,
all
formulas,
the supplementary
material section text and the article title). Second, using TextBlob (Loria et al. 2013), we
tokenized each article and lemmatized each token.
Third,
we removed all
tokens which
contained only numeric characters, which did not have at least one alphanumeric character
in it
or
which belonged to NLTK’s (Bird 2006)
list
of
English stopwords.
Fourth,
we
filtered all tokens which appeared 10 or less times in the entire corpus.
Once the LDA algorithm was trained and each article in the corpus was represented as a
mixture of
topics,
we inferred semantic dissimilarity between pairs of
articles as the
observed dissimilarity in the distribution of
topics between articles.
As in Hall
et al.
(2008),
for
measuring dissimilarity in the distribution of
topics we used the Jensen–
Shannon divergence (JSD) metric (see Brie
¨
t
and Harremoe
¨
s 2009).
The JSD is a metric
designed to capture the dissimilarity between two probability distributions and has the
twofold advantage of being symmetric and always finite.
Formally,
given two discrete
probability distributions P and Q (which in our case stand for estimated mixture of topics
for two given articles),
the JSD metric between them is calculated as the average of the
Kullback–Leibler divergence of each distribution to the average of the two distributions as
follows:
Table 2
Number of articles per
PLOS subject area
PLOS subject area
n
Share (%)
Biology and life sciences
121,681
29.31
Medicine and health sciences
93,993
22.64
Research and analysis methods
80,543
19.40
Physical sciences
44,414
10.70
People and places
20,858
5.02
Social sciences
14,909
3.59
Computer and information sciences
10,211
2.46
Ecology and environmental sciences
9944
2.39
Engineering and technology
9547
2.30
Earth sciences
8820
2.12
Science policy
296
0.07
Scientometrics (2017) 111:1801–1812
1805
123
JSDðP k QÞ ¼
1
2
D
KL
ðP k MÞ þ
1
2
D
KL
ðQ k MÞ
ð1Þ
where M is equal to
1
2
ðP þ QÞ and D
KL
stands for the Kullback–Leibler divergence metric,
which is defined as:
D
KL
ðPkMÞ ¼
X
i
PðiÞ log
PðiÞ
MðiÞ
ð2Þ
Self-citation identification and additional article pairing
As stated in ‘‘Data’’ section, building up the intra-PLOS citation network using the citation
data retrieved from the API
is straightforward.
Once this network was available,
we
divided intra-PLOS citations into two groups:
self-citations and non-self-citations.
The
self-citations group comprises all pairs of articles in which citing and cited articles share at
least one author full name.
The non-self-citations group comprises all pairs of articles in
which citing and cited articles share no author full
name.
Table 3 details the number of
intra-PLOS citations and self-citations for the whole corpus and by PLOS subject area.
From Table 3 it
can be seen that
self-citations are quite regular
in the intra-PLOS
citation network,
as they account
for almost
one every four intra-PLOS citations.
When
focusing on different PLOS subject areas, it can be seen that this ratio is quite stable across
them.
One additional type of article pairing which we analyse is the one in which two articles
share authors but are not connected by any cite.
We refer to the group formed by all of
them as the shared authors & no citation set. For identifying articles falling into this group
we checked for every pair of PLOS articles not connected by a cite if they shared at least
two author full names; if they did, they belonged to this set. This last group is composed of
124,796 pairs of articles. We opted to restrict this group to articles which share at least two
authors
because,
as
many different
authors
share names,
considering a single author
Table 3
Intra-PLOS citations and self-citations for the whole corpus and by PLOS subject area
PLOS subject area
Intra-PLOS citations
Self-citation
Share (%)
Biology and life sciences
112,341
28,407
25.29
Medicine and health sciences
85,125
22,522
26.46
Research and analysis methods
75,269
18,588
24.70
Physical sciences
34,137
8880
26.01
People and places
19,616
5499
28.03
Computer and information sciences
11,679
2527
21.64
Social sciences
10,614
2874
27.08
Ecology and environmental sciences
8833
2099
23.76
Earth sciences
8283
2101
25.37
Engineering and technology
6643
1793
26.99
Science policy
227
37
16.30
Whole corpus
114,451
29,008
25.35
1806
Scientometrics (2017) 111:1801–1812
123
resulted in a high number of false positives detections (i.e., considering articles as sharing
authors when they actually do not).
Results
Having trained the LDA model
as previously described,
we can inspect
each topic’s
semantic content
by exploring its active tokens.
For this purpose,
we follow how LDA
obtained topics are visualized in Eichstaedt et al. (2015). Figure 1 plots the composition of
a few hand-picked topics obtained by the trained model.
In this figure,
each word cloud
contains the words that compose a particular topic, being the size of each word related to
its importance in the topic.
Some interesting results can be seen in Fig.
1. First, the LDA model effectively seems
to capture the multidisciplinary spirit of the PLOS journals. Take, for example, the ample
spectrum that goes from word cloud ‘‘G’’ (which contains tokens related to graph theory)
to word cloud ‘‘D’’ (which relates to statistical
learning classifiers),
to word cloud ‘‘E’’
(which relates to chronobiology terms), or to word cloud ‘‘C’’ (which presumably includes
terms related to sexually transmitted infections). Second, the trained LDA model seems to
capture topics which are commonly related to research subjects areas (e.g.,
word clouds
Fig. 1
Topic composition for a hand-picked sample of topics. Each word cloud contains the words which
compose a particular topic.
The size of each word relates to its importance in the topic
Scientometrics (2017) 111:1801–1812
1807
123
‘‘A’’,
‘‘E’’,
and ‘‘G’’) as well as topics which are commonly related to articles’ method-
ology and data analysis (e.g.,
word clouds ‘‘D’’,
‘‘I’’,
and ‘‘H’’).
More in line with our research purpose, Fig.
2 plots the empirical distribution functions
of the JSD metric for all pairs of articles in the self-citations group, the non-self-citations
group and the shared authors & no citation group.
To further validate our results,
Fig.
2
also plots the empirical
distribution function of the semantic distance for a million ran-
domly sampled pairs of articles which are not connected by any type of cite and share no
author,
we refer to this last group as not related.
A first result observed in Fig.
2 comes from the fact that,
compared to the other three
groups,
the mass of the distribution of the JSD metric for the not
related group lies at
higher values. This is reassuring, as one would expect pairs of articles in which one cites
the other, which share authors, or where both things happen, to be more similar in content
than completely unrelated ones.
More relevant to the aim of this work, Fig.
2 also shows
that articles which cite each other and share at least one author tend to be more similar in
semantic terms than articles linked by a cite in which both articles share no author (a
Mann–Whitney U test rejects the null hypothesis that both samples come from the same
population with p  0:01). Finally, it can be seen that articles which share authors but do
not cite each other have a larger semantic distance than those which cite each other but
share no author (a Mann–Whitney U test rejects the null hypothesis that both samples come
from the same population with p  0:01).
As citation behavior has been shown to vary across fields and sub-fields of research (see
Anauati
et al.
2016;
Snyder
and Bonzi
1998),
we analyse if
the observed patterns are
stable across PLOS subject areas. Figure 3 plots kernel density estimation (KDE) functions
of the JSD metric distribution taking into account
the whole corpus and dividing citing
Fig.
2
Empirical
distribution functions of
the JSD metric.
Each curve plots the empirical
distribution
function for a different set of pair of articles. Self-citations refers to pairs of articles which are connected by
a self-citation.
Non-self-citations refers to pairs of
articles which are connected by a non-self-citation.
Shared authors & no citation refers to pairs of articles which share authors but none cites the other.
Not
related refers to pairs of articles which are not connected by any cite and which share no author
1808
Scientometrics (2017) 111:1801–1812
123
articles by PLOS subject areas.
3
For pairs of papers included in the not related group, we
assign each pair of articles to the PLOS subject areas of the older one.
From Fig.
3 it
can be seen that
the patterns observed for the whole corpus are quite
stable across PLOS subject
areas.
In no case the distribution of self-citations lies at
the
right of the non-self citations one. Only in science policy both distributions are markedly
superimposed; but note that, as the sample of articles belonging to this PLOS subject area
is relatively small,
estimations may be less precise.
When one focuses on articles which
3
KDE functions are a non-parametric way of estimating probability density functions of a random variable.
In contrast to Fig. 2, where we were unable to present the data using KDE functions as the JSD metric for
the not related group is highly concentrated around 0.7; in Fig.
3, as we do not plot the distribution for this
group,
we chose to plot KDE functions instead of empirical distribution functions,
because they are more
straightforward to interpret.
Fig. 3
Kernel density estimation functions of the JSD metric across PLOS subject areas and for the whole
corpus.
Each curve plots kernel
density function estimations of the JSD metric distribution across PLOS
subject areas and for the whole corpus. Each panel considers articles linked by self-citations, articles linked
by non-self-citations and articles which share authors but
are not
linked by any citation.
Numbers in
parentheses represent the total pair of articles considered in each panel
Scientometrics (2017) 111:1801–1812
1809
123
share authors and which are not linked by any cite, it can be seen that in all PLOS subject
areas this group tends to have a larger JSD metric than the non-self-citations one.
Discussion and conclusions
As stated in Wojick et al.
(2006),
scientific publication is in itself
a huge system of
deliberate knowledge diffusion.
In consequence,
citations,
by playing a key role in how
researchers access to new knowledge,
are considered to be one of the main drivers of
scientific knowledge diffusion (Yu et al. 2010). Nevertheless, given the important role that
citations play at the moment of assessing the performance of research agents (see Born-
mann and Daniel 2008), many strategic factors are believed to influence in how researchers
cite previous work.
Of course,
these strategic factors do not necessarily go hand in hand
with knowledge diffusion.
In particular,
author
self-citation is
a practice regarded as
controversial,
as their
share relative to all
citations is considered by some authors as
excessive (see MacRoberts and MacRoberts 1989).
In this work we empirically studied the role of author self-citation as a mechanism of
knowledge diffusion. Using as our main corpus all articles published in the PLOS journals
up to October
2015 we trained a topic discovery model
which allowed us to measure
semantic dissimilarity for every possible pair of articles contained in it. By dividing intra-
PLOS citations into self-citations and non-self-citations we observed that
self-citations
tend to connect articles which share more semantic content than the ones non-self-citations
do.
Additionally,
we found that our results are not driven by the fact that authors tend to
specialize in particular areas of research,
make use of specific research methodologies or
simply have particular styles of writing.
Taken as a whole, our results point towards the idea that, in aggregate terms, authors do
not cite themselves in an irrelevant way and,
when faced with the opportunity,
they cite
articles
of
their
authorship which effectively are
close
in content
(not
citing more
semantically distant ones). Assuming shared content as a proxy for usefulness of citations,
our results indicate that the practice of author self-citation has been employed as a valid
mechanism of relevant knowledge diffusion.
As a corollary,
this suggests that excluding
self-citations
from citation counts
on the basis
of
them being purely strategic is
not
empirically justified.
Although these results place the practice of author self-citation in a pretty good spot,
caution should be taken at
interpolating them to scientific publication as a whole.
Even
when the corpus used in the present
study is large and covers vastly different
fields of
research, one important caveat should be issued: in this study we only analyse intra-PLOS
citations, so the external validity of our results must be furthered checked in future research
using an even larger
and more heterogeneous
corpus
of
scholarly research articles.
Additionally, and as further future work, we believe that analysing the dissimilarity in the
distribution of latent topics of scholarly articles is a promising line of research in scien-
tometrics. Possible applications could be (1) to suggest relevant citations which might have
been omitted in a manuscript by focusing on papers detected as semantically similar, (2) to
detect
possible spurious citations in an article by focusing on cited papers detected as
drastically dissimilar (of course this could be narrowed to the detections of spurious self-
citations),
or (3) to flag possible cases of plagiarism and self-plagiarism by focusing on
excessively similar papers in terms of topics distribution.
1810
Scientometrics (2017) 111:1801–1812
123
References
Aksnes,
D.
(2003).
A macro study of self-citation.
Scientometrics,
56(2),
235–246.
Anauati, V., Galiani, S., & Ga
´
lvez, R. H. (2016). Quantifying the life cycle of scholarly articles across fields
of economic research.
Economic Inquiry,
54(2),
1339–1355.
Ball,
P.
(2005).
Index aims for fair ranking of scientists. Nature,
436(7053),
900–900.
Bartneck,
C.,
& Kokkelmans,
S.
(2011).
Detecting h-index manipulation through self-citation analysis.
Scientometrics,
87(1),
85–98.
Bird,
S.
(2006).
Nltk:
the natural
language toolkit.
In Proceedings of
the COLING/ACL on interactive
presentation sessions, COLING-ACL ’06 (pp. 69–72). Stroudsburg, PA: Association for Computational
Linguistics.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning
Research,
3,
993–1022.
Bornmann,
L.,
& Daniel,
H.-D.
(2008).
What
do citation counts measure? A review of studies on citing
behavior. Journal of Documentation,
64(1),
45–80.
Brie
¨
t, J., & Harremoe
¨
s, P. (2009). Properties of classical and quantum Jensen–Shannon divergence. Physical
Review A,
79(5),
052311.
Chamberlain, S., Boettiger, C., & Ram, K. (2015). rplos: Interface to the Search ‘API’ for ‘PLoS’ Journals.
R package version 0.5.4.
Eichstaedt,
J.
C.,
Schwartz,
H.
A.,
Kern,
M.
L.,
Park,
G.,
Labarthe,
D.
R.,
Merchant, R.
M.,
et al.
(2015).
Psychological language on twitter predicts county-level heart disease mortality. Psychological Science,
26(2),
159–169.
Engqvist, L., & Frommen, J. (2008). The h-index and self-citations. Proceedings of the National academy of
Sciences of the United States of America,
99,
11270–11274.
Estabrooks,
C.
A.,
Derksen,
L.,
Winther,
C.,
Lavis,
J.
N.,
Scott,
S.
D.,
Wallin,
L.,
et
al.
(2008).
The
intellectual structure and substance of the knowledge utilization field: A longitudinal author co-citation
analysis,
1945 to 2004. Implementation Science,
3(1),
49.
Fagerberg, J., Srholec, M., & Verspagen, B. (2010). Chapter 20—innovation and economic development. In
B.
H.
Hall & N.
Rosenberg (Eds.),
Handbook of the economics of innovation (Vol.
2,
pp.
833–872).
Amsterdam: North-Holland.
Gla
¨
nzel,
W.,
& Thijs,
B.
(2004).
The influence of author self-citations on bibliometric macro indicators.
Scientometrics,
59(3),
281–310.
Hall,
D.,
Jurafsky,
D.,
& Manning,
C.
D.
(2008).
Studying the history of ideas using topic models.
In
Proceedings of the conference on empirical methods in natural language processing, EMNLP ’08 (pp.
363–371). Stroudsburg, PA: Association for Computational Linguistics.
Hirsch,
J.
E.
(2005).
An index to quantify an individual’s scientific research output.
Proceedings of
the
National academy of Sciences of the United States of America,
102(46),
16569–16572.
Hu, D. J. (2009). Latent dirichlet allocation for text, images, and music. http://cseweb.ucsd.edu/*dhu/docs/
research_exam09.pdf.
Last checked on January 16,
2017.
Hudson, J. (2007). Be known by the company you keep: Citations–quality or chance? Scientometrics, 71(2),
231–238.
Hyland,
K.
(2003).
Self-citation and self-reference:
Credibility and promotion in academic publication.
Journal of the American Society for Information Science and Technology,
54(3),
251–259.
Knorr-Cetina,
K.
(1981).
The manufacture of
knowledge.
An essay on the constructivist
and contextual
nature of science.
Oxford: Pergamon Press.
Kulkarni,
A.
V.,
Aziz,
B.,
Shams,
I.,
& Busse,
J.
W.
(2011).
Author self-citation in the general medicine
literature.
PLoS ONE,
6(6),
e20885.
Lawani,
S.
M.
(1982).
On the heterogeneity and classification of
author
self-citations.
Journal
of
the
American Society for Information Science,
33(5),
281.
MacRoberts, M. H., & MacRoberts, B. R. (1989). Problems of citation analysis: A critical review. Journal of
the American Society for Information Science,
40(5),
342–349.
Maliniak,
D.,
Powers,
R.,
& Walter,
B.
F.
(2013).
The gender
citation gap in international
relations.
International Organization,
67(4),
889–922.
Merton, R. K. (1973). Sociology of science: Theoretical and empirical investigations. Chicago: University
of Chicago Press.
Motamed,
M.,
Mehta,
D.,
Basavaraj,
S.,
& Fuad,
F.
(2002).
Self citations and impact factors in otolaryn-
gology journals. Clinical Otolaryngology & Allied Sciences,
27(5),
318–320.
Park,
H.
W.,
Hong,
H.
D.,
& Leydesdorff,
L.
(2005).
A comparison of the knowledge-based innovation
systems in the economies of South Korea and the Netherlands using Triple Helix indicators.
Scien-
tometrics,
65(1),
3–27.
Scientometrics (2017) 111:1801–1812
1811
123
Public Library of Science.
(2015).
Plos subject
area thesaurus.
https://github.com/PLOS/plos-thesaurus.
Last checked on January 16,
2017.
R
ˇ
ehu
˚
r
ˇ
ek,
R.,
& Sojka,
P.,
(2010).
Software framework for topic modelling with large corpora.
In Pro-
ceedings of the LREC 2010 workshop on new challenges for NLP frameworks (pp.
45–50).
Valletta:
ELRA.
Rogers,
E.
M.
(2003).
Diffusion of innovations (5th ed.).
New York,
NY: Free Press.
Schreiber,
M.
(2007). Self-citation corrections for the Hirsch index. Europhysics Letters,
78(3),
30002.
Seglen,
P.
O.
(1992).
The skewness of science.
Journal of the American Society for Information Science,
43(9),
628–638.
Snyder,
H.,
& Bonzi,
S.
(1998).
Patterns of self-citation across disciplines (1980–1989). Journal of Infor-
mation Science,
24(6),
431–435.
Loria,
S.,
Keen,
P.,
Honnibal,
M.,
Yankovsky,
R.,
Karesh,
D.,
Dempsey,
E.,
et
al.
(2013).
TextBlob:
Simplified text processing.
https://textblob.readthedocs.io/en/dev/.
Last checked on January 16,
2017.
Tagliacozzo,
R.
(1977).
Self-citations in scientific literature. Journal of Documentation,
33(4),
251–265.
Wojick, D. E., Warnick, W. L., Carroll, B. C., & Crowe, J. (2006). The digital road to scientific knowledge
diffusion.
D-Lib Magazine,
12(6),
1082–9873.
Yu,
G.,
Wang,
M.-Y.,
& Yu,
D.-R.
(2010).
Characterizing knowledge diffusion of nanoscience & nan-
otechnology by citation analysis.
Scientometrics,
84(1),
81–97.
1812
Scientometrics (2017) 111:1801–1812
123
