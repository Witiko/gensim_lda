Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 685–693,
Taipei, Taiwan, November 27 – December 1, 2017
c
2017 AFNLP
Bilingual Word Embeddings for Bilingual Terminology Extraction from
Specialized Comparable Corpora
Amir Hazem
1
Emmanuel Morin
1
1
LS2N - UMR CNRS 6004, Universit
´
e de Nantes, France
{
amir.hazem,emmanuel.morin
}
@univ-nantes.fr
Abstract
Bilingual lexicon extraction from compa-
rable corpora is constrained by the small
amount
of
available data when dealing
with specialized domains.
This aspect pe-
nalizes the performance of distributional-
based approaches,
which is
closely re-
lated to the reliability of word’s cooccur-
rence counts extracted from comparable
corpora. A solution to avoid this limitation
is to associate external resources with the
comparable corpus.
Since bilingual word
embeddings have recently shown efficient
models for
learning bilingual
distributed
representation of words,
we explore dif-
ferent word embedding models and show
how a general-domain comparable corpus
can enrich a specialized comparable cor-
pus via neural networks.
1
Introduction
Bilingual lexicon extraction from comparable cor-
pora has shown substantial growth since the sem-
inal work of Fung (1995) and Rapp (1995).
Com-
parable corpora,
which are comprised of
texts
sharing common features such as domain,
genre,
sampling period, etc.
and without having a source
text/target
text
relationship (McEnery and Xiao,
2007),
are more abundant
and reliable resources
than parallel
corpora.
On the one hand,
parallel
corpora are difficult
to obtain for language pairs
not involving English.
On the other hand, as par-
allel
corpora are comprised of
a pair
of
trans-
lated texts, the vocabulary appearing in the trans-
lated texts is highly influenced by the source texts.
These problems are aggravated in specialized and
technical domains.
Although it
is
easier
to build large general-
domain comparable corpora,
specialized compa-
rable corpora are often of
modest
size (around
1 million words)
due to the difficulty to obtain
many specialized documents in a language other
than English.
Consequently,
word co-occurrence
counts of the historical
context-based projection
approach, known as the standard approach (Fung,
1995;
Rapp,
1995),
dedicated to bilingual
lexi-
con extraction from comparable corpora are un-
reliable in specialized domain.
This problem per-
sists with other paradigms such as Canonical Cor-
relation Analysis (CCA) (Gaussier et
al.,
2004),
Independent Component Analysis (ICA) (Hazem
and Morin,
2012) and Bilingual
Latent
Dirichlet
Allocation (BiLDA) (Vuli
´
c et al., 2011).
A solution to avoid this limitation and to in-
crease the representativity of distributional repre-
sentations is to associate external
resources with
the specialized comparable corpus.
These re-
sources can be lexical
databases such as Word-
Net
which allows the disambiguation of
trans-
lations
of
polysemous
words
(Bouamor
et
al.,
2013) or general-domain data to improve word co-
occurrence counts of specialized comparable cor-
pora (Hazem and Morin, 2016).
Our work is in this line and attempts to find out
how a general-domain data can enrich a special-
ized comparable corpora to improve bilingual ter-
minology extraction from specialized comparable
corpora.
Since bilingual
word embeddings have
recently provided efficient
models
for
learning
bilingual distributed representation of words from
large general-domain data (Mikolov et al., 2013),
we contrast
different
popular
word embedding
models for this task. In addition, we explore com-
binations of word embedding models as suggested
by Garten et al. (2015) to improve distributed rep-
resentations.
We compare the results obtained
with the state-of-the-art
context-based projection
approach.
Our results show under which condi-
tions the proposed model can compete with state-
685
of-the art approaches.
To the best of our knowl-
edge,
this is the first
time that
word embedding
models have been used to extract
bilingual
lexi-
cons from specialized comparable corpora.
The remainder of this paper is organized as fol-
lows.
Section 2 presents the two state-of-the-
art
approaches used to extract
bilingual
lexicons
from comparable corpora.
Section 3 describes
the two data combination approaches adapted to
Skip-gram and CBOW models (Mikolov et
al.,
2013).
Section 4 describes the different
linguis-
tic resources used in our experiments.
Section 5
is then devoted to a large-scale evaluation of the
different
proposed methods.
Finally,
Section 6
presents our conclusion.
2
State-of-the-Art Approaches
In this section, we describe the two state-of-the-art
approaches used to extract bilingual lexicons from
comparable corpora.
These approaches are both based on monolin-
gual lexical context analysis and relies on the dis-
tributional hypothesis (Harris, 1968) which postu-
lates that a word and its translation tend to appear
in the same lexical contexts. This is the hypothesis
that tends to be reduced to the famous sentence of
the British linguist J.
R.
Firth (1957,
p.
11) who
said:
“You shall know a word by the company it
keeps.”
even if the context was related to collo-
cates.
The two approaches are known as distributional
and distributed semantics (according to Hermann
and Blunsom (2014)).
The first
one is based on
vector space models while the second one is based
on neural language models.
2.1
Context-Based Projection Approach
The historical context-based projection approach,
known as the standard approach, has been studied
by a number of researchers (Fung,
1998;
Rapp,
1999;
Chiao and Zweigenbaum,
2002;
Morin
et al., 2007; Prochasson and Fung, 2011; Bouamor
et al., 2013; Morin and Hazem, 2016, among oth-
ers).
Its implementation can be carried out by ap-
plying the following steps:
1.
For each word
w
of the source and the target
languages, we build a context vector (resp.
s
and
t
for source and target languages) con-
sisting in the measure of association of each
word that appears in a short window of words
around
w
.
The association measures tradi-
tionally studied are Mutual Information, Log-
likelihood, and the Discounted Odds-Ratio.
2.
For a word
i
to be translated, its context vec-
tor
i
is projected from the source to the tar-
get language by translating each element of
its context vector thanks to a bilingual seed
lexicon.
3.
The translated context vector
i
is compared to
each context vector
t
of the target language
using a similarity measure such as Cosine or
weighted Jaccard.
The candidate translations
are then ranked according to the scores of a
given similarity measure.
This approach is very sensitive to the choice of
parameters.
We invite readers to consult the study
of Laroche and Langlais (2010) in which the influ-
ence of parameters such as the size of the context,
the choice of the association and similarity mea-
sures have been examined.
In order to improve the quality of bilingual ter-
minology extraction from specialized compara-
ble corpora,
Hazem and Morin (2016) have pro-
posed two ways to combine specialized compara-
ble corpora with external resources.
The hypoth-
esis is that
word co-occurrences learned from a
large general-domain corpus for general words im-
prove the characterisation of the specific vocabu-
lary of the specialized corpus.
The first
adapta-
tion called Global
Standard Approach (GSA) is
basic and consists in building the context
vec-
tors from a comparable corpus composed of the
specialized and the general
comparable corpora.
The second adaptation called Selective Standard
Approach (SSA) is more sophisticated and com-
prises:
i) independently building context
vectors
of the specialized and general comparable corpus
and then ii) merging under certain conditions, spe-
cialized and general context vectors of the words
belonging to the specialized corpus when they ap-
pear in the general corpus.
2.2
Word Embedding Based Approach
Bilingual word embeddings has become a source
of great
interest
in recent
times (Mikolov et
al.,
2013;
Vuli
´
c and Moens,
2013;
Zou et
al.,
2013;
Chandar et al., 2014; Gouws et al., 2014; Artetxe
et al., 2016, among others).
Mikolov et al. (2013)
was the first
to propose a method to learn a lin-
ear transformation from the source language to the
686
target language to improve the task of lexicon ex-
traction from bilingual corpora.
During the training time of Mikolov’s method,
for all
{x
i
, z
i
}
n
i=1
bilingual word pairs of the seed
lexicon,
the word embedding
x
i
∈ R
d
1
of word
i
in the source language and the word embedding
z
i
∈ R
d
2
of its translation in the target language
are computed. A transformation matrix
W
such as
Wx
i
approximates
z
i
is then learned by the objec-
tive function as follows:
min
W
n
X
i=1
kW x
i
− z
i
k
2
(1)
At
prediction time,
we can transfer the word
embedding
x
for a word to be translated in the tar-
get language using the translation matrix such as
z = W x
.
The candidate translations are obtained
by ranking the closest target words to
z
according
to a similarity measure such as the Cosine mea-
sure.
Recently,
Artetxe et
al.
(2016)
presented an
approach
for
learning
bilingual
mappings
of
word embeddings that preserves monolingual in-
variance using several
meaningful
and intuitive
constraints
related to other
proposed methods
(Faruqui and Dyer, 2014; Xing et al., 2015). These
constraints are orthogonality,
vectors length nor-
malization for maximum cosine and mean center-
ing for
maximum covariance.
Monolingual
in-
variance tends to preserve the dot
products af-
ter mapping,
in order to avoid performance drop
in monolingual tasks, while dimension-wise mean
centering tends to insure that two randomly taken
words would not be semantically related. This ap-
proach has shown meaningful
improvements for
both monolingual and bilingual tasks.
Other work has focused on learning bilingual
word representations without word-to-word align-
ments
of
comparable corpora.
Chandar
et
al.
(2014) and Gouws et al.
(2014) use multilingual
word embeddings based on sentence-aligned par-
allel data whereas Vuli
´
c and Moens (2015, 2016)
propose a model to induce bilingual word embed-
dings directly from document-aligned non-parallel
data.
Theses works are based on sentence-
or
document-aligned of general-domain comparable
corpora and are outside the scope of this study.
It
is unlikely, not to say impossible, to find this type
of alignment in a specialized comparable corpus.
3
Data Combination Using Neural
Networks
Recently,
Hazem and Morin (2016) have shown
that
using external
data drastically improves the
performance of the traditional distributional-based
approach for
the task of
bilingual
lexicon ex-
traction from specialized
comparable
corpora.
Mikolov et
al.
(2013) have also shown that
dis-
tributed vector representations over large corpora
in a continuous space model capture many linguis-
tic regularities and key aspects of words.
Based
on these findings, we pursue the preceding works
and propose different
ways to combine special-
ized and general domain data using neural network
models.
We adapt the two data combination ap-
proaches proposed in Hazem and Morin (2016)
(see Section 2.1)
using Skip-gram and CBOW
models (Mikolov et
al.,
2013).
Inspired by the
work of Garten et
al.
(2015) which studied dif-
ferent combinations of distributed vector represen-
tations for
word analogy task,
we also propose
different Skip-gram and CBOW models combina-
tions over specialized and general domain data.
3.1
Global Data Combination Using Neural
Network Models
This approach can be seen as similar to the GSA
approach (Hazem and Morin,
2016),
the differ-
ence is that instead of using the context-based pro-
jection approach to build context vectors,
we use
the distributed Skip-gram or Continuous Bag-of-
Words (CBOW) models proposed in Mikolov et al.
(2013).
Given a specialized and a general domain
corpus, we create a new corpus which is the com-
bination of both. We then learn a Skip-gram model
(respectively a CBOW model) using this new gen-
erated corpus.
We denote this approach by GSG
for the global
1
Skip-gram model and GCBOW for
the global CBOW model.
After combining the two corpora,
the steps for
extracting bilingual lexicons are as follows:
1.
We first build a CBOW (respectively a Skip-
gram) model for source and target languages.
2.
Then,
we apply bilingual
mapping (Artetxe
et al., 2016) between the source and the target
CBOW models (respectively the Skip-gram
models).
The mapping step needs a bilin-
gual dictionary to compute the mapping ma-
1
The term global refers to a global combination of data
without any specific criterion.
687
trix. We used a dictionary subset of the 5,000
more frequent
translation pairs.
Different
sizes of the seed dictionary have been stud-
ied and discussed in Jakubina and Langlais
(2016).
It
should be noted that
in our ex-
periments, no great impact has been observed
when varying the size of the dictionary sub-
set.
3.
For each word to be translated,
we compute
a Cosine similarity between its mapped em-
bedding vector and the embedding vectors of
all the target words.
4.
Finally,
we rank the candidates according to
their similarity score.
3.2
Specific Data Combination Using Neural
Network Models
This approach is in the line of the SSA (Hazem and
Morin, 2016) approach but the idea is not exactly
the same.
Similarly to them we build two sepa-
rate representations.
One learned from a special-
ized domain corpus and the second learned from
a general-domain corpus, but unlike them we con-
catenate the distributed models while they merge
2
distributional context vectors.
Our goal is to capture the two word characteri-
sations thanks to CBOW/Skipgram models.
One
is
issued from the specialized domain and the
other one from the general domain, to finally com-
bine both representations in the perspective of ob-
taining a better word representation. Our approach
is as follows:
1.
We first build a CBOW (respectively a Skip-
gram) model for both specialized and general
domain source and target languages.
2.
Then, we concatenate source CBOW vectors
(respectively Skip-gram vectors) of the spe-
cialized and the general
domain data.
We
apply the same process for specialized and
general-domain target data.
3.
We apply bilingual mapping (Artetxe et al.,
2016) between the source and target concate-
nated vectors.
4.
For each word to be translated,
we compute
a Cosine similarity between its mapped em-
bedding vector and the embedding vectors of
all the target words.
2
The merging process can be seen as a simple vector ad-
dition.
5.
Finally,
we rank the candidates according to
their similarity score.
3.3
Combining Distributed Representations
We follow the findings of
Garten et
al.
(2015)
where they have shown substantial improvements
on a standard word analogy task,
combining dis-
tributed vector representations (more specifically,
vectors concatenation).
They compared their hy-
brid methods and have shown their advantages es-
pecially when training data is limited, which is the
main problem in the task of extracting bilingual
terminology from specialized comparable corpora.
In our case,
word embedding models lead to
three different
ways of concatenation.
The first
one is a CBOW model concatenation between the
specialized and the general domain data.
The sec-
ond one is a Skip-gram model concatenation and
the third one is a concatenation of both CBOW and
Skip-gram models.
If for instance we have a 100 dimension spe-
cialized CBOW model and a 200 dimension gen-
eral
domain CBOW model.
The concatenation
will
lead to a resulting 300 dimension CBOW
model.
If
we also have a 100 dimension spe-
cialized Skip-gram model
and a 200 dimension
general
domain Skip-gram model.
The concate-
nation will
lead to a resulting 300 dimension
Skip-gram model.
Finally,
if we concatenate the
CBOW and Skip-gram models,
this will
result
in a 600 dimension combined model.
This fi-
nal
concatenation process allows to take advan-
tage of both CBOW and Skip-gram models and to
learn a mapping matrix of the combined models.
To our knowledge this is the first attempt to first
encode CBOW
_
CBOW
3
, Skip-gram
_
Skip-gram
and CBOW
_
Skip-gram models before learning a
bilingual mapping matrix.
4
Data and Resources
In this section, we briefly outline the different tex-
tual resources used for our experiments: the com-
parable corpora,
the bilingual
dictionary and the
terminology reference list. These textual resources
are a subset of the data used in Hazem and Morin
(2016).
3
CBOW
_
CBOW stands for the concatenation of two
CBOW vectors issued from two different training datasets.
688
4.1
Comparable corpora
The specialized comparable corpus consists of sci-
entific papers collected from the Elsevier website
4
.
The scientific papers were taken from the medi-
cal domain within the sub-domain of “breast can-
cer”.
The breast cancer comparable corpus (BC)
is composed of 103 English documents and 130
French documents.
The four
general-domain comparable corpora
are of different types and sizes often used in mul-
tiple evaluation campaigns such as WMT.
News
commentary corpus consists of political and eco-
nomic commentary crawled from the web (NC),
Europarl
corpus
is
a parallel
corpus
extracted
from the proceedings of the European Parliament
(EP7),
JRC acquis corpus is a collection of leg-
islative European Union documents (JRC)
and
Common Crawl corpus (CC) which encompasses
over petabytes of web crawled data collected over
seven years. It should be noted that we do not take
advantage of the parallel information encoded in
the parallel corpora.
Table 1 shows the number of content words (#
content words) for each corpus.
Comparable corpus
# content words
FR
EN
BC
8,221
79.07
NC
5.7M
4.7M
EP7
61.8M
55.7M
JRC
70.3M
64.2M
CC
91.3M
81.1M
Table 1: Characteristics of the specialized compa-
rable corpus and the external data.
The documents were pre-processed through ba-
sic linguistic steps such as tokenization,
part-of-
speech tagging and lemmatization using the TTC
TermSuite
5
tool that applies the same method to
several
languages including English and French.
Finally,
the function words were removed thanks
to a stopword list and the hapax
6
were discarded.
4.2
Bilingual Dictionary
The bilingual dictionary is the French/English dic-
tionary ELRA-M0033
7
(243,539 entries). This re-
4
www.elsevier.com
5
code.google.com/p/ttc-project
6
Tokens that appear only once in the corpus.
7
catalog.elra.info/product_info.php?
products_id=666
source is a general language dictionary which con-
tains only a few terms related to the medical do-
main.
4.3
Gold Standard
The bilingual terminology reference list required
to evaluate the quality of
bilingual
terminology
extraction from comparable corpora has been de-
rived from the UMLS
8
meta-thesaurus.
The ter-
minology reference list
is composed of 248 sin-
gle word pairs for
which each word appears at
least 5 times in the comparable corpus.
This list
is of
a standard size compared to other
works
such as Chiao and Zweigenbaum (2002): 95 single
words, Morin et al. (2007):
100 single words and
Bouamor et al. (2013): 125 and 79 single words.
5
Experiments and Results
The first
piece of
work comparing methods for
identifying translation pairs in comparable cor-
pora
was
presented in Jakubina
and Langlais
(2016).
However the evaluation was conducted
on Wikipedia,
which is a general domain corpus.
In our case,
we are interested in specialized do-
mains where there is a lack of specialized data.
Our experiments aim at
exploring word embed-
dings performance in specialized comparable cor-
pora, which is to our knowledge, the first attempt
at tackling this problem.
Moreover,
and as it has
been pointed out in Mikolov et al. (2013), applica-
tions to low resource domains is a very interesting
topic where there is still much to be explored.
In this section, we compare different word em-
bedding representations for the extraction of bilin-
gual terms from specialized comparable corpora.
We contrast Skip-gram and CBOW models as well
as different ways of combining them over special-
ized and general domain corpora.
5.1
Word2vec
For word2vec, we used as settings a window size
of 10,
negative sampling of 5,
sampling of 1e-3
and training over 15 iterations.
We applied both
Skip-gram and CBOW models
9
to create vectors
of 100 dimensions.
We used hierarchical softmax
for training the Skip-gram model.
Other settings
were assessed but on average the chosen ones tend
to give the best results on our data.
8
www.nlm.nih.gov/research/umls
9
To train word embedding models we used the gensim
toolkit (Rehurek and Sojka, 2010)
689
Corpus
CBOW
SG
Concat
BC
17.1
12.8
20.8
NC
33.9
31.2
33.6
EP7
42.3
40.8
43.1
JRC
40.3
40.5
43.4
CC
60.9
56.0
61.0
BC
∪
NC
42.9
37.7
46.3
BC
∪
EP7
47.2
49.0
53.3
BC
∪
JRC
49.9
46.5
53.0
BC
∪
CC
67.7
63.2
68.4
BC
_
(
BC
∪
NC
)
45.5
30.7
48.1
BC
_
(
BC
∪
EP7
)
51.6
35.7
53.8
BC
_
(
BC
∪
JRC
)
53.7
36.3
56.1
BC
_
(
BC
∪
CC
)
70.7
40.2
70.9
Table 2: Results (MAP %) of word2vec using the Skip-gram model (noted SG), the Continuous Bag of
Words model (noted CBOW) and the concatenation of both models (noted Concat).
The window size
was set to 10 and the vector size to 100.
5.2
Bilingual Mappings of Word Embeddings
For mapping words of the source language to the
target
language we follow the method presented
in Artetxe et
al.
(2016) where they presented an
efficient
exact
method to learn the optimal
lin-
ear
transformation that
gives the best
results in
translation induction.
While we contrasted dif-
ferent configurations of there framework, we only
present the best results.
We used the orthogonal
mapping with length normalization and mean cen-
tering of vectors
10
.
5.3
Results
Table 2 shows the results of word2vec according
to different configurations.
The first column rep-
resents the corpora that
have been used to train
word2vec models.
The first bloc lines compares
the performance of word2vec models on the spe-
cialized breast
cancer corpus (BC) and the four
external
data that
are:
news commentary (NC),
EuroParl (EP7),
JRC acquis (JRC) and common
crawl (CC).
The second bloc lines compares the
models trained on the combination of
BC with
each external
corpus.
For
instance BC
∪
EP7
consists of training word2vec on the combination
of
BC and EP7.
Finally,
the third bloc lines
shows the concatenation of two models,
the first
one trained on the specialized corpus (BC) and the
second one trained on the combination of BC with
a given external corpus (BC
∪
EP7 for instance).
10
These parameters have also shown the best
results in
Artetxe et al. (2016).
This is noted by BC
_
(
BC
∪
EP7
)
for the concate-
nation (represented by
_
) of vectors of BC with
vectors of BC
∪
EP7. The first column of the sec-
ond and third blocs shows CBOW concatenation
while the second shows Skip-gram concatenation.
The third column shows CBOW and Skip-gram
concatenation.
Concatenate BC with BC
∪
EP7
instead of BC with EP7 (noted BC
_
EP7) for in-
stance, insures the presence of all specialized do-
main words in both models.
The first observation that can be seen from Ta-
ble 2 is that
the results are very low when us-
ing the specialized BC corpus only (the maximum
obtained MAP is 20.8% for the Concat
model).
The second observation is that using external data
only gives better results.
The best obtained MAP
score is 61.0% when using CC corpus.
This is
certainly due to the presence of
medical
terms
in the general
domain corpora which have been
crawled from the web and can contain scientific
pages.
Over the first bloc lines results, we can see
that CBOW model outperforms Skip-gram model
in most cases which is not surprising in the sense
that CBOW aims at characterising a word accord-
ing to its context while Skip-gram characterises a
context according to a given word. From this point
CBOW is more appropriate for our task. However,
combining both CBOW and Skip-gram as shown
by the Concat model,
always improves the MAP
scores. This is an important finding that shows that
both models are complementary.
According to the second bloc lines results,
we
690
Word2vec + Mapping
BC
concat
CC
concat
U nconstrained + Original
11.4
48.0
Orthogonal + Original
19.5
69.9
U nconstrained + U nit
11.8
50.4
Orthogonal + U nit
19.0
70.1
U nconstrained + U nit + Center
12.3
54.0
Orthogonal + U nit + Center
20.8
70.9
Table 3: Results (MAP %) of word2vec using different mapping techniques.
BC
NC
EP7
JRC
CC
SA
27.0
45.3
48.5
52.0
75.5
GSA
-
58.9
58.3
61.7
80.2
SSA
-
58.9
60.8
66.6
82.3
GCBOW
17.1
42.9
47.2
49.9
67.7
GSG
12.8
37.7
49.2
46.5
63.2
GCBOW + GSG
20.8
46.3
53.3
53.0
68.4
SCBOW
-
45.5
51.6
53.7
70.7
SSG
-
30.7
35.7
36.3
40.2
SCBOW + SSG
-
48.1
53.8
56.1
70.9
Table 4:
Results (MAP %) of the Standard Approach (
SA
), the Global Standard Approach (
GSA
) and
the Selective Standard Approach (
SSA
) for the breast cancer corpus (BC) using the different external
data (the improvements indicate a significance at the 0.001 level using the Student t-test).
observe that
combining the specialized bilingual
corpus (BC) with external data always improves
the results.
This can be noticed for the BC
∪
CC
corpus
where we increase the MAP score for
CBOW from 60.9% to 67.7%,
the MAP score
for Skip-gram from 56% to 63.2% and the MAP
score for the Concat approach from 61% to 68.4%.
These are also interesting results which coincide
with the observations of Hazem and Morin (2016).
Hence,
combining specific and general
domain
corpora before applying any model always bene-
fits the task of bilingual terminology extraction.
Finally,
for
the
third bloc
lines
where
we
combine the model
issued from the first
bloc
(the BC model) and models issued from the sec-
ond bloc lines (BC
∪
EP7 for instance), two obser-
vations need to be pointed out. The first one is that
for CBOW model
concatenation we still
get
im-
provements as we can see for CC where we gain
3 points (from 67.7% to 70.7% of MAP score).
The second surprising observation is that
Skip-
gram concatenation decreases the results.
This
drop may suggest
that
Skip-gram concatenation
is not
appropriate for
this configuration.
How-
ever the concatenation of CBOW and Skip-gram
models (the Concat
approach) still
improves the
results as we can see for BC
_
(
BC
∪
JRC
)
where
we move from 53% to 56.1% of MAP score and
for BC
_
(
BC
∪
CC
)
where we increase the MAP
score from 68.4% to 70.9%.
For this last
result
the gain is not
that
important
compared to the
CBOW
_
CBOW model that obtains a MAP score
of 70.7%.
In Table 3 we report
a comparison of
differ-
ent mappings as studied in Artetxe et al.
(2016).
We contrast
unconstrained and orthogonal
map-
pings using original
as well
as length normal-
ization (noted
unit
)
and mean centering (noted
Center
). As we can see, the best results are those
obtained using orthogonal
mapping with length
normalization and mean centering.
We reach the
same conclusions as Artetxe et al. (2016).
In Table 4 we present the results from Hazem
and Morin (2016) of the standard approach (noted
SA
) using only specialized comparable corpora
(BC) or using only external data (NC, EP7, JRC
and CC), in addition to the two adapted standard
approaches (noted
GSA
and
SSA
) using the com-
bination of
each specialized comparable corpus
with each corpus of the external
data.
We also
691
report
the best
results of each of the three word
embedding methods that we introduced earlier in
Table 2.
Even if we obtained improvements over our dif-
ferent
embedding models,
we are still
below the
standard approach as seen in Table 4.
The lack of
specialized data may partially explain these lower
results as well as the multiple tuning parameters of
CBOW and Skip-gram models.
6
Conclusion
In this paper,
we have proposed and contrasted
different data combinations using neural networks
for bilingual terminology extraction from special-
ized comparable corpora.
We have shown un-
der which conditions external resources as well as
Skip-gram and CBOW models can be jointly used
to improve the performance of bilingual terms ex-
traction.
If the results are encouraging,
we were
unable to compete with the results of the histori-
cal context-based projection approach.
However,
our findings may suggest a starting point of apply-
ing word embeddings and the multiple proposed
variants to specialized domains as well as to other
tasks. We hope this work will help to lead the way
in exploring low resource domains such as special-
ized comparable corpora.
Acknowledgments
The
research leading to these
results
has
re-
ceived funding from the
French National
Re-
search Agency under grant
ANR-17-CE23-0001
ADDICTE (Distributional analysis in specialized
domain).
References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings
of
the 2016 Conference on Em-
pirical
Methods in Natural
Language Processing
(EMNLP’16), pages 2289–2294, Austin, TX, USA.
Dhouha
Bouamor,
Nasredine
Semmar,
and Pierre
Zweigenbaum.
2013.
Context
vector disambigua-
tion for
bilingual
lexicon extraction from compa-
rable corpora.
In Proceedings of
the 51st
Annual
Meeting of the Association for Computational Lin-
guistics (ACL’13), pages 759–764, Sofia, Bulgaria.
A.
P.
Sarath
Chandar,
Stanislas
Lauly,
Hugo
Larochelle,
Mitesh M.
Khapra,
Balaraman Ravin-
dran, Vikas C. Raykar, and Amrita Saha. 2014.
An
autoencoder
approach to learning bilingual
word
representations.
In Proceedings
of
the
Annual
Conference
on
Neural
Information
Processing
Systems
(NIPS’14),
pages
1853–1861,
Montreal,
Quebec, Canada.
Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.
Looking for
candidate translational
equivalents in
specialized, comparable corpora.
In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING’02), pages 1208–1212, Tapei,
Taiwan.
Manaal Faruqui and Chris Dyer. 2014.
Improving vec-
tor
space word representations using multilingual
correlation.
In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational
Linguistics (EACL’14),
pages 462–471,
Gothenburg, Sweden.
John Rupert Firth. 1957.
A synopsis of linguistic the-
ory 1930–1955.
In Studies in Linguistic Analysis
(special volume of the Philological Society),
pages
1–32. Blackwell, Oxford.
Pascale Fung.
1995.
Compiling bilingual lexicon en-
tries from a non-parallel english-chinese corpus.
In
Proceedings of
the 3rd Annual
Workshop on Very
Large Corpora (VLC’95),
pages
173–183,
Cam-
bridge, MA, USA.
Pascale Fung.
1998.
A statistical
view on bilingual
lexicon extraction:
From parallel
corpora to non-
parallel
corpora.
In Proceedings of
the 3rd Con-
ference of the Association for Machine Translation
in the Americas on Machine Translation and the In-
formation Soup (AMTA’98), pages 1–17, Langhorne,
PA, USA.
Justin Garten, Kenji Sagae, Volkan Ustun, and Morteza
Dehghani. 2015.
Combining distributed vector rep-
resentations for words.
In Proceedings of
the 1st
Workshop on Vector Space Modeling for Natural
Language Processing,
pages 95–101,
Denver,
CO,
USA.
Eric. Gaussier, Jean-Michel Renders, Irena. Matveeva,
Cyril. Goutte, and Herv
´
e D
´
ejean. 2004.
A geomet-
ric view on bilingual lexicon extraction from com-
parable corpora.
In Proceedings of
the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL’04),
pages 526–533,
Barcelona,
Spain.
Stephan Gouws,
Yoshua Bengio,
and Greg Corrado.
2014.
Bilbowa:
Fast
bilingual
distributed rep-
resentations
without
word
alignments.
CoRR,
abs/1410.2455.
Z.
S.
Harris.
1968.
Mathematical
Structures of
Lan-
guage.
Interscience Publishers.
Amir
Hazem and Emmanuel
Morin.
2012.
Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora.
In Proceedings of
the Eight
International
Conference on Language Resources
and Evaluation (LREC’12),
pages 288–292,
Istan-
bul, Turkey.
692
Amir Hazem and Emmanuel
Morin.
2016.
Efficient
data selection for bilingual
terminology extraction
from comparable corpora.
In Proceedings of
the
26th International
Conference on Computational
Linguistics (COLING’16),
pages 3401–3411,
Os-
aka, Japan.
Karl Moritz Hermann and Phil Blunsom. 2014.
Mul-
tilingual
models for
compositional
distributed se-
mantics.
In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’14), pages 58–68, Baltimore, Maryland.
Laurent
Jakubina and Phillippe Langlais.
2016.
A
comparison of methods for identifying the transla-
tion of words in a comparable corpus:
Recipes and
limits.
Computaci
´
on y Sistemas, 20(3):449–458.
Audrey Laroche and Philippe Langlais.
2010.
Revis-
iting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora.
In
Proceedings of
the 23rd International
Conference
on Computational Linguistics (COLING’10), pages
617–625, Beijing, China.
Anthony McEnery and Zhonghua Xiao.
2007.
Paral-
lel and comparable corpora:
What are they up to?
In Gunilla Anderman and Margaret Rogers, editors,
Incorporating Corpora:
Translation and the Lin-
guist, Multilingual Matters, chapter 2, pages 18–31.
Clevedon, UK.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities
among languages
for
ma-
chine translation.
CoRR, abs/1309.4168.
Emmanuel
Morin,
B
´
eatrice Daille,
Koichi
Takeuchi,
and Kyo Kageura.
2007.
Bilingual
Terminology
Mining – Using Brain,
not brawn comparable cor-
pora.
In Proceedings of
the 45th Annual
Meet-
ing of the Association for Computational Linguistics
(ACL’07), pages 664–671, Prague, Czech Republic.
Emmanuel
Morin and Amir Hazem.
2016.
Exploit-
ing unbalanced specialized comparable corpora for
bilingual lexicon extraction.
Natural Language En-
gineering, 22(4):575–601.
Emmanuel Prochasson and Pascale Fung. 2011.
Rare
Word Translation Extraction from Aligned Compa-
rable Documents.
In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL’11), pages 1327–1335, Portland, OR,
USA.
Reinhard Rapp.
1995.
Identify Word Translations in
Non-Parallel Texts.
In Proceedings of the 35th An-
nual Meeting of the Association for Computational
Linguistics (ACL’95), pages 320–322, Boston, MA,
USA.
Reinhard Rapp.
1999.
Automatic Identification of
Word Translations from Unrelated English and Ger-
man Corpora.
In Proceedings of
the 37th Annual
Meeting of the Association for Computational Lin-
guistics (ACL’99),
pages 519–526,
College Park,
MD, USA.
Radim Rehurek and Petr Sojka. 2010.
Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of
the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA.
Ivan Vuli
´
c, Wim De Smet, and Marie-Francine Moens.
2011.
Identifying word translations from compara-
ble corpora using latent topic models.
In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics (ACL’11), pages 479–
484, Portland, OR, USA.
Ivan Vuli
´
c and Marie-Francine Moens. 2013.
A study
on bootstrapping bilingual vector spaces from non-
parallel data (and nothing else).
In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP’13), pages 1613–
1624, Seattle, WA, USA.
Ivan Vuli
´
c and Marie-Francine Moens.
2015.
Bilin-
gual word embeddings from non-parallel document-
aligned data applied to bilingual
lexicon induc-
tion.
In Proceedings of
the 53rd Annual
Meet-
ing of the Association for Computational Linguistics
(ACL’15), pages 719–725, Beijing, China.
Ivan Vuli
´
c and Marie-Francine Moens. 2016.
Bilingual
distributed word representations
from document-
aligned comparable data.
Journal
of
Artificial
In-
telligence Research (JAIR), 55(1):953–994.
Chao Xing,
Dong Wang,
Chao Liu,
and Yiye Lin.
2015.
Normalized word embedding and orthog-
onal
transform for
bilingual
word translation.
In
Proceedings of
the 2015 Conference of
the North
American Chapter of the Association for Computa-
tional Linguistics:
Human Language Technologies
(NAACL’15), pages 1006–1011, Denver, CO, USA.
Will
Y.
Zou,
Richard Socher,
Daniel
M.
Cer,
and
Christopher
D.
Manning.
2013.
Bilingual
word
embeddings for
phrase-based machine translation.
In Proceedings
of
the 2013 Conference on Em-
pirical
Methods in Natural
Language Processing
(EMNLP’13), pages 1393–1398, Seattle, WA, USA.
693
