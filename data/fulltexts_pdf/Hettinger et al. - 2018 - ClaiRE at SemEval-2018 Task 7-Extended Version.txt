arXiv:1804.05825v3 [cs.CL] 15 May 2018
ClaiRE at SemEval-2018 Task 7 - Extended Version
Lena Hettinger, Alexander Dallmann, Albin Zehe, Thomas Niebler and Andreas Hotho
DMIR Group
University of Wuerzburg
{
hettinger,dallmann,zehe,niebler,hotho
}
@informatik.uni-wuerzburg.de
Abstract
In this paper we describe our post-evaluation
results
for
SemEval-2018 Task 7 on clas-
sification of
semantic relations
in scientific
literature for
clean (subtask 1.1)
and noisy
data (subtask 1.2).
This is an extended ver-
sion of our workshop paper (Hettinger et al.,
2018) including further technical details (Sec-
tions
3.2
and
4.3)
and
changes
made
to
the preprocessing step in the post-evaluation
phase (Section 2.1).
Due to these changes
Classification of Relations using Embeddings
(ClaiRE)
achieved an improved F1 score of
75
.
11 %
for the first subtask and
81
.
44 %
for
the second.
1
Introduction
The goal of SemEval-2018 Task 7 is to extract and
classify semantic relations between entities into
six categories that
are specific to scientific liter-
ature (G´
abor et
al.,
2018).
In this work,
we fo-
cus on the subtask of classifying relations between
entities in manually (subtask 1.1) and automati-
cally annotated and therefore noisy data (subtask
1.2).
Given a pair of related entities, the task is to
classify the type of their relation among the fol-
lowing options:
Compare,
Model-Feature,
Part Whole, Result, Topic or Usage.
Re-
lation types are explained in detail in the task de-
scription paper (G´
abor et al., 2018).
The follow-
ing sentence shows an example of a Result rela-
tion between the two entities combination meth-
ods and system performance:
Combination methods
are
an effec-
tive way of improving system perfor-
mance.
This sentence is a good example for two chal-
lenges we face in this task.
First,
almost half of
all entities consist of noun phrases which has to be
considered when constructing features.
Secondly,
the vocabulary is domain dependent and therefore
background knowledge should be adopted.
Previous approaches for semantic relation clas-
sification tasks mainly employed two strategies.
Either they made use of a lot of hand-crafted fea-
tures or they utilized a neural network with as few
background knowledge as possible.
The winning
system of an earlier SemEval challenge on relation
classification (Hendrickx et al., 2009) adopted the
first approach and achieved an F1 score of 82.2%
(Rink and Harabagiu,
2010).
Later,
other works
outperformed this approach by using CNNs with
and without
hand-crafted features (Santos et
al.,
2015; Xu et al., 2015) as well as RNNs (Miwa and
Bansal, 2016).
Approach
We
present
two
approaches
that
use different
levels
of
preliminary information.
Our
first
approach is
inspired by the winning
method of the SemEval-2010 challenge (Rink and
Harabagiu,
2010).
It
models semantic relations
by describing the two entities, between which the
semantic relation holds,
as well as the words be-
tween those entities.
We call
those in-between
words the context
of the semantic relation.
We
classify relations by using an SVM on lexical fea-
tures,
such as part-of-speech tags.
Additionally
we make use of semantic background knowledge
and add pre-trained word embeddings to the SVM,
as word embeddings have been shown to improve
performance in a series of
NLP tasks,
such as
sentiment analysis (Kim, 2014), question answer-
ing (Chen et al., 2017) or relation extraction (Dli-
gach et
al.,
2017).
Besides using existing word
embeddings generated from a general corpus,
we
also train embeddings on scientific articles that
better reflect scientific vocabulary.
In contrast, our second approach relies on word
embeddings
only,
which are fed into a convo-
lutional
long-short
term memory (C-LSTM) net-
work,
a model
that
combines convolutional
and
recurrent
neural
networks
(Zhou et
al.,
2015).
Therefore no hand-crafted features are used.
Be-
cause both CNN and RNN models have shown
good performance for this task,
we assume that a
combination of them will positively impact clas-
sification performance compared to the individual
models.
Results
By combining Lexical information and
domain-Adapted Scientific word Embeddings, our
system ClaiRE originally achieved an F1 score of
74.89% for
the first
subtask with manually an-
notated data and 78.39% for the second subtask
with automatically annotated data (Hettinger et al.,
2018).
Improving our
preprocessing lifted this
performance to 75.11% and 81.44% respectively.
Our results make a strong case for domain-specific
word embeddings,
as using those improved our
score by close to 5%.
Paper Structure
In Section 2,
we describe the
features that we used to characterize semantic re-
lations.
Section 3 shows how we classify the re-
lation using an SVM and a C-LSTM neural
net-
work. Section 4 presents the results, which are dis-
cussed in Section 5.
Finally,
Section 6 concludes
this work.
2
Features
In this section, we describe the features which are
used in our two approaches.
All sentences are first
preprocessed before constructing boolean lexical
features on the one hand and word embedding vec-
tors on the other. Both feature groups are based on
the entities of relations as well
as the context
in
which those entities appear.
Apart from the Compare relation,
all relation
types are asymmetric, and therefore the distinction
between start and end entity of a relation is impor-
tant. If entities appear in reverse order, that means
the end entity of a relation appears first in the sen-
tence, this is marked by a direction feature which
is part of the data set.
In our entrance example,
combination meth-
ods denotes the start entity, system performance
the end entity,
and are an effective way of im-
proving the context.
2.1
Preprocessing
Early experiments showed that
it is beneficial
to
filter the vocabulary of our data and reduce noise
by leaving out infrequent context words.
The best
setting was found to be a frequency threshold of
5
on lemmatized words.
Therefore we discard a
context word if its lemma appears less than
5
times
in the corpus of the respective subtask.
Post-Evaluation changes
Lemmas as well
as
POS tags were extracted with the help of SpaCy.
1
We started and finished the challenge with version
2.0.2 and afterwards updated to version 2.0.9.
This version update lead to a change of POS tags,
with which our results improved.
During post-
evaluation we also noticed an error in the prepro-
cessing that caused two feature sets (
bow
and
pos
)
to intermix.
Both the lemmas of pronouns as well
as the POS-tags of pronouns were mapped to the
same symbol ’PRON’, therefore we had to explic-
itly separate these two sets.
After resolving this
intersection our results improved further.
2.2
Context features
First we will explain feature construction based on
the context
of a relation.
Abbreviations for fea-
ture names are denoted in brackets.
Context
is
defined as the words between two entities.
Early
tests showed that using those words described the
relation better than the words surrounding the re-
lation entities.
Lexical
We
construct
several
lexical
boolean
features which are illustrated in Table 1.
First
we apply a bag of words (
bow
) approach where
each lemmatized word forms one boolean feature,
which for example takes 1 as value if the lemma
improve is present and 0 if it is not. Second we de-
termine whether the context words contain certain
part-of-speech (POS) tags (
pos
),
such as VERB.
To represent the structure of the context phrase we
add a path of POS tags feature, which contains the
order in which POS tags appear (
pospath
).
The
distance feature depicts whether the POS-path and
therefore the context
phrase has a certain length
(
dist
).
Additionally we add background knowledge by
extracting the top-level Levin classes of intermedi-
ary verbs from VerbNet
2
(
lc
), a verb lexicon com-
patible with WordNet. It contains explicitly stated
1
https://spacy.io/
2
http://verbs.colorado.edu/
˜
mpalmer/projects/verbnet.html
Example Sentence: Combination methods are an effective way of improving system performance.
Lexical Feature Set
Exemplary Boolean Features
BagOfWords (bow)
an, be, effective, improve, of, way
POS tags (pos)
ADJ, ADP, DET, NOUN, VERB
POS path (pospath)
VDANAV
Distance (dist)
6
Levin classes (lc)
45
Entities without order (ents)
combination methods, methods, system performance, performance
Start entity (startEnt)
combination methods, methods
End entitiy (endEnt)
system performance, performance
Similarity (sim100)
0.43
Similarity bucket (simb)
q50
Table 1: Examples for lexical context and entity features.
syntactic and semantic information,
using Levin
verb classes to systematically construct lexical en-
tries (Schuler,
2005).
For example the verb im-
prove belongs to class 45.4,
which is described
by Levin as consisting of “alternating change of
state“ verbs.
3
Embeddings
Aside
from lexical
features
we
also use word embedding vectors to leverage in-
formation from the context
of
entities (
c
).
For
each filtered context word we extract its word em-
beddding from a pre-trained corpus, where out-of-
vocabulary words (OOV) are represented by the
zero vector.
The individual word vectors are later
applied to train a C-LSTM.
In contrast, for use in an SVM we found it ben-
eficial to represent the context embedding features
as the average over all context word embeddings.
2.3
Entity features
In the second set of features, we model the relation
entities themselves as they may be connected to
a certain relation class.
For example,
the token
performance or one form of it mostly appears as
an end entity of a Result relation, and in the rare
case when it represents a start entity,
it is almost
always part of a Compare relation.
Therefore we
leverage information about entity position for the
creation of lexical and embedding entity features.
Lexical
For the creation of boolean lexical fea-
tures,
we first take the lowercased string of each
entity and construct
up to three distinct
features
from it.
One feature which marks its general ap-
pearance in the corpus without
order (
ents
) and
one each if it
occurs as start
(
startEnt
) or end
(
endEnt
) entity of a relation,
taking its direction
into account.
Additionally we add the head noun
3
http://www-personal.umich.edu/
˜
jlawler/levin.verbs
to the respective feature set if the entity consists of
a nominal phrase to create greater overlap between
instances.
Furthermore we measure the semantic similar-
ity of the relation entities using the cosine of the
corresponding word embedding vectors (
sim
100
).
While the cosine takes every value from [-1, 1] in
theory, we cut off after two digits to reduce the fea-
ture space and get 99 boolean similarity features
for our corpus.
To again enable learning across
instances we additionally discretize the similar-
ity range and form another five boolean similar-
ity features (
simb
) that capture into which of the
following buckets the similarity score falls:
q
0 =
[
−
1
,
0)
, q
25 = [0
,
0
.
25)
, q
50 = [0
.
25
,
0
.
5)
, q
75 =
[0
.
5
,
0
.
75)
, q
100 = [0
.
75
,
1]
(values below zero
are very rare in this corpus).
Embeddings
Similar to the context features we
also want to add word embeddings of entities to
our entity feature set.
This is not straighforward
as more than 44% of all entities consist of nomi-
nal phrases, while a word embedding usually cor-
responds to a single word. By way of comparison,
the proportion of nominals in the relation classifi-
cation corpus of the SemEval-2010 challenge was
only 5%. Thus we tested different strategies to ob-
tain a word embedding for nominal
phrases and
found that averaging over the individual word vec-
tors of the phrase yielded the best results for this
task.
These word embeddings for start
(
e
s
) and
end (
e
e
) entities of relations were then presented
to our two classification methods,
which will
be
described in detail in the following section.
3
Classification Methods
We utilize two different models for classifying se-
mantic relations: an SVM which incorporates both
the lexical
and embedding features described in
Section 2 and a Convolutional
Long Short
Term
Memory (C-LSTM) neural network that only uses
word embedding vectors
3.1
SVM
To fully exploit
our hand-crafted lexical
features
we employ a traditional
classifier.
In compari-
son to Naive Bayes,
Decision Trees and Random
Forests we found a Support Vector Machine to per-
form best for this task.
Instead of utilizing the de-
cision function of the SVM to predict
test labels
we decided to make use of the probability esti-
mates according to Wu et al. (2004) as this proved
to be more successful.
As mentioned before,
the
lexical features are fed into the SVM as boolean
features whereas the word embeddings are nor-
malized using MinMax-Scaling to the range
[0
,
1]
to make it easier for the SVM to handle both fea-
ture groups (Fig. 1).
3.2
C-LSTM
In contrast
to SVM,
neural
network models do
not
necessarily rely on handcrafted features and
are therefore faster
to implement.
We experi-
ment with C-LSTM (Zhou et al., 2015) which ex-
tracts a sentence representation by combining one-
dimensional
convolution and an LSTM network
and uses the representation to perform a classifi-
cation.
C-LSTM extracts a sentence representation in
the following steps.
First
embeddings
for
all
words
w
i
∈
R
v
are obtained from a pre-computed
embedding table
E
∈
R
v×|V |
where
v
is the em-
bedding size and
|
V
|
denotes the size of the vo-
cabulary.
For entities that are nominal phrases the
average over the individual
word embeddings is
used.
This results in a sequence of embedding
vectors
s
= [
e
s
, w
1
, w
2
,
· · ·
, w
n
, e
e
]
of length
l
s
where
e
1
, e
2
∈
R
v
are embeddings representing
entities and the
w
i
represent the context word em-
beddings.
Next
the embedding vectors in
s
are
concatenated to form an input matrix
I
∈
R
v×l
s
for
the one-dimensional
convolution layer.
For
computational
reasons a matrix
ˆ
I
∈
R
v×l
max
is
obtained by right padding
I
with a zero token to
the maximum sequence length
l
max
in the corpus.
After that
k
feature maps
f
i
∈
R
m
with
m
being
the number of features in each map are computed
over
ˆ
I
using a one-dimensional convolution layer
with
k
filters of window size
ws
and stride
st
. The
resulting feature map matrix
C
∈
R
k×m
is then
split along the second axis into a sequence
c
with
label
subtask 1.1
subtask 1.2
total
COMPARE
95 (
8%)
41 (
3%)
136 (
5%)
MODEL-F.
326 (27%)
174 (14%)
500 (20%)
PART W.
234 (19%)
196 (16%)
430 (17%)
RESULT
72 (
6%)
123 (10%)
195 (
8%)
TOPIC
18 (
1%)
243 (20%)
261 (11%)
USAGE
483 (39%)
468 (38%)
951 (38%)
Table 2: Distribution of class labels for training data as
absolute and relative values.
individual elements
c
i
∈
R
k
and length
l
c
=
m
.
Finally
c
is used as input to an LSTM network with
the last output being a representation of the input
sequence.
A softmax layer is used to compute la-
bel scores from the sentence representation.
See
Figure 2 for an illustration of the model.
4
Evaluation
After describing the two models we employ for re-
lation classification,
we now portray the data set
we use and present results for both SVM and C-
LSTM in detail.
Results are reported as micro-F1
and macro-F1.
The latter is the official evaluation
score of the SemEval Challenge.
We describe the
experimental setup for both models and compare
different feature sets and pre-trained embeddings.
4.1
Data and Background Knowledge
We evaluate our approach on a set of scientific ab-
stracts,
D
test
. It consists of 355 semantic relations
for each subtask which are similarly distributed as
its respective training data set. As training data we
received
350
abstracts of scientific articles per sub-
task, which resulted in
1228
labeled training rela-
tions for subtask 1.1 and
1245
training instances
for subtask 1.2 (c.f.
Table 2).
We combine data
sets of both subtasks for training, resulting in
2473
training examples in total (
D
train
).
Background Knowledge
In our
experiments,
we compare different
pre-trained word embed-
dings as a source of background knowledge.
As
a baseline,
we employ a publicly available set of
300-dimensional
word embeddings trained with
GloVe (Pennington et al.,
2014) on the Common
Crawl data
4
(CC). To better reflect the semantics
of scientific language,
we trained our own scien-
tific embeddings using word2vec (Mikolov et al.,
2013) on a large corpus of papers collected from
arXiv.org
5
(arXiv).
4
http://commoncrawl.org/
5
https://arxiv.org
bow
pos
pospath
dist
lc
1129
13
965
23
44
c
e
s
e
e
300
300
300
ents
startEnt
endEnt
sim100
simb
dir
3097
1831
1783
99
5
1
2174 lexical context
6816 lexical entity
900 embedding
context
entity
9886 features
Figure 1: Feature vector used in the SVM. Numbers hold true for subtask 1.1, including 1.2 data
[e
s
, w
1
, w
2
, w
3
, w
4
, w
5
, w
6
, e
s
]
concatenate embeddings

lter f
i
1-D CNN
feature maps
w=3
Figure 2:
An illustration of the model architecture of
C-LSTM.
In order to create the scientific embeddings, we
downloaded L
A
T
E
X sources for all papers published
in 2016 on arXiv.org using the provided dumps.
6
After
originally trying to extract
the plain text
from the sources,
we found that it was more fea-
sible to first compile the sources to pdf (exclud-
ing all
graphics etc.)
and then use pdftotext
7
to
convert the documents to plain text.
This resulted
in a dataset of about
166 000
papers.
Using gen-
sim (
ˇ
Reh˚
uˇrek and Sojka,
2010),
for
each docu-
ment
we extracted tokens of minimum length 1
with the wikicorpus tokenizer and used word2vec
to train 300-dimensional word embeddings on the
6
https://arxiv.org/help/bulk_data
7
https://poppler.freedesktop.org
context
+ entities
data
macro F1
micro F1
macro F1
micro F1
1.1
45.10
59.15
48.96
65.35
+1.2
46.95
61.97
66.03
70.14
CC
51.14
64.79
70.31
73.24
arXiv
51.55
64.79
75.11
77.46
Table 3: SVM results for subtask 1.1.
data.
We kept most hyper-parameters at their de-
fault values,
but limited the vocabulary to words
occurring at least 100 times in the dataset,
reduc-
ing for example the noise introduced by artifacts
from equations.
4.2
SVM
After an extensive grid search per cross validation
the best parameters for the SVM were found to be
a rbf-kernel with
C
= 100
and
γ
= 0
.
001
for both
tasks.
Our post-evaluation results of the SVM for sub-
task 1.1.
are shown in Table 3.
Adding entity
features proves to be very beneficial compared to
using only context features,
as we could improve
macro-F1 by 16 points on average.
Results are
further
improved by enlarging the data set
with
the training samples of subtask 1.2 and by adding
word embeddings to the feature set. While adding
the CC embeddings
enhances
the micro-F1 by
more than 4 points, our domain-adapted arXiv em-
beddings prove to perform even better and deliver
the best result with a macro-F1 score of
75
.
11 %
and a micro-F1 of
77
.
46 %
.
Similar observations can be made for subtask
1.2.,
as is pictured in Table 4.
Originally we
achieved a micro-F1 score 74.89% for
the first
subtask and 78.39% for the second but adding the
changes noted in Section 2.1 led to an improve-
ment of
2 %
on average (Hettinger et al., 2018).
context
+ entities
data
macro F1
micro F1
macro F1
micro F1
1.2
68.61
71.27
73.49
81.41
+1.1
61.09
69.01
78.63
83.66
CC
62.74
70.42
76.80
85.63
arXiv
63.29
70.99
81.44
85.07
Table 4: SVM results for subtask 1.2.
parameter
min
max
selected
number of filters
10
500
384
filter width
2
5
3
rnn cell units
16
500
93
dropout rate
0.0
0.5
0.23
l2 normalization scale
0.0
3.0
0.79
Table 5:
C-LSTM parameters and settings selected by
random search from search ranges of [min, max].
4.3
C-LSTM
We fix the batch size and number of epochs to
128
and
100
respectively for all trained models. Words
are encoded using either arXiv or CC embeddings.
The embeddings are not further optimized during
training. Cross-entropy is used as the loss function
and the model is optimized using Adam (Kingma
and Ba, 2014) with the initial learning rate set to
lr
= 0
.
002
,
β
1
= 0
.
9
,
β
2
= 0
.
999
,
ε
= 10
−8
.
To find the optimal
hyperparameter configura-
tion,
we perform a random search (Bergstra and
Bengio, 2012) on the hyper-parameters number of
filters, filter width, rnn cell units, dropout rate and
l2 norm scale.
For this study,
we sample
10 %
stratified from the training set
to serve as a val-
idation set.
All
parameters were chosen from a
uniformly random discrete or continuous distribu-
tion.
The ranges and the parameters yielding the
best performance on the validation set are given in
Table 5.
Using the determined optimal
parameter
set-
tings, models with both types of embeddings were
trained on the full
training set
and evaluated on
the test
set.
Table 6 shows that
the C-LSTM
model performs well on the scientific embeddings,
but
consistently worse than the SVM model
us-
ing handcrafted features and achieves a macro-F1
score of
67
.
49
and
67
.
02
for subtask 1.1 and sub-
task 1.2 respectively.
5
Discussion
We briefly discuss our approach during the train-
ing phase of the SemEval-Challenge and how label
distribution and evaluation measure influences our
subtask 1.1
subtask 1.2
macro F1
micro F1
macro F1
micro F1
CC
54.42
67.61
74.42
78.87
arXiv
67.49
70.96
67.02
74.37
Table 6:
Results for C-LSTM models trained with CC
and arXiv embeddings on both subtasks.
results.
Ahead of the final evaluation phase where
the concealed test data
D
test
was presented to the
participants we were given a preliminary test par-
tition
D
pre
as part of the training data
D
train
.
To
be able to estimate our performance we evaluated
it on
D
pre
as well as for a 10-fold stratified cross
validation setting.
We chose this procedure to be
sure to pick the best system for submission at the
challenge.
As
some
classes
were
strongly
underrepre-
sented in the training corpus and
D
pre
,
we as-
sumed that
this is also true for the final
test
set
D
test
.
When in doubt we therefore chose to opti-
mize according to
D
pre
as cross validation is based
on a slightly more balanced data set (of train data
for subtask 1.1 + 1.2).
The best system we sub-
mitted for subtask 1.1 of the challenge achieved a
macro-F1 of 75.05% on
D
pre
during the training
phase which shows that we were able to estimate
our final result pretty closely.
During training we also noticed that for heavily
skewed class distributions as in this case,
macro-
F1 as an evaluation measure strongly depends on
a good prediction of very small classes.
For exam-
ple, macro-F1 of subtask 1.1 increases by 5 points
if we correctly predict one Topic instance out of
three instead of none.
Thus we pick a configura-
tion that optimizes the small classes.
We also omitted some lexical feature sets from
our system as performance on the temporary and
final test set showed that they did not improve re-
sults.
These features were hypernyms of context
and entity tokens from WordNet and dependency
paths between entities.
Using tf-idf normalization
instead of boolean for lexical features also wors-
ened our results.
The C-LSTM model performes quite well, con-
sidering it only relies on very limited information,
the sequence of entity and word embedding vec-
tors,
to perform the classification.
For example
the model
has no way of determining the direc-
tion of the relation and we speculate that increas-
ing the model complexity to include such informa-
tion might increase the performance further.
Ad-
ditionally,
the results for subtask 1.2 show that in
contrast to the SVM model, C-LSTM does not per-
form consistently better with arXiv embeddings,
which warrants further investigation.
6
Conclusion
In this
paper,
we described our
SemEval-2018
Task 7 system to classify semantic relations in sci-
entific literature for clean (subtask 1.1) and noisy
(subtask 1.2) data and its results during the post-
evaluation phase.
We constructed features based
on relation entities and their context by means of
hand-crafted lexical features as well as word em-
beddings.
To better
adapt
to the scientific do-
main,
we trained scientific word embeddings on
a large corpus of scientific papers obtained from
arXiv.org.
We used an SVM to classify rela-
tions and additionally contrasted these results with
those obtained from training a C-LSTM model on
the scientific embeddings.
Due to improved pre-
processing we were able to obtain a macro-F1
score of
75
.
11 %
on clean data and
81
.
44 %
on
noisy data.
We finished the challenge as 4th out
of 28 (subtask 1.1) and 6th out of 20 (subtask 1.2)
though the results from Hettinger et al. (2018) are
applied.
In future work,
we will improve the tokeniza-
tion of the scientific word embeddings and also
take noun compounds into account,
as they make
up a large part
of the scientific vocabulary.
We
will also investigate more complex neural network
based models,
that can leverage additional
infor-
mation,
for example relation direction and POS
tags.
References
James Bergstra and Yoshua Bengio.
2012.
Random
search for hyper-parameter optimization.
Journal of
Machine Learning Research, 13:281–305.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017.
Reading wikipedia to answer open–
domain questions.
In ACL (1),
pages 1870–1879.
Association for Computational Linguistics.
Dmitriy Dligach,
Timothy Miller,
Chen Lin,
Steven
Bethard, and Guergana Savova. 2017.
Neural tem-
poral relation extraction.
In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers, volume 2, pages 746–751.
Kata G´
abor,
Davide Buscaldi,
Anne-Kathrin Schu-
mann,
Behrang QasemiZadeh,
Ha¨ıfa Zargayouna,
and Thierry Charnois.
2018.
Semeval-2018 Task
7:
Semantic relation extraction and classification in
scientific papers.
In Proceedings of
International
Workshop on Semantic Evaluation (SemEval-2018),
New Orleans, LA, USA.
Iris
Hendrickx,
Su Nam Kim,
Zornitsa
Kozareva,
Preslav Nakov,
Diarmuid
´
O S´
eaghdha,
Sebastian
Pad´
o,
Marco Pennacchiotti,
Lorenza Romano,
and
Stan Szpakowicz. 2009.
Semeval-2010 task 8: Mul-
ti-way classification of semantic relations between
pairs of
nominals.
In Proceedings of
the Work-
shop on Semantic Evaluations:
Recent
Achieve-
ments and Future Directions, pages 94–99. Associ-
ation for Computational Linguistics.
Lena Hettinger,
Alexander
Dallmann,
Albin Zehe,
Thomas Niebler, and Andreas Hotho. 2018.
Claire
at semeval-2018 task 7:
Classification of relations
using embeddings.
Yoon Kim.
2014.
Convolutional neural networks for
sentence classification.
In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing,
EMNLP 2014,
October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.
Diederik P.
Kingma and Jimmy Ba.
2014.
Adam:
A method for
stochastic
optimization.
CoRR,
abs/1412.6980.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado,
and Jeff Dean. 2013.
Distributed representa-
tions of words and phrases and their compositional-
ity.
In NIPS, pages 3111–3119. Curran Associates,
Inc.
Makoto Miwa and Mohit Bansal. 2016.
End-to-end re-
lation extraction using lstms on sequences and tree
structures.
Cite arxiv:1601.00770Comment:
Ac-
cepted for publication at the Association for Compu-
tational Linguistics (ACL), 2016. 13 pages, 1 figure,
6 tables.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning.
2014.
Glove:
Global
vectors for word
representation.
In EMNLP, volume 14, pages 1532–
1543.
Radim
ˇ
Reh˚
uˇ
rek and Petr Sojka. 2010.
Software Frame-
work for Topic Modelling with Large Corpora.
In
Proceedings of
the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA.
Bryan Rink and Sanda Harabagiu.
2010.
Utd:
Clas-
sifying semantic relations by combining lexical and
semantic resources.
In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation,
pages
256–259.
Association for
Computational
Linguis-
tics.
Cicero Nogueira dos Santos,
Bing Xiang,
and Bowen
Zhou. 2015.
Classifying relations by ranking with
convolutional neural networks.
Proceedings of the
7th International Joint Conference on Natural Lan-
guage Processing [IJCNLP].
Karin Kipper
Schuler.
2005.
Verbnet:
A Broad-
coverage, Comprehensive Verb Lexicon.
Ph.D. the-
sis,
University of Pennsylvania,
Philadelphia,
PA,
USA.
AAI3179808.
Ting-Fan Wu, Chih-Jen Lin, and Ruby C. Weng. 2004.
Probability estimates for
multi-class classification
by pairwise coupling.
Journal of Machine Learning
Research, 5(Aug):975–1005.
Kun
Xu,
Yansong
Feng,
Songfang
Huang,
and
Dongyan Zhao. 2015.
Semantic relation classifica-
tion via convolutional neural networks with simple
negative sampling.
In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing [EMNLP], pages 536–540.
Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran-
cis C.
M.
Lau.
2015.
A c-lstm neural network for
text classification.
CoRR, abs/1511.08630.
