An Integrated Framework of Text and Visual Analytics to Facilitate Information 
Retrieval towards Biomedical Literature 
Dissertation 
Presented in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy 
in the Graduate School of The Ohio State University 
By 
Xiaonan Ji, M.S. 
Graduate Program in Computer Science and Engineering 
The Ohio State University 
2018 
Dissertation Committee 
Dr. Alan Ritter, Advisor 
Dr. Po-Yin Yen, co-Advisor 
Dr. Raghu Machiraju 
2 
Copyrighted by 
Xiaonan Ji 
2018 
i 
Abstract 
Digitalized scientific literature, as a special type of text articles, is considered valuable 
knowledge repository in widespread academic and practical settings. Biomedical literature 
has specifically played an important role in supporting evidence-based medicine and 
promoting quality healthcare. Given an information need such as a patient problem, 
information retrieval towards biomedical literature has been focusing on the identification 
of high relevant articles to support up-to-date knowledge synthetization and reliable 
decision making. In particular, high recall, high precision, and human involvement are 
expected for a rigorous information retrieval in healthcare. Despite the critical information 
needs requiring high effectiveness and efficiency, the information overload from the large 
volume and heterogeneous biomedical literature has placed challenges on that. 
In this dissertation, we propose an integrated and generalizable framework of text 
and visual analytics to facilitate the significant domain application of biomedical literature 
retrieval. We focus on the unmet and most challenging aspect of identifying high relevant 
articles from a text corpus, which is typically an article collection obtained via exhaustive 
literature search. We convert extensive biomedical articles to effective representations that 
encode 
underlying 
article 
meanings 
and 
indicate 
article 
relevancies; 
and 
promote 
advantageous visualizations to exploit and explore article representations so that humans 
can get involved in not only task accomplishment but also knowledge discovery. 
ii 
We first implement text analytics to generate machine-understandable article 
features and representations, and promote their effectiveness with multiple knowledge and 
computational resources. Consider the special format of biomedical literature, we start by 
investigating the fundamental lexical feature space consisting of diverse article elements 
and examine their usefulness in predicting article relevancy. We then proceed to semantic 
analysis of the most informative article titles and abstracts. We develop an ontology-based 
semantic method exploiting gold-standard domain knowledge in UMLS ontologies, and 
build a concept modelling process to represent articles with optimized and enriched UMLS 
concepts. We also embrace the unprecedented computational power of neural networks, 
and develop a corpus-based semantic method with a neural document embedding model. 
This model is trained with multiple tasks to not only capture context semantics, but also 
integrate task specifications with minimal supervision. The effectiveness of our approaches 
is demonstrated through a downstream application of active article recommendation. Our 
approaches are also affordable and generalizable in the biomedical and clinical community. 
We then implement visual analytics to exploit and explore established article 
representations in a human involving manner. We start with a concept demonstration that 
2D visualizations of article representations (similarities) can reveal visual patterns that are 
beneficial to biomedical literature retrieval. To promote effective visualizations, we 
implement multiple visualization schemes, including sparsified article networks and article 
maps, and propose a new network sparsification scheme that preserves important article 
relationships 
and 
results 
in 
favorable 
2D 
embeddings 
(placements) 
of 
articles. 
Furthermore, 
we 
expose 
the 
visualizations 
to 
real-world 
settings 
where 
scalability, 
iii 
interpretability, and interactivity are expected. Under this notion, we propose a visual 
analytics system which is built upon effective visualizations and equipped with interactive 
features to meet the visual analytics Mantra. It is also extendible to assist in visual 
evaluation 
and 
interpretation 
of 
text 
analytics 
results, 
such 
as 
semantic 
article 
representations learned with neural embedding models, to overcome the black-box nature 
and gain insights into the underlying mechanism such as semantic properties. We use 
experiments and use cases to demonstrate the usefulness of our visualizations and the visual 
analytics system in expediting biomedical literature retrieval. 
iv 
Dedication 
This is dedicated to the ones I love, my parents, my husband, my grandparents, my 
mentors, and my friends. 
v 
Acknowledgments 
This dissertation is completed not only by my work, it is the result of all the help and 
support I got during my days at The Ohio State University. There are so many people that 
I want to express my deepest gratitude to. Without their help and support, it would not have 
been possible for me to finish this dissertation. 
Foremost, I want to give special thanks to my advisors and research committee 
members, Dr. Po-Yin Yen, Dr. Alan Ritter, and Dr. Raghu Machiraju. They brought me 
into the very interesting area of text analysis and visualization, and introduced me to the 
wonderful journal in medical and clinical informatics. They have been giving me their 
generous support in my research and study. Their broad knowledge and insightful guidance 
have helped me establish my research interest and develop my problem identification and 
solving capabilities. I am so grateful that because of them, I love what I am doing, and I 
have the firmest confidence in that. Again, I sincerely appreciate all their support, without 
which it would have been hard for me to pass through the ups and downs during my Ph.D. 
study. I also want to thank Dr. Han-Wei Shen and Dr. Huan Sun, who have also given me 
their generous help and great encouragements in my research and thesis preparation. 
Thanks to my dearest husband, Yiding Zhang, and my dearest parents, Xiuying Han 
and Baosheng Ji, for their unconditional love, support, kindness, and patience that helped 
me out through the hard times I had. For some days, I felt disappointed in myself and lost 
vi 
my direction, it is with them I catch up with my energy and determination, it is with them 
I keep moving on. I would also like to thank my dear mentor, Po-Yin Yen, for her strongest 
spiritual and financial support. She is not only a successful educator but also an excellent 
example of my life. I also appreciate all the helps I have received from the Department of 
Computer Science and Engineering and the Department of Biomedical Informatics, the 
friendly directors and coordinators, and the lovely colleagues and lab mates. Finally, I want 
to thank my friends and relatives who have always been supportive, and our cat Pipi who 
has been my closest accompany in many silent hardworking nights. 
vii 
Vita 
June 2010………… B.S. Software Engineering, Beihang University 
June 2012………… M.S. Computer Science and Engineering, The Ohio State University 
2012……...……..... Student Research Assistant, Department of Biomedical Informatics, 
The Ohio State University 
2012……...……..... Graduate Teaching Associate, Department of Computer Science and 
Engineering, The Ohio State University 
2012……..……..... Research and Develop, Department of Biomedical Informatics, 
Wexner Medical Center, The Ohio State University 
2013 to 2014........... Application Consultant and System Engineer, IBM China 
2014 to 2017……... Graduate Research Associate, Department of Biomedical 
Informatics, The Ohio State University 
2018 to present…… Graduate Teaching Associate, Department of Computer Science and 
Engineering, The Ohio State University 
viii 
Publications 
Xiaonan 
Ji
, 
Han-Wei 
Shen, 
Raghu 
Machiraju, 
Alan 
Ritter, 
Po-Yin 
Yen. 
Visual 
Exploration of Neural Document Embeddings: What are Behind the Neural Dimensions? 
IEEE VAST 2018. 
Submitted and Under Review 
Xiaonan Ji
, Raghu Machiraju, Alan Ritter, Po-Yin Yen. Visualizing Article Similarities 
via Sparsified Article Network and Map Projection for Systematic Reviews. Studies in 
health technology and informatics. 2017;245:422-6. (Award: Winner of Student Best Paper 
Competition at MedInfo 2017) 
Xiaonan Ji
, Alan Ritter, Po-Yin Yen. Using Ontology-based Semantic Similarity to 
Facilitate the Article Screening Process for Systematic Reviews. Journal of Biomedical 
Informatics. 2017 May 31;69:33-42. 
Xiaonan Ji
, Raghu Machiraju, Alan Ritter, Po-Yin Yen. Examining the Distribution, 
Modularity, and Community Structure in Article Networks for Systematic Reviews. In 
AMIA Annual Symposium Proceedings 2015 (Vol. 2015, p. 1927). American Medical 
Informatics Association. 
Xiaonan 
Ji
, 
Po-Yin 
Yen. 
Using 
MEDLINE Elemental 
Similarity 
to 
Assist 
Article 
Selection for Systematic Reviews. JMIR medical informatics. 2015 Jul;3(3). 
Albin, Aaron, 
Xiaonan Ji
, Tara B. Borlawsky, Zhan Ye, Simon Lin, Philip RO Payne, Kun 
Huang, and Yang Xiang. Enabling Online Studies of Conceptual Relationships between 
Medical terms: Developing an Efficient Web Platform." JMIR medical informatics. 2014 
Jul;2(2). 
Xiaonan Ji
, Spencer Davis, Erikson Hardesty, Xu Liang, Sabuj Saha, Hai Jiang. Towards 
Utilizing Remote GPUs for CUDA Program Execution. IEEE Transactions on Wireless 
Communications. 2011 May;10(4-10):1232-41. 
Fields of Study 
Major Field: Computer Science and Engineering
ix 
Table of Contents 
Abstract ............................................................................................................................ i
Dedication ...................................................................................................................... iv
Acknowledgments ........................................................................................................... v
Vita ............................................................................................................................... vii
List of Tables ............................................................................................................... xiv
List of Figures ............................................................................................................... xv
Chapter 1. Introduction .................................................................................................... 1
1.1 A Significant Problem: Information Retrieval towards Biomedical Literature ........ 4
1.1.1 Biomedical Literature ..................................................................................... 4
1.1.2 Biomedical Literature Retrieval ...................................................................... 6
1.1.3 Meet the Challenges ...................................................................................... 11
1.2 Inspirations from State-of-the-art Techniques ...................................................... 13
1.2.1 Text Analytics ............................................................................................... 13
1.2.2 Visual Analytics ............................................................................................ 16
1.3 Related Works and Established Gaps ................................................................... 18
1.3.1 Automated Article Classification................................................................... 18
1.3.2 Bibliometric Information Visualization ......................................................... 22
1.4 Dissertation Overview ......................................................................................... 24
1.4.1 Objective, Scope, and Hypotheses ................................................................. 25
1.4.2 Outline .......................................................................................................... 29
1.4.3 Statement ...................................................................................................... 33
Chapter 2. Lexical Analysis on Multiple Article Elements of Biomedical Literature ...... 34
2.1 Research Questions .............................................................................................. 35
2.2 Background and Related Works ........................................................................... 36
2.2.1 Article Elements of Biomedical Literature .................................................... 36
2.2.2 Lexical Feature with BoW and n-gram .......................................................... 37
2.2.3 Vector Space Model and Cosine Similarity ................................................... 38
2.3 Lexical Features of Biomedical Literature ............................................................ 39
x 
2.3.1 Selecting and Processing Article Elements .................................................... 39
2.3.2 Weighting the Combinations of Multiple Article Elements ............................ 40
2.4 An Interactive Article Recommendation Model with Active Learning ................. 41
2.4.1 Integrating the Concept of Active Learning ................................................... 41
2.4.2 Implementation of Interactive Article Recommendation ................................ 43
2.5 Experimental Settings and Results ....................................................................... 44
2.5.1 Datasets ........................................................................................................ 44
2.5.2 Evaluation Measures ..................................................................................... 45
2.5.3 Single Element Performance ......................................................................... 46
2.5.4 Composited Elements Performance ............................................................... 49
2.5.5 Performance Comparisons with Related Works ............................................. 50
2.6 Summary ............................................................................................................. 51
Chapter 3. Ontology-based Semantic Analysis with UMLS Knowledge Resource ......... 54
3.1 Research Questions .............................................................................................. 55
3.2 Background and Related Works ........................................................................... 56
3.2.1 UMLS Metathesaurus ................................................................................... 56
3.2.2 SNOMED-CT and MeSH ............................................................................. 56
3.2.3 MetaMap ...................................................................................................... 58
3.2.4 Semantic Similarity Measures ....................................................................... 59
3.3 Semantic Concepts Representation Development Process .................................... 61
3.3.1 Concept Annotation ...................................................................................... 61
3.3.2 Concept Optimization ................................................................................... 62
3.3.3 Concept Expansion ....................................................................................... 64
3.4 Experimental Settings and Results ....................................................................... 66
3.4.1 Experimental Settings ................................................................................... 66
3.4.2 Performance Report ...................................................................................... 68
3.4.3 Performance Comparisons with Lexical Approaches ..................................... 70
3.4.4 Performance Comparisons with Corpus-based Semantic Approaches ............ 72
3.5 Summary ............................................................................................................. 73
Chapter 4. Corpus-based Semantic Analysis with Neural Document Embedding ........... 76
4.1 Research Questions .............................................................................................. 77
4.2 Background and Related Works ........................................................................... 79
xi 
4.2.1 An Overview of Neural Embedding .............................................................. 79
4.2.2 A Design Process of Neural Embedding ........................................................ 80
4.2.3 Existing Neural Embedding Models .............................................................. 83
4.2.4 Task-Specific Interests .................................................................................. 86
4.3 Model Design ...................................................................................................... 87
4.3.1 Linguistic Aspects for Information Retrieval ................................................. 88
4.3.2 Task-Specific Interests and Annotations ........................................................ 90
4.3.3 Model Architecture Selection ........................................................................ 91
4.3.4 Multi-Task Paragraph Vector (PV-MT) Model .............................................. 92
4.4 Experiment Settings and Results .......................................................................... 94
4.4.1 Experimental Settings ................................................................................... 94
4.4.2 Experimental Results .................................................................................... 95
4.5 Summary ............................................................................................................. 96
Chapter 5. Demonstrating the Usefulness of Visualizing Article Relationships for 
Biomedical Literature Retrieval ................................................................................... 100
5.1 Research Questions ............................................................................................ 102
5.2 Background and Related Works ......................................................................... 103
5.2.1 Network Visualization with Force-Directed Algorithm ............................... 103
5.2.2 Structural Network Properties ..................................................................... 104
5.2.3 Community Detection with the Louvain Method ......................................... 105
5.3 Visualization of Article Similarity Network ....................................................... 107
5.3.1 Article Similarity Network .......................................................................... 107
5.3.2 Network Visualization ................................................................................ 108
5.3.3 Community Detection (Network Clustering) ............................................... 110
5.4 Experimental Settings and Results ..................................................................... 111
5.4.1 Datasets ...................................................................................................... 111
5.4.2 Results on Graph Diameter and Closeness Centrality .................................. 112
5.4.3 Results on Modularity and Community ....................................................... 114
5.5 Summary ........................................................................................................... 115
Chapter 6. Promoting Effective Visualizations with Sparsified Article Networks and 
Article Maps................................................................................................................ 119
6.1 Research Questions ............................................................................................ 122
6.2 Background and Related Works ......................................................................... 123
xii 
6.2.1 Graph-based Network Drawing ................................................................... 123
6.2.2 Network Sparsification with Edge Sampling ............................................... 124
6.2.3 Distance-based Map Projection ................................................................... 125
6.2.4 t-Distributed Stochastic Neighbor Embedding (t-SNE) ................................ 126
6.3 Article Network Sparsification and Visualization ............................................... 128
6.3.1 Revisiting Article Network ......................................................................... 128
6.3.2 Configurable Article Network Sparsification ............................................... 129
6.3.3 Visualization of Sparsified Article Network ................................................ 132
6.4 Article Map Projection ....................................................................................... 133
6.5 Experimental Settings and Results ..................................................................... 135
6.5.1 Evaluation Methods .................................................................................... 135
6.5.2 Results on Network Properties .................................................................... 137
6.5.3 Results on Clustering Patterns ..................................................................... 138
6.6 Summary ........................................................................................................... 140
Chapter 7. A Visual Analytics System for Literature Retrieval and Text Corpus 
Exploration.................................................................................................................. 143
7.1 Research Questions ............................................................................................ 146
7.2 Background and Related Works ......................................................................... 147
7.2.1 Hierarchical Clustering and Graph Summarization ...................................... 147
7.2.2 Visual Analytics of Text Corpus ................................................................. 149
7.2.3 Visual Analytics of Large Dataset ............................................................... 150
7.2.4 Visual Analytics of Neural Networks .......................................................... 153
7.3 A Visual Analytics System Design .................................................................... 154
7.3.1 Article Collection Visualization .................................................................. 154
7.3.2 Adjustable Hierarchical Clustering - Multilevel Structure ........................... 156
7.3.3 Dynamic Topic Synthesis ............................................................................ 158
7.3.4 Hierarchical Navigation .............................................................................. 159
7.3.5 Other Interactive Features ........................................................................... 161
7.3.6 Use Case ..................................................................................................... 163
7.4 Visual Evaluation of Neural Document Embeddings .......................................... 167
7.4.1 Internal and External Validation .................................................................. 168
7.4.2 Use Case ..................................................................................................... 169
7.5 Visual Interpretation of Neural Document Embeddings ..................................... 171
xiii 
7.5.1 Visualization of Dimension Behaviors ........................................................ 173
7.5.2 Exploration of Dimension Semantics .......................................................... 175
7.5.3 Exploration of Dimension Contributions ..................................................... 180
7.5.4 Use Case ..................................................................................................... 183
7.5 Summary ........................................................................................................... 185
Chapter 8. Conclusion ................................................................................................. 188
8.1 Conclusion......................................................................................................... 188
8.2 Contributions ..................................................................................................... 189
8.3 Discussions and Future Works ........................................................................... 191
8.3.1 Article Annotations ..................................................................................... 192
8.3.2 Additional Article Elements ........................................................................ 193
8.3.3 Information Extraction ................................................................................ 195
8.2.4 Deep Neural Document Embeddings ........................................................... 197
8.2.5 UMLS Metathesaurus with Neural Embedding ........................................... 199
8.2.6 Long-term Feedback of Visual Analytics System ........................................ 201
8.2.7 Visual Analytics of Dynamic Neural Embeddings ....................................... 201
Bibliography ............................................................................................................... 202
xiv 
List of Tables 
Table 2.1: 15 DERP datasets and their sizes and rates of inclusion (relevancy) .............. 44
Table 2.2: Single element WSS95 performance ............................................................. 47
Table 2.3: Single element F1 performance ..................................................................... 48
Table 2.4: Composited elements WSS95 performance ................................................... 49
Table 2.5: Composited elements F1 performance........................................................... 50
Table 2.6: WSS95 performance comparisons with existing works ................................. 51
Table 3.1: Performance report with different ontologies ................................................ 69
Table 3.2: Performance comparisons with lexical approaches ........................................ 71
Table 3.3: Performance comparisons with other semantic approaches ............................ 73
Table 4.1: Performance report and comparisons ............................................................ 95
Table 5.1: Five Cochrane datasets and their sizes and rates of inclusion (relevancy) .... 112
Table 5.2: Graph diameter and closeness centrality ...................................................... 113
Table 5.3: Modularity and detected communities ......................................................... 115
Table 6.1: Preservation of network properties .............................................................. 137
Table 6.2: Article distribution and clustering of high relevant articles .......................... 139
xv 
List of Figures 
Figure 1.1: A sample biomedical article (clinical trial) ..................................................... 5
Figure 1.2: Illustrated information retrieval towards biomedical literature ....................... 7
Figure 1.3: The specific biomedical literature retrieval and related issues ...................... 12
Figure 1.4: A general workflow of text mining .............................................................. 15
Figure 1.5: An integrated framework of text and visual analytics ................................... 28
Figure 1.6: Dissertation overview .................................................................................. 32
Figure 1.7: Dissertation statement.................................................................................. 33
Figure 2.1: Illustrated interactive article recommendation with active learning .............. 42
Figure 2.2: Illustrated high WSS95 and F1 .................................................................... 46
Figure 3.1: Overview of the semantic concepts representation development process ...... 61
Figure 3.2: Concept categorization process .................................................................... 64
Figure 3.3: Illustrated concept expansion ....................................................................... 65
Figure 3.4: Illustrated optimized and enriched concept set ............................................. 66
Figure 3.5: Ontology Depth threshold and resulting WSS95 performance ...................... 68
Figure 3.6: Performance curves with SNOMED-CT + MeSH ........................................ 70
Figure 4.1: An illustration of the Paragraph Vector model [90] ...................................... 84
Figure 4.2: Linguistic aspects and their applicability ..................................................... 88
Figure 4.3: Illustration of task-specific interests for ACE Inhibitor dataset [26] ............. 91
Figure 4.4: Illustration of Multi-Task Paragraph Vector (PV-MT) model ...................... 93
Figure 5.1: Illustrated article network (ADHD dataset) colored by article classes ........ 109
Figure 5.2: Illustrated article network (ADHD dataset) colored by communities .......... 111
Figure 6.1: Visualization techniques and the intuitions behind ..................................... 122
Figure 6.2: Illustrated “complete” article network with “hairballs” .............................. 128
Figure 6.3: Article network sparsification with different scoring schemes .................... 131
Figure 6.4: Illustrated sparsified article networks on the ADHD dataset....................... 133
Figure 6.5: Illustrated article map on the ADHD dataset .............................................. 134
Figure 6.6: Illustrated clustering of high relevant articles ............................................. 135
Figure 7.1: Multiple 2D visualization schemes ............................................................ 155
Figure 7.2: Illustrated 2D visualization with clusters and topics ................................... 156
Figure 7.3: Visual enhancement of clustering patterns ................................................. 158
Figure 7.4: A global structure with super points representing clusters .......................... 161
Figure 7.5: Identification of focal points and contextual sets of potential interests ....... 162
Figure 7.6: Identification and highlight of citations and references .............................. 163
Figure 7.7: A global overview of clusters and topics on the ADHD dataset.................. 165
Figure 7.8: A detailed view of sub-clusters and sub-topics ........................................... 166
Figure 7.9: Internal validation using silhouette coefficient ........................................... 170
xvi 
Figure 7.10: External validation using the distribution of benchmark high-quality articles
.................................................................................................................................... 170
Figure 7.11: Visualization of neural dimension behaviors ............................................ 175
Figure 7.12: Exploration of neural dimension semantics – hypothesis generation ........ 178
Figure 7.13: Exploration of neural dimension semantics – hypothesis refinement ........ 180
Figure 7.14: Exploration of neural dimension contributions ......................................... 182
Figure 7.15: Use case for neural dimension exploration on the ADHD dataset ............. 183
Figure 8.1: A conclusive dissertation overview ............................................................ 189
1 
Chapter 1. Introduction 
In this chapter, we introduce the storyline of this dissertation, including the motivation for 
resolving a significant biomedical domain problem with computer science and information 
techniques, and the inspirations from the state-of-the-art techniques to design and develop 
an effective, affordable, and generalizable solution to the real-world issue. 
First 
and 
foremost, 
our 
research 
work 
is 
motivated 
by 
the 
significance 
of 
biomedical 
literature 
in 
supporting 
Evidence-based 
Medicine 
(EBM) 
and 
informing 
targeted clinical or research decisions for quality healthcare. As a special type of digitalized 
scientific literature, biomedical literature is considered an important repository maintaining 
high-quality biomedical and clinical knowledge, evidence, and experiences [1], [2]. 
Therefore, given an information need such as an awaiting patient problem or a research 
question (e.g. a drug effectiveness review request), rigorous information retrieval towards 
biomedical literature has been intending to identify high relevant articles to support reliable 
decision 
making 
or 
up-to-date 
knowledge 
synthetization 
[3]–[5]. 
Because 
of 
the 
particularity of quality healthcare, high recall, high precision, and human involvement are 
usually 
expected 
to 
achieve 
effective, 
efficient, 
and 
reliable 
information 
retrieval. 
However, 
the 
information 
overload 
resulted 
from 
the 
overwhelming 
volume 
of 
heterogeneous biomedical literature has placed severe challenges on that, and made an 
information retrieval process time consuming and labor intensive [6], [7]. There are notable 
2 
gaps between extensive biomedical articles, and high relevant articles that can be identified 
and utilized in targeted scenarios. [8], [9]. In response to this critical issue, we propose a 
framework of biomedical literature mining to bridge the recognized gaps in an effective, 
affordable, and generalizable manner. For doing so, our research is further inspired by the 
technological trends related to text analytics and visual analytics in recent decades. 
Text analytics, which is also known as text mining in a broader sense, has been 
widely used to draw high-quality information from texts written in natural language. It is 
also demonstrated to derive underlying meanings and pinpoint task-specific characteristics 
of text segments [10]–[13]. Under this notion, many text mining approaches convert 
unstructured texts into structured feature representations encoding fine information and 
patterns in a machine understandable manner, which turn to enable and even enhance many 
downstream applications, including information retrieval [14]–[16]. On the other hand, 
visual 
analytics, 
as 
an 
extension 
to 
information 
visualization, 
is 
a 
study 
of 
visual 
representations of abstract data and is shown to expedite data exploitation and exploration 
by leveraging the most reliant human cognition and analytic reasoning capabilities [17]–
[20]. Visual analytics not only reveals underlying patterns to human eyes but also enables 
insights for knowledge discovery. In other words, it not only promotes the involvement of 
humans in task accomplishments, but also advances information communications by 
keeping humans in the loop and integrating human expressed interests and triggered 
interactions for superior performance. Therefore, if text analytics is to bridge the gap 
between unstructured extensive text documents and structured high-quality information 
(e.g. features, representations, similarity) that is generated to reveal underlying meanings 
3 
and patterns, visual analytics is an important follow-up to further bridge the gap between 
the abstract fine information and advantageous exploitation and exploration involving 
humans in real-world settings. 
Based on the above-mentioned facts, in our proposed framework of biomedical 
literature mining, we integrate text and visual analytics to design and develop an effective, 
affordable (time and cost efficient), and generalizable solution to facilitate 
information 
retrieval towards high relevant biomedical literature
, which will also be referred to as 
biomedical literature retrieval
in this dissertation. This integrated framework is considered 
the primary contribution made by this dissertation, which addresses the significant 
biomedical 
domain 
problem 
with 
state-of-the-art 
computer 
science 
and 
information 
techniques. In light of this real-world problem, we also extend state-of-the-art text and 
visual analytics approaches with several innovative contributions made to the computer 
science and information domain, reflecting how superior knowledge and computational 
resources, task specifications of information retrieval, and human involvements (e.g. 
cognition and interaction) can advance the effectiveness of text and visual analytics. 
In the following part of this chapter, we introduce the background information and 
the problematic identification process with respect to the significant issue of biomedical 
literature retrieval. We then present an overview of the state-of-the-art techniques of text 
and visual analytics, which inspire our integrated solution to facilitate biomedical literature 
retrieval, and present some related works and discusses the limitations and gaps. After that, 
we formulate our research questions, objectives, and scopes, propose a framework of 
biomedical literature mining, and illustrate the rationalities with a logic diagram. We also 
4 
present an overview of this dissertation, especially for its structure, organization, and major 
components, which are in correspondence with the later chapters of this dissertation. 
1.1 A Significant Problem: Information Retrieval towards Biomedical Literature 
1.1.1 Biomedical Literature 
Digitalized scientific literature, which is a special type of text documents or articles written 
in natural language, is a valuable knowledge repository and intermediary in a wide range 
of academic and practical settings, especially with respect to the modern information era 
driven by the cultivation and communication of knowledge, evidence, and experiences. 
Biomedical literature has more specifically performed an important role in the biomedical 
and clinical settings, in promoting the development of knowledge, intervention, and 
technologies for better healthcare delivery at the point-of-care [1]. In particular, the 
systematic review of biomedical literature, as one significant application in quality 
healthcare, critically appraises and synthesizes knowledge from high relevant literature in 
an exhaustive and complete manner, and is considered the highest-quality resources of 
Evidence-based Practice (EBP) [2], [8], [21]. According to the Institute of Medicine (IOM), 
90% clinical decisions are expected to be evidence-based by 2020 [22]. 
Considering the heterogeneous nature of biomedical literature, there have been 
many different types of biomedical literature, including analytical (experimental) studies, 
observational studies, health outcomes, review studies, etc. [1] For instance, clinical trials 
as analytic studies, determine the relationships between therapeutic agents and responses 
in a group of patients, and are used to guide extensive clinical practices [23]. Figure 1.1 (a) 
shows 
a 
sample 
clinical 
trial, 
with 
its 
title 
and 
abstract 
level 
information, 
about 
5 
Atomoxetine treatment effectiveness for adults with Attention Deficit and Hyperactivity 
Disorder 
(ADHD). 
Digitalized 
biomedical 
literature 
is 
maintained 
and 
indexed 
in 
electronical bibliographic databases, among them, MEDLINE/PubMed is recognized as 
the most comprehensive and approachable database of biomedical literature [7]. With 
MEDLINE, biomedical literature is formatted and standardized with multiple MEDLINE 
article elements, such as title (TI), abstract (AB), Medical Subject Heading Terms (MH), 
author provided keywords (OT), authors (AU), publication types (PT), etc., and is usually 
exportable to a MEDLINE or XML format for further utilizations (e.g. analysis and 
evaluation [24]). Figure 1.1 (b) shows the same sample article in the MEDLINE format. 
(a) Title and abstract level information 
Figure 1.1: A sample biomedical article (clinical trial) 
6 
Figure 1.1 continued 
(b) Formatted MEDLINE article elements 
1.1.2 Biomedical Literature Retrieval 
Given an information need, such as an awaiting patient problem or a research question e.g. 
for drug effectiveness review, information retrieval (IR) towards biomedical literature has 
been specifically intending to identify high relevant articles to support reliable decision 
making or knowledge synthetization in a comprehensive and precise manner [3], [4]. In 
general, information retrieval in a biomedical perspective can include multiple aspects. The 
aspect of retrieving “possibly relevant literature” from “all literature” is being addressed 
by many search engines or search interfaces [25], and can be approached with raw keyword 
search for high recall and completed in seconds or minutes. However, the unmet and 
critical aspect lies in identifying “high relevant literature” or “definitely relevant literature” 
from an article collection (text corpus) of “possibly relevant literature”. This process 
involves rigorous criteria to meet the expectation of high recall and precision, and usually 
requires intensive manual work or relies on sophisticated computational tools [3], [5]. As 
one significant application lies in this setting, systematic literature review (SR) aims to 
7 
comprehensively summarize clinical evidence or research knowledge by exhaustively 
identifying and appraising all high relevant articles from a pre-obtained article collection 
[8]. SRs are considered the preferred sources of Evidence-based Practice (EBP) that utilizes 
the most up-to-date evidence to inform clinical decisions [21], however, SRs on hundreds 
or thousands of articles can take weeks and even months to complete by human researchers. 
In Figure 1.2 (a), we present a well-known diagram from Hersh [3], [5] to illustrate 
the problem identification upon a big picture of information retrieval in a biomedical 
perspective. And in Figure 1.2 (b), we present a sample systematic review task, drug 
effectiveness review of 
ACE Inhibitors
, to demonstrate how the information retrieval is 
narrowed down from all literature, possibly relevant literature, to high relevant literature. 
(a) Information retrieval in a biomedical perspective 
Figure 1.2: Illustrated information retrieval towards biomedical literature 
Approached with many 
search engines on 
electronic databases 
A critical and 
challenging aspect, 
especially considering 
SRs and EBPs. 
Information Retrieval 
Information Extraction 
Text Mining 
Identification of relevant high-
quality articles for complete 
knowledge synthetization and 
reliable decision making 
8 
Figure 1.2 continued 
(b) A sample systematic review task 
Therefore, we recognize the challenging and significant aspect of 
information retrieval 
towards biomedical literature
, which focuses on the identification of high relevant articles 
from a large text corpus or a pooled article collection (of possibly relevant articles), which 
can be obtained via preliminary exhaustive literature search, expecting to receive high 
recall and precision for up-to-date knowledge synthetization and reliable decision making. 
We will also refer to this information retrieval task as 
biomedical literature retrieval
in this 
dissertation with several specifications, which are confirmed by our collaborative domain 
experts in biomedical and clinical informatics. We present the specifications as below: 
1) Information need. The information need can be given as a patient problem, a 
clinical question, or a research protocol. Meanwhile, it can be a complexity consisting of 
multiple analytic aspects (facets) or indicating a diverse scope; it can even require iterative 
refinements during an information retrieval process. For instance, in the drug effectiveness 
review project (DERP) [26] and Cochrane systematic review project [27], an information 
need is usually formulated as a research protocol consisting of a list of key questions and 
9 
inclusion criteria. Furthermore, a set of keywords or key phrases can be extracted from the 
descriptive texts of questions and criteria, and categorized to reflect the different facets of 
an information need. Specifically, in the clinical domain, a categorization typically follows 
the 
PICO
format [28], representing 
Patient
(e.g. popular, disease, symptom), 
Intervention
(e.g. treatment, drug name, procedure), 
Comparison
, and 
Outcome
(e.g. effectiveness, 
safety, adverse effect). In this sense, the information need of a drug effectiveness review 
would be a text synopsis about the targeted disease, drugs, expected outcomes, etc. 
2) Information resource. The information resource is usually a large-scale text 
corpus of biomedical literature (pooled article collection) obtained with preliminary 
exhaustive literature search on electronic databases, such as MEDLINE, via a search 
engine or interface, such as PubMed [7], [25]. Recall the diagram of Figure 1.1, we focus 
on the unmet and critical aspect of identifying “high relevant literature” from “possibly 
relevant literature”. Additional reasons of taking an article collection as our information 
resource can be explained by the recognized objectives, procedures, and pain points in 
biomedical literature retrieval. Firstly, a high recall of relevant articles is essentially 
important to reach a comprehensive and up-to-date knowledge synopsis and inform reliable 
decisions in quality healthcare. And it has been a common procedure to conduct exhaustive 
literature search with raw keywords as a preliminary step to obtain an all-inclusive article 
collection, which could have a size ranges from several hundreds to tens of thousands [29]. 
Secondly, because of the complication of an information need and the heterogeneous nature 
of 
biomedical 
literature (e.g. 
diverse 
biomedical 
terminologies 
[3], 
[4]), 
the 
article 
screening process un a large article collection to identify high relevant articles has become 
10 
the most time and labor intensive process. According to existing studies, a manual article 
screening process can take several weeks or months to complete for human researchers, 
however, only a small percentage of articles (<1% to 27%) from the corpus are identified 
as high relevant with respect to the strict requirement of high precision [29]. Although 
some automated or semi-automated approaches have been developed by related studies, 
they have limited effectiveness or generalization to different tasks or datasets. Therefore, 
the pain point of information retrieval towards biomedical literature lies in the rigorous 
process upon a large text corpus of possibly relevant articles, instead of feeding keywords 
to an electronic database, though a reliable search engine would be needed for that. 
3) Expected outcomes. Given an information need, the expected outcomes are a set 
of high relevant biomedical articles identified from the information resource, e.g. a text 
corpus or an article collection. A high recall and a high precision are necessary to reach 
comprehensive and up-to-date knowledge synthetization and reliable decision making. 
4) With biomedical literature being considered as the objective of this information 
retrieval task, the title and abstract level information of an article is primarily utilized for 
analysis to determine the inclusion (relevancy) or exclusion (irrelevancy). Firstly, the title 
and abstract level information, including title, abstract, authors, keywords (e.g. Medical 
Subject Heading terms), and publication types, are considered the most informative and 
concise 
article 
elements, 
especially 
considering 
the 
format 
and 
property 
of 
many 
biomedical literature e.g. clinical trials [30], [31]. Specifically, an abstract can provide key 
information for readers to rapidly understand the scope, methods, and findings of an article. 
On the other hand, the full-text information is much noisier due to the extensive narratives, 
11 
deviating scope, and diverse facets. For example, the contents related to background, 
associated work, patient recruitment, and experiment setup might not directly contribute to 
the underlying meanings or relevancy of an article. Secondly, the full-text level information 
is typically approached after the completion of title and abstract level information retrieval 
to manually examine the full-text content of already-included articles. Therefore, full-text 
level information retrieval is usually triggered at a post-stage [29]. 
1.1.3 Meet the Challenges 
The recognized application of biomedical literature retrieval is an important activity in 
bridging high relevant articles with subsequent knowledge synthetization and decision 
making for better healthcare delivery, such as SRs and EBPs. However, the volume of 
biomedical literature has met rapid growth with over 26 million biomedical articles stored 
in electronical bibliographic databases (e.g. MEDLINE), and the number of articles 
published each week is over 12,000, including more than 300 randomized clinical trials [6], 
[7]. Although many search engines of electronic databases have been integrated with 
advanced functionalities to simplify the search process and promote the quality of searched 
results [25], a typical exhaustive literature search would still yield hundreds or tens of 
thousands of articles [29]. As illustrated in Figure 1.3, with a large article collection as the 
text corpus, it is an intensive process to conduct rigorous information retrieval and identify 
high relevant articles via thorough appraises to meet the expectations of high recall and 
precision 
(i.e. 
high 
comprehensiveness 
and 
accuracy). 
More 
specifically, 
an 
article 
screening process involved in this process can take weeks or even months to complete for 
human researchers [29]. However, only a small percentage of articles (e.g. <1% to 27%) 
12 
might be identified as high relevant (inclusion) due to the heterogeneous nature of 
biomedical literature and the complexity of an information need. In other words, human 
researchers would spend most of their time on excluding irrelevant articles that fail to meet 
an information need. 
Figure 1.3: The specific biomedical literature retrieval and related issues 
All these situations, including the information overload, heterogeneousness of biomedical 
literature, and complexity of an information need, have placed severe challenges on 
biomedical literature retrieval with effectiveness and efficiency, thus leaving important 
research questions and urgent clinical problems unsolved, and limiting the production of 
SRs and the implementation of EBPs for quality healthcare. While existing studies [29], 
[32]–[38] 
have 
proposed 
to 
use 
automated 
or 
semi-automated 
article 
classification 
approaches to facilitate the identification of high relevant articles, they usually have limited 
Relevant high-quality articles 
Up-to-date Knowledge Synthetization 
Reliable Decision Making 
(SR and EBP) 
Articles 
(Article Collection) 
The most time and 
labor-intensive process 
Rigorous retrieval of 
biomedical literature 
An effective, efficient, and 
generalizable “automation” 
is needed 
13 
effectiveness (computationally resources or capabilities), efficiency (model selection), or 
generalization (dependencies on prior supervised dataset). 
Therefore, we have identified a significant domain problem, which is recognized 
as 
information retrieval towards high relevant biomedical literature
, and is referred to as 
biomedical literature retrieval
. There are critical requirements to improve its effectiveness 
and efficiency to support a wide range of applications, including knowledge synthetization 
and decision making in the biomedical and clinical domain, and to approach the objective 
of Evidence-based Medicine and meet the goal of IOM. 
1.2 Inspirations from State-of-the-art Techniques 
In this section, we present a retrospect of the state-of-the-art techniques that have inspired 
our research work of using biomedical literature mining to facilitate biomedical literature 
retrieval. In particular, many text analytics (text mining) approaches show to generate high-
quality information and patterns from an extensive text corpus to assist in downstream 
applications including information retrieval. Meanwhile, visual analytics (information 
visualization) approaches show to enable an advantageous exploitation and exploration of 
abstract data, such as text analytics results, with leveraged human cognition, analytic 
reasoning, and interactions. The integration of text analytics and visual analytics has a high 
potential to promote more favorable performance in task accomplishment and expedite 
human involvement in further knowledge exploration. 
1.2.1 Text Analytics 
Text analytics, which is also referred to as text mining in a broader sense, is an analysis of 
data contained in natural language, such as unstructured texts ranging from words, phrases, 
14 
sentence, 
paragraphs, 
to 
documents 
(articles), 
to 
derive 
and 
discover 
high-quality 
information in a machine understandable manner [11], [39]. More specifically, the high-
quality information is typically precise, decisive, relevant, novel, or interesting information 
that enables insights into the underlying values of texts. For example, the essential content 
meaning, the relevancy in a given information retrieval task, the similarity to others, the 
intensity, the sentiment, and so on. Such high-quality information can be further utilized to 
derive interesting patterns, such as the clustering patterns across a text corpus. While labor-
intensive manual text mining approaches were arisen in early years, the rapid development 
of computational techniques have advanced this field during the past decade [40]. 
Modern text analytics usually involves three basic processes [12], including to (1) 
structure texts with transformation methods (e.g. parsing) to extract useful linguistic 
features, (2) derive high-quality information and underlying patterns from structured data 
with statistical or machine learning methods, and (3) utilize, evaluate, or interpret the 
established information and patterns in downstream applications, such as information 
retrieval involving text classification, clustering, matching, ranking, recommendation, etc. 
In 
this 
sense, 
text 
analytics 
is 
also 
considered 
an 
interdisciplinary 
that 
draws 
on 
computational linguistics, natural language processing, machine learning, data mining, 
statistics, and information retrieval. It intends to address the diversity of textual patterns 
(i.e. lexical, syntax, and semantic) from different resources, and derive the most effective 
information in an automated or semi-automated manner. In our application scenario of 
information retrieval, our ultimate purpose of text analytics is to convert unstructured text 
15 
into structured features or representations encoding high-quality information for in-depth 
analysis [39]. Figure 1.4 shows a general workflow of text mining (text analytics). 
Worth mentioning, text analytics has drawn increasing interests during the past 
years with respect to the challenge of exploiting the large volume of heterogeneous 
information that is organized as unstructured texts, and the necessity of utilizing large 
digitalized text collections to discover new facts and trends about the world itself [41]. Text 
analytics demonstrates to empower a better, faster, and more productive support for 
knowledge discovery and decision making in many domains, including social media, 
digital humanities, computational sociology, and biomedical and clinical settings [11]. 
Figure 1.4: A general workflow of text mining 
In particular, the biomedical domain has met the foremost requirements on analyzing an 
increasing number of biomedical articles for comprehensive and up-to-date knowledge 
This diagram is derived from 
The Common Text Mining Workflow by Ricky 
Ho. 
16 
synthetization. Existing applications in this area include identifying biological, medical, or 
chemical 
entities 
(e.g. 
gene, 
protein, 
disease, 
or 
drug 
name), 
discovering 
specific 
relationships between entities, and extracting other information of interests (e.g. medical 
procedure) [13], [42]–[44]. Other applications include biomedical literature annotation, 
categorization, and classification [1], [45]. In the recent years, text analytics naturally 
extends to produce high-quality information and patterns from biomedical articles, such as 
clinical trials and electronic health records, to facilitate the identification, interpretation, 
and utilization of relevant high-quality articles [46]. In this dissertation, we will be 
specifically focusing on the generation of effective article features, representations, and 
similarities that are able to encode the underlying article meaning and indicate article 
relevancies with respect to the information retrieval towards biomedical literature. 
1.2.2 Visual Analytics 
Visual analytics, which is an extension to information visualization, produces visual 
representations of abstract data to improve and leverage human cognition, reasoning, and 
interactions in analytics tasks. It also enables information exploration and knowledge 
discovery from multiple levels (elementary, intermediate, and overall) and intuitively 
reveals the internal structure and causal relationships to human eyes [17], [18]. Information 
visualization and visual analytics have been increasingly important disciplines in data 
analysis and Human-Computer Interaction. As pictures and pixels can provide more 
information with less clutter in less space, information visualization improves human’s 
capability of accessing and analyzing largescale datasets and information repositories. 
Moreover, with the integration of interactive techniques, such as panning, zooming, 
17 
selection, multi-view, and manipulation, visual analytics further allows human to explore 
a large amount of information from any designated or customizable facets [19]. 
There 
are 
many 
different 
forms 
of 
information 
visualization. 
In 
particular, 
numerical data can be presented by bar charts, pie charts, line charts, and scatter plots; 
geographic data can be encoded as maps, and relational data can be visualized as graph-
based networks or distance-based map projections [47]. Among these visualizations, 
networks and maps have been commonly used to visualize relationships among data items 
[17], for instances, the collaboration among authors, the citing and being-cited among 
scientific publications, the similarity or relatedness among text documents, the voting and 
being-voted among webpages (e.g. Wikipedia), the interaction among social network users 
or events (e.g. Facebook and Twitter), co-purchasing among products (e.g. Amazon), 
etc.[48] By encoding the most important data relationships into the most compelling spatial 
channel, graphs and maps intuitively reveal the internal structure and implicit patterns 
within a data collection. Besides, graphs and maps have also shown implications in 
hypothesis testing, problem solving, and decision making, with respect to the imperative 
requirements of analyzing largescale datasets [49]–[51]. 
In this dissertation, we exploit the advantages of visual analytics in literature mining. 
By integrating visual analytics as a follow-up of text analytics, we can intuitively display, 
interpret, and explore the high-quality information and patterns derived from text analytics, 
and obtain knowledge and insights which are originally limited by the abstract information. 
More 
specifically, 
by 
using 
the 
article 
features, 
representations, 
and 
similarities 
(relationships) produced by text analytics, we can visualize an article collection into a 
18 
visual article distribution which can perform as a mind map that reveals the internal 
structure and the important clustering patterns to human eyes. Such a visualization will 
facilitate the identification of high relevant articles if they are aggregated into visual 
densities or clusters due to their strong similarities in underlying content meanings or task-
specific relevancies. Besides exploiting the visual patterns for task accomplishment, human 
researchers 
can 
also 
conduct 
interactive 
visual 
explorations 
to 
expedite 
knowledge 
discovery. Furthermore, the visual analytics can also help evaluate and understand the text 
analytics results, such as the applied article representations, which might have been used 
as a black-box. The visual feedback would advise the application and text analytics results 
and potential refinement of text analytics models in a qualitative and comprehensive way. 
1.3 Related Works and Established Gaps 
Literature mining with text and visual analytics can assist in literature retrieval by deriving 
high-quality information and patterns, and further enable knowledge discovery with 
improved cognition. Under this notion, existing studies have proposed a wide range of 
approaches to facilitate the identification of relevant articles, or to resolve a related problem 
of enhancing the cognition of article collections. The best-known and most widely used 
approaches 
include 
automated 
article 
classification 
and 
bibliometric 
information 
visualization. In this section, we summarize the related works and identify some limitations 
and gaps in biomedical literature retrieval. 
1.3.1 Automated Article Classification 
Automated or semi-automated article classification is most commonly used to facilitate the 
identification of high relevant biomedical articles from a text corpus. The classifiers are 
19 
normally trained as supervised machine learning (ML) models to classify articles as 
relevant or irrelevant. Article features are usually derived from article titles and abstracts. 
Existing studies have shown that automated article classification is a valuable tool to 
accelerate the identification of relevant articles and save manual workloads. 
Cohen 2006 [29] proposed a voting perceptron-based automated classification 
system to classify each article as containing high-quality and drug-specific evidence or not. 
Cohen 2008 [32] implemented an SVM classifier for a similar task with Cohen 2006. 
Wallace 2010 [34] used an ensemble of SVMs to automatically discriminate relevant 
articles from irrelevant articles. Matwin 2010 [33] implemented a factorized version of the 
complement Naïve Bayes classifier with weight engineering techniques (FCNB/WE) to 
reduce the time spent by experts in identifying journal articles of drug class efficacy for 
disease treatment. Miwa 2014 [38] applied an SVM classifier and a logistic regression 
classifier for the classification of clinical medicine and social science (public health) 
research. Khabsa 2016 [37] utilized a random forest classifier with heuristically configured 
parameters to predict relevant studies. In all of these studies, the problem of identifying 
relevant articles is modelled as an imbalanced data classification task where the cost of 
misclassifying the minority (relevant) class is higher than the cost of misclassifying the 
majority (irrelevant) class. These approaches were evaluated on benchmark datasets, such 
as the 15 DERP datasets for drug effectiveness review [29] which were annotated by 
domain experts, and have demonstrated to yield good performance in precision, recall, F1 
score, area under the receiver operating curve (AUC), and/or workload saved over 
sampling at 95% recall (WSS95). 
20 
However, supervised machine learning approaches rely on the availability of 
expensive manual annotations, and the learned models have limited generalizability to new 
topics or questions. Besides, an optimization process or intra-topic training process might 
be required to present the best performance in different datasets. For instance, Cohen 2008 
[32] applied intra-topic training for individual datasets and concluded that topic-specific 
training was required for best performance. Matwin 2010 [33] used weight engineering to 
produce optimized feature weights, and concluded that weight parameters should be 
tunable and modified with respect to different datasets. The heuristic method for parameter 
assignment in Khabsa 2016 [37] also depended on a specific dataset. In order to promote 
the usability in practical scenarios, (semi-supervised) active learning has been introduced 
to make effective use of human efforts. It interactively queries humans for classification 
labels as training datasets, and then triggers an automated classification process. For 
instance, Wallace 2010 [34] and Wallace 2012 [52] presented an online classification 
strategy upon an SVM classifier which was trained interactively. Miwa 2014 [38] 
performed a pool-based active learning method where sparse positive instances were 
identified and presented for annotations with a higher priority. Other enhancements of 
active learning models include weighting, detection of proper training size, covariate shift, 
etc. 
With the aforementioned article classification approaches, lexical features derived 
from article titles and abstracts were commonly used. Specifically, unigram words or n-
gram phrases (normally 2-gram or 3-gram) are tokenized from titles and abstracts after 
some preprocessing steps such as the removal of stop-words and stemming. These words 
21 
and phrases, along with their term frequency (TF) and/or term frequency–inverse document 
frequency (TF-IDF), are used as lexical article features. Unigram features have been most 
widely used for existing article classifiers and demonstrated to bring comparable or better 
performance to n-gram features. [32], [37] In addition, medical subject headings (MeSH) 
and publication types also show to indicate article relevancy and are used in some scenarios. 
On the other hand, syntactic features such as part-of-speech (POS) hasn’t been commonly 
used in article classification. While Kim 2011 [53] incorporated POS taggers to unigrams 
for sentence classification to support evidence-based medicine, no obvious performance 
improvement was achieved. 
Besides lexical article features, some recent studies also incorporated semantic 
features to advance the feature space and improve the classification performance. For 
instance, article titles and abstracts in natural language may include different terminologies 
(e.g. dry mouth and xerostomia) that have a similar semantic sense (e.g. oral symptom), 
which is beyond the capacity of exact matching or lexical features. Jonnalagadda 2013 [36] 
and Khabsa 2016 [37] used semantic features learned with word embedding and Brown 
clustering which capture distributional semantics from text corpus. While these approaches 
leveraged corpus-based word semantics, they were unable to capture the semantics of more 
sophisticated linguistic aspects, such as local contexts in article sentences. Besides, the 
corpus-based word semantics usually relies on a large and high-quality text corpus. While 
there is generic word semantics that has been pre-trained (e.g. word2vec and GloVe), its 
accuracy in the biomedical and clinical domain remains uncertain. 
22 
In summary, automated article classification is shown to be a valuable tool in 
facilitating the identification of relevant articles from an article collection. However, it has 
limited generalization due to the dependency on prior supervised training dataset. To 
improve the generalization and make the best use of human efforts, semi-supervised active 
learning is drawing increasing interests to train classifiers in an interactive manner. On the 
other hand, existing classifiers are mainly based on lexical features and open-domain 
(generic) semantic features derived from article titles and abstracts. Experimental results 
from the existing approaches also suggested that (1) semantic features have a high potential 
to advance information retrieval towards biomedical articles, and (2) a well-developed 
feature system has a high potential to outperform the others, regardless of the selection of 
downstream models. 
1.3.2 Bibliometric Information Visualization 
The visualization of bibliometric information shows to be a useful method to analyze large 
collections 
of 
scientific 
publications 
(published 
articles). 
In 
particular, 
bibliometric 
networks encode particular bibliometric relationships (e.g. citation and collaboration) 
among articles, and show to enable a rapid understanding of article collections as well as 
the status and evolution of a research topic or research community. It is also considered 
that visual analytics of bibliometric information can supplement the peer-review process 
to demonstrate the value of scientific research. [54] 
The visual elements in a bibliometric network include nodes and edges, where 
nodes represent the entities (e.g. articles, words, and authors) being visualized and edges 
represent a certain type of relationship between endnodes. Based on the information being 
23 
visualized, the most commonly studied bibliometric networks include citation networks, 
keyword co-occurrence networks or semantic networks, and collaboration networks.[55] 
Specifically, citation networks show the relationships among articles based on their 
citations. There are three sub-types of citation relations, including the direct citation 
relation, co-citation relation, and bibliographic coupling relation. Citation networks are 
considered one of the most effective methods to describe and evaluate a collection of 
articles, and can reveal clusters of articles that share similar research topics and the impact 
of individual articles. On the other hand, keyword co-occurrence networks, which are also 
known as semantic networks, aim to analyze keywords specified by authors or derived 
from article contents. By representing words as nodes and representing the number of word 
co-occurrences as weighted edges, keyword co-occurrence networks reveal the most 
common words with high centralities. Besides, nodes with high centralities and clusters 
with high clustering coefficients tend to reflect focuses and trends of the examined research 
area. Finally, collaboration networks show the collaboration of authors, institutions, or 
countries across an article collection. Among the different types of collaboration networks, 
the co-authorship network has been most commonly studied, where nodes represent 
authors and edges represent the number of articles that two authors have collaborated. A 
co-authorship network also reveals the authors who have high collaborations with others, 
and exhibit clusters representing research communities working on specific research topics. 
Gaps between nodes or clusters might also indicate opportunities for further collaborations. 
There are three fundamental approaches to visualize bibliometric information, 
including the graph-based method, distance-based method, and timeline-based method. [55] 
24 
With the graph-based method, article nodes and edges (article similarities) are positioned 
into a 2D space by graph drawing algorithms e.g. the force-directed algorithm. Topological 
properties such as the graph diameter and node centralities are available for analysis and 
utilizations with a graph-based method. With the distance-based method, article points are 
projected into a 2D space via dimensionality reduction algorithms, e.g. Multidimensional 
Scaling (MDS) and t-Distributed Scholastic Neighbor Embedding (t-SNE). For both graph-
based and distance-based methods, the 2D placements of article nodes (points) are 
approximated based on their theoretical relationships (similarities) in the original feature 
space. Finally, the timeline-based method locates articles to specific time points (e.g. 
reflecting the publication dates) on the vertical dimension, and utilizes the horizontal 
dimension to encode article relatedness. Other visualization approaches include the circular 
visualization and self-organization maps. 
While the visualization and visual analytics of bibliometric information have shown 
to promote the understanding and exploration of scientific publications (articles), there are 
limited investigations into the articles contents, nor to facilitate the information retrieval 
which values the underlying meanings of articles. 
1.4 Dissertation Overview 
Motivated by the significant domain application of biomedical literature retrieval, and 
inspired by the implications of text and visual analytics in resolving related problems, we 
propose to facilitate the identification of high relevant articles and address the gaps 
established in related works. Under this notion, we aim to advance the effectiveness and 
human involvement in biomedical literature retrieval, as well as promote an efficient and 
25 
generalizable solution. In this section, we describe the objective and scope of this 
dissertation, draw research questions and hypotheses, and present an outline of our 
biomedical literature mining framework, which consists of multiple text and visual 
analytics components to exploit different resources or design factors. 
1.4.1 Objective, Scope, and Hypotheses 
The fundamental objective of this dissertation can be formulated as: implement biomedical 
literature mining to facilitate biomedical literature retrieval in an effective, affordable (time 
and resource efficient), and generalizable manner. Under this notion, we further specify 
our scope as below with respect to the recognized problem, and describe several hypotheses 
for the design of the proposed framework of biomedical literature mining. 
Firstly, we focus on the significant domain application of biomedical literature 
retrieval, which is also referred to as information retrieval towards high relevant biomedical 
literature. As we have described in Section 1.1, this application specifically focuses on the 
rigorous process to identify high relevant biomedical articles from a text corpus (pooled 
article collection) for up-to-date knowledge synthetization and reliable decision making, 
expecting to achieve a high recall and precision. Because the pain points mainly lie in the 
information overload and the limited time and labor resources, which are further challenged 
by the complexity of an information need as well as the heterogeneous of biomedical 
literature, it is of critical importance to facilitate the biomedical literature retrieval process 
with effectiveness and efficiency for quality healthcare. 
Secondly, our proposed framework of biomedical literature mining integrates and 
extends state-of-the-art techniques of text analytics (text mining) and visual analytics 
26 
(information visualization) to bridge the crucial gaps between extensive biomedical 
literature and effective and efficient applications in practical settings for quality healthcare. 
In 
particular, 
given 
a 
text 
corpus 
(article 
collection) 
consisting 
of 
extensive 
and 
unstructured biomedical articles (text documents): 1) We propose to utilize text analytics 
to generate structured article features, representations, and similarities (relationships) that 
can effectively encode the underlying articles meanings and indicate the article relevancies 
in 
information 
retrieval 
tasks. 
For 
doing 
so, we 
implement 
multiple 
text 
analytics 
approaches to leverage different knowledge and computational resources, aiming to 
provide effective, affordable, and generalizable model(s) that can be used in widespread 
applications 
in 
the 
biomedical 
and 
clinical 
domain. To 
evaluate 
our 
text 
analytics 
approaches and employ the text analytics results, we develop an interactive article 
recommendation model with respect to the recognized application of biomedical literature 
retrieval. At this point, we 
hypothesize
that the text analytics approaches can facilitate 
biomedical literature retrieval by bridging the gaps between (unstructured) text articles and 
(structured) effective article features, representations, and similarities. 2) We then propose 
to utilize visual analytics to enable advantageous exploitation and exploration of the text 
analytics results, which would be high-quality but abstract information, in an approachable 
and human-involving manner. For doing so, we implement multiple visualization schemes 
to visualize article representations and relationships in a 2D space. We also promote 
effective visualizations and expose them to real-world settings to meet the expectations of 
scalability, interpretability, and interactivity. We use empirical experiments and case 
studies to evaluate and demonstrate the usefulness of the visualizations and visual 
27 
analytics. At this point, we 
hypothesize
that the visualizations and visual analytics of article 
features, representations, and similarities (relationships) can overcome the black-box 
nature of text analytics, thus facilitate biomedical literature retrieval as well as further 
knowledge discovery, by levering human cognition and analytic reasoning and bridging 
the gaps between abstract information and advantageous exploitation and exploration. In 
addition, we 
hypothesize
that such visualizations and visual analytics can also promote 
intuitive visual evaluation and interpretation of the applied text analytics results. 
Therefore, by putting things together, our 
top-level hypothesis
is: such an integrated 
framework of 
biomedical literature mining
with both text analytics and visual analytics 
can facilitate the significant domain application of 
biomedical literature retrieval
, by 
bridging the gaps between extensive biomedical literature and high-quality information 
that can be exploited and explored to facilitate the identification of high relevant articles 
and knowledge discovery. Under this notion, this framework would also perform as an 
effective, affordable, and generalizable solution to a wide range of literature retrieval, 
analysis, and exploration tasks in the biomedical and clinical settings. 
Figure 1.5 (a), (b), and (c) presents a storyline and overviews about how we design 
and develop this integrated framework of both text and visual analytics, including the 
rationalities and workflows, the multiple components to explore the breadth and depth of 
a solution space, the knowledge and computational resources being exploited, and the 
design factors being considered. 
28 
(a) An illustration of the integrated framework of text and visual analytics 
(b) An overview of text analytics 
(c) An overview of visual analytics 
Figure 1.5: An integrated framework of text and visual analytics 
29 
1.4.2 Outline 
As shown in Figure 1.5 (b) and (c), our integrated framework of biomedical literature 
mining consists of multiple components of text analytics and visual analytics to investigate 
the breadth and depth of a solution space. In other words, while text analytics and visual 
analytics are considered as two mutually supportive stages with respect to the information 
flow and evolution, for each of the two analytics stages, we implement multiple techniques 
in 
sequence 
or 
in 
parallel 
to 
leverage 
advantageous 
knowledge 
and 
computational 
resources and account for different design factors. In this section, we present an outline of 
this dissertation which is organized by the multiple components of text analytics and visual 
analytics respectively. 
Part 1: Text Analytics 
We 
utilize 
text 
analytics 
to 
generate 
effective 
article 
features, representations, 
and 
similarities that can encode underlying article meanings and indicate article relevancies, to 
facilitate the downstream application of biomedical literature retrieval. Meanwhile, we also 
promote the affordability (time and resource efficiency) and generalization of applied text 
analytics with respect to the widespread applications in biomedical and clinical settings. 
Lexical Analysis on Multiple Article Elements of Biomedical Literature
: We 
investigate the fundamental lexical space consisting of multiple article elements regarding 
the special format and property of biomedical literature. We then evaluate the usefulness 
of single and combined article elements in predicting article relevancy, and demonstrate 
the dominance of article titles and abstracts, which are considered the most informative 
30 
and concise article elements written in natural language. This motivates us to consider 
article titles and abstracts for further semantic analysis. 
Ontology-based Semantic Analysis with Gold-Standard Domain Knowledge
: 
We propose an ontology-based semantic approach that leverages the gold-standard domain 
knowledge encoded in UMLS ontologies i.e. SNOMED-CT and MeSH, to overcome the 
limitation of lexical analysis and capture the underlying article meanings. Under this 
notion, we develop a novel pipelined concept modelling process that represents biomedical 
articles with optimized and enriched UMLS concepts, and demonstrate the advantageous 
of this ontology-based semantic approach comparing to the lexical baselines and existing 
corpus-based semantic methods. 
Corpus-based Semantic Analysis with Neural Document Embedding
: We 
propose a corpus-based semantic approach that leverages the superior computational power 
of neural networks. Under this notion, we develop a new neural document embedding 
model with multi-task learning that not only captures the context semantics but also 
integrates task specifications of information retrieval with minimal supervision. Without 
relying 
on 
external 
knowledge 
or 
full 
supervisions, 
we 
demonstrate 
the 
improved 
effectiveness of this corpus-based semantic approach comparing to the lexical baseline as 
well as the neural embedding model (e.g. Paragraph Vector) without task specifications. 
Part 2: Visual Analytics 
We then utilize visual analytics to enable advantageous exploitation and exploration of the 
text analytics results, such as article representations and similarities, to further expedite the 
31 
application of biomedical literature retrieval in a human-involving manner, and meet the 
real-world expectations of scalability, interpretability, and interactivity. 
Demonstrating 
the 
Usefulness 
of 
Visualization 
in 
Biomedical 
Literature 
Retrieval
: We demonstrate the 2D visualization of established article representations and 
similarities can reveal visual patterns, such as the article distribution (visual positions) and 
clusters (visual densities), and facilitate the identification of high relevant articles with 
leveraged human cognition and analytical reasoning. We utilize a basic visualization 
scheme and employ network properties and network clustering patterns to demonstrate the 
centralization (aggregation) of high relevant articles in the 2D visualization. 
Promoting Effective Visualizations with Sparsified Article Networks and 
Article Maps
: We promote effective visualizations of article collections with compelling 
visual patterns (e.g. 2D placements of article points) to facilitate the identification of high 
relevant articles. We compare the visualization types of graph-based article networks and 
distance-based article maps, which preserve certain article similarities in the approximated 
2D visualizations. We then focus on the unmet and configurable network sparsification 
schemes applied to the new field of article similarity networks, and propose a new 
sparsification scheme of article networks, which shows to preserve important article 
similarities and results in effective 2D placements for biomedical literature retrieval. 
A Usable Visual Analytics System for Literature Retrieval and Text Corpus 
Exploration
: We further expose visualizations and visual analytics to real-world settings 
to meet the expectations of scalability, interpretability, and interactivity. Therefore, we 
follow the Visual Analytics Mantra and propose a visual analytics system for human 
32 
researchers to conduct literature retrieval and knowledge exploration. This system can also 
be used for visual evaluations and interpretations of the applied text analytics results. 
In Figure 1.6 below, we present a graphic overview of this dissertation with respect 
to the recognized problem, the proposed objective and scope, and the outline. 
Figure 1.6: Dissertation overview 
33 
1.4.3 Statement 
Information retrieval towards biomedical literature has high significances in quality 
healthcare, especially for the Evidence-based Medicine, but is challenged by the extensive 
and heterogeneous information recourses and leaving critical information needs unmet. 
This dissertation develops an integrated framework of text and visual analytics to bridge 
the gaps among extensive biomedical articles, high-quality article representations, and 
advantageous exploitations and explorations involving humans. This integrated framework 
not only facilitates the identification of high relevant articles in biomedical literature 
retrieval, but also extends state-of-the-art techniques in computer and information science 
to meet real-world settings by reinforcing advanced resources and merits, and providing an 
effective, affordable, and generalizable solution (Figure 1.7). 
Figure 1.7: Dissertation statement
34 
Chapter 2. Lexical Analysis on Multiple Article Elements of Biomedical Literature 
In this chapter, we explore the fundamental lexical space of biomedical literature, with 
respect to its special format consisting of multiple article elements, including title, abstract, 
Medical Subject Heading Terms (standardized keywords), publication types, and authors, 
which have been commonly used to pinpoint the characteristics or indicate the meaning 
and relevancy of biomedical articles [24]. As our initial attempt to conduct text analytics 
on biomedical articles, we aim to evaluate and demonstrate the usefulness of different 
article element(s) in facilitating the identification of high relevant articles, given the 
information retrieval tasks towards biomedical literature. This lexical analysis would 
perform as a foundation for the later semantic analysis. 
Generally speaking, lexical analysis is a process to convert a stream of text (e.g. 
articles or documents) into words, phrases, symbols, or other meaningful elements which 
are called tokens, via tokenization or text segmentation. Therefore, our purpose of lexical 
analysis is to tokenize and utilize effective words and phrases as lexical article features 
from selected article element(s), which can be a single element or a combination of multiple 
elements. The extracted features further compose lexical article representations and 
establish lexical article similarities in the feature space, which can be readily applied to the 
downstream application of biomedical literature retrieval. 
35 
Throughout the text analytics part in this dissertation, we simulate an interactive 
article 
recommendation 
process 
with 
a 
straightforward 
active 
learning 
model 
as 
a 
downstream application of biomedical literature retrieval, to employ and evaluate the 
established article representations and similarities. As this active learning model doesn’t 
rely on prior supervised training data or sophisticated model tuning, we are able to promote 
a downstream application with better efficiency and generalization. In this chapter, we 
conduct experiments on benchmark datasets and demonstrate utilizing article lexical 
features, representations, and similarities to facilitate the identification of high relevant 
articles. While the effectiveness (e.g. predictive capability) of different article elements and 
their combinations differs, article title and abstract are shown to be the most informative 
and concise elements with dominant contributions in predicting the article relevancy. 
2.1 Research Questions 
We formulate and highlight our research questions as below: 
•
Consider the special format (property) of biomedical articles (scientific publications), 
which article elements can be utilized to derive effective article feature, representations, 
and similarities in the fundamental lexical space, with respect our targeted application 
of biomedical literature retrieval? 
•
If different article elements have diverse advantages and are valued in distinct 
scenarios, will a collaboration or combination of multiple article elements produce 
more favorable performance? If so, what combinations and corresponding weight 
settings can be recommended for general-purpose utilizations? 
36 
•
With generated article features and representations in the lexical space, will our active 
learning model of interactive article recommendation, as a downstream application for 
biomedical literature retrieval, be as effective (comparable or competitive) as other 
well-recognized supervised learning models? 
2.2 Background and Related Works 
2.2.1 Article Elements of Biomedical Literature 
Digitalized biomedical literature [1] is considered a special type of text documents or text 
articles with well-established organization, arrangement, and indexing. Like many other 
scientific 
publications, 
biomedical 
articles 
have 
standardized 
formats 
consisting 
of 
recognizable article elements. Specifically, article elements are also known as article fields 
that document the major pieces of information of an article. While there are many different 
types of article elements, different resources (e.g. bibliographic databases) can provide 
different coverages and classifications of mandatory elements and optional elements. [24] 
In this dissertation, we access to the Medical Literature Analysis and Retrieval 
System (MEDLINE) [7], [56], which is the primary electronical bibliographic database of 
life sciences and biomedical information, and use it as our bibliographic resource. 
MEDLINE maintains and displays over 60 types of article elements, which are also known 
as MEDLINE elements, tagged and encoded in the MEDLINE format [24] which is both 
human-readable and machine-readable. Among the article elements, some are commonly 
used for the purpose of information retrieval, including: title (TI), abstract (AB), Medical 
Subject Heading Term (MH), author-supplied keywords (KY or OT), publication type 
(PT), author (AU), etc. [33], [57] Please note that the author-supplied keywords are not 
37 
always available in many biomedical articles such as clinical trials, instead, MHs [58] as 
standardized terminologies are typically assigned to index articles for information retrieval 
[59]. In this sense, author-supplied keywords are named as other terms (OT) in the 
MEDLINE format. 
2.2.2 Lexical Feature with BoW and n-gram 
In lexical analysis of texts, the 
bag-of-words
(BoW) model is a simplifying and orderless 
representation 
of 
texts 
(e.g. 
sentences, 
paragraphs, 
and 
documents 
or 
articles) 
that 
transforms texts into a bag of its words, disregarding the grammar and word order. Along 
with words in the bag, BoW also counts word frequency and uses that for the weighting of 
words. The most commonly used weighting schemes include the basic 
term frequency
(TF) 
and the more advanced 
term frequency-inverse document frequency
(TF-IDF), which 
reflect how important a word is to a document in a corpus. Under this notion, BoW has 
been widely used for feature generation to characterize texts in natural language processing 
and information retrieval. By creating a lexical feature space with BoW, each text piece 
from a text corpus can be represented as a feature vector of numerical values encoding the 
weights of lexical features e.g. words. Therefore, BoW is readily used to prepare input texts 
to 
many 
downstream 
applications, 
such 
as 
text 
classification, 
clustering, 
ranking, 
recommendation, matching, and other machine learning or statistical models. 
One important extension of the orderless BoW model is the n-gram model, which 
intends 
to 
preserve 
some 
local 
sequential 
(spatial) 
information, 
for 
example, 
the 
composition of a phrase with multiple words in order. More specifically, in computational 
linguistics and probability, an n-gram is a contiguous sequence of n items (e.g. words) from 
38 
a sequence of text. An n-gram of size 1 is referred to as a unigram, which has the same 
utilization of the BoW model. Other commonly used n-gram models are the size-2 bigram 
model and the size-3 trigram model. Besides parsing texts into n-word units and store the 
frequency of each unit in a feature space like we mentioned earlier, an n-gram model is 
also used as a probabilistic language model (e.g. sequence labeling) to predict the next item 
given a prior sequence of (n-1) items. 
While the BoW model and the n-gram model are shown to produce effective lexical 
features for information retrieval tasks especially regarding long text pieces (as strong 
lexical baselines), they do suffer from a notorious curse of high dimensionality [60], which 
is roughly proportional to the size of a corpus or the corresponding word vocabulary. We 
will address this issue in the later part of this dissertation with semantic text analysis. 
2.2.3 Vector Space Model and Cosine Similarity 
Vector space model, which is also known as term vector model, is an algebraic model that 
represents a set of text documents as vectors in a common vector space for
information 
retrieval and other operations. A feature space of a document corpus established with a 
BoW or n-gram model can be considered as a vector space, where each dimension 
corresponds to a specific feature, and each document is represented as a feature vector such 
that if a feature occurs in the document, its value in the feature vector will be a non-zero 
representing the frequency or weight of the feature. 
One important application of the vector space model is to determine document 
similarities (relationships) or relevancy to an information need. For doing so, associative 
coefficients based on the (normalized) inner product of document feature vectors are 
39 
measured, generally based on the overlap or co-occurrence of features. The most popular 
similarity measure is the cosine similarity (cosine coefficient), which measures the cosine 
of the angle between a pair of vectors, as indicated by the formulas below. It reflects the 
degree of similarity based on the presence and frequency of features in each text. The 
resulting similarity score ranges from 0 to 1, where 0 indicates independence and 1 means 
exactly the same. Other measures include the Jaccard coefficient and Dice coefficient. 
𝐱 ∙ 𝐲 =< 𝐱, 𝐲 >=<
(
𝑥
*
, 𝑥
+
, … , 𝑥
-
)
,
(
𝑦
*
, 𝑦
+
, … , 𝑦
-
)
>= 0𝑥
1
𝑦
1
-
12*
cos
(
θ
)
=
𝐱 ∙ 𝐲
7
|
𝒙
|
7
|
|
𝒚
|
|
=
∑
𝑥
1
𝑦
1
-
12*
<
∑
𝑥
1
+
-
12*
<
∑
𝑦
1
+
-
12*
2.3 Lexical Features of Biomedical Literature 
2.3.1 Selecting and Processing Article Elements 
We generate lexical article features from multiple article elements, which are standardized 
by MEDLINE [24]. As the most informative elements, title (TI), abstract (AB) and Medical 
Subject Heading Terms (MH) have been widely used in related work to build feature spaces 
for information retrieval, more specifically, article classification and clustering in the 
applied areas [32]. Publication type (PT) is also selected by some studies [33], [57] as it 
can be a key factor to determine the inclusion and exclusion of an article for some 
information needs. In addition, in our preliminary work we found that author (AU) 
information also had predictive values in indicating article relationships and relevancy, 
especially considering that the collaborations and contributions of particular research 
groups are attracting increasing interests in the recent decades. Therefore, we consider five 
40 
article (MEDLINE) elements, including title (TI), abstract (AB), Medical Subject Heading 
Term (MH), publication type (PT) and author (AU), to generate lexical features for 
biomedical articles. 
To tokenize the article elements and extract article features, we preprocess the texts 
of TI and AB, which are written in natural language, by removing stop-words [61] and 
stemming the remaining words with the classic Porter Stemmer [62]; for MH, PT and AU, 
we use the exact strings as they are already standardly encoded. With the BoW approach, 
we record the term frequency (TF) of the unigram word from TI and AB as well as the 
multi-word string from MH, PT and AU, which will be used to generate a lexical feature 
space with an adjustable scheme. Please note that we use unigrams from TI and AB because 
existing studies have commented that n-grams or NLP-based features (e.g. POS tagging 
and IOB tagging) didn’t bring obvious performance improvement comparing to the 
unigram baseline in the applications of biomedical article classification [32], [33]. In this 
study, we investigate feature spaces with adjustable features derived from a single article 
element or a (weighted or unweighted) combination of multiple article elements. 
2.3.2 Weighting the Combinations of Multiple Article Elements 
We try to approach weighted combinations when multiple article elements are considered. 
Because different information needs can have diverse scopes, for example, one may require 
sufficient descriptive evidence from AB while another may have strict requirements on PT, 
we are interested in whether different weighting schemes can affect or significantly alter 
the performance. In our preliminary study, we found that when one element’s weight was 
increased to achieve a higher performance for some datasets, some other datasets would 
41 
have dropped performance. Overall, we could not find a universal weight setting that 
benefits all datasets, and this might be explained by the diverse scopes of different 
biomedical literature retrieval tasks. Besides, although some weighted combinations bring 
better global performance, the enhancement from the baseline, where elements in the 
combination are equally weighted, is limited. A similar idea was also stated in [33]. 
In this sense, we use an equally weighted combination of multiple article elements 
to reach a composited lexical feature space, where article representations are established, 
and cosine similarities can be calculated. In other words, when a combination of 
n
article 
elements is selected for utilization, the resulting article similarity is calculated as an 
equally-weighted sum of the 
n
element-level similarities. With the cosine similarity, each 
element-level similarity ranges from 0 to 1, therefore, a composited article similarity ranges 
from 0 to 
n
. Therefore, when all of the 5 article elements are selected, the composited 
article similarity will range from 0 to 5. 
2.4 An Interactive Article Recommendation Model with Active Learning 
2.4.1 Integrating the Concept of Active Learning 
Upon lexical article features generated with the above-mentioned method, we develop a 
downstream application to utilize and evaluate the established article representations (i.e. 
feature vectors) and similarities with respect to biomedical literature retrieval. While many 
downstream applications for information retrieval are realized with machine learning and 
statistical models, such as Naïve Bayes (NB), Support Vector Machine (SVM), perceptron, 
etc. [29], [32]–[34], [63], we aim to promote an efficient and generalizable application that 
doesn’t rely on prior supervised training data or sophisticated model tunings in practical 
42 
settings. Therefore, we incorporate the concept the active learning (semi-supervised 
learning) to simulate an interactive article recommendation process, where articles are 
iteratively recommended, and relevancy feedback [64] is simulated with gold-standard 
labels (relevancy and irrelevancy) on the benchmark datasets. 
As shown in Figure 2.1, we first calculate article similarities in the lexical feature 
space with adjustable weights. With our active learning model, a recommended article is 
considered as a chosen data point querying for its label, and our model is iteratively refined 
based on the acquired labels. This scheme of exploiting the unlabeled data and determining 
the most informative data points to label is motivated by the nature of literature retrieval, 
where articles that are most likely to be relevant are recommended to reviewers with higher 
priorities. In this respect, we choose and recommend the most confident data points 
(articles) that are away from the delineated margin (hyperplane), instead of choosing the 
most uncertain data points that are close to the margin [65], [66]. 
Figure 2.1: Illustrated interactive article recommendation with active learning 
43 
2.4.2 Implementation of Interactive Article Recommendation 
In the implementation of this interactive article recommendation (active learning) model, 
we first recommended an article based on the keywords extracted from an information need 
(as described in Section 1.1.2). Once a recommended article is classified as “included” 
(relevance) or “excluded” (irrelevance), an inclusion list (IN) or an exclusion list (EX) is 
created. We then iteratively recommend articles based on the similarity to IN. Assuming 
V is the set of all articles, and U (U=V-IN-EX) is the set of articles that have never been 
recommended, the recommendation can be formulated as: 
Recommendation = arg max
H∈J
(
0
𝑣𝑥
LLLL⃗ ∙ 𝑤LL⃗
O∈PQ
)
In the formula, 
𝑣𝑥
LLLL⃗
is a n-dimensional vector with each dimension representing an element-
level similarity between article v and x; n is the number of article elements being 
considered in this composited lexical feature space. 
𝑤LL⃗
is a weight parameter that controls 
the contribution of each article element. With this model, we recommend the article of the 
highest similarity to the recognized relevant (included) articles. We also have a secondary 
model which also incorporates the penalty from recognized irrelevant (excluded) articles. 
However, it is not encouraged because of the highly-imbalanced nature of many datasets 
of biomedical literature retrieval, with only 0.55%~27.04% (7.67% on average) of relevant 
articles. If the penalty from irrelevant articles is taken into account, it is likely to override 
the contributions from the small portion of relevant articles. 
Recommendation = arg max
H∈J
(𝛼
0
𝑣𝑥
LLLL⃗ ∙ 𝑤LL⃗
O∈PQ
− 𝛽
0
𝑣𝑥
LLLL⃗ ∙ 𝑤LL⃗
O∈UV
)
44 
2.5 Experimental Settings and Results 
2.5.1 Datasets 
We use 15 benchmark datasets produced by the Drug Effectiveness Review Project team 
(DERP) [26]. The datasets were made publicly available by Cohen 2006 [29], and were 
utilized by many related studies, i.e., Cohen 2008 [32], Matwin 2010 [33], Jonnalagadda 
2013 [36], Miwa 2014 [38], and Khabsa 2016 [37]. These 15 datasets were completed as 
systematic review (SR) reports by experienced and knowledgeable human researchers, 
with inclusion (relevance) and exclusion (irrelevance) decisions made by at least two 
expert reviewers. Table 2.1 shows the total sizes and numbers/percentages of articles 
included at the (1) abstract level and (2) full-text level. Using the PubMed Identifier 
(PMID), we download the articles in MEDLINE format and extracted article elements. We 
consider the full-text level included articles as high relevant articles. 
Dataset 
Size 
Abstract N (%) 
Full-text N (%) 
ACE Inhibitors 
2544 
183 
(7.19%) 
41 
(1.61%) 
ADHD
851 
84 
(9.87%) 
20 
(2.35%) 
Antihistamines
310 
92 
(29.68%) 
16 
(5.16%) 
Atypical Antipsychotics
1120 
363 
(32.41%) 
146 
(13.04%) 
Beta Blockers
2072 
302 
(14.58%) 
42 
(2.03%) 
Calcium Channel Blockers
1218 
279 
(22.91%) 
100 
(8.21%) 
Estrogens
368 
80 
(21.74%) 
80 
(21.74%) 
NSAIDS
393 
88 
(22.39%) 
41 
(10.43%) 
Opioids
1915 
48 
(2.51%) 
15 
(0.78%) 
Oral Hypoglycemics
503 
139 
(27.63%) 
136 
(27.04%) 
Proton Pump Inhibitors
1333 
238 
(17.85%) 
51 
(3.83%) 
Skeletal Muscle Relaxants
1643 
34 
(2.07%) 
9 
(0.55%) 
Statins
3465 
173 
(4.99%) 
85 
(2.45%) 
Triptans
671 
218 
(32.49%) 
24 
(3.58%) 
Urinary Incontinence
327 
78 
(23.85%) 
40 
(12.23%) 
Table 2.1: 15 DERP datasets and their sizes and rates of inclusion (relevancy) 
45 
2.5.2 Evaluation Measures 
To evaluate the usefulness of the generated lexical article features, representations, and 
similarities, as well as the applied active learning model for article recommendation, we 
use two performance measures: work saved over sampling at 95% recall (WSS95) and F-
measure. These measures are commonly used for evaluating similar works. 
WSS95: WSS95 is a performance measure first proposed by Cohen [29] to 
calculate the overall labor saving while maintaining the recall at 95%. This assumes that a 
recall of >=0.95 is necessary for a document classification system. Precision should be as 
high as possible, as long as recall is at least 0.95. 
WSS =
(𝑇𝑁 + 𝐹𝑁)
𝑁
− 1 +
𝑇𝑃
𝑇𝑃 + 𝐹𝑁
WSS95 =
(𝑇𝑁 + 𝐹𝑁)
𝑁
− 0.05
In the above formulas, TP is the number of true positive (relevant) articles, TN is the 
number of true negative (irrelevant) articles, FN is the number of false negative (relevant) 
articles, and N is the total number of articles in each dataset. 
F-measure (F1): F-measure is a measure of information retrieval accuracy. It 
considers both precision and recall and combines them into a weighted harmonic mean. 
When precision and recall are weighted equally, the balanced F-measure is also called F1, 
where it reaches its best value at 1 and the worst value at 0. Since F1 is dynamically 
changed over time, we can detect the highest F1 from the steepness of performance curves. 
𝐹
*
= 2 ∙
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ∙ 𝑟𝑒𝑐𝑎𝑙𝑙
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑟𝑒𝑐𝑎𝑙𝑙
46 
Figure 2.2 presents performance curves with the y-axis representing the recall and the x-
axis representing the recommendation rate. The orange curve illustrates a high WSS95 
(overall), and the green curve illustrates a high F1 (early-stage). 
Figure 2.2: Illustrated high WSS95 and F1 
2.5.3 Single Element Performance 
We first evaluate the performance of lexical features derived from a single article element 
using WSS95. We present the results of each of the 15 datasets in Table 2.2. 
As shown in Table 2.2, TI gets the best average WSS95 performance (34.01%), 
followed by PT (33.41%) and AB (33.3%). MH has a much lower WSS95 than other 
elements (25.31%). AU receives 0% workload saved due to the dispersion among articles’ 
authorship. If there is no authorship similarity between articles, we are not able to 
recommend relevant articles based on solely the AU element. Using PT also brings good 
47 
performance; we speculate it is a key consideration when conducting biomedical literature 
retrieval e.g. systematic reviews. However, repeated ANOVA shows that the WSS95 
performances across TI, AB, PT, MH are not statistically different (p=0.079). 
Dataset 
TI 
AB 
PT 
AU 
MH 
ACE inhibitors 
76.49 
71.07 
33.22 
0 
47.37 
ADHD 
80.26 
65.10 
22.56 
0 
47.00 
Antihistamines 
13.55 
15.81 
32.58 
0 
2.58 
Atypical antipsychotics 
17.23 
20.54 
19.64 
0 
9.46 
Beta blockers 
44.74 
49.95 
43.77 
0 
28.67 
Calcium channel blockers 
19.38 
16.34 
18.64 
0 
20.94 
Estrogens 
29.35 
29.08 
17.93 
0 
38.59 
NSAIDS 
63.36 
66.67 
58.27 
0 
33.84 
Opioids 
8.30 
9.82 
37.23 
0 
6.48 
Oral hypoglycemics 
11.73 
12.13 
22.27 
0 
7.55 
Proton pump inhibitors 
43.74 
15.60 
35.48 
0 
20.56 
Skeletal muscle relaxants 
0 
36.03 
74.68 
0 
42.85 
Statins 
25.52 
30.17 
13.31 
0 
13.68 
Triptans 
45.60 
42.47 
28.17 
0 
33.23 
Urinary incontinence 
30.89 
18.65 
43.43 
0 
26.91 
Average 
34.01 
33.30 
33.41 
0 
25.31 
Table 2.2: Single element WSS95 performance 
Furthermore, Table 2.3 shows the highest F1 performance and its corresponding time-
stamp during the article recommendation process, which is reflected by the percentage of 
articles that have been recommended or screened. 
An earlier occurrence of a high F1 usually indicates a more promising performance 
that a high precision and recall are both achieved before more workload is contributed. In 
other words, a rapid identification of high relevant articles is achieved. We find that AB 
and PT gain the best F1 (0.3683 and 0.3437 respectively); MH and TI have lower F1 scores 
(0.3116 and 0.3039 respectively). Again, AU gets the worst F1, which is only 0.1365. We 
48 
also examine the corresponding time-stamp of the highest F1. We observe that the best F1 
value appears approximately when 5%~20% of articles are screened, which is at the early 
stage of recommendation. MEDLINE elements with a higher F1 score and a smaller 
percentage of articles screened in correspondence indicate high-accuracy performances 
during the early stage of recommendation (i.e. AB). We conclude that AB and PT bring 
the best early stage performance, in other words, the recommendation accuracy of AB and 
PT in the beginning is better than that of the other elements. However, repeated ANOVA 
shows that the F1 performances across TI, AB, PT, MH is not statistically different 
(p=0.073). Pairwise comparisons only find a significant difference between TI and AB 
(AU is not considered due to its inferior performance). 
Dataset 
TI 
F
1
(%)
AB 
F
1
(%)
PT 
F
1
(%)
AU 
F
1
(%)
MH 
F
1
(%)
ACE inhibitors 
0.3444 (4) 
0.3121 (4) 
0.2182 (<1) 
0.1872 (6) 
0.2368 (1) 
ADHD 
0.2885 (10) 
0.3824 (6) 
0.2963 (<1) 
0.0909 (<1) 
0.5556 (4) 
Antihistamines 
0.2593 (12) 
0.4000 (3) 
0.2759 (<1) 
0.1111 (<1) 
0.3333 (3) 
Atypical antipsychotics 
0.3447 (26) 
0.4248 (14) 
0.4363 (5, 12
a
) 
0.0135 (<1) 
0.3113 (40) 
Beta blockers 
0.1972 (1) 
0.2710 (5) 
0.2105 (<1) 
0.0417 (<1) 
0.0957 (19) 
Calcium channel blockers 
0.2026 (10) 
0.2672 (11) 
0.2662 (15) 
0.1261 (9) 
0.2579 (2) 
Estrogens 
0.5140 (36) 
0.5612 (29) 
0.4937 (18) 
0.0244 (<1) 
0.5536 (39) 
NSAIDS 
0.4368 (34) 
0.5870 (13) 
0.6761 (8) 
0.4853 (24) 
0.3650 (24) 
Opioids 
0.2727 (<1) 
0.1429 (<1) 
0.2222 (<1) 
0.1111 (<1) 
0.2500 (<1) 
Oral hypoglycemics 
0.4509 (88) 
0.4603 (76) 
0.5019 (78) 
0.0145 (<1) 
0.4527 (53) 
Proton pump inhibitors 
0.3333 (1) 
0.3860 (5) 
0.1299 (42) 
0.0377 (<1) 
0.1775 (25) 
Skeletal muscle relaxants 
0.1429 (<1) 
0.1981 (<1) 
0.2286 (2) 
0.1429 (<1) 
0.2222 (<1) 
Statins 
0.2278 (6) 
0.2479 (1) 
0.4019 (4) 
0.1484 (12) 
0.1563 (1) 
Triptans 
0.1739 (10) 
0.360 (4) 
0.2569 (13) 
0.0690 (<1) 
0.2750 (8) 
Urinary incontinence 
0.3697 (24) 
0.5243 (19) 
0.5405 (10) 
0.4444 (13) 
0.4317 (30) 
Average
b
0.3039 (18) 
0.3683 (13) 
0.3437 (14) 
0.1365 (5) 
0.3116 (17) 
a
Both 5% and 12% have F
1
= 0.4363. The average of 5% and 12% (8.5%) is taken to calculate the 
average value on the last row of the table. 
b
<1% is considered as 1% for calculating the average percentage. 
Table 2.3: Single element F1 performance 
49 
2.5.4 Composited Elements Performance 
Different article elements can play different roles in a literature retrieval process and their 
performances vary greatly as we have described above. To further explore the possibility 
of performance improvement brought by the collaboration (combination) of multiple 
elements, we examine 22 different combinations inspired by the single element results, and 
choose the top 6 combinations and report the composited performance in Table 2.4 and 
Table 2.5. We also conduct statistical analysis with repeated ANOVA for the composited 
performance. For WSS95, the results show that there is no statistical difference across the 
6 combinations (p=0.332). For F1, there is also no statistically significant difference across 
the 6 combinations (p=0.069) as well. 
Dataset 
TI+AB 
TI+AB 
+MH 
TI+AB 
+PT 
TI+AB 
+AU 
TI+AB 
+PT+AU 
TI+AB+MH 
+PT+AU 
ACE inhibitors 
76.38 
76.85 
74.29 
75.79 
73.70 
75.08 
ADHD 
80.38 
79.79 
67.92 
80.14 
67.92 
56.17 
Antihistamines 
16.13 
10.65 
24.52 
16.13 
24.52 
18.39 
Atypical antipsychotics 
20.89 
14.20 
17.95 
20.63 
17.77 
14.38 
Beta blockers 
60.14 
60.09 
65.01 
60.96 
64.72 
65.21 
Calcium channel blockers 
18.23 
18.64 
17.32 
18.39 
17.49 
22.82 
Estrogens 
33.42 
36.14 
22.55 
33.97 
22.55 
29.08 
NSAIDS 
72.26 
75.57 
77.35 
70.48 
76.34 
77.86 
Opioids 
6.01 
11.75 
8.98 
5.95 
8.98 
12.17 
Oral hypoglycemics 
11.33 
13.12 
13.52 
11.13 
13.52 
12.72 
Proton pump inhibitors 
19.20 
21.31 
19.65 
19.05 
19.65 
20.11 
Skeletal muscle relaxants 
41.94 
46.44 
58.55 
41.87 
58.49 
60.01 
Statins 
29.10 
27.11 
27.80 
30.96 
27.71 
26.07 
Triptans 
48.29 
51.71 
39.64 
50.52 
39.79 
40.98 
Urinary incontinence 
12.84 
11.01 
20.80 
12.84 
20.80 
14.37 
Average 
36.44 
36.96 
37.06 
36.59 
36.93 
36.35 
Table 2.4: Composited elements WSS95 performance 
50 
Dataset 
TI+AB 
F
1
(%) 
TI+AB 
+MH 
F
1
(%) 
TI+AB 
+PT 
F
1
(%) 
TI+AB 
+AU 
F
1
(%) 
TI+AB 
+PT+AU 
F
1
(%) 
TI+AB+MH 
+PT+AU 
F
1
(%) 
ACE inhibitors 
0.4156 (1) 
0.4000 (2) 
0.4051 (1) 
0.3902 (2) 
0.3971 (4) 
0.3774 (3) 
ADHD 
0.4000 (3) 
0.4688 (5) 
0.5455 (4) 
0.4286 (6) 
0.5306 (3) 
0.5818 (4) 
Antihistamines 
0.3226 (5) 
0.3333 (10) 
0.2903 (15) 
0.3226 (5) 
0.2903 (15) 
0.2813 (15) 
Atypical antipsychotics 
0.4364 (16) 
0.4241 (15) 
0.4887 (15) 
0.4411 (17) 
0.4856 (15) 
0.4606 (15) 
Beta blockers 
0.2800 (3) 
0.3043 (2) 
0.3590 (2) 
0.2667 (3) 
0.3596 (2) 
0.3333 (3) 
Calcium channel blockers 
0.2335 (8) 
0.2620 (11) 
0.2804 (9) 
0.2323 (8) 
0.2816 (9) 
0.2995 (9) 
Estrogens 
0.6000 (30) 
0.6237 (29) 
0.6047 (25) 
0.5979 (31) 
0.6118 (24) 
0.6171 (26) 
NSAIDS 
0.6667 (16) 
0.6154 (16) 
0.6966 (12) 
0.6471 (16) 
0.6809 (13) 
0.6667 (15) 
Opioids 
0.3000 (0) 
0.3158 (<1) 
0.3000 (<1) 
0.3000 (<1) 
0.3000 (<1) 
0.3158 (<1) 
Oral hypoglycemics 
0.4497 (90) 
0.4541 (88) 
0.4553 (86) 
0.4489 (92) 
0.4561 (75) 
0.4635 (82) 
Proton pump inhibitors 
0.4384 (7) 
0.4737 (5) 
0.5172 (5) 
0.4552 (7) 
0.5455 (5) 
0.5079 (6) 
Skeletal muscle relaxants 
0.2222 (1) 
0.2353 (<1) 
0.2500 (<1) 
0.2222 (1) 
0.2500 (<1) 
0.2667 (<1) 
Statins 
0.2994 (2) 
0.3281 (1) 
0.3382 (1) 
0.2959 (2) 
0.3358 (2) 
0.3465 (1) 
Triptans 
0.3636 (3) 
0.3913 (3) 
0.3556 (3) 
0.3556 (3) 
0.3529 (4) 
0.3913 (3) 
Urinary incontinence 
0.5063 (12) 
0.5347 (19) 
0.5505 (21) 
0.5263 (11) 
0.5507 (9) 
0.5843 (15) 
Average
a
0.3956 (13) 
0.4110 (14) 
0.4291 (14) 
0.3954 (14) 
0.4286 (12) 
0.4329 (13) 
a
<1% is considered as 1% for calculating the average percentage. 
Table 2.5: Composited elements F1 performance 
2.5.5 Performance Comparisons with Related Works 
We compare our interactive article recommendation model with existing supervised 
machine learning models which are trained on lexical features of biomedical articles. 
Because F1 is not commonly used in those studies, we only consider WSS95 for 
performance comparisons. In addition, as TI+AB+PT has the simplest combination and its 
performance is equivalent or better than others, we use it to compare against other models. 
The repeated ANOVA test shows significant differences across four studies 
(p=0.005). The pairwise comparison with Hommel adjustment shows that there is no 
significant 
difference 
between 
our 
study 
and 
either 
Cohen 
2008 
or 
Matwin 
2010 
51 
(p=0.4979, 0.4979), but is significantly better than Cohen 2006 (p=0.0475). In summary, 
our active learning model provides competitive results to conventional supervised models. 
Dataset 
Cohen 2006 
(VP) 
Cohen 2008 
(SVM) 
Matwin 2010 
(FCNB/WE) 
Our model 
(TI+AB+PT) 
ACE inhibitors 
56.61 
73.30 
52.30 
74.29 
ADHD 
67.95 
52.60 
62.20 
67.92 
Antihistamines 
0 
23.60 
14.90 
24.52 
Atypical antipsychotics 
14.11 
17.00 
20.60 
17.95 
Beta blockers 
28.44 
46.50 
36.70 
65.01 
Calcium channel blockers 
12.21 
43.00 
23.40 
17.32 
Estrogens 
18.34 
41.40 
37.50 
22.55 
NSAIDS 
49.67 
67.20 
52.80 
77.35 
Opioids 
13.32 
36.40 
55.40 
8.98 
Oral hypoglycemics 
8.96 
13.60 
8.50 
13.52 
Proton pump inhibitors 
27.68 
32.80 
22.90 
19.65 
Skeletal muscle relaxants 
0 
37.40 
26.50 
58.55 
Statins 
24.71 
49.10 
31.50 
27.80 
Triptans 
3.37 
34.60 
27.40 
39.64 
Urinary incontinence 
26.14 
43.20 
29.60 
20.80 
Average 
23.43 
40.80 
33.50 
37.06 
a
VP: voting Perceptron-based automated citation classification system 
b
FCNB/WE: factorized complement naïve Bayes with weight engineering 
c
SVM: support vector machine 
Table 2.6: WSS95 performance comparisons with existing works 
2.6 Summary 
In this chapter, we generate lexical article features and representations using 5 informative 
article elements from the biomedical literature, including title (TI), abstract (AB), medical 
subject heading term (MH), publication type (PT), and author (AU). For all article pairs in 
a text corpus (article collection), we establish an element-level similarity for each of the 5 
article elements, as well as composited article similarities for different combinations of 
multiple article elements. To employ and evaluate the effectiveness of different article 
elements (corresponding article features, representations, and similarities) in an efficient 
52 
and generalizable manner, we incorporate the concept of active learning and develop an 
interactive article recommendation model, with respect to the application of biomedical 
literature retrieval. 
We investigate the effectiveness (predictive capability) of different article elements 
in indicating article relevancies in the information retrieval tasks. For the performance of 
a single element, our experimental results reveal that TI and PT have the best WSS95, AB 
and PT result in the best F1 during the early stage of a recommendation process, and TI 
and AB have a better predictive capability in general. Therefore, we consider TI and AB 
as the most effective (useful) elements in approaching article relevancy in an informative 
and concise manner. 
Moreover, 
collaborative 
elements 
(combinations) 
can 
bring 
performance 
improvement in both F1 and WSS95. In particular, a simple equally weighted combination 
of TI+AB+PT could achieve a WSS95 performance of 37%. However, because different 
literature retrieval tasks can have very different scopes, we cannot find one universal 
weight setting that can be successfully applied to any datasets. Therefore, instead of finding 
the best weighting for each dataset, flexible and customizable weight parameters for human 
reviewers to adjust based on their scopes and interests would be more practically useful. 
Comparing to the conventional supervised machine learning models for automated 
article classification, our active learning model for interactive article recommendation can 
perform as a more efficient and generalizable method in practical settings. Meanwhile, our 
active learning model is able to achieve competitive performances with existing supervised 
models in identifying high relevant articles and saving workloads. In summary, we have 
53 
demonstrated an effective, efficient, and generalizable solution using lexical article 
features for biomedical literature retrieval. 
Highlight of Contribution 
We investigate the effectiveness (usefulness) of different article elements from biomedical 
articles (scientific publications), including both single elements and combinations of 
multiple elements, in indicating article relevancies in information retrieval. We also 
develop an active learning model for interactive article recommendation to employ and 
evaluate 
the 
lexical 
article 
features, 
representations, 
and 
similarities. 
Together, 
we 
demonstrate an effective, efficient, and generalizable solution using lexical article features 
for biomedical literature retrieval. 
This work has been published in JMIR 2015: 
Xiaonan Ji, Po-Yin Yen. Using MEDLINE Elemental Similarity to Assist Article Selection 
for Systematic Reviews. JMIR medical informatics. 2015 Jul;3(3). 
54 
Chapter 3. Ontology-based Semantic Analysis with UMLS Knowledge Resource 
In the previous chapter, we demonstrate the effectiveness of lexical article features, which 
are derived from multiple article elements, in assisting with biomedical literature retrieval 
using our (active) interactive article recommendation model. Among the multiple article 
elements, title and abstract are shown to be the most informative and concise ones in 
indicating article relevancies. In this chapter, we proceed to semantic analysis on article 
title and abstract to develop advanced semantic features, representations, and similarities, 
expecting to achieve improved effectiveness and accuracy. Specifically, we intend to 
capture the inherent linguistic senses (semantic domains) and underlying meanings of 
articles, as opposed to being limited to the lexical or morphological level information. 
As a motivating example, 
apple
and 
orange
are parts of the same semantic domain 
of 
fruit
; likewise, article titles and abstracts may contain different terminologies, i.e. 
dry 
mouth
and 
xerostomia
which are associated with the same semantic concept of 
oral 
symptom
. However, such semantic relationships are beyond the capacity of lexical features 
or morphological matchings. In this sense, semantic features such as the concept level 
information would empower us with better capabilities to explain how texts are related or 
similar to each other. Consequently, semantic representations and similarities would 
further facilitate the identification of articles that share similar underlying meanings but 
are with different terminologies or languages styles in biomedical literature retrieval. 
55 
Therefore, how can we access proper semantic knowledge that connects related 
texts? Meystre 2008 [67] conducted a thorough review on natural language processing 
(NLP) approaches applied to the biomedical and clinical domain, and summarized that 
standard ontologies and terminologies could be utilized as background knowledge for 
information extraction and semantic analysis. Motivated by the advantageous of UMLS 
[68] concepts and ontologies in assembling biomedical knowledge, in this chapter, we 
build a pipelined semantic process (model) to map an article into an optimized and enriched 
UMLS concepts representation with three steps: concept annotation, concept optimization, 
and concept expansion. Throughout the process, we leverage concept relations maintained 
in UMLS ontologies (SNOMED-CT [69] and MeSH [59]) and utilize ontology-based 
concept similarities to prompt semantic features (concepts) in each article’s representation. 
Augmented article similarities with higher accuracies are readily established and applied 
to our interactive article recommendation model to facilitate the identification of high 
relevant articles for biomedical literature retrieval. 
3.1 Research Questions 
We formulate and highlight our research questions as below: 
•
Both 
SNOMED-CT 
and 
MeSH 
are 
considered 
well-known 
and 
comprehensive 
ontologies in the UMLS Metathesaurus (biomedical knowledge base), which of them 
would be more useful for us to gain semantic knowledge to facilitate biomedical 
literature retrieval, e.g. the identification of high relevant biomedical articles? 
•
By leveraging the gold-standard semantic knowledge from UMLS ontologies (i.e. 
SNOMED-CT and MeSH), can we derive effective semantic features and generate 
56 
high-quality semantic article representations and similarities to facilitate biomedical 
literature retrieval? 
•
Can our ontology-based semantic approach (a pipelined semantic model levering 
ontology-based semantics) outperform the lexical baselines and existing corpus-based 
semantic approaches in biomedical literature retrieval? 
3.2 Background and Related Works 
3.2.1 UMLS Metathesaurus 
UMLS [68] is a compendium of many controlled vocabularies in the biomedical domain, 
as well as a comprehensive thesaurus of biomedical concepts. The UMLS Metathesaurus 
consists of over two million UMLS concepts with a mapping structure from nearly 200 
different vocabularies (ontologies). Specifically, it is organized by concepts (senses or 
meanings) associating with attributes and names in source vocabularies, and is enumerated 
via 
Concept 
Unique 
Identifiers 
(CUIs). 
The 
Metathesaurus 
also 
preserves 
useful 
relationships between concepts from each vocabulary. In summary, UMLS integrates, 
classifies, and distributes key terminologies in biomedical informatics, and has been a well-
recognized resource for knowledge processing. We use MetaMap [70]–[72] to retrieve 
UMLS concepts from biomedical texts. MetaMap assigns CUIs to words and terms using 
rules and patterns. Thereby, we can overcome the barrier of when the same concepts are 
expressed in a variety of ways in different contexts or by different authors. 
3.2.2 SNOMED-CT and MeSH 
Systematized 
Nomenclature 
of 
Medicine-Clinical 
Terms 
(SNOMED-CT) 
[69] 
and 
Medical Subject Headings (MeSH) [59] are two of the most widely used knowledge 
57 
sources within the UMLS framework. SNOMED-CT has been the main ontology used in 
the biomedical domain [73]. It provides a global and broad hierarchical terminology that 
models clinical data (i.e. storage, encoding, and retrieval) in healthcare. Specifically, the 
US edition of SNOMED-CT covers more than 300,000 unique concepts classified in 19 
top-level hierarchies. MeSH is a controlled vocabulary for indexing published studies from 
the MEDLINE database. It provides over 25,000 biomedical concepts hierarchically 
classified in 16 categories. MeSH has been widely used for information retrieval, but can 
also be used for other purposes, such as encoding and storing biomedical data. Although 
MeSH is not explicitly claimed as an ontology, it provides broader-narrower relations in 
its hierarchy [74]. Several studies have utilized MeSH as an ontology for semantic analysis 
and achieved high correlations with human ratings; among them, some also applied cross-
ontology or multi-ontology approaches by using MeSH together with SNOMED-CT and 
obtained improved performance over using SNOMED-CT alone [75]–[80]. Inspired by the 
properties of MeSH and prior studies, in our study, we use the vocabulary concepts and the 
hierarchical (hyponym/hypernym) relations in both SNOMED-CT and MeSH as our 
ontology knowledge sources. By loading the Rich Release Format (RRF) [81] set of UMLS 
Metathesaurus into MySQL database, we access to the vocabularies and concept relations 
in SNOMED-CT and MeSH from relational tables (i.e. MRREL and MRHIER). Using 
SNOMED-CT and MeSH together tend to promote a broader concept and knowledge 
coverage. In this study, we use the UMLS Metathesaurus Version 2015AB. 
58 
3.2.3 MetaMap 
MetaMap [70]–[72] is a program of natural language processing (NLP) to map biomedical 
texts to UMLS concepts, with the purpose to improve biomedical text retrieval and text 
mining in a knowledge intensive manner. MetaMap is highly configurable across multiple 
dimensions, including the 
data options
to choose the source vocabularies and data model, 
the 
output options
to determine the output format, and the 
processing options
to control the 
algorithms to process input text and determine mapping concepts. 
More specifically, input texts to MetaMap is 
processed through lexical and 
syntactical analysis consisting of several steps, including sentence segmentation and 
tokenization, parts-of-speech (POS) tagging, lookup of lexical words in the SPECIALIST 
lexicon 
[82] 
(common 
words 
as 
well 
as 
biomedical 
terms 
with 
their 
syntactic, 
morphological, and orthographic information), syntactic analysis with a shallow parser to 
identify phrases, etc. Identified phrases are further proceeded to determine the mapping 
concepts from the UMLS Metathesaurus. This mapping process also involves several 
aspects, including the generation of phrase variants (and word variants) via table lookup, 
identification of candidate concepts via string matching, evaluation and ranking of the best 
candidate concepts, and word sense disambiguation (WSD) to promote the semantic 
consistency between the mapped concepts and the surrounding texts. 
MetaMap enables a thorough mapping between biomedical texts and UMLS 
concepts because of its aggressive generation of word variants, the linguistically principled 
approach of the lexical and syntactic analysis, and the evaluation metric for concept scoring 
and ranking. Because of its flexible configuration and customizability, MetaMap can also 
59 
be 
applied 
to 
many 
other 
domains. 
However, 
MetaMap 
also 
suffers 
from 
several 
weaknesses, including the limited applicability to other languages beyond English, and the 
slow processing speed which places barriers on real-time applications. A parallel re-
implementation of would be necessary to meet the requirements of time efficiency. 
3.2.4 Semantic Similarity Measures 
Concept or word level semantics are considered the pre-requisites or building blocks of 
sentence or document level semantics. There have been different semantic similarity 
measures 
(SSM) 
proposed 
to 
capture 
relationships 
among 
concepts 
or 
words, 
for 
applications in information retrieval, information extraction, natural language processing, 
etc. Based on the resource of semantic knowledge, SSMs can be roughly categories into 
knowledge (ontology) based approaches and distributional (corpus) based approaches [73]. 
Knowledge (ontology) based approaches utilize standard and pre-existing resources 
such as dictionaries, ontologies, taxonomies, etc., and it has been the most widely used in 
the biomedical domain to leverage the semantic knowledge encoded as concept hierarchies 
or relations in the UMLS ontologies [73], [83]–[87]. Under this notion, given the 
taxonomic structure of an ontology, the semantic similarity between an inherent pair of 
concepts would be measured in two ways [73]: (1) Path-based measures using concepts’ 
co-location, distance (shortest path), depth (distance to the root), etc. The path-based 
measures are simple, straightforward, and efficient, but might fail to take a comprehensive 
picture of an ontology into consideration. (2) Intrinsic information content (IC) based 
measure that considers concepts’ informativeness (or specificity), which can be reflected 
by e.g. the number of ancestors and children/leaves in the taxonomic tree of an ontology. 
60 
Intrinsic IC derived from an ontology could better represent the taxonomic structure in a 
more comprehensive manner. The formulas below show the computation a concept’s IC, 
and the similarity between a pair of concepts using their ICs with the Lin’s method [88]. 
simi
(
𝑐
*
, 𝑐
+
)
=
+∗Po(poq(r
s
,r
t
))
Po
(
r
s
)
uPo
(
r
t
)
(Lin’s formula) 
IC
(
c
)
= −log
|yz{|z}(~)|
|
}•€}••z‚
(
~
)|
u*
ƒ„ƒ…†_†ˆ…Hˆ‰ u*
(Intrinsic IC) 
Distributional (corpus) approaches quantify semantic similarities between linguistic terms 
based on their distributional properties, e.g. co-occurrence or frequencies, in large and 
high-quality corpus, so that terms with similar distributions would have similar meanings 
[89]–[93]. Under this notion, as traditional methods, the distribution of terms within a 
corpus is utilized to compute their semantic similarities via the corpus IC or context vectors 
[91]. While corpus IC is a measure of concept specificity derived from concept frequencies 
within a corpus; context vector, on the other hand, is based on the assumption that words 
that appear in similar contexts are related. A similar idea is used in Brown clustering [92], 
where words appear in similar contexts are grouped together in a hierarchical manner. 
More recent distributional based semantic approaches, such as word embedding [89], [93], 
[94], has been shown to generate low-dimensional vector representations of terms by 
capturing their distributional semantics, and can be used in a wide range of information 
retrieval and extraction tasks. However, additional studies are needed to demonstrate its 
capabilities in effectively modeling biomedical knowledge. 
In summary, ontology-based approaches, such as intrinsic information content (IC), 
have been shown to provide better performance than corpus-based approaches, and 
61 
compute similarities highly correlated with human ratings in the biomedical domain [73], 
[83]. Existing studies, as we have mentioned in Section 3.2.2, also successfully employed 
UMLS concept semantic similarities derived from biomedical ontologies in applications of 
information retrieval and extraction. 
3.3 Semantic Concepts Representation Development Process 
We create a pipelined three-step process (Figure 3.1) to develop a UMLS concept-based 
semantic representation of an article, with respect to its 
title
and 
abstract 
which are written 
in natural language. The three-step process consists of: (1) concept annotation, (2) concept 
optimization, and (3) concept expansion. The output of this process is an optimized and 
enriched concepts (concept-based) representation for a given article. 
Figure 3.1: Overview of the semantic concepts representation development process 
3.3.1 Concept Annotation 
We annotate all terms and words in the title and abstract from each article with MetaMap 
API [72], and map them to UMLS CUIs. When using MetaMap, we do not restrict the 
mapping score or semantic type in order to investigate a more generalizable approach. We 
MetaMap 
Parser
All-inclusive 
concepts 
representation 
UMLS Ontologies
Tunable 
Parameters
Categorization
Optimized 
concepts 
representation
Expand with 
Hypernyms
Concept 
Similarity
Enriched 
concepts 
representation
SR Article 
Collection
Concept Annotation
Concept Optimization
Concept Expansion
62 
then parse the MetaMap output to create a list of concepts. In the results, each article can 
be represented as two separate lists of concepts, one as the title concept list and the other 
as the abstract concept list. At this stage, we include all CUIs that matched or unmatched 
to SNOMED-CT and MeSH. We call this the all-inclusive concepts representation. In 
addition, each concept contains three key components: CUI, preferred concept name 
(identified by UMLS), and frequency (N). The frequency (N) indicates the total number of 
times that a concept is mapped from words and phrases in the title or abstract. 
After the concept annotation, we are able to establish an all-inclusive concepts 
representation for each article, which includes biomedical-specific concepts, as well as 
other more general concepts. For instance, concept “C188335: To [Qualitative Concept],” 
derived from the trivial stop-word “to”, can be seen as a noise; analogous concepts 
“C1706408: 
PLACEBO 
[Research 
Activity]”, 
“C0032042: 
Placebo 
[Therapeutic 
or 
Preventive Procedure]”, and “C1696465: PLACEBO [Biomedical or Dental Material],” 
derived 
from 
“placebo” 
in 
the 
same 
context, 
lead 
to 
overmatching; 
and 
concept 
“C00449851: Methods [Techniques]” is too general to provide meaningful semantic 
connections. The contribution of these concepts should depend on task scopes. 
3.3.2 Concept Optimization 
To optimize the concept list, a tunable process of concept categorization (Figure 3.2) is 
implemented to control noises, overmatching concepts, and concepts with low specificity. 
We utilize SNOMED-CT and MeSH to identify concepts commonly used in the biomedical 
and clinical settings. Concepts not matched in SNOMED-CT and MeSH are considered as 
unmatched concepts, which are usually noise or overmatching as above mentioned. 
63 
Concepts matched to SNOMED-CT and MeSH proceed to the next stage to be categorized 
as general concepts or specific concepts. In an ontology, concepts are arranged in a 
hierarchical structure, which is also a directed acyclic graph (DAG) with a root node 
(considered as a taxonomic structure, or a taxonomic tree). Concepts with lower depths, 
located close to the root, have broader meanings; concepts with higher depths, located away 
from the root, are hyponyms with more specific meanings. We use a depth threshold to 
categorize concepts based on their depths in the taxonomic tree. In this sense, concepts 
with a depth lower than the depth threshold are categorized as general concepts; concepts 
with a depth equal to or higher than the threshold are categorized as seed concepts. Because 
of the poly-hierarchical structure in SNOMED-CT and MeSH, one concept can have 
multiple parents and depths. We calculate both its smallest and largest depths, but use the 
smallest depth for concept categorization and weights. This help avoids biases resulting 
from undesired lengthy paths assigned to some general concepts. 
We also assign weights to concepts and use parameters to control the contribution 
of each concept category. We assign weights to seed concepts based on their frequency 
from the annotation stage and depth in the taxonomic tree (
𝑤𝑒𝑖𝑔ℎ𝑡
‰ˆˆ•
= 𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦 ∙
𝑑𝑒𝑝𝑡ℎ
). For general concepts, we have a ratio, α (
0 ≤ α ≤ 1
), to control their contributions 
(
𝑤𝑒𝑖𝑔ℎ𝑡
”ˆ-ˆ•…†
= 𝛼 ∙ 𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦 ∙ 𝑑𝑒𝑝𝑡ℎ
). For unmatched concepts, we use a controlling 
ratio, β (
0 ≤ β ≤ 1
), (
𝑤𝑒𝑖𝑔ℎ𝑡
—-˜…ƒr™ˆ•
= β ∙ 𝑓𝑟𝑒𝑞𝑢𝑒𝑛𝑐𝑦
). While various datasets can 
have very different scopes, criteria, or preferences, there are no universal parameters that 
can be applied to all scenarios. To leverage prior knowledge in biomedical literature 
retrieval (e.g. researchers’ experience), parameters should be flexibly adjustable. 
64 
Figure 3.2: Concept categorization process 
3.3.3 Concept Expansion 
The purpose of concept expansion is to overcome the limitation of exact concept matching 
and provide an exhaustive concepts representation for each article [85], [95], [96]. Consider 
a scenario when two articles are semantically related but share no identical concepts in 
their concept sets. With exact concept matching, it would be difficult to find connections, 
thus missing potentially important semantic features. A concept expansion to include 
related concepts (i.e. hyponyms, hypernyms, and siblings) in the ontology may solve the 
problem. 
We 
leverage 
concept 
relations 
maintained 
in 
the 
taxonomic 
structure 
of 
SNOMED-CT and MeSH. For each seed concept, we expand it by including all ancestors 
(hypernyms) that have a higher depth than the assigned depth threshold. To access concept 
relations in the taxonomic structure, we load an RRF [81] set of UMLS Metathesaurus into 
a MySQL database. We use C++ to index the path-to-root (PTR) from the table MRHIER, 
which contains concept relation information of the taxonomic structure. 
Figure 3.3 illustrates a concept expansion example. C4 (
Animal bite
) and C6 (
Stung 
by sea anemone
) are seed concepts representing in two different articles; both C4 and C6 
are hyponyms of C1 (
accident due to animal or plant
) as shown in Figure 3.3 (a). If a task 
Article 
All 
Inclusive 
Concept 
Set
MetaMap 
Match with 
SNOMED-CT 
& MeSH
Unmatched 
Concepts 
Matched 
Concepts
Depth 
Threshold
General 
Concepts
Specific/Seed 
Concepts
65 
scope is to analyze accidents due to environment factors, C4 and C6 should contribute to 
the similarity of the two articles. In Figure 3.3 (b), we expand C4 with C1 and C2, and 
expand C6 with C1 and C3. The two expanded concept sets now share a commonality, C1, 
which grants similarity between the two articles. 
Figure 3.3: Illustrated concept expansion 
To control the contribution of expanded concepts, we assign weights to them based on their 
semantic similarity to the seed concepts. Since the expansion only utilizes concept relations 
between a hypernym and a hyponym, we deploy a simple version of an ontology-based 
method using the distance (shortest path) between concepts, to capture concept semantic 
similarity [73]. In this sense, the weight of an expanded concept is calculated with the 
formulas below. If one expanded concept acts as a hypernym of multiple seed concepts, 
then the highest weight was used. 
Similarity
›
𝐶
‰ˆˆ•
, 𝐶
ˆO•…-‰1„-
ž
=
1
𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒
›
𝐶
‰ˆˆ•
, 𝐶
ˆO•…-‰1„-
ž
+ 1
𝑊𝑒𝑖𝑔ℎ𝑡
ˆO•…-‰1„-
= 𝑊𝑒𝑖𝑔ℎ𝑡
‰ˆˆ•
∙ Similarity
›
𝐶
‰ˆˆ•
, 𝐶
ˆO•…-‰1„-
ž
Depth 
Threshold 
Depth 
Threshold
(a) Before expansion
(b) After expansion
Seed concept 
Expanded concept
Other concept
C1
C1
C1
C1
C2
C3
C4
C5
C6
C2
C3
C4
C5
C6
C2
C3
C4
C5
C6
C2
C3
C4
C5
C6
Article 1
Article 2
Article 1
Article 2
66 
At the end of this process, each title or abstract is represented with an optimized and 
enriched set of UMLS concepts; the entire article collection (text corpus) can then be 
represented as a weighted semantic vector space that comprises all derived concepts as 
semantic features. Figure 3.4 shows an example of a specific title along with its optimized 
and enriched concept set. Due to the space limitation, we only list part of the concepts 
under each category. For each concept, we list its CUI, name, and weight based on the 
above-mentioned formula(s). 
Figure 3.4: Illustrated optimized and enriched concept set 
3.4 Experimental Settings and Results 
3.4.1 Experimental Settings 
We use the 15 DERP datasets [26], [29] (described in Section 2.5.1) and the interactive 
article recommendation model (introduced in Section 2.4) to evaluate the concept-based 
semantic article features and representations in facilitating the identification of high 
relevant articles. We implement a restrictive experiment setting that assigns a zero ratio 
(
α, β = 0
) to unmatched concepts and general concepts to eliminate their contributions in 
PMID-12811500: Controlled, double blind, multicentre clinical trial to investigate long-
term tolerability and efficacy of trospium chloride in patients with detrusor instability. 
Specific/Seed Concepts 
C0013072 Double-Blind Method 
7
C1096776 Multicenter Study 
5
C0077408 Trospium Chloride 
6
C1261562 Detrusor Instability 
8 
Expanded Concepts 
C0242896 Anticholinergic Agent 
1.5
C0005686 Urinary Bladder Disease 
4 
General Concepts
C1292732 Investigate 
𝛼 ∙ 6
C1280519 Efficacy 
𝛼 ∙ 4
C0030705 Patients 
𝛼 ∙ 5
Unmatched Concepts
C1883351 To 
β ∙ 1
C0443252 Long-term 
β ∙ 1
C3274448 Tolerability Study 
β ∙ 1
67 
the feature space. Since no universal weight parameters can be identified as we discussed 
before, we use an equal weight setting for title similarity and abstract similarity (
𝑤LL⃗ = [1,1]
). 
As for the depth threshold, with respect to the diversity of task scopes and the 
different structures in SNOMED-CT and MeSH, we examine the WSS95 performance 
using SNOMED-CT (Figure 3.5 a) and MeSH (Figure 3.5 b) separately and jointly (Figure 
3.5 c) with a series of depth thresholds (1-15). In Figure 3.5, the 15 DERP datasets are 
represented by performance curves of different colors. As shown in Figure 3.5 a and Figure 
3.5 b, although SNOMED-CT and MeSH have different structures, they result in the same 
range of depth thresholds (1-9) with non-zero WSS95. We find that each individual dataset 
has its own best depth threshold that led to the highest WSS95. SNOMED-CT has an 
average best depth threshold of 3.53; MeSH has an average best depth threshold of 4.87. 
When combining SNOMED-CT and MeSH together, the average best depth threshold is 
3.53. As different depth thresholds result in varying WSS95 performance for individual 
datasets, and there is no universal best depth threshold that could be used for all datasets, 
a flexible depth threshold is needed. In this experiment, we conclude that we could apply 
the average best depth threshold (i.e. 3) or the depth threshold that results in higher 
performance in most datasets (i.e. 1) as the initial default setting for later adjustments. 
68 
Figure 3.5: Ontology Depth threshold and resulting WSS95 performance 
3.4.2 Performance Report 
Table 3.1 presents the WSS95 performance with different settings: (1) SNOMED-CT, (2) 
MeSH, and (3) SNOMED-CT + MeSH. For each setting, we compare the performance 
resulting from a fixed depth threshold, 1 (DT=1), for all datasets, and a flexible depth 
threshold, the best depth threshold, for each individual dataset. As shown in Table 3.1, 
flexible depth thresholds bring improved performance over the fixed depth threshold with 
a WSS95 improvement of 2.40% on average. We then evaluate the WSS95 performance 
across different settings using flexible depth thresholds. Specifically, SNOMED-CT results 
in an average WSS95 of 41.15%; MeSH results in an average WSS95 of 36.97%. While 
SNOMED-CT brings better performance than MeSH in 10 datasets, MeSH outperforms 
SNOMED-CT in 5 datasets (e.g. Triptans). With SNOMED-CT + MeSH as the knowledge 
sources, we achieve an improved average WSS95 of 43.81%. Specifically, we obtain an 
improved performance in 10 datasets with SNOMED-CT + MeSH, compared to using 
SNOMED-CT alone. In summary, even though flexible depth thresholds result in the best 
performance, we can still receive competitive results using a fixed depth threshold. In 
69 
addition, SNOMED-CT + MeSH can achieve better WSS95 compared to using SNOMED-
CT or MeSH alone. 
SNOMED-CT 
MeSH 
SNOMED-CT + MeSH 
Dataset 
WSS95 % 
(DT=1) 
WSS95 % 
(DT=F) 
WSS95 % 
(DT=1) 
WSS95 % 
(DT=F) 
WSS95 % 
(DT=1) 
WSS95 % 
(DT=F) 
ACE Inhibitors 
83.33 
84.39 (3) 
59.83 
59.98 (4) 
82.70 
85.57 (3) 
ADHD 
81.67 
83.67 (4) 
80.73 
81.67 (6) 
82.14 
83.55 (4) 
Antihistamines 
17.10 
22.26 (8) 
14.52 
27.42 (7) 
17.74 
23.87 (7) 
Atypical Antipsychotics 
30.36 
31.70 (3) 
26.16 
26.96 (4) 
33.57 
35.18 (3) 
Beta Blockers 
63.85 
63.85 (1) 
54.87 
57.48 (4) 
68.48 
68.48 (1) 
Calcium Channel Blockers 
11.08 
11.74 (7) 
12.97 
13.22 (5) 
12.81 
25.53 (7) 
Estrogens 
27.17 
31.25 (4) 
33.15 
33.42 (2) 
28.53 
32.07 (4) 
NSAIDS 
48.60 
48.60 (1) 
69.21 
69.21 (1) 
64.12 
64.89 (4) 
Opioids 
23.24 
23.24 (1) 
8.30 
13.84 (7) 
17.39 
17.39 (1) 
Oral Hypoglycemic 
16.10 
16.10 (1) 
9.94 
10.54 (7) 
15.90 
15.90 (1) 
Proton Pump Inhibitors 
20.78 
27.23 (8) 
15.45 
18.15 (6) 
21.01 
25.81 (8) 
Skeletal Muscle Relaxants 
31.22 
38.89 (4) 
28.48 
30.86 (5) 
29.88 
36.15 (4) 
Statins 
43.30 
43.30 (1) 
38.16 
39.23 (5) 
43.74 
43.74 (1) 
Triptans 
45.75 
46.35 (3) 
53.50 
57.82 (9) 
54.10 
54.10 (1) 
Urinary Incontinence 
44.04 
44.65 (4) 
14.68 
14.68 (1) 
41.59 
44.95 (4) 
Average 
39.17 
41.15 
34.66 
36.97 
40.91 
43.81 
DT: Depth Threshold; F: Flexible 
Table 3.1: Performance report with different ontologies 
Figure 3.6 presents the recall performance curves for all 15 datasets using SNOMED-CT 
+ MeSH. We notice that the speed (efficiency) of identifying relevant articles differs from 
one dataset to another. For example, ACE Inhibitors and ADHD identify relevant articles 
rapidly at the early screening stage and achieve a high recall immediately; Atypical 
Antipsychotics and Urinary Inconvenience maintain a more stable recall speed for the 
entire process; Antihistamines has a speed acceleration in a much later stage. Overall, with 
a 40% article screening progress, 12 datasets can reach a recall of > 70%. 
70 
Figure 3.6: Performance curves with SNOMED-CT + MeSH 
3.4.3 Performance Comparisons with Lexical Approaches 
We compare our semantic approach with prior works using lexical features, including (1) 
our lexical approach introduced in Chapter 2, and (2) representative supervised machine 
learning approaches, Cohen 2008 [32] and Matwin 2010 [33], who employed lexical 
features in their training and testing process. In these works, an optimization process or 
intra-topic training process applied adjustable parameters to present the best performance 
in different datasets. To be comparable with these works, we use the results obtained from 
the flexible depth thresholds. Table 3.2 shows the WSS95 comparisons. We do not include 
71 
Cohen 2006 [29] for side by side comparison as it is an earlier work of Cohen 2008. Cohen 
2006 reported an average WSS95 of 23.43%, using a machine learning approach of voting 
perceptron. 
Dataset 
Cohen 2008 
(SVM) 
Matwin 2010 
(FCWB/WE) 
Our Lexical 
Approach 
Our Semantic 
Approach 
ACE Inhibitors 
73.3 
52.3 
74.29 
85.57 
ADHD 
52.6 
62.2 
67.92 
83.55 
Antihistamines 
23.6 
14.9 
24.52 
23.87 
Atypical Antipsychotics 
17 
20.6 
17.95 
35.18 
Beta Blockers 
46.5 
36.7 
65.01 
68.48 
Calcium Channel Blockers 
43 
23.4 
17.32 
25.53 
Estrogens 
41.4 
37.5 
22.55 
32.07 
NSAIDS 
67.2 
52.8 
77.35 
64.89 
Opioids
36.4 
55.4 
8.98 
17.39
Oral Hypoglycemic 
13.6 
8.5 
13.52 
15.90 
Proton Pump Inhibitors 
32.8 
22.9 
19.65 
25.81 
Skeletal Muscle Relaxants 
37.4 
26.5 
58.55 
36.15 
Statins 
49.1 
31.5 
27.8 
43.74 
Triptans 
34.6 
27.4 
39.64 
54.10 
Urinary Incontinence 
43.2 
29.6 
20.8 
44.95 
Average 
40.8 
33.5 
37.06 
43.81 
Total 
611.70 
502.20 
555.85 
657.18 
SVM: Support Vector Machine 
FCWB/WE: Factorized Complement Naïve Bayes with Weight Engineering 
Table 3.2: Performance comparisons with lexical approaches 
The repeated ANOVA test shows significant differences across the four studies (p=0.035), 
but the pairwise comparisons with the Hommel adjustment do not find significant 
differences (p>0.05). However, when examining each individual dataset, we outperform 
Cohen 2008 in 8 datasets, outperform Matwin 2010 in 13 datasets, and outperform our 
lexical approach in 12 datasets. We also have the highest average WSS95 of 43.81% and 
the highest total WSS95 of 657.18%. 
72 
3.4.4 Performance Comparisons with Corpus-based Semantic Approaches 
With the same goal to facilitate the identification of high relevant biomedical articles, 
Jonnalagadda 2013 [36] and Khabsa 2016 [37] used corpus-based semantic approaches 
with word embedding and brown clustering respectively. With the same DERP dataset, 
Jonnalagadda 2013 reported WSS95 using the abstract level decision, and Khabsa 2016 
reported WSS at a various recall level using the full-text level decision. We examine 
performances at both abstract and full-text levels to have side-by-side comparisons with 
them. In addition, because Khabsa 2016 reported WSS with various recall levels, ranging 
from 
75.00% 
to 
99.50%, 
for 
each 
individual dataset, 
we 
also 
generate 
WSS 
with 
corresponding recall levels to make exact comparisons with them. Again, we use the results 
obtained from the flexible depth thresholds 
As shown in Table 3.3, we have a better WSS95 performance on average and in 
total than Jonnalagadda 2013 (19.24% vs 13.17%, and 288.66% vs 197.56%) and Khabsa 
2016 (40.98% vs 31.25%, and 614.68% vs 468.8%). With pairwise comparisons, we do 
not find a significant difference between our approach and Jonnalagadda (2013) (p=0.072), 
but find a significant difference between our approach and Khabsa 2016 (p=0.031). In 
addition, when examining each individual dataset, we outperform Jonnalagadda 2013 in 
13 datasets, and outperform Khabsa 2016 in 10 datasets. In summary, our UMLS ontology-
based semantic approach provides a better performance in identifying high relevant articles 
in the biomedical and clinical domain than the existing corpus-based semantic approaches. 
73 
Table 3.3: Performance comparisons with other semantic approaches 
3.5 Summary 
In this chapter, we demonstrate using ontology-based semantics to derive semantic article 
features and representations, to facilitate the identification of high relevant articles with 
respect to the application of biomedical literature retrieval. In particular, we leverage the 
semantic knowledge encoded in UMLS ontologies i.e. SNOMED-CT and MeSH, and 
create a novel pipelined process to map each article title and abstract to an optimized and 
enriched set of UMLS concepts as semantic article features. Within a weighted semantic 
vector space consisting of UMLS concepts, we obtain semantic article representations and 
calculate semantic article similarities, which then perform as inputs to our interactive 
article recommendation model to facilitate the identification of relevant articles. Instead of 
using distributional semantics learned from corpuses preferentially for specific topics, our 
Dataset 
Abstract level decisions 
WSS95 (%) 
Full-text level decisions 
WSS (%) at various recall levels 
Jonnalagadd
a 2013 
Our 
Approach 
a
Recall 
Khabsa 
2016 
Our 
Approach 
ACE Inhibitors 
13.40 
18.00 
98.6 
46.90 
53.34 
ADHD 
29.63 
78.03 
98.00 
44.70 
81.55 
Antihistamines 
16.45 
22.90 
75.00 
3.00 
37.42 
Atypical Antipsychotics 
6.07 
9.11 
96.00 
19.90 
28.39 
Beta Blockers 
7.24 
9.51 
94.70 
36.10 
68.48 
Calcium Channel Blockers 
8.13 
9.77 
98.00 
28.70 
16.34 
Estrogens 
30.16 
32.07 
98.30 
18.00 
16.03 
NSAIDS 
12.98 
20.87 
99.50 
40.40 
52.93 
Opioids 
6.11 
5.12 
93.30 
45.50 
39.11 
Oral Hypoglycemic 
15.31 
16.30 
97.10 
7.40 
5.77 
Proton Pump Inhibitors 
10.80 
13.73 
97.60 
28.80 
21.08 
Skeletal Muscle Relaxants 
6.15 
16.37 
82.20 
37.10 
50.64 
Statins 
15.15 
16.74 
94.10 
40.00 
44.54 
Triptans 
13.56 
13.11 
95.80 
31.20 
54.10 
Urinary Incontinence 
6.42 
7.03 
94.00 
41.10 
44.95 
Average 
13.17 
19.24 
94.15 
31.25 
40.98 
Total 
197.56 
288.66 
-- 
468.8 
614.68 
a 
Various recall levels used in Khabsa 2016[14], ranging from 75.00% to 99.50% 
74 
approach can easily apply to any topics in the biomedical and clinical settings with gold-
standard and background knowledge from the UMLS ontologies. 
Our results show that using SNOMED-CT and MeSH together provide better 
performance than using SNOMED-CT or MeSH alone. While flexible depth thresholds are 
preferred 
in 
resulting 
the 
best 
performance 
for 
different 
datasets, 
we 
also 
obtain 
competitive results when using a fixed depth threshold. When comparing to other studies, 
we are able to achieve an average of 43.81% WSS95, which is better than other approaches 
using lexical features. In addition, we also confirm that our ontology-based semantic 
approach provided better performance than existing corpus-based semantic approaches. 
Worth mentioning, our performance relies on the scope, coverage, and quality of 
the ontologies, which determine the knowledge that we can obtain to annotate texts and 
capture contributing semantics. However, SNOMED-CT and MeSH are the most widely 
used standards in related studies. We consider SNOMED-CT and MeSH as sufficient 
resources for our current task in promoting relationships among biomedical articles and 
assisting with biomedical literature retrieval. Besides, like we have discussed in Chapter 2, 
due to the fact that different tasks have various scopes, we could not identify a universal 
parameter setting i.e. depth threshold. Therefore, a tunable depth threshold is especially 
helpful to meet different criteria or expectations in concept specificity. In addition, it is 
likely that we could further improve the performance through adjustable weights for 
concept categorizations (
α, β
) and the contribution of title and abstract (
𝑤LL⃗
). 
Highlight of Contribution 
75 
We develop a novel semantic concept modelling process (e.g. a pipelined process with 
three stages) to derive optimized and enriched concepts as semantic features to effectively 
represent a biomedical article, by levering relevant concepts and concept relations encoded 
in UMLS ontologies. This ontology-based semantic approach demonstrates to outperform 
not only lexical approaches but also some existing corpus-based semantic approaches. 
This work has been published in JBI 2017: 
Xiaonan Ji, Alan Ritter, Po-Yin Yen. Using Ontology-based Semantic Similarity to 
Facilitate the Article Screening Process for Systematic Reviews. Journal of Biomedical 
Informatics. 2017 May 31;69:33-42.
76 
Chapter 4. Corpus-based Semantic Analysis with Neural Document Embedding 
In the previous chapters of text analytics, we presented both a lexical approach and an 
ontology-based 
semantic approach to generate 
structured article representations and 
similarities to assist in biomedical literature retrieval. While the lexical approach is 
established as a strong baseline, the ontology-based semantic approach gains significant 
performance improvement by leveraging external semantic knowledge from UMLS. In this 
chapter, we continue promoting effective article representations with an affordable and 
generalizable approach, which is motivated by the superior computational power of neural 
networks (NN). In particular, as a group of neural networks, neural embeddings not only 
ease the notorious curse of dimensionality, but also show to capture essential linguistic 
aspects 
(e.g. 
semantics) 
of 
text 
segments 
with 
fixed-length 
vector 
representations 
(embeddings), thus benefit a wide range of downstream applications, including information 
retrieval, natural language processing (NLP), machine translation, etc. [14]–[16], [97] 
With respect to the application of biomedical literature retrieval, we propose a 
neural document embedding model to learn semantic representations of biomedical 
articles, which are considered text documents of variable-length. In order to promote the 
effectiveness and efficiency, we advance the general-purpose Paragraph Vector (PV) 
model [90], which is a well-known 2-layer neural network, with a multi-task learning 
scheme to integrate the interest of specific information retrieval via minimal supervision. 
77 
As this model mainly derives semantics from text corpus, it is referred to as a corpus-based 
semantic approach in this dissertation. As it is trained with multi-task objectives to capture 
not only the context semantics but also the interests of specific information retrieval, it is 
named as a Multi-Task Paragraph Vector model (PV-MT). 
In this chapter, we follow the storyline to develop a neural network model [98], and 
address the rationality of selecting the model architecture (i.e. the PV model) and 
employing the multi-task learning scheme, with respect to the important linguistic aspects 
for the application of biomedical literature retrieval. We evaluate the PV-MT model with 
the downstream applications of interactive article recommendation, and demonstrate the 
improved effectiveness comparing to the strong lexical baseline as well as the original PV 
model. Although this corpus-based semantic approach is not as competitive as the 
ontology-based semantic approach, it is independent of external knowledge resources and 
is 
more 
time 
and 
resource 
efficient. 
In 
addition, 
the 
generated 
article 
semantic 
representations are low-dimensional, fixed-length, and dense vectors (embeddings), which 
can be readily applied to downstream applications along with machine learning and 
statistical models. 
4.1 Research Questions 
Before going to more details into this chapter, we formulate and highlight our research 
questions. Firstly, by following the storyline of designing a neural embedding model, we 
propose two preliminary questions, which are addressed by knowledge and findings 
summarized from a review of recent and related works. 
78 
•
Text semantics can be approached from multiple linguistic aspects by corresponding 
learning objectives, and the linguistic aspects are valued differently in specific tasks 
(applications). Besides, text semantics can be retrofitted with the interests of specific 
tasks, such as supervision from desired tasks. Consider our recognized application of 
information retrieval towards biomedical articles, which are long texts (e.g. documents) 
consisting of multiple sentences, which linguistic aspects shall be valued to promote 
effective semantic article representations? 
•
Among the many types of neural network architectures, including shallow neural 
networks (e.g. PV), convolutional neural networks (CNN), and recurrent neural 
networks (RNN) and their variants (e.g. LSTM and GRU), which model architecture is 
likely to capture important linguistic aspects, produce effective representations of 
biomedical articles, and being cost efficient and generalizable at the same time? 
Secondly, after addressing the aforementioned preliminary questions and determined the 
linguistic aspect (learning tasks) and model architecture, we propose our PV-MT model, 
and include the following questions to validate our model design and its performance. 
•
By advancing the PV model with multi-task learning, which integrates the interests of 
specific information retrieval, will the PV-MT model produce improved performance 
in biomedical literature retrieval comparing to the original PV model? 
•
With the PV-MT model applied to biomedical literature retrieval, can we promote more 
effective article representations comparing to the lexical baseline? In addition, can we 
achieve comparable or competitive performance to the ontology-based semantic 
approach which takes advantages of the gold-standard domain knowledge? 
79 
4.2 Background and Related Works 
4.2.1 An Overview of Neural Embedding 
Neural embedding can be considered as a collective name of a group of neural network 
models for language modelling and feature learning, where text pieces are mapped to 
vectors of real numbers. It typically involves a mathematical embedding that converts text 
pieces from their original space, which is likely to have a high dimensionality, to a 
continuous vector space with a much lower dimensionality. Besides the dimensionality 
reduction, neural embedding also intends to resolve one critical question of knowledge 
representation in language understanding, for instance, how to capture the underlying 
meaning of text pieces in a machine understandable manner [99]? While atomic words as 
the fundamental type of texts, are first studied with word embedding models, more recent 
models are developed to address other types of texts with variable-length, ranging from 
phrases, sentences, paragraphs, and documents (articles) [90], [99], [100]. 
Text 
semantics 
can 
be 
approached 
from 
different 
linguistic 
aspects. 
Many 
unsupervised neural embedding models leverage the distributional semantics of words and 
terms (e.g. n-grams) across the corpus to capture the semantics of short-term (local) 
context. The basic intuition behind is that terms tend to appear in similar contexts are likely 
to be semantic similar or related to each other, considering their similar linguistic properties 
and contributions. Under this notion, a large and high-quality corpus is usually in need to 
perform as the semantic knowledge resource in proving extensive distributional semantics 
and context information. On the other hand, many supervised neural embeddings models 
80 
are trained to capture more sophisticated linguistic aspects, such as long-term dependencies 
and sentence structures, or to capture salient features by encoding task-specific interests. 
Neural 
embedding 
models 
are 
shown 
to 
produce 
high-quality 
semantic 
representations of texts, which are typically low-dimensional, fixed-length, and dense 
vectors. Such text representations can be readily applied to facilitate many downstream 
applications, including (1) information retrieval which usually involves text matching, 
classification, clustering, ranking, etc., (2) NLP tasks such as sentiment analysis, sentence 
modeling, sentence parsing, etc., (3) machine translation such as the translation between 
English and French, and the translation among images, audios, and languages. Neural 
embedding models have also demonstrated superior performance comparing to lexical and 
conventional embedding methods [101][102] such as principal component analysis (PCA), 
singular 
vector 
decomposition 
(SVD), 
latent 
Dirichlet 
allocation 
(LDA) 
[103], 
probabilistic 
latent 
semantic 
analysis 
(PLSA) 
[104]. 
They 
also 
show 
to 
augment 
performance of many machine learning models, such as Naïve Bayes, SVM, logistic 
regression, k-means, t-SNE, etc., and effective 2D visualizations which rely on proper 
knowledge representations [105]. Some related research topics include feature generation, 
latent semantic analysis, dimensionality reduction, vector space model, and so on. 
4.2.2 A Design Process of Neural Embedding 
An advisable design of a neural embedding model should be guided by the targeted text 
type and the objective application. The quality of a model design can directly influence the 
effectiveness of the resulting text representations. In specific, a model design consists of 
81 
two perspectives: (1) selecting the neural network architecture, and (2) selecting the task(s) 
for model training. 
Different model architectures can have diverse capabilities in capturing different 
linguistic aspects, which are valued differently in objective applications. In other words, a 
model architecture might be preferred in particular applications [98], [106], [107]. There 
have been many types of neural network architectures, and a common categorization is 
based on the complexity and depth of neural networks. In particular, shallow networks with 
simpler architectures tend to capture short-term (local) linguistic features, and usually have 
a 
better 
efficiency, 
simplicity, 
and 
scalability [108]. 
For 
example, 
the 
well-known 
Paragraph Vector model (PV) has been widely used in information retrieval applications 
towards 
texts 
of 
variable-length 
and 
achieved 
competitive 
performance 
with 
high 
generalization [90], [109]. Besides, existing works also showed that direct manipulations 
(e.g. weighted average or alignment) of word vectors, which are learned with a shallow 
model, can also achieve effective and efficient performance in sentence classification 
[110]. On the other hand, deep networks with more sophisticated architectures tend to 
capture 
long-term 
and 
complicated 
linguistic 
features, 
and 
usually 
have 
higher 
requirements on computational intensity and parameter tuning [106]–[108], [111]. For 
example, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) 
are deep networks that are used in widespread applications. CNNs are commonly used in 
text classifications, while RNNs are more commonly used in language modeling and 
sentiment analysis [107], [111], [112]. 
82 
With a selected model architecture, the next question is how and on what tasks (and 
resources) should the model be properly trained. Basically, a model can be trained in an 
unsupervised or supervised manner [111]. An unsupervised method usually comes with 
unsupervised objectives, which don’t directly optimize desired tasks which are derived 
from the intended applications. Under this notion, auxiliary tasks are generated using the 
context information across the text corpus, such as word co-occurrence, adjacent phrases, 
and sentence pairs (original and corrupted sentence, and adjacent sentences). Without 
depending on task supervision, an unsupervised method is usually more generalizable 
across different tasks, however it might have a weaker predictive power on specific tasks. 
Most shallow models (e.g. Skip-gram, CBOW [94], [100], PV [90]) and some deep models 
(e.g. Skip-Thought [113], SDAE [106], FastSent [106]) are trained in an unsupervised 
manner. On the contrary, a supervised method comes with supervised objectives, which 
usually directly optimize desired tasks, and requires supervised training datasets. While 
supervised methods can fully leverage labeled information designed for specific objectives 
and result in stronger predictive power on specific tasks, they have limited generalization 
because of the high dependency on prior supervised training data. Most deep networks are 
trained in a supervised (or semi-supervised) manner, for example, recursive networks, 
recurrent networks, convolutional networks, and recursive-convolutional networks [98], 
[108], [109], [112], [114]–[121]. And these deep models are shown to handle the labeled 
data effectively with their high computational and representational power. In summary, the 
differences between unsupervised and supervised models lie their different capabilities to 
incorporate general information or value specific features useful for the tasks at hand [98]. 
83 
For both types of the models, multi-task objectives are likely to outperform a single-
task objective [111], [116]. Specifically, when the training data is extremely large, training 
all parameters from a single-task would be adequate; otherwise, it’s better to learn semantic 
representations from multi-task objectives, which could leverage multiple information 
resources and capture multiple linguistic aspects. 
4.2.3 Existing Neural Embedding Models 
We present an overview of the most commonly used and representative neural embedding 
models, which generate semantic representations of texts, ranging from phrases, sentences, 
to documents, for many downstream applications including information retrieval. 
Paragraph Vector (PV)
[90], [99] is a 2-layer neural network inspired by the 
word2vec model [89], [94]. Paragraph is referring to any length of text, ranging from 
phrase, 
sentence, 
paragraph, 
to 
document. The 
PV 
model 
learns 
distributed 
vector 
representations in an unsupervised manner. Specifically, it leverages the distributed 
semantics conveyed by the context information, with the intuition that terms with similar 
contexts are likely to be similar. Because of the encoding of salient features in contexts, it 
also demonstrates to produce competitive performance in information retrieval especially 
for long text pieces e.g. documents. In the PV model, paragraph (article) representations 
are trained to predict words in the paragraphs. There are two types of PV models: (1) the 
distributed memory model (PV-DM), and (2) the distributed bag of words model (PV-
DBOW). As PV-DM is shown to generate better performance in most scenarios, we use 
PV-DM throughout this work. As shown in Figure 4.1, PV-DM concatenates or averages 
the paragraph vector with a set of context word vectors to predict the next word in the 
84 
context window. A context is fixed-length and sampled from a sliding window over the 
given paragraph. The paragraph vector is shared among all contexts (training instances) 
and is repeatedly updated towards the objective, thus it is considered a memory of all 
contexts and a topic of the paragraph. The PV-DM model has two key advantages: 1) it 
inherits the semantics from word vectors, and 2) it handles word orders and captures the 
semantics of local contexts. 
Figure 4.1: An illustration of the Paragraph Vector model [90] 
Convolutional neural network
is a class of deep hierarchical models. It’s generally 
trained on the top of pre-established word vectors to capture local contexts and short-term 
word dependencies (i.e. n-grams) with a sliding window, and results in multiple feature 
detectors for each window size on the convolutional layer [109]. Max pooling or k-max 
pooling are usually used to form global features. While conventional CNNs are mainly 
85 
designed to capture salient local features such as consecutive words forming n-gram terms, 
they might not able to capture long-term dependencies or sophisticated sentence structures. 
Therefore, some advanced variants are developed to capture richer features, such as 
regional features [122], dependency-tree-based features [117], and hierarchical features 
[118], and dynamic k-max pooling [123] and dynamic multi-pooling [124] are used to 
synthesize more sophisticated global features. CNNs have been widely used in many 
classification tasks with full-supervision or semi-supervision. A simple CNN with one 
convolutional layer also shows to produce favorable performance in information retrieval, 
sentence classification, sentiment prediction, etc. [109], [122] A simple model could be 
advanced by integrating more layers, regional features, and syntactic features. 
Recurrent neural network is a class of deep sequential models. It treats a sentence 
as a sequence of words with internal structures, and reads in and maps each word (from 
left to right) into a hidden space with accumulated history information. In this sense, it 
encodes a sentence vector in an incremental manner, and the hidden vector for the last word 
accumulates all information of the entire sentence. Supervisions on sentence-level labels 
are usually required to train an RNN, for example, paraphrases [98], [113] and query-
document pairs [115]. To promote the efficiency and generalization of RNNs, recent works 
have proposed unsupervised schemes that automatically generate sentence pairs, such as 
adjacent sentences [113], and original VS. corrupted sentences [106]. Long short-term 
memory (LSTM) cells and gated recurrent units (GRU) can be integrated to better capture 
sophisticated linguistic features, such as sentence structures and long-term dependencies 
86 
[98], 
[115]. 
RNNs 
has 
demonstrated 
to 
produce 
high-quality 
results 
for 
language 
modelling, language textual entailment, sentiment analysis, QA, etc. [107], [111], [112] 
Recursive neural network aims to capture long-term dependency (compositionality) 
of sentences, and relies on language specific parsing information e.g. a dependency tree 
with various syntactic relations [119], [125]. Thus, it is usually limited to single sentences. 
4.2.4 Task-Specific Interests 
For many applications (tasks), task-specific interests or task specifications are important 
considerations to explicitly indicate the task values and enable model specialization [108], 
[116]. One motivating example is about the keywords and noises lie in an information 
retrieval task [115]. Given an information need, not every word is equally important; and 
a word that is important to a document might not be relevant to a given information need. 
Therefore, we only want to remember the salient words and emphasize the task-relevant 
words. With task specifications, we could tell which words are bearing higher importance, 
and generate more effective representations for the intended applications. 
In particular, the quality of a supervised model can be significantly impacted by the 
tasks on which it is trained. And additional attention mechanisms which are directly related 
to task objectives can be integrated to enhance the performance targetedly [119][120][121]. 
The unsupervised models, on the other hand, have been criticized about the ignorance of 
task-specific interests. A general-purpose model which is trained on unsupervised corpus 
might not always produce favorable performance [115]. Related studies have proposed to 
integrate additional task-specific interests into existing neural embedding models, and 
many of the schemes are considered as Attention Mechanisms. For example, attention-
87 
based sentence embedding emphasizes and assigns high weights to recognized key parts of 
a sentence, which can be a set of keywords highly associated with the task-specific 
interests. In particular, this attention mechanism has been utilized in a weight-sharing CNN 
with an attention-based matrix to weight the convolutions [120], and a biRNN-LSTMs with 
an 
attention-based 
weighting 
on 
LSTM 
hidden 
states 
[121]. 
Besides, 
aspect-based 
sentiment analysis [119] allows users to provide particular sentiment aspects reflecting the 
task-specific interests. The model then emphasizes the entities related to the aspects along 
with the corresponding opinions being expressed. 
Inspired by the related studies, we summarize that: (1) The essential meanings of 
text pieces can be contributed by particular key parts (e.g. keywords and key terms) which 
are highly associated with the task-specific interests, (2) It is possible that even a “shallow” 
component related to the task-specific interests (e.g. attentions) can be more important than 
sophisticated components designed for advanced syntactic or semantic features in some 
application scenarios, (3) The key parts can be emphasized by alignment (matching) and 
weighting, and (4) In some cases, additional dimensions or model components can be used 
to encode, capture, or highlight the key parts. However, while there have been studies 
advancing CNNs and RNNs with task-specific interests (e.g. attention mechanisms) and 
achieved impressive performance, there is a notable gap in promoting the well-known and 
general-purpose PV model with task-specific interests. 
4.3 Model Design 
Inspired by the background information and recent trends in neural embeddings, in this 
section, 
we 
present 
our 
model 
design 
with 
respect 
to 
the 
intended 
application 
of 
88 
information retrieval towards high relevant biomedical articles, expecting to promote an 
effective, efficient (time and resource affordable) and generalizable model. 
4.3.1 Linguistic Aspects for Information Retrieval 
Linguistic aspects (features) are corresponding to the level or depth of semantics we could 
derive from texts, and different linguistic aspects can be favored by different applications. 
In this study, we approach potential linguistic aspects of interests based on our intended 
application of information retrieval, as well as the targeted text type of biomedical articles. 
Figure 4.2: Linguistic aspects and their applicability 
Firstly, we review and summarize the applicability of different linguistic aspects in 
practical settings based on related studies as we have mentioned in Section 4.2. As shown 
in Figure 4.2 (a), we categorize the linguistics aspects into four levels: elementary word, 
bag-of-words, local context and short-term dependency (e.g. n-grams), and long-term 
89 
compositionality and dependency (e.g. sentence structure). Besides the elementary word 
semantics which performs as the foundation of the higher-level semantics, we connect and 
assign other linguistic aspects from the left box to their preferable applications in the right 
box. According to the demonstrated associations between the linguistic aspects and the 
spectrums of applications, bag-of-words and local context are favorable in information 
retrieval applications, where keywords and key terms are valued. On the other hand, long-
term 
compositionality 
and 
dependency 
are 
mostly 
used 
in 
applications 
where 
the 
comprehension of an entire sentence is needed. Consider our intended application of 
biomedical literature retrieval, we mainly focus on the semantics of local context. 
Secondly, we work towards the special format of a biomedical article, which 
consists of a title and an abstract, thus is considered a long text document with multiple 
sentences. Existing studies have mainly addressed words, phrases, and sentences; although 
the PV model has demonstrated to work around text pieces of variable-length, including 
paragraphs and documents [99], there is limited understanding of its performance on text 
pieces with multiple sentences. In fact, a text document tends to carry richer and more 
diverse information because of its length and the composition of multiple sentences, and it 
is more challenging to derive the essential meanings with respect to an information need. 
As different sentences and terms can contribute differently to a document’s essential 
meanings, it is important to identify and emphasize the key parts which positively 
contribute to or highly associated with task-specific interests. Therefore, to promote our 
information retrieval applications, we propose to integrate task-specific interests to capture 
the additional aspects of: (1) intra-document emphasis of the key parts, and (2) inter-
90 
document alignment of the key parts. More details on the task-specific interests will be 
described in the next section. 
4.3.2 Task-Specific Interests and Annotations 
In the previous section, we have elaborated the necessity of integrating task-specific 
interests in our objective application of information retrieval towards biomedical articles. 
In this section, we describe the identification of document key parts (e.g. keywords and 
key terms) with respect to the task-specific interests. 
Inspired by related works of attention mechanism, we identify a set of salient words 
and noun phrases from the descriptive texts of an information need, which can be a set of 
key questions for biomedical literature retrieval. In particular, for the applied DERP 
datasets of drug effectiveness reviews [26], key questions have been made available and 
maintained in research protocols. As we mentioned in Chapter 2, the extracted key terms 
typically follow the PICO format, which covers the most important facets of a biomedical 
article in evidence-based medicine and clinical decision making, including Population (e.g. 
patient, disease, symptom), Intervention (e.g. treatment, drug name), and Outcome (e.g. 
effectiveness, safety, adverse effect). As illustrative examples in Figure 4.3, we present the 
original key questions and the extracted key terms for the ACE Inhibitor dataset and task. 
We then annotate biomedical articles with the recognized key terms, which will be 
used to locate the key parts, thus capture the intra-document emphasis and inter-document 
alignments. Because the task-specific interests don’t rely on prior supervised training data 
or expensive labels on individual articles, only the overall information needs are examined 
91 
to extract high-level key terms and automatically annotate articles, we consider it as a 
minimal supervision that makes the best use of available information in an efficient way. 
(a) Key questions of information retrieval 
(b) Extracted key terms and their categories 
Figure 4.3: Illustration of task-specific interests for ACE Inhibitor dataset [26] 
4.3.3 Model Architecture Selection 
While there are many different types of neural model architectures, we select the model 
architecture based on two criteria: 1) it is able to capture the proposed linguistic aspects in 
92 
Section 4.3.1 and 4.3.2 and promote effective performance for our intended application of 
biomedical literature retrieval, and 2) it is a cost efficient and resource efficient (affordable) 
model that can be utilized in widespread biomedical and clinical settings. 
In this sense, we decide to employ the 2-layer architecture of the PV model [90] 
(Figure 4.1). The rationalities can be explained from multiple perspectives: (1) The shallow 
PV model is designed to capture the fundamental linguistic features, e.g. semantics of local 
contexts such as n-grams, for information retrieval tasks, and has been shown to achieve 
competitive performance comparing to deep models, especially for long texts. (2) The 
shallow model is more scalable and efficient. Unlike deep models such CNNs and RNNs, 
it doesn’t rely on intensive computations or extensive parameter tuning, thus is more 
affordable and approachable in practical settings. (3) The shallow model is more commonly 
used in unsupervised settings, instead, many deep models tend to have a higher dependency 
on supervised settings, which is not always available. Furthermore, task-specific interests, 
such as attention mechanisms, have been used in CNNs and RNNs and demonstrated 
improved performances. However, to the best of our knowledge, there still is a gap between 
the classic and general-purpose PV model and the integration of task specifications. 
4.3.4 Multi-Task Paragraph Vector (PV-MT) Model 
We propose a Multi-Task Paragraph Vector model (PV-MT) with the design rationalities 
described in the previous sections. As shown in Figure 4.4, the PV-MT model leverages 
the 2-layer architecture of the general-purpose PV model to capture the fundamental 
context semantics in an unsupervised manner, and integrates the interests of our intended 
information retrieval (IR) application with the minimal supervision of high-level keywords 
93 
and corresponding annotations. We employ curriculum learning [126], which is a multitask 
learning schemes with a sequential order that can result in improved performance for a 
small model or low resource setup, to train the PV-MT model. More specifically, two 
auxiliary tasks are implemented in sequence to capture the valued linguistic aspects, 
including word semantics, local context semantics (n-grams), intra-document weighting of 
key parts, and inter-document alignment of corresponding key parts. 
Figure 4.4: Illustration of Multi-Task Paragraph Vector (PV-MT) model 
•
Task 1
: Using a document token (document vector) and a set of context words (word 
vectors) from a sliding window to predict the target word in the sliding window. This 
task is inspired by the PV model with distributed memory (PV-DM). In this sense, the 
document token is trained to inherit the word semantics and further encode the local 
context semantics, such as n-grams with orders. 
94 
•
Task 2
: Using a document token (document vector) and a recognized keyword (word 
vector) or key term (averaged word vectors) to predict the keyword’s or term’s context 
words. We consider the entire sentence which contains a keyword (term) as the context 
of the keyword, according to the annotations described in Section 4.3.2. 
4.4 Experiment Settings and Results 
4.4.1 Experimental Settings 
We use the 
doc2vec
component from the 
genism
toolkit [127], [128] to initiate the PV-MT 
model. The PV-MT model was first trained on the text corpus for the Task 1, which can be 
considered as a realization of the PV-DM model. Our text corpus consists of not only article 
titles (sentences) and article abstracts (paragraphs), but also individual sentences extracted 
from articles abstracts to enrich the text corpus for more training instances. Sentences and 
paragraphs from the same articles were assigned with the same labels. For the parameter 
setting, we use a window size of 8, as a window size between 5 and 12 is recommended 
for the PV-DM model, and no obvious improvement is achieved by other window sizes in 
our preliminary experiments. We take an embedding size (vector dimensionality) of 200, 
as the PV-DM is less sensitive to the embedding size and no obvious improvement is 
achieved by other embedding sizes in our preliminary experiments. Finally, we implement 
20 epochs of training with shuffled training instances, and apply an initial learning rate of 
0.025, which is decreased by 0.001 at each epoch. 
For the Task 2, we generate additional training instances using the recognized 
keywords and the corresponding annotations of the text corpus. For example, a training 
instance can be a sentence with an annotated keyword. These instances are also labeled by 
95 
the articles where the sentences are extracted. The model which has been trained on Task 
1 is now further trained on Task 2, as an integration of the information retrieval application 
specific interests. For the parameter setting, we keep the same embedding size of 200. 
Besides, we implement 5 epochs of training with shuffled training instances, and apply an 
initial learning rate of 0.025, which is decreased by 0.005 at each epoch. 
4.4.2 Experimental Results 
Similar to the previous chapters, we employ the interactive article recommendation model 
to evaluate semantic article representations learned with PV-MT. Specifically, we conduct 
experiments on the 15 DEPR datasets and use WSS95 as the performance measure. Table 
4.1 shows the results and side-by-side comparisons to the original PV model, our lexical 
baseline approach, and the ontology-based semantic approach. 
Dataset 
PV 
PV-MT 
Lexical 
Baseline 
Ontology-based 
Semantic 
ACE Inhibitors 
0.6155 
0.6800 
0.7429 
0.8557 
ADHD 
0.7117 
0.7413 
0.6792 
0.8355 
Antihistamines 
0.0044 
0.2062 
0.2452 
0.2387 
Atypical Antipsychotics 
0.1886 
0.2417 
0.1795 
0.3518 
Beta Blockers 
0.3948 
0.4360 
0.6501 
0.6848 
Calcium Channel Blockers 
0.1558 
0.2804 
0.1732 
0.2553 
Estrogen 
0.2900 
0.3059 
0.2255 
0.3207 
NSAIDS 
0.5587 
0.5431 
0.7735 
0.6489 
Opioids 
0.4373 
0.5091 
0.0898 
0.1739 
Oral Hypoglycemics 
0.1084 
0.1468 
0.1352 
0.1590 
Proton Pump Inhibitors 
0.2140 
0.3151 
0.1956 
0.2581 
Skeletal Muscle Inhibitors 
0.0683 
0.4903 
0.5855 
0.3615 
Statins 
0.2400 
0.3225 
0.2780 
0.4374 
Triptans 
0.2794 
0.4360 
0.3964 
0.5410 
Urinary Incontinence 
0.2942 
0.3853 
0.2080 
0.4495 
Average 
0.3041 
0.4027 
0.3705 
0.4381 
PV: Paragraph Vector Model 
PV-MT: Multi-Task Paragraph Vector Model 
Table 4.1: Performance report and comparisons 
96 
As shown in Table 4.1, PV-MT competes PV in 14 out of 15 datasets. The average 
performance of PV-MT is 0.4027, while the average performance of PV is 0.3041. This 
demonstrates that our advanced PV-MT model with the integration of task-specific 
interests can promote the effectiveness of semantic representations in information retrieval. 
In addition, PV-MT competes our lexical approach in 10 out of 15 datasets. The 
average performance of PV-MT is 0.4027, while the average performance of the lexical 
approach is 0.3705. Because lexical approaches are usually considered strong baselines in 
information retrieval towards long texts, the advantageous performance of PV-MT is very 
encouraging. 
Finally, PV-MT competes our ontology-based semantic approach only in 4 out of 
15 datasets, while being comparable in 3 out of 15 datasets (< 4% difference). The average 
performance of the ontology-based semantic approach is as high as 0.4381. We consider 
such results are explainable. Firstly, our PV-MT (also PV) model doesn’t rely on any 
external knowledge resource, instead, the task corpus itself performs as the main semantic 
resource. Secondly, the UMLS ontologies are gold-standard semantic knowledge resources 
in the biomedical domain. As the quality of semantic resources greatly impacts the 
effectiveness of learned semantic representations, it is not surprising that the ontology-
based semantic approach can result in more favorable performance. 
4.5 Summary 
In 
this 
chapter, 
we 
implement 
a 
corpus-based 
semantic 
approach 
to 
facilitate 
the 
application of biomedical literature retrieval, and explore the possibility of promoting an 
effective, 
efficient, 
and 
generalizable 
solution. 
Being 
motivated 
by 
the 
superior 
97 
computational capability of neural networks, we propose a new neural embedding model, 
Multi-task Paragraph Vector (PV-MT), to learn effective semantic representations of 
biomedical articles. Under this notion, we not only leverage the Paragraph Vector model 
to capture the fundamental context semantics from the text corpus, but also integrate the 
information retrieval task-specific interests to overcome the limitation of general-purpose 
models and advance the performance. 
We 
utilize 
the 
learned 
semantic 
representations 
in 
the 
active 
article 
recommendation model, and evaluate the effectiveness comparing to other approaches. 
Specifically, our PV-MT model outperforms the PV model in 14 out of 15 DERP datasets, 
which demonstrates that the integrated task-specific interests can improve the effectiveness 
of neural document embeddings in information retrieval. It also validates our hypothesis 
that the emphasis and alignment of key document parts are important linguistic aspects in 
addition to the local semantics captured in the PV model. Besides, as a corpus-based 
semantic approach that doesn’t rely on any external knowledge resources, our PV-MT 
model shows to result in better performance in 10 out15 datasets and better overall 
performance comparing to the strong lexical baseline approach. Finally, although our PV-
MT model only outperforms the ontology-based semantic approach in 4 out of 15 datasets, 
it can be a strong alternative semantic approach when (1) an external ontology or 
knowledge resource is not available, (2) a cost-efficient and generalizable solution is 
needed, or (3) effective semantic representations that are of fixed-length and low-
dimensionality are needed for downstream applications. 
98 
Before closing this chapter, we would also like to present some discussions that 
could motivate or be considered in future work. Firstly, the selection of keywords or key 
terms can be an important influential factor for the resulting performance. In most cases, a 
key term shouldn’t be too specific, for example, a categorical term (e.g. “estrogen”) could 
work better than a specific drug name. At the same time, the selection of key terms should 
avoid noises from some common or frequent terms, as these terms can come with extremely 
large, diverse, and noisy contexts across the text corpus. 
Secondly, existing neural embedding models are mainly developed to handle text 
documents written in general natural languages, such as questions and answers, movie 
reviews, google news, Wikipedia articles, and so on. However, many scientific literature 
(e.g. Beta Blockers) can consist of proper or complicated elements, for example, numeral 
values, formulas, professional terminologies, etc., which might negatively affect the 
performance of both PV and PV-MT. In addition, the scope of some information retrieval 
tasks on scientific literature (e.g. Skeletal Muscle Relaxants) can be too diverse to identify 
particular key terms, even with the gold-standard high relevant articles. For example, the 
high relevant articles might cover a wide range of drugs and symptoms under a high-level 
scope, which would be retrieving articles mentioning comparisons among any drugs. PV 
and PV-MT tend to result in inferior results in such datasets and tasks, partially due to their 
mechanisms of capturing local semantics from sentences, instead of long-term or global-
structure-based semantics. Therefore, potential future directions include: (1) investigate the 
advantages and disadvantages of existing neural embedding models (e.g. PV, PV-MT, 
CNN, RNN) in handling different types of texts, e.g. natural language texts vs. scientific 
99 
literature, and (2) investigate how to properly encode scientific literature with respect to its 
special formats and language styles? 
Highlight of Contribution 
We 
advance 
an 
effective 
and 
efficient 
neural 
embedding 
model 
to 
learn 
semantic 
representations of biomedical articles. The Multi-Task Paragraph Vector (PV-MT) model 
advances the general-purpose Paragraph Vector (PV) model with the task-specific interest, 
which is realized via minimal supervision and multi-task learning. This corpus-based 
semantic approach demonstrates to outperform lexical baselines and can be a strong 
alternative to the ontology-based semantic approach. 
This work is planned to be submitted to a conference/journal of information retrieval with 
a cross-interest in biomedical informatics.
100 
Chapter 5. Demonstrating the Usefulness of Visualizing Article Relationships for 
Biomedical Literature Retrieval 
In the previous chapters through Chapter 2 to Chapter 4, we have developed a series of text 
analytics 
approaches 
to 
convert 
unstructured 
and 
extensive 
biomedical 
articles 
into 
structured representations in a machine understandable manner. More importantly, we use 
three different schemes to promote effective article representations and high-quality article 
relationships, which encode the underlying article meanings and can be employed to 
facilitate the identification of high relevant articles in biomedical literature retrieval. The 
article features, representation, and similarities (relationships) have been performing as 
inputs to our interactive article recommendation model, and can be applied to other 
downstream applications for information retrieval. 
While downstream applications are usually realized with machine learning models, 
such as text matching, clustering, classification, ranking, recommendation, etc. [32]–[34], 
[36]–[38], their approachability is typically limited to human researchers in the biomedical 
and clinical settings, because of the intensive training, parameter tuning, or dependencies 
on supervision. Meanwhile, human researchers cannot necessarily understand the abstract 
knowledge 
(e.g. 
article 
representations 
and 
relationships) 
and 
a 
human-involving 
exploratory process is discouraged or obstructed during biomedical literature retrieval. 
To bridge the gaps between human researchers and the established knowledge, 
especially article representations and relationships encoding essential article meanings and 
101 
indicating article relevancies, we use visualization and visual analytics to enable more 
advantageous exploitation and exploration of the knowledge in an approachable and 
human-involving manner. By leveraging the most reliant human cognition and analytic 
reasoning, a visualization of article representations and relationships is likely to reveal the 
underlying 
patterns 
and 
internal 
structures 
to 
human 
eyes, 
thus 
enable 
intuitive 
explorations and obtain insights for knowledge discovery [20], [47], [129], [130]. In other 
words, with such a mental image of visual article distribution, human researchers will have 
increased capability and confidence in identifying articles of importance or interests. 
In this chapter, we will first have a concept-level demonstration (concept proof) 
that visualizations of a biomedical article collection (text corpus) using established article 
representations and similarities (relationships) can promote biomedical literature retrieval. 
More specifically, the article distribution e.g. the structure of an article collection, and the 
clustering patterns revealed by the visual patterns in a 2D visualization can facilitate the 
identification of high relevant articles, with leveraged human cognition and analytic 
reasoning. This concept proof will perform as a preliminary work for the later chapters, 
which further promote effective and usable visualizations in practical settings. 
For the purpose of concept proof, we utilize a basic visualization scheme that 
applies the force-directed graph drawing algorithm [131], [132] to visualize a lexical article 
similarity network in a 2-dimensional (2D) space. The graph-base network has been 
commonly used to visualize entity relationships and relational patterns [47], [133]. Under 
this notion, we are able to employ several structural (topological) network properties, e.g. 
graph diameter and closeness centrality, to mathematically examine the article distribution 
102 
especially for the aggregation (centralization) of high relevant articles. To obtain a better 
insight into the clustering of similar articles, we implement a community detection 
(network 
clustering) 
algorithm, 
the 
Louvain 
method 
[134], 
to 
reveal 
the 
inherent 
community structure and examine the clustering patterns related to high relevant articles. 
Together, with the structural centralization properties and network clustering properties, 
we will be able to infer the distribution and the associated visual patterns (in the 2D space) 
of high relevant articles from benchmark datasets, and analyze whether such visual patterns 
would facilitate the identification of high relevant articles from human researchers’ views. 
5.1 Research Questions 
We formulate and highlight our research questions with respect to the purpose of evaluating 
and demonstrating the usefulness of visualizations (the established visual patterns) in 
promoting biomedical literature retrieval, i.e. the identification of high relevant articles. 
•
With a 2D visualization of an article similarity network, do high relevant articles from 
benchmark datasets tend to aggregate together in the 2D space, which can be reflected 
by the network centralization properties such as the diameter and closeness centrality? 
•
With a 2D visualization of an article similarity network, do high relevant articles tend 
to be clustered into the same and/or adjacent communities (clusters) in the 2D space, 
which can be approached with a community detection (network clustering) algorithm? 
•
Will the afore-mentioned visual patterns, such as the visual densities illustrated by the 
aggregation and clustering of high relevant articles, facilitate the progress of identifying 
high relevant articles from human researchers’ views? 
103 
5.2 Background and Related Works 
5.2.1 Network Visualization with Force-Directed Algorithm 
In graph theory, graphs are mathematical structures used to model pairwise relations 
between 
objects 
[135]. 
A 
graph 
in 
this 
context 
is 
made 
up 
of vertices, nodes, 
or points which are connected by edges, arcs, or lines. As a graph-based approach, network 
visualization is one of the most widely used techniques in information visualization, 
especially for the visualization of relationships or relational patterns [47]. With the graph-
based approach, nodes and edges are positioned in a 2D space based on a layout algorithm, 
which is also known as a graph drawing algorithm [131]. 
Force-directed graph drawing algorithms can draw networks in an aesthetically 
pleasing way [131], [132]. As a spatial layout, it places nodes and edges by simulating a 
physical system that assigns spring-like attractive forces to attract pairs of endnodes, and 
electrical repulsive forces to separate nodes. The layout criteria is converted to a cost 
function that can be optimized to find the layout for energy minima. When the system 
comes to a mechanical equilibrium state, the pairwise geometric distance between the 
drawn nodes tends to match with the graph theoretic pairwise distance. Specifically, similar 
article nodes tend to aggregate together while dissimilar article nodes are drawn apart. 
Structural or topological properties such as the graph diameter and node centralities can be 
accessed and analyzed with a graph-based approach. 
104 
5.2.2 Structural Network Properties 
Networks have nice properties that can be utilized to analyze their characteristics in a 
mathematical way [48]. Below we describe some of the most commonly used properties in 
evaluating the topological structure and patterns of large and complex networks. 
In a connected graph, there is a natural distance metric between all pairs of nodes, 
defined by the length of their shortest paths. 
Graph diameter
is the length of the shortest 
path between the most distanced nodes. In other words, a graph’s diameter is the largest 
number of nodes needed to travel from one node to another when paths which backtrack, 
detour, or loop are excluded from the consideration. A disconnected graph has an infinite 
diameter. As the graph diameter is the maximum eccentricity of any nodes in the graph, it 
is widely used to measure the topology and concentration (centralization) of a graph. A 
more concentrated graph usually comes with a smaller diameter. 
Closeness centrality
measures the farness from one node to all other nodes and is 
considered an indicator of network centralization. The more central a node is located, the 
lower is its total distances to all other nodes. In other words, closeness centrality can be 
used as a measure of how long it takes to spread information from a node to all other nodes 
sequentially. In this study, each node has a closeness centrality value. 
𝐶
o
(
𝑥
)
=
1
∑
𝑑(𝑥, 𝑣)
H∈¥
Clustering coefficient
measures the degree to which nodes tend to cluster together. 
More precisely, the clustering coefficient of a node is the ratio of existing links connecting 
a node's neighbors to each other to the maximum possible number of such links. The 
105 
clustering coefficient of the entire network is typically the average of the clustering 
coefficients of all nodes. Nodes with higher clustering coefficients have higher transitivity 
(triangles) in the neighborhood. A high clustering coefficient of a network is another 
indication of a small world. 
𝐶𝐶
(
𝑥
)
=
2 ∙ |
¦
𝑒
1§
: 𝑣
1
, 𝑣
§
∈ 𝑁
O
, 𝑒
1§
∈ 𝐸
ª
|
𝑘
O
(𝑘
O
− 1)
Modularity
, as a measure of network structure, is designed to measure the strength 
of division of a network into modules, communities, or clusters. Modularity is the fraction 
of the edges that fall within the given groups minus the expected fraction if edges were 
distributed at random. 
Communities
, which are also referred to as modules or clusters, are 
densely connected groups of nodes, with only sparser connections between groups. 
Networks with a high modularity have dense connections among the nodes within the same 
communities but sparse connections among nodes in different communities. 
𝑄 =
1
2𝑚
0 ®
𝐴
H°
−
𝑘
H
𝑘
°
2𝑚
±
𝛿
(
𝑐
H
, 𝑐
°
)
=
0
(𝑒
11
− 𝑎
1
+
)
r
12*
H°
𝑒
11
=
0
𝐴
H°
2𝑚
1
H∈r
³
1
°∈r
³
H°
𝑎
1
=
𝑘
1
2𝑚
=
0
𝑒
1§
§
5.2.3 Community Detection with the Louvain Method 
Network community detection is also known as network clustering, with the purpose to 
assigns a set of objects (nodes) to communities (clusters) such that objects in the same 
communities are more similar to each other than to those in other communities. The 
106 
"similarity" is defined based on the network topology, i.e. intra-cluster density and inter-
cluster sparsity. Communities reveal a non-trivial internal organization of a network and 
allow people to infer special relationships among nodes [136]. They have been shown to 
have 
significant 
real-world 
meaning 
[136]–[138]. 
While 
a 
diversity 
of 
community 
detection algorithms are proposed, the quality of community detection is usually measured 
by standalone quality metrics (e.g. modularity) and information recovery metrics based on 
benchmark graphs [138]–[140]. 
The Louvain method proposed in Blondel 2008 [134] is a modularity optimization 
based heuristic method for fast uncovering of communities in large networks. This 
algorithm has several advantages in easy implementation, fast computation speed, and the 
capability to handle large and weighted networks. More importantly, comparing to other 
methods [141]–[143], the Louvain method has demonstrated to provide high-quality results 
for community detection. In particular, when working on several commonly used test-case 
networks with a size ranges from 34 to 118 million, the Louvain method performs the best 
in modularity and efficiency compared to other algorithms. The Louvain method has also 
been successfully applied to the well-known Girvan and Newman (GN) benchmarks [137] 
and Lancichinetti and Fortunato and Radicchi (LFR) benchmarks [138]. The performance 
of the Louvain method is among the best when considering more than 10 popular 
community detection algorithms, including Rosvall 2008 [144] and Newman’s works 
[137], [142]. The Louvain method has also been applied to some popular social networks 
with millions of nodes like LinkedIn and Twitter [134], [145]. Therefore, in this study, we 
107 
consider the Louvain method as a preferred approach to examine network structure and 
detect communities with high quality. 
Besides the Louvain method, some other state-of-the-art community detection 
algorithms 
include: 
(1) 
smart 
local 
moving 
[146], 
a 
recently 
proposed 
modularity 
optimization method that attains high levels of modularity on large graphs, (2) infomap 
[147], which is based on the principle of information theory, aims to reach an optimal 
clustering that minimizes the length of information descriptions, and (3) label propagation 
[148] that iteratively identifies communities by adopting the most common label(s) among 
the neighbors. Worth mentioning, there is not an always-the-best community detection 
algorithm, instead, the size of a network and the way of defining communities (clusters) 
are essential and influential factors. Besides, there can be disagreements between certain 
metrics, such as stand-alone metrics and information recovery metrics [138]–[140]. 
5.3 Visualization of Article Similarity Network 
5.3.1 Article Similarity Network 
We conceptualize an article collection with its established article relationships (similarities) 
as an article similarity network, or article network, which is different from traditional 
bibliometric networks. With this concept, each article is modeled as a node in the network 
and the similarity between two articles is modeled as a weighted edge connecting them. 
The similarity metric can be calculated based on any of our text analytics approaches. 
Basically, we use article similarities calculated by the baseline lexical approach, with the 
equally weighted collaboration (combination) of all the 5 article elements, including title, 
abstract, medical subject heading term (MeSH), publication types and authors 
108 
Theoretically, an article similarity network is almost a complete graph with 
𝑛(𝑛 − 1)
+
edges, in which every pair of articles are connected by an edge representing a 
non-zero similarity between them, because the article collection (text corpus) could be 
obtained via a preliminary literature search and the returned results would share certain 
similarities especially in the narrative contents of titles and abstracts. Large and complete 
networks are usually complex and unreadable because of the extreme clutter present (e.g. 
hairballs [149]). To provide a human readable network, we use a global threshold to filter 
edges 
representing 
low 
similarity 
values. 
Please 
note 
that 
here 
we 
only 
use 
a 
straightforward 
network 
sparsification 
scheme 
for 
concept 
demonstration. 
More 
sophisticated network sparsification schemes will be approached in the next chapter, 
expecting to preserve important article similarities and promote effective visualizations. 
5.3.2 Network Visualization 
We implement article network visualization in Gephi [150], which is an open source 
network analysis and visualization software package written in Java. It provides an 
interactive visualization and exploration platform for all kinds of networks, including 
complex, dynamic, and hierarchical networks. We use a force-directed graph drawing 
algorithm [131], which is realized as the Force Atlas algorithm [132] in Gephi, to visualize 
article similarity networks. In the resulting network visualization drawn by this algorithm 
(layout), the pairwise geometric distances between the displayed nodes tend to match the 
graph theoretic pairwise distances, which are corresponding to article similarities in the 
original feature space. With such a visualization, similar articles tend to be placed close 
together, in other words, adjacent nodes tend to represent articles with high similarities. 
109 
As an illustrative example, Figure 5.1 (a) shows a visualized article network of the 
ADHD dataset from DEPR [26], with article similarities calculated based on an equally 
weighted combination of lexical features of title, abstract, MeSH term, publication type, 
and author. Figure 5.1 (b) shows a network of the same dataset but is using the ontology-
based semantic features instead. In particular, the ADHD dataset has a total of 851 articles; 
among which 20 articles are recognized as high relevant articles included at the full-text 
level, 767 articles are irrelevant articles excluded at the abstract level, and the remaining 
84 articles, which are included at the title/abstract level but excluded at the full-text level, 
are also considered as irrelevant articles. In Figure 5.1, article nodes are colored by the 
inclusion/exclusion (relevant/irrelevant) class labels. Therefore, 20 nodes (included and 
relevant articles) are labeled in green, 64 nodes (half-included and irrelevant articles) are 
labeled in yellow, and 767 nodes (excluded and irrelevant articles) are labeled in red. 
Figure 5.1: Illustrated article network (ADHD dataset) colored by article classes 
(a) Lexical similarities 
(b) Ontology-based semantic similarities
Full-text Level Inclusions (Relevant) 
Abstract Level Inclusions (Irrelevant) 
Exclusions (Irrelevant) 
110 
5.3.3 Community Detection (Network Clustering) 
For community detection in the article network, Gephi supports the Louvain method [134] 
as its modularity function. There are three parameters for this modularity function, 
including the resolution, randomization, and weight. We use the original (default) setting 
of the Louvain method, therefore, the parameters are set to be “
resolution=1.0
” and 
“
randomization=yes
”. Specifically, this setting is corresponding to the nature of the 
Louvain method that a random node order is selected for iteration rather than an 
unnecessarily fixed order. Although different runs might bring slightly different results, the 
differences are trivial, and the overall structure remains highly similar. In addition, our 
article networks use article similarities as edge weights, so “
weight=yes
” is applied. 
In Figure 5.2, we illustrate the same network as Figure 5.1 (a), but with a different 
coloring scheme. Here, article nodes are colored based on their assigned communities by 
the Louvain method. A total of 73 communities are found with our algorithm setting as 
above mentioned. Eight out of 73 communities contain more than 17 nodes, which is 2% 
of all articles in the ADHD dataset. The rest of 65 communities are considered minor 
communities with only a small number of nodes or even one single node inside. Please 
note that the rough network sparsification method being used in this chapter can break a 
network into several disconnected sub-networks (communities) or single nodes. The top 
three communities, which contain 31.02%, 18.21% and 17.04% of all articles, are colored 
in red, green, and yellow respectively. With this partition, we observe that 18 out of 20 
relevant articles tend to aggregate in the green community, with the exception of two outlier 
articles. In other words, the green community in Figure 5.2 covers 90% of high relevant 
111 
articles. The ratio of high relevant articles in this green community is significantly greater 
than that in the entire network (20/851=2.35%). Therefore, if the article screening process 
begins with this community, we could rapidly identify 90% (18 out of 20) relevant articles. 
Figure 5.2: Illustrated article network (ADHD dataset) colored by communities 
5.4 Experimental Settings and Results 
5.4.1 Datasets 
We use similar datasets to evaluate the visualization of article similarity networks as what 
we have used to evaluate our text analytics approaches. Besides the 15 DERP datasets [26] 
described in Section 2.5.1, we further include 5 additional datasets which are produced by 
the Cochrane Collaboration [27] (Table 5.1). These 5 datasets [151]–[155] are also 
completed systematic reviews conducted by experienced and knowledgeable human expert 
researchers, with inclusion (relevancy) and exclusion (irrelevancy) decisions made by at 
112 
least two experts. Biomedical articles (in the MEDLINE format) in the Cochrane datasets 
are retrieved using the publicly available search strategies on PubMed, articles from other 
bibliographic resources (e.g. EMBASE/Ovid) are not used. For the purpose of concept 
proof, the article similarities utilized generate article networks and assign edge weights are 
calculated as the equally weighted sum of the 5 lexical element-level similarities, i.e. title, 
abstract, MeSH, author, and publication type, which range from 0 to 5. 
Dataset 
Total 
Abstract N (%) 
Full-text N (%) 
Antibiotic [151] 
412 
74 (17.96%) 
10 (2.43%) 
Antineoplastic [152] 
1294 
N/A N/A 
19 (1.47%) 
Antiretrovirals [153] 
749 
N/A N/A 
38 (5.07%) 
Hearing Loss [154] 
467 
13 (2.78%) 
3 (0.64%) 
Leukaemia [155] 
328 
11 (3.35%) 
7 (2.13%) 
Table 5.1: Five Cochrane datasets and their sizes and rates of inclusion (relevancy) 
5.4.2 Results on Graph Diameter and Closeness Centrality 
To evaluate the centralization (aggregation) of high relevant articles in the visualized 
article network, we examine the sub-network of high relevant articles, calculate its graph 
diameter and the range/distribution of closeness centrality, and compare that with the 
largest connected network (Table 5.2). Because the Skeletal Muscle Relaxants dataset [26] 
has a graph diameter of 1, which is not meaningful for calculation, it is not included in the 
following analysis due to the fracture network. For the remaining 19 datasets, the average 
value of the largest connected networks’ diameter is 8.26 (SD = 1.76), while the average 
graph diameter value of the sub-network for relevant articles is only 3.84 (SD = 1.54). The 
113 
results show that the graph diameter of the sub-network of relevant articles is significantly 
smaller than that of the largest connected network (paired-t test, p<0.001). And for the 
closeness centrality range of the largest connected network, the largest one is 1~9 while 
the smallest one is 1~4. The largest closeness centrality range of the sub-network for 
relevant articles is 1~5 while the smallest one is only 1~2. 
Dataset 
Largest connected 
network 
Sub-network for 
relevant articles 
GD 
CCR 
GD 
CCR 
ACE Inhibitors 
7 
1~5 
3 
1~3 
ADHD 
10 
1~7 
2 
1~2 
Antihistamines 
8 
1~6 
3 
1~3 
Atypical Antipsychotics 
8 
1~6 
7 
1~5 
Beta Blockers 
8 
1~6 
4 
1~4 
Calcium Channel Blockers 
8 
1~6 
7 
1~5 
Estrogens 
7 
1~5 
6 
1~4 
NSAIDS 
11 
1~9 
3 
1~3 
Opioids 
7 
1~5 
3 
1~3 
Oral Hypoglycemics 
5 
1~4 
5 
1~4 
Proton Pump Inhibitors 
8 
1~7 
3 
1~3 
Skeletal Muscle Relaxants 
13 
1~9 
1
a
1~1 
Statins 
9 
1~6 
4 
1~4 
Triptans 
10 
1~7 
5 
1~4 
Urinary Incontinence 
8 
1~6 
4 
1~3 
Antibiotic 
6 
1~5 
3 
1~3 
Antineoplastic 
9 
1~7 
4 
1~3 
Antiretrovirals 
6 
1~5 
3 
1~3 
Hearing Loss 
11 
1~8 
2 
1~2 
Leukemia 
11 
1~8 
2 
1~2 
Average 
8.26 
1~6 
3.84
*
1~3 
GD = Graph Diameter; CCR= Closeness Centrality Range. 
a
=Fracture network, only 2 out of 9 nodes are connects. Therefore, the report was not 
included in the average value. 
*
Significant smaller than the largest connected network (p<0.001)
Table 5.2: Graph diameter and closeness centrality 
In summary, the graph diameter of the sub-network of high relevant articles is smaller than 
that of the largest connected network. More than half of the datasets even have less than a 
114 
half size of the graph diameter. Besides, the closeness centrality range also shows a smaller 
value and tighter distribution in the sub-network of high relevant articles comparing to the 
largest connected network, (1~3 vs. 1~6). The results confirm that high relevant articles 
tend to aggregate into a few small areas in the visualization, and these “visual densities” 
cover 93.75% high relevant articles in average from the 19 datasets in our study. 
5.4.3 Results on Modularity and Community 
Table 5.3 shows modularity and community results after an implementation of the Louvain 
method [134] on the 20 datasets. We count towards the number of communities (C), the 
number of communities with high relevant articles (CR), and the number of dominant 
communities with more than 10% high relevant articles (CRD). We also report the top two 
communities, which contain the top numbers of high relevant articles. 
Although a large number of communities are detected for the largest connected 
network (average C = 86), high relevant articles are only found in a few communities 
(average CR = 4), and even less are classified as the dominant communities (average CRD= 
2). This implies that high relevant articles have strong similarities and tend to cluster in the 
same communities. Moreover, the number of relevant articles found in Community 1 (C1, 
the top community for each dataset) provides additional aggregation evidence. In average, 
C1 covers 25.5% articles overall, but 67.81% relevant articles. With C1 and C2 together, 
they total cover 89.03% (67.81%+21.24%=89.03%) relevant articles. Due to the space 
limitation, 
we 
only report 
the 
top 
two 
communities 
in 
Table 
5.3. 
However, 
other 
communities except dominant communities contain few relevant articles. In many cases, 
there is only one relevant article in a community which might be considered as an outlier. 
115 
Dataset 
Community 
Community 1 
Community 2 
C 
CR 
CRD 
Articles 
Covered 
Relevant 
Articles 
Covered 
Articles 
Covered 
Relevant 
Articles 
Covered 
# 
%
a
# 
%
b
# 
%
a
# 
%
b
ACE Inhibitors 
72 
2 
1 
707 
27.97% 
40 
97.56% 
796 
31.29% 
1 
2.44% 
ADHD 
73 
3 
1 
159 
18.68% 
18 
90% 
50 
5.88% 
1 
5% 
Antihistamines 
43 
4 
3 
83 
26.77% 
9 
56.25% 
74 
23.87% 
3 
18.75% 
Atypical Antipsychotics 
83 
8 
4 
187 
16.7% 
50 
34.25% 
200 
17.86% 
45 
30.82% 
Beta Blockers 
89 
4 
2 
400 
19.26% 
26 
61.90% 
476 
22.97% 
12 
28.57% 
Calcium Channel Blockers 
68 
5 
3 
274 
22.5% 
36 
36% 
333 
27.34% 
32 
32% 
Estrogens 
42 
6 
3 
75 
20.38% 
34 
42.5% 
66 
17.93% 
24 
30% 
NSAIDS 
44 
3 
2 
78 
19.85% 
32 
78.05% 
42 
10.69% 
7 
17.07% 
Opioids 
90 
3 
2 
298 
15.56% 
11 
73.33% 
592 
30.91% 
3 
20% 
Oral Hypoglycemics 
31 
7 
3 
161 
32.01% 
66 
48.53% 
146 
29.03% 
33 
24.26% 
Proton Pump Inhibitors 
51 
6 
2 
449 
33.68% 
37 
72.55% 
405 
30.38% 
7 
13.73% 
Skeletal Muscle Relaxants 
322 
3 
2 
826 
50.27% 
6 
66.67% 
184 
11.20% 
2 
22.22% 
Statins 
191 
4 
2 
1384 
39.95% 
67 
78.82% 
1160 
33.49% 
14 
16.47% 
Triptans 
43 
2 
2 
205 
30.55% 
17 
70.83% 
183 
27.27% 
7 
29.17% 
Urinary Incontinence 
57 
7 
1 
67 
20.49% 
30 
75% 
18 
5.5% 
3 
7.5% 
Antibiotic 
25 
3 
3 
45 
10.92% 
6 
60% 
125 
30.34% 
3 
30% 
Antineoplastic 
195 
5 
3 
144 
11.13% 
9 
47.37% 
191 
14.76% 
4 
21.05% 
Antiretrovirals
*
13 
1 
1 
331 
44.19% 
38 
100% 
– 
– 
– 
– 
Hearing Loss 
87 
2 
2 
116 
24.84% 
2 
66.67% 
100 
21.41% 
1 
33.33% 
Leukaemia
*
92 
1 
1 
80 
24.39% 
7 
100% 
– 
– 
– 
– 
Average 
86 
4 
2 
303 
25.50% 
27 
67.81% 
286 
21.78% 
11 
21.24% 
C= number of communities; CR= number of communities that contain relevant articles; CRD= number of dominant 
communities with more than 10% relevant articles; 
*
The report has only one community that contains relevant articles. Therefore, there is no Community 2 (C2) 
a
= percentage of articles covered in the community; 
b
= percentage of relevant articles covered in the community. 
Table 5.3: Modularity and detected communities 
5.5 Summary 
In this chapter, we visualize an article collection, with respect to established lexical article 
similarities (relationships) from text analytics, using graph-based network visualization. 
Specifically, this article similarity network, which is also referred to as an article network, 
is drawn by a force-directed algorithm into a 2D visualization space, where the geometric 
distances among article nodes in the 2D space approximately match with the theoretical 
distances i.e. article similarities in the original feature space. 
116 
As a primary purpose of this study, we demonstrate the usefulness of such a 
visualization, the resulting visual patterns, in assisting with biomedical literature retrieval. 
For doing so, we examine several network properties, including the graph diameter and 
closeness centrality, and confirm the centralization of high relevant articles in the visual 
distribution. We also examine the clustering patterns established in the visualization space 
by using the Louvain method for community detection, and further confirm the clustering 
of high relevant articles into the same or adjacent communities (clusters). In particular, the 
sub-networks of high relevant articles have significantly smaller graph diameters than those 
of the largest connected networks (3.84 vs. 8.26, p<0.001), and smaller distributions in 
closeness centralities (1~3 vs. 1~6). Although many communities are detected for the 
largest connected network (n=86) after the implementation of the Louvain method, high 
relevant articles only distribute into a few communities (n=4), and are found in even fewer 
dominant communities (n=2). More importantly, the top two communities covered 89.03% 
high relevant articles. Since most high relevant articles in our research cohort exhibit a 
tendency to aggregate to specific regions, early identifying and exploring of these regions 
would accelerate the identification of high relevant articles for biomedical literature 
retrieval, from human researchers’ views. 
Also, we are able to further confirm the effectiveness of our established article 
features, representations, and similarities in capturing the underlying article meanings thus 
capturing the strong relationships among high relevant articles. Through the visualization 
of article networks, we demonstrate exploiting article representations and similarities in a 
more intuitive way, such that the article distribution and important patterns (e.g. clustering 
117 
patterns) are revealed to human eyes with expedited cognition and analytic reasoning. We 
also discover other advantages of visualizing article collections that are infeasible from the 
conventional and abstract text-based approaches. For example, if there are multiple notable 
aggregated regions (e.g. visual densities) for high relevant articles in a visualization, a 
branching of information retrieval scopes may be needed. Integrating the visualization as 
a decision support tool in biomedical literature retrieval will enable researchers to discover 
particular patterns and communities; thus, accelerating the application with high efficiency 
and effectiveness. 
In summary, we anticipate that the visualization of an article collection, using text 
analytics results, can be applied to (1) identify multiple key communities (clusters) when 
the topics of articles are diversified; (2) assign high priority to communities with known 
relevant articles and screen articles from the closest neighbors; (3) assign low priority to 
communities that contain several known irrelevant articles to save unnecessary workload; 
(4) customize the network structure with different article similarity schemes (calculation, 
sparsification, etc.) for edge weights that align to specific aims or sub-aims of an 
information retrieval scope, e.g. authorship, publication type, keywords; (5) provide 
potential knowledge discovery from unexpected communities; (6) gain overview and 
insights into the associated research topics and communities. 
Highlight of Contribution 
We demonstrate the usefulness of visualizations of article relationships (representations) 
in assisting with the information retrieval towards biomedical articles. Specifically, for the 
concept proof using a visualized article similarity network, the revealed 2D visual patterns, 
118 
including the centralization (aggregation) and clustering of high relevant articles, can 
facilitate the identification of high relevant articles by leveraging human cognition and 
analytic reasoning. 
This work has been published in AMIA 2015: 
Xiaonan Ji, Raghu Machiraju, Alan Ritter, Po-Yin Yen. Examining the Distribution, 
Modularity, and Community Structure in Article Networks for Systematic Reviews. In 
AMIA Annual Symposium Proceedings 2015 (Vol. 2015, p. 1927). American Medical 
Informatics Association.
119 
Chapter 6. Promoting Effective Visualizations with Sparsified Article Networks and 
Article Maps 
In the previous chapter, we have demonstrated that the visualization of article similarities 
(relationships), which are established upon text features and representations, can facilitate 
the identification of high relevant articles in biomedical literature retrieval, by leveraging 
human cognition to exploit the beneficial visual patterns e.g. article distribution and 
clustering. In this chapter, we aim to promote effective visualizations and the associated 
visual patterns to better assist in biomedical literature retrieval with respect to the practical 
settings. Meanwhile, an effective visualization will also support advantageous exploitation 
and exploration of the applied article features, representations, and similarities [17], [18], 
[20]. In this sense, we consider that an effective visualization should display articles and 
reveal their relationships in a human interpretable, attribute intuitive, and spatial scalable 
manner, and more importantly, provide compelling visual patterns (e.g. distribution and 
clustering) that can expedite the identification of high relevant articles. 
For doing so, we consider the most commonly used visualization types for entity 
relationships or relational information, and implement two types of visualizations: graph-
based network drawing (article network), and distance-based or geometry-based map 
projection (article map) [20], [47]. Other visualization types, such as adjacency matrices, 
hypergraphs, and circular graphs, are not included because of their limited structural 
analytics or spatial scalability. With a 2D visualization of article network or article map, 
120 
articles are represented as visual marks such as nodes or points, and their relationships are 
reflected by the attractive visual channel of positions, with or without explicit connections 
(edges) among articles. Therefore, the effectiveness of a visualization is highly associated 
with the 2D placements of articles points (nodes), such as the 2D aggregation of truly 
similar articles, which are determined by the applied visualization techniques. In this study, 
we focus on: (1) the force-directed graph drawing algorithm [131], [132] for article 
networks, and (2) the dimensionality reduction algorithm of t-distributed Stochastic 
Neighbor Embedding [105] (t-SNE) for article maps. 
The visualization techniques convert articles from a high-dimensional feature space 
to a 2D space that can be properly visualized. During this process, there is inevitable 
information loss, and the visualization techniques have an intuition to preserve the “most 
important” article similarities (relationships) and relax the others. As shown in Figure 6.1, 
The dimensionality reduction algorithm of t-SNE for article maps preserves article 
similarities with a probability distribution, and is considered a state-of-the-art technique. 
However, for article networks where article similarities are explicitly taken as weighted 
edges, there are limited studies about network sparsification schemes that are applicable to 
the special article networks to determine article similarities to be preserved. Therefore, we 
further focus on multiple network sparsification schemes [149], [156] which are more 
configurable and understandable (comparing to the probability distribution based method), 
and 
can 
result 
in 
different 
article 
network 
visualizations. 
Meanwhile, 
the 
network 
sparsification not only eliminates the clutter presents (e.g. hairballs) of an almost complete 
article network as we have mentioned in Section 5.3.1, but also reveals and even highlights 
121 
important structures and patterns to human eyes. Under this notion, we implement three 
network sparsification methods to reduce the network size via edge sampling, expecting to 
preserve edges (article similarities) bearing important conceptual or structural information. 
The three network sparsification methods include 1) the established article similarity (AS) 
that predominantly retains edges for strong article similarities, 2) the derived algebraic 
distance (AD) [157], [158] that values edges within neighborhoods and tends to preserve a 
network’s local structure, and 3) the derived local degree (LD) [159] that values edges 
leading to hub (highly important) nodes and tends to preserve the global structure. We 
visualize each type of the sparsified article networks with the force-directed algorithm, and 
evaluate and compare the effectiveness in biomedical literature retrieval. 
In summary, both the force-directed algorithm [131], [132] on sparsified article 
networks and the t-SNE algorithm [105] applied to article maps are designed to work 
around article similarities to produce approximated 2D placements (2D distances) of article 
nodes or points. In this sense, only a portion of article similarities are preserved and utilized 
during the computation. While t-SNE automatically utilizes probability distributions to 
determine which article similarities are preserved, our network sparsification schemes are 
more understandable and configurable (customizable) in explicitly preserving interesting 
or important article similarities regarding the task scopes. Therefore, besides comparisons 
among different article network sparsification schemes, we also evaluate and compare the 
sparsified article networks with article maps. 
122 
Figure 6.1: Visualization techniques and the intuitions behind 
6.1 Research Questions 
We formulate and highlight our research questions with respect to the purpose of promoting 
effective 
visualizations 
of 
an 
article 
collection 
(using 
article 
representations 
and 
similarities from text analytics) to facilitate biomedical literature retrieval, considering the 
two well-recognized visualization types of article networks and article maps, and three 
network sparsification approaches of AS, AD, and LD. 
•
As the fundamental intuition of network sparsification is to preserve edges bearing 
important structural or conceptual information, will the sparsification (edge sampling) 
approaches, AS, AD, and LD, applied to article networks preserve the original network 
structure and the article distribution? 
•
Among the three network sparsification approaches, which one can produce a more 
effective visualization for the identification of high relevant articles? Will the selection 
of network sparsification schemes noticeably influence the performance? 
123 
•
Among the two visualization types, (sparsified) article network and article map, which 
one can produce a more effective visualization? Will a sparsified article network and 
an article map result in noticeably similar or different performance? 
6.2 Background and Related Works 
6.2.1 Graph-based Network Drawing 
In the previous chapter, we have demonstrated graph-based network visualization with the 
force-directed graph drawing algorithm. Here we highlight some additional background 
information, as well as some advantages and limitations lie in the graph-based visualization 
and the drawing algorithms, in a more systematic manner. 
Graphs [135] are actually considered mathematical structures to model pairwise 
relations between objects, and are usually drawn as node-link-diagrams where the vertices 
(nodes) are represented as disks and the edges are represented as lines or arcs. As a graph-
based approach, network visualization enables the exploration of graph topologies and the 
utilization of graph-based algorithms for advanced visual analytics [20]. More specifically, 
we can investigate a wide range of network properties, which are based on the network 
topology (node connectedness) in the graph theory, and obtain a rapid understanding and 
evaluation of a network from a global or a local perspective. Besides, we can also 
implement many effective graph-based algorithms, such as network traversal, cut and flow, 
centrality, ranking, and clustering, thus promote our capabilities of visual analytics. 
Because a graph-based approach relies on explicit assignments (definitions) of edges to 
node pairs, for article similarity networks, we need to calculate article similarities in a 
vector feature space and take the established article similarities as (weighted) edges. 
124 
Graph drawing is to derive two-dimensional depictions of graphs with a pictorial 
representation of the visual elements, i.e. nodes and edges. There have been a variety of 
graph 
drawing 
algorithms 
(layouts) 
that 
arrange 
graph 
elements 
in 
different 
ways, 
including the force-based layout, tree layout, circular layout, arc diagram, etc. Among 
them, the force-based layout, or the force-directed graph drawing algorithm, is proven to 
draw networks in an aesthetically pleasing way [131]. It places nodes and edges by 
simulating a physical system that assigns spring-like attractive forces to attract pairs of 
endnodes, and electrical repulsive forces to separate nodes. The layout criteria are 
converted to a cost function that can be optimized to find the layout for energy minima. 
When the system comes to a mechanical equilibrium state, the pairwise geometric distances 
between the drawn nodes tend to match the graph theoretic pairwise distances. Specifically, 
similar article nodes tend to aggregate together while dissimilar article nodes are drawn 
apart. While the force-directed layout works well for graphs with a reasonable size (e.g. 
1000), it has limited scalability to graphs with more than one or two thousand vertices and 
can result in unreadable drawings. In addition, force-directed algorithms typically have a 
time complexity of
O(𝑛
µ
)
, and the processing can be computational intensive. The 
efficiency can be improved by approximations [160]. 
6.2.2 Network Sparsification with Edge Sampling 
A graph-based network consists of a set of nodes and edges connecting endnodes. The 
purpose of network sparsification, which is also known as edge sampling, is to retain just 
a fraction of edges while still preserving important network structures [158], [159], [161]. 
The benefits of sparsification are three-fold, including 1) improving the efficiency in 
125 
computational data processing and analysis, 2) compressing large networks for better 
spatial scalability while preserving important or interesting information, and 3) revealing 
and even highlighting important structures and patterns to human eyes and enabling 
insights for better interpretation and evaluation. The core idea of edge sampling lies in that 
not all edges are equally important with respect to their structural properties or conceptual 
properties driven by task-specific interests. Therefore, the importance of edges can be 
quantified using the edge score, and a general sparsification process consists of two major 
steps: (1) scoring the edges, and (2) filtering the edges based on the assigned scores. 
There have been many different sparsification schemes that value edges from 
different perspectives [161]. Some recognized methods include: Random Edge (RE) that 
samples edges randomly, Triangles that scores edges based on the number of triangles that 
an edge belongs to, Local Similarity (LS) and Algebraic Distance (AD) that score edges 
based on the overlap between endnodes’ neighborhoods (Jaccard measure), Local Degree 
(LD) 
that 
scores 
edges 
based 
on 
endnodes’ 
degree, 
Backbones 
(TS 
& 
QLS) 
that 
discriminates edges within dense subgraphs and across dense subgraphs, Edge Forest Fire 
(EFF), etc. The quality of a sparsified network is evaluated by the comparisons to the 
original network, with respect to the preservation of network properties such as diameter, 
degree, betweenness centrality, PageRank, clustering coefficient, the number of connected 
components, and modularity. 
6.2.3 Distance-based Map Projection 
From a traditional point-of-view, a map projection is a systematic transformation of the 
three-dimensional surface onto a two-dimensional plane, e.g. representing the latitudes and 
126 
longitudes on the earth with locations on a plane. Map projection is a mathematical 
procedure to create maps. Map projection is now extended to embed high-dimensional data
，
such as article features and representations, into a space of human interpretable and 
visualizable dimensions via dimensionality reduction, so that the between-object distances 
are preserved as well as possible. In this sense, map projection is utilized to visualize the 
level of similarities originated from a high-dimensional space by placing data objects onto 
a lower dimensional space with properly assigned coordinates or geometric positions. 
A 
variety 
of 
dimension 
reduction 
techniques 
for 
the 
visualization 
of 
high-
dimensional data have been developed in the recent decades [162][105]. Traditional 
techniques, which are mostly linear approaches that focus on drawing apart dissimilar data 
objects, include Principal Component Analysis (PCA) [163] and multidimensional scaling 
(MDS) [164], [165]. To keep similar data objects closer and preserve the local structures, 
many nonlinear techniques are also developed, including the Sammon mapping, curvilinear 
components analysis (CCA), Stochastic Neighbor Embedding (SNE) [166], Isomap [167], 
Maximum Variance Unfolding (MVU), Locally Linear Embedding (LLE), Laplacian 
Eigenmaps, and t-distributed Stochastic Neighbor Embedding (t-SNE) [105]. 
6.2.4 t-Distributed Stochastic Neighbor Embedding (t-SNE) 
t-Distributed 
Stochastic 
Neighbor 
Embedding 
(t-SNE) 
[105] 
is 
a 
machine 
learning 
algorithm 
for 
nonlinear 
dimensionality 
reduction, 
specifically, it 
is 
a 
technique for 
visualizing similarity data by embedding high dimensional data into a space of two or three 
dimensions. The resulting visualization is considered a map or a scatter plot with 
distributed data points. The t-SNE approach retains data structures by keeping similar data 
127 
points close together while pushing dissimilar points far apart. Therefore, it has been 
proven to preserve and reveal important structures such as the clusters. 
As a non-linear algorithm for dimensionality reduction, t-SNE establishes the high-
dimensional Euclidean distance (similarity) between data points, 
𝑥
1
and 
𝑥
1
, and converts 
the corresponding similarity into a probability, 
𝑝
1§
. The probability density is proportional 
to the student’s t-distribution with 1 degree of freedom centered at 
𝑥
1
. In the low-
dimensional (e.g. 2D) space, there are counterpart map points 
𝑦
1
and 
𝑦
§
, as well as a 
probability 
𝑞
1§
. Student’s t-distribution is used as it leads to simpler gradients for faster 
computation, and its heavy tail can alleviate the crowding problem as more space is needed 
in the low-dimensional space to accommodate nearby points. To minimize the difference 
or mismatch between 
𝑝
1§
and 
𝑞
1§
, a cost function, which is based on the Kullback–Leibler 
(KL) divergence measuring how 
𝑞
1§
diverges from 
𝑝
1§
, is minimized via gradient descent. 
𝛿𝐶
𝛿𝑦
1
= 4
0
(𝑝
1§
− 𝑞
1§
)(𝑦
1
− 𝑦
§
)(1 +
¶
𝑦
1
− 𝑦
§
¶
+
)
·*
§
Worth mentioning, the gradient of the cost function can be interpreted as forces between 
the map points just like the force-directed graph drawing for networks. Considering the 
theoretical similarity (p) versus the visualized 2D distance (q), the force can be attractive 
or repulsive. In other words, t-SNE sets strong attractions for similar points that are put far 
away, 
and 
strong 
repulsions 
for 
dissimilar 
points 
that 
are 
put 
close 
together. The 
computational and memory complexity is quadratic to the number of data points O(
𝑁
+
). 
128 
6.3 Article Network Sparsification and Visualization 
6.3.1 Revisiting Article Network 
An article network G(V,E) consists of a set of nodes (v
∈
V) representing articles and a set 
of weighted edges (e
∈
E) representing article similarities between corresponding endnodes. 
In this study, we utilize article similarities established in the lexical features space of an 
equally weighted combination of 5 article elements, including title, abstract, medical 
subjective heading term (MeSH), publication type, and author. By summing up the 5 
element-level 
similarities 
(cosine 
similarity 
or 
Euclidean 
distance), 
the 
resulting 
composited article similarity ranges from 0 to 5. 
Figure 6.2: Illustrated “complete” article network with “hairballs” 
Because of the existence of non-zero similarities among most articles that are obtained via 
preliminary literature search with similar or related keywords, an article similarity network 
is almost a complete network that exhibits connections (edges with non-zero weights) 
between most article pairs. A direct visualization of such a network is limited by human 
129 
perception and important visual patterns are inaccessible, in other words, visualizing such 
a network is meaningless because of the extreme clutter presents e.g. “hairballs” [149]. 
Figure 6.2 illustrates an almost complete article network with “hairballs” using the ADHD 
dataset from DERP [26], which has 851 articles. 
6.3.2 Configurable Article Network Sparsification 
To overcome the limitation of visualizing an almost complete network, we implement three 
network sparsification methods, Article Similarity (AS), Algebraic Distance (AD) [157], 
[158], and Local Degree (LD) [159], with an expectation to reduce the number of edges, 
but preserve edges with conceptual or structural importance. The sparsification process, 
which has configurable schemes and parameters, consists of three steps: (1) network pre-
pruning, (2) edge scoring, and (3) edge sampling. 
Step 1. Network pre-pruning.
In our preliminary experiments, we find that 
because of the particularity of article similarity networks, directly calculating AD or LD 
edge scores using neighborhood information in an almost complete network result in 
indiscriminate edge scores as most nodes share similar neighbors. Therefore, we initiate a 
relaxed pre-pruning step to keep the top neighbors (edges) based on edge weights for each 
node. We test a series of pre-pruning parameters from 50% to 5% and find that keeping the 
top 10% edges leads to a better balance of avoiding the mass of trivial edges and retaining 
important edges. We then use this coarsely pruned 10% network as the 
baseline network
. 
The 10% baseline network, although it sounds satisfying, is still insufficient because the 
pre-pruning only cuts off very trivial edges. As many datasets for biomedical literature 
retrieval contain more than 3,000 articles (some are more than 10,000) and result in a 
130 
quadratic number of edges, network sparsification is needed to retain the most important 
edges. Thus in the following steps we further aggressively reduce the number of edges. 
Step 2. Edge scoring.
With the baseline network, we calculate edge scores to 
further capture the edge importance from different perspectives using AS, AD, and LD. 
Figure 6.3 briefly illustrates the edge scoring schemes. 
•
Article Similarity (AS).
To aggressively retain edges corresponding to strong article 
similarities, we directly use the established article similarities as edge scores. In other 
words, the edge weights are used as edge scores. 
•
Algebraic Distance (AD).
AD was first proposed by Chen 2011 [157]. John 2016 [158] 
used AD to preserve strong connections in terms of local structures. It generalizes the 
idea of estimating the Jaccard coefficient for neighborhoods through lazy random walks 
to determine the strength of connections of the edges. Specifically, nodes with similar 
neighbors are considered strongly connected. With the AD approach, these nodes 
converge to similar values via information propagation within the neighborhood and 
lead to high AD edge scores, which represent strong local connections. With the 
algorithm below, we run multiple rounds (R=20) to obtain synthesized results. 
Input: Parameter 
ω (ω = 0.5)
, weighted adjacency 
matrix (for weights 
𝑤
1§
and neighbors 
𝑁
1
), and 
randomly initialized vector 
𝑥
(¹)
with |V| elements. 
for k = 1, 2, …do 
∀i ∈ V 𝑥
1
(¼)
← ω𝑥
1
(¼·*)
+ (1 − ω)
∑
°
³¾
O
¾
(¿Às)
¾∈Á
³
∑
°
³¾
¾∈Á
³
∀ij ∈ E
𝑠
1§
(¼)
=
7
𝑥
1
(¼)
− 𝑥
§
(¼)
7
end for 
Algorithm: Computing algebraic distance (AD) 
131 
•
Local Degree (LD)
. LD was proposed by Lindner 2015 [159] to emphasize “hub” nodes, 
which are nodes with relatively high degrees. The hub nodes and the connections to the 
hubs are important to present a network’s global structure. Because LD used the 
unweighted degree, in our study, we extend it to the weighted degree for our weighted 
article networks. For each node, we score an incident (associated) edge based on the 
weighted degree of the other endnode. The LD approach assigns high scores to edges 
that lead to the hub nodes, and preserves the network “hub backbone”. 
Figure 6.3: Article network sparsification with different scoring schemes 
Step 3. Edge sampling.
After the edge scores are calculated, we sample the edges 
based on the edge scores. For each node 
v ∈ V
, we include the top 
⌊
𝑑𝑒𝑔𝑟𝑒𝑒(𝑣)
ˆ
⌋
edges 
sorted by edge scores in descending order, where 
𝑑𝑒𝑔𝑟𝑒𝑒(𝑣)
is the degree of node v, and e 
(
0 ≤ e ≤ 1
)
controls the strength of sampling (filtering). We ensure that at least one incident 
edge is kept for each node. In this study, we use a sparsification parameter 
e = 0.5
to 
preserve at least 
Ç
𝑑𝑒𝑔𝑟𝑒𝑒(𝑣)
¹.È
É
edges for each node. 
132 
6.3.3 Visualization of Sparsified Article Network 
After the network sparsification, we use the force-directed algorithm [131], [132] to draw 
sparsified article networks in an aesthetically pleasing way in a 2D space. As a spatial 
layout, it places nodes and edges by simulating a physical system. When the system comes 
to a mechanical equilibrium state, the pairwise geometric distances between the drawn 
nodes match the graph theoretic pairwise distances. Specifically, similar article nodes tend 
to aggregate together while dissimilar article nodes are drawn apart. We implement the 
algorithm in Gephi [150] with the built-in Force Atlas layout [132]. 
We 
illustrate 
the 
baseline 
network 
and 
sparsified 
networks 
with 
the 
abovementioned sparsification schemes using the ADHD dataset [26] as an example. The 
ADHD dataset has a total of 851 articles: 20 are relevant articles that are included at the 
full-text level, 64 irrelevant articles are only included at the title/abstract level, and 767 
irrelevant articles are excluded at the title/abstract level. As shown in Figure 6.4, the 
baseline network has clutter presents that limit human perception without explicit visual 
patterns e.g. structures. All sparsified networks provide more interpretable structures and 
reveal the clustering of relevant articles (green nodes). Specifically, AS provides the most 
manifest community structure with meticulous separations, where relevant articles are 
highly 
concentrated. 
AD 
retains 
local 
connections 
and 
leads 
to 
densely 
connected 
neighborhoods. LD preserves the hub backbone structure by concentrating nodes towards 
hubs and forming bridges among hubs. 
133 
Figure 6.4: Illustrated sparsified article networks on the ADHD dataset 
6.4 Article Map Projection 
In an article map, articles are represented as a set of points and their similarities are encoded 
by the spatial positions of (distances among) article points. We use t-SNE [105] to generate 
(a) baseline 
(b) AS 
(c) AD 
(d) LD 
Green nodes: relevant articles (full-text level inclusion) 
Yellow nodes: irrelevant articles (abstract level inclusion only) 
Blue nodes: irrelevant articles (exclusion) 
134 
article maps by providing t-SNE with the established lexical article representations (feature 
vectors). The t-SNE approach establishes article similarities (Euclidean distances) in the 
high-dimensional feature space and creates an article map by projecting article similarities 
down to a 2D space. The resulting visualization is considered a map or a scatter plot with 
data point distribution. The t-SNE approach also retains data structures (e.g. clusters) by 
keeping similar data points close together while pushing dissimilar points far apart. Besides, 
t-SNE has been shown to create higher-quality visualizations than linear methods (i.e. PCA 
and MDS) and other nonlinear methods (i.e. SNE and Isomap). 
We implement t-SNE in MATLAB. As shown in Figure 6.5, we illustrate an article 
map generated by t-SNE using the ADHD dataset. We can observe the concentration of 
high relevant articles as indicated by the aggregated green points (visual densities). 
Figure 6.5: Illustrated article map on the ADHD dataset 
135 
In Figure 6.6, we also illustrate communities in the AS sparsified network and clusters in 
the article map. Article nodes (points) are colored by communities (clusters). Dominant 
sets (communities or clusters) of high relevant articles are marked by green rectangles, 
which further demonstrate the effective clustering of high relevant articles. 
Figure 6.6: Illustrated clustering of high relevant articles 
6.5 Experimental Settings and Results 
6.5.1 Evaluation Methods 
We use the 15 DERP datasets [26] for performance evaluation and comparisons, which are 
measured with structural (topological) network properties and clustering patterns [48]. 
Specifically, we evaluate the quality of sparsified article networks considering their 
preservation of the original network structure i.e. article distribution, which can be 
(a) AS article network 
(b) article map 
136 
measured by a wide range of network properties. We also evaluate the clustering patterns 
of high relevant articles for the intended application of biomedical literature retrieval. 
For 
structural 
network 
properties, 
we 
consider 
graph 
diameter, 
clustering 
coefficient, communities, and modularity. 
Graph diameter
is the length of the shortest path 
between the most distanced nodes. A smaller diameter indicates a stronger concentration 
of a graph. 
Clustering coefficient
measures the degree to which nodes tend to cluster 
together. 
Nodes 
with 
higher 
clustering 
coefficients 
have 
higher 
transitivity 
in 
the 
neighborhood. 
Communities
are subsets of nodes that are internally densely connected but 
externally sparsely connected. 
Modularity
is designed to measure the strength of division 
of a network into communities. A high modularity corresponds to a better community 
structure. We use the Louvain method for community detection which is proven to provide 
high-quality results. A graph’s community structure and modularity also reflect its local 
structure with respect to intra-community connections. In addition, a graph’s global 
structure can be reflected by the diameter, averaged clustering coefficient, and the number 
of communities. However, these topological (structural) properties are unavailable in 
article maps. 
For clustering patterns, we evaluate them on both sparsified article networks and 
article maps. For article networks, we utilize communities detected by the Louvain method 
[134]. For article maps, we apply k-means clustering to identify clusters based on the 2D 
map computed by t-SNE. For convenience, we use 
set
to refer to 
community
and 
cluster
. 
We evaluate the clustering patterns of high relevant articles that have been identified in the 
benchmark datasets (external criteria). Because of the highly-imbalanced dataset with only 
137 
0.55%~27.04% (7.67% on average) high relevant articles, we identify the set that contains 
at least 10% of relevant articles as 
a dominant set
. We examine the 
coverage (recall)
and 
proportion (precision)
of relevant articles in all dominant sets, and calculate the balanced 
F-measure (F1 score).
This is inspired by the classic measure of clustering quality that 
evaluates how well the clustering matches the gold standard classes and interprets the 
clustering as a series of decisions. Other measures such as the purity, normalized mutual 
information, and Rand index are not suitable for the highly-imbalanced datasets. 
6.5.2 Results on Network Properties 
We report the network properties of the baseline network and the sparsified networks with 
the averaged results on the 15 DERP datasets (Table 6.1). Because edge sampling is 
implemented based on individual nodes, the number of edges after the sampling are not the 
same when using different sparsification methods. The clustering coefficient is also the 
average of all article nodes for each dataset. 
Baseline 
AS 
AD 
LD 
Edge Number 
176,492 
12,706 
13,323 
15,722 
Diameter 
3 
6 
5 
4 
Clustering Coefficient 
0.4038 
0.2581 
0.1169 
0.3806 
Modularity 
0.4103 
0.6231 
0.4336 
0.3860 
Community Number 
5 
10 
7 
5 
Table 6.1: Preservation of network properties 
As shown in Table 6.1, the number of edges is significantly reduced from 176,492 to 
approximately 13,000 after sparsification. All sparsification methods result in increased 
138 
diameters and decreased clustering coefficients because of the removal of most edges. 
However, they also show differences. AS brings the highest modularity with a larger 
number of communities, but alters the baseline graph diameter and clustering coefficient 
to a greater extent. Similarly, AD alters the baseline diameter and clustering coefficient, 
but provides a slight gain in modularity. LD retains a similar graph diameter, clustering 
coefficient, modularity, and community number compared to the baseline. In summary, AS 
results in a better community structure; AD tends to retain the baseline local structure 
(slightly higher modularity); LD performs the best in preserving baseline global structure. 
6.5.3 Results on Clustering Patterns 
We examine the clustering patterns of high relevant articles in both sparsified article 
networks and article maps. The optimized number of communities in an article network is 
determined by the Louvain method (default resolution setting). For the k-means clustering 
in an article map, we apply the knee (elbow) method to identify a proper value range for 
the number of clusters, k. We find the resulting range approximately aligns to the number 
of communities detected by the Louvain method in AS networks. Thus for each dataset, 
we have k equal to the number of communities in the corresponding AS network. 
In Table 6.2, we report the total number of 
sets
(communities or clusters), the 
number of 
relevant sets 
that contain at least one relevant article, and the number of 
dominant sets
that contain at least 10% of relevant articles. We also report the overall size 
of all dominant sets by calculating the ratio of articles contained by the dominant sets. We 
calculate the corresponding recall, precision, and F1 score regarding the relevant articles 
in all dominant sets. Again, all the results are averaged from the 15 DERP datasets. 
139 
Article Networks 
Article Map 
Baseline 
AS 
AD 
LD 
Total Set # 
5 
10 
7 
5 
10 
Relevant Set # 
4 
6 
5 
4 
8 
Dominant Set # 
3 
3 
3 
3 
3 
Dominant Size 
63.10% 35.13% 56.35% 65.59% 
31.36% 
Recall 
94.87% 83.16% 90.14% 92.99% 
76.45% 
Precision 
10.84% 17.53% 12.40% 10.08% 
17.36% 
F1 Score 
0.1823 
0.2623 
0.1983 
0.1713 
0.2618 
Table 6.2: Article distribution and clustering of high relevant articles 
As shown in Table 6.2, the AS network and article map generated by t-SNE have the largest 
number of 
sets
(10) and 
relevant sets
(6 and 8), but the number of 
dominant sets
is only 3. 
Their 
dominant sets
cover 83.16% and 76.45% of relevant articles with a size of 35.13% 
and 31.36% of entire articles. Both of them have lower recalls but the highest precisions 
and F1 scores (0.2623 and 0.2618). They achieve a good quality of clustering relevant 
articles by decomposing articles into finely separated sets. The LD network behaves 
similarly to the baseline network, with the relevant articles spreading into coarsely divided 
sets
. With the highest recalls, 3 out of 5 
sets
act as 
dominant sets
and cover over 90% of 
relevant articles. However, their precisions and F1 scores (0.1713 and 0.1823) are lower 
than others because of the conservative discrimination. The AD network brings moderate 
performance in the recall, precision, and F1 (0.1983). Specifically, 90.14% of relevant 
articles are covered by 
dominant sets
, with a size of 56.35%. 
In summary, network sparsification leads to a more recognizable network structure 
by distributing articles into distinct sets. Per the F1 score, the quality of clustering high 
relevant articles is improved with AS and AD. In addition, the AS network and article map 
140 
have the highest precisions and F1 scores, but the AS network has a better recall than the 
article map. Overall, the sparsified article networks and the article map provide more 
interpretable distribution and clustering patterns. 
6.6 Summary 
Network Sparsification Schemes
: AD and LD are applied to sparser networks (e.g. social 
networks and citation networks) in early works. To our knowledge, we are the first to apply 
AD and LD to article network sparsification. Due to the densely-connected nature of article 
networks, a relaxed pre-pruning step is used. In this section, we find that AD retains the 
local network structure, LD preserves the global network structure (also supported by other 
works), and AS performs the best in revealing the community structure. In addition, 
considering the clustering of high relevant articles, AD and LD have lower precisions but 
higher recalls, resulting from their integration of the network structure; while AS has a 
lower recall, but higher precision because it aggressively concentrates relevant articles. 
Another encouraging finding is that by keeping only 1-3% of edges (13,000 edges on 
average) 
from 
the 
original 
networks 
(1,200,000 
edges 
on 
average), 
we 
can 
reveal 
meaningful network structures and important clustering patterns. 
Sparsified Article Network vs. Article Map:
Article networks sparsified by AS and 
articles maps generated by t-SNE achieve similar results in revealing article distribution 
and clustering patterns. Both of them aggregate the majority of high relevant articles into 
finely separated set(s). While the AS network has a slightly lower precision than the article 
map, it has a higher recall which is very important for biomedical literature retrieval. For 
article networks, a sparsification process is needed to eliminate clutter presents; but we can 
141 
flexibly configure the sparsification scheme towards task-specific interests, and explore the 
graph topology and apply graph-based algorithms for advanced visual analytics, such as 
community detection and graph traversal. Article maps are generated by t-SNE or other 
dimensionality reduction algorithms, but topological analysis is not available. A future 
investigation could include a user study to gather feedback regarding the selection of 
visualization types for biomedical literature retrieval. 
In summary, to promote effective visualizations and visual patterns for biomedical 
literature retrieval, we have implemented visualizations of both sparsified article networks 
and article maps. We mainly focus on configurable article network sparsification schemes, 
which value and preserve important article similarities from different perspectives (e.g. 
task-specific interests), and demonstrate their effectiveness in preserving meaningful 
articles structures and exhibiting clustering of high relevant articles in an intuitive and 
interpretable manner. We also compare the sparsified article network with the article map, 
both are shown to be effective visualization approaches with compelling visual patterns to 
facilitate the identification of high relevant articles in biomedical literature retrieval. In 
other words, by revealing and even highlighting the important structure and distribution of 
articles, human researchers can conduct biomedical literature retrieval in an effective and 
efficient manner. Consider a scenario of screening hundreds of articles in the ADHD 
dataset, researchers can rapidly identify the most of high relevant articles by screening only 
a small percent of articles once the dominant sets (visual densities) are located. 
Highlight of Contribution 
142 
We promote effective visualizations of article collections (text corpuses) for biomedical 
literature retrieval (information retrieval) with multiple visualization approaches, including 
sparsified article networks and article maps. We develop a novel and configurable article 
network sparsification scheme which is shown to preserve important article relationships 
and result in effective visual patterns, e.g. the distribution and compelling clustering of 
high relevant articles. 
This work has been presented and published in MedInfo 2017: 
Xiaonan Ji, Raghu Machiraju, Alan Ritter, Po-Yin Yen. Visualizing Article Similarities via 
Sparsified Article Network and Map Projection for Systematic Reviews. International 
medical informatics conference (MedInfo). 2017;245:422-6. (Award: 1st Place of Student 
Best Paper Competition at MedInfo 2017)
143 
Chapter 7. A Visual Analytics System for Literature Retrieval and Text Corpus 
Exploration 
In the previous two chapters, we have demonstrated the usefulness of visualizing article 
collections, utilizing article representations and similarities form text analytics, to facilitate 
biomedical literature retrieval, and promoted effective visualizations using both sparsified 
article networks and article maps. In this chapter, we further expose the visualizations to 
real-world settings, where scalability, interpretability, and interactivity are of essential 
importance [17], [18], [20], [130]. Specifically, an article collection for literature retrieval 
usually contains hundreds, thousands, or tens of thousands of articles, however, we might 
only interpret visualizations of a few hundreds of articles at once with respect to the 
capability of a human brain. To improve the usability of effective visualizations in practice, 
we are motivated by the state-of-the-art theorem of finding a compromise of what to show 
at what level of details given a largescale dataset [168], which can be realized by integrating 
human interactions to visual analytics. 
Under this notion, we develop a usable visual analytics system for literature 
retrieval as well as visual document analytics in real-world settings. This system will 
exploit text analytics results and reinforce the involvement of human researchers to 
facilitate both task accomplishment and knowledge exploration, with expedited scalability, 
interpretability, and interactivity. In light of the visual analytics theorem [168], we consider 
that: 1) A visual analytics session of biomedical literature retrieval consists of a sequence 
144 
of different tasks (a similar idea was stated in [169]), for example, an overview of an entire 
article collection and a close examination of a subset of important or interesting articles. 
Accordingly, a single view won’t be enough or competent for the multiple tasks, and it 
would be necessary to switch among multiple views with proper and smooth transitions. 2) 
A large dataset can be reduced into a more manageable size with respect to a specific 
analytics task, and such a reduction can be realized by filtering or abstraction/aggregation 
(a similar idea was stated in [170]). In this sense, interactive features would allow us to 
meet the visual budget while satisfying a visual analytics task towards a particular 
information need. Humans will consequently have improved capability and confidence to 
explore large article collections in a scalable, interpretable, and interactive way. 
Following the guideline of the Visual Analytics Mantra [171]: analyze first, show 
the important, zoom, filter and analyze further, details on demand, we implement a series 
of interactive features upon effective visualizations, including: (1) multilevel structure with 
hierarchical clustering and an adjustable resolution, (2) dynamic cluster topic synthesis, (3) 
hierarchical navigation with Pan and Zoom (smooth transitions among multiple views), 
and (4) additional features for information discovery and acquisition. We develop this 
visual analytics system as a web-based system, where the visualizations and interactions 
are realized with JavaScript and D3.js [172]. 
To evaluate how the visual analytics system can assist human researchers in 
conducting 
literature 
retrieval, 
such 
as 
systematic 
literature 
review 
and 
document 
exploration, we conduct a case study using a sequence of analytics tasks which are derived 
from the intended application of biomedical literature retrieval. In particular, these tasks 
145 
include: (1) obtain a high-level overview of an entire article collection or text corpus, (2) 
identify or narrow down to subsets (clusters) of articles of potential interests, (3) identify 
high relevant articles according to the precise information need, and (4) optionally, 
generate new knowledge or identify new associations of interests. 
As an extensional work
, we are also interested in whether our visual analytics 
system can be utilized by computer science engineers and biomedical informaticians to 
evaluate and interpret the applied text analytics results, such as the article representations 
generated by neural document embedding models, and obtain visual feedbacks and insights. 
Specifically, despite the state-of-the-art performance of neural document embedding, it is 
typically used as a black-box and little is known about how the performance is achieved 
and how the semantics is encoded. While many studies attempt to evaluate neural 
embedding 
models 
and 
their 
performances, 
they 
usually 
rely 
on 
trial-and-error 
or 
benchmark datasets with limited characteristics. [106] Due to the presence of boundless 
analytic facets and significant noises in many neural embeddings, it is also challenging to 
make thorough interpretations even with sophisticated mathematical tools. Therefore, we 
leverage the advantageous of visual analytics to evaluate and interpret neural document 
embeddings in a comprehensive and interactive manner. Users can benefit from improved 
analytics to recognize and gain insights into effective document embeddings (article 
representations) and promote more favorable performance. Related works have used visual 
analytics to other neural networks, such as Skip-gram [173], CNN [174], and RNN [175]. 
Among the different types of neural document embedding models, we decide to use 
the classic Paragraph Vector (PV) model and our Multi-Task Paragraph Vector (PV-MT) 
146 
model as described in Chapter 4. We then formulate our visual analytics goals to :1) 
evaluate the usefulness of article representations (document embeddings) learned with PV 
and PV-MT respectively, and understand their performances in our information retrieval 
application i.e. biomedical literature retrieval, and 2) interpret the underlying embedding 
space (semantic vector space), with respect to the neural dimensions (semantic features) 
and their behaviors, associated semantics, and contributions to the performance, using PV 
as a pilot study. Our study design and approaches should be extensible to other neural 
embedding models, such as PV-MT, CNNs, RNNs, etc. 
7.1 Research Questions 
We formulate and highlight our research questions with respect to the objective to develop 
a usable visual analytics system to assist in literature retrieval and knowledge exploration 
in real-world settings, and provide visual feedback and insights to mutually support text 
analytics e.g. prevalent neural document embedding for semantic text analytics. 
•
By integrating effective visualizations (e.g. sparsified article network and article map) 
with a series of interactive features, can the visual analytics system meet the real-world 
expectations of better scalability, interpretability, and interactivity? 
•
More specially, with the visual analytics system, can human researchers gain enhanced 
capability to conduct literature retrieval and knowledge exploration with respect to a 
sequence of analytics tasks, which are derived from real-world settings? 
•
Additionally, can our visual analytics system provide useful feedback and insights to 
support the visual evaluation and interpretation of the applied text analytics results, 
specifically, article representations (embeddings) by the PV and PV-MT models. 
147 
7.2 Background and Related Works 
7.2.1 Hierarchical Clustering and Graph Summarization 
In this section, we describe a closely related technique of hierarchical clustering, which 
produces clustering patterns and results in a multilevel structure for a given dataset, and 
the clustering patterns are considered an important guidance for interpreting and exploring 
the dataset. We also describe a related concept of graph summarization, which produces a 
compressed visualization as a tradeoff between the visual budgets and the preservation of 
important information. Graph summarization inspires the idea of the multilevel structure. 
Hierarchical clustering [176] constructs clusters by recursively partitioning the data 
instances in either a top-down (divisive) or bottom-up (agglomerative) manner using the 
connectivity information. The division or merging of clusters is performed based on a 
particular similarity measure (among clusters), such as the single-link, complete-link, 
average-link, and the advanced ward-mode. The result of a hierarchical clustering can be 
represented as a dendrogram, where the number of clusters and the corresponding 
resolution can be flexibly selected by cutting the dendrogram at any desired level (height). 
The quality of a hierarchical clustering method can be reflected by its versatility, especially 
regarding some special patterns, such as non-isotropic (no-concentric) clusters and nested 
multi-partitions. 
Hierarchical 
clustering 
methods 
typically 
have 
a 
time 
complexity 
proportional to 
O(𝑛
+
)
. For the special case of hierarchical network clustering (community 
detection), the Louvain method [134] agglomeratively merges clusters (communities) with 
a greedy heuristics for a local optimization of modularity. In particular, the algorithm 
consists of two major steps that are iteratively implemented until convergence: (1) 
148 
screening the nodes and assigning a node to a community if that leads to an increase in 
modularity, and (2) taking formed communities as super-nodes and repeating the first step. 
Although the underlying computational problem is NP-hard, the Louvain method relies on 
an efficient and effective heuristic to balance the speed and quality (modularity). Therefore, 
it has a computational complexity that scales roughly linearly with the number of edges. In 
summary, the Louvain method has demonstrated to have several advantages in easy 
implementation, fast computation, handling large and weighted networks. The Louvain 
method is also shown to provide high-quality results for network clustering. 
The purpose of graph summarization [149], [156] is to produce a compressed 
representation of an input graph or an adjacency matrix (e.g. similarity matrix) to promote 
human’s capability of identifying important patterns. Given a graph without any external 
information, the expected outcomes of graph summarization can be a set of structures or a 
compressed data structure that concisely describe the given graph. In specific, graph 
summarization techniques can be categorized as: (1) Grouping-based methods, which are 
also known as (hierarchical) network clustering, aggregate nodes into super-nodes (clusters) 
and generate a super-graph. (2) Simplification-based methods that simplify a graph by 
removing less important nodes (or edges) and generate a sparsified graph. For example, 
graph sampling, edge sampling (network sparsification), node sampling, sketches, etc. (3) 
Compression-based methods that cast the summarization problem as a compression 
problem with the goal to minimize the number of bits needed to describe a graph, and 
produce a graph summary with a significantly smaller size. For example, the Graph 
Dedensification model compresses the neighborhoods around high-degree nodes. (4) 
149 
Influence-based methods, which are commonly used in social information propagating and 
influence analysis, discover a short representation of the influence flow in largescale graphs, 
and can be formulated as an optimization process. For example, a compact network 
representation with nodes representing communities and directed edges representing 
influence relationships. (5) Pattern-mining based methods that summarize a network with 
structural patterns and replace frequent patterns (e.g. bipartite cores) with super-nodes. 
7.2.2 Visual Analytics of Text Corpus 
This study is related to visual analytics of text corpus (document or article collection) to 
visually interpret and exploit established text features or representations. Related work 
summarizes document collections with their properties and patterns in a global manner 
[177], and many types of visualizations are utilized for different purposes. For instance, 
stream and river-based visualizations are used to explore dynamic patterns. [178]–[180] 
Graph-based document networks [181], [182] and distance-based document maps [177], 
[183], [184] are widely utilized to lay out documents in 2D space with document 
relationships being encoded by the most compelling spatial channel. For 2D embedding 
approaches that map high-dimensional document representations into a 2D space, the t-
distributed stochastic neighbor embedding (t-SNE) algorithm [105] shows to retain 
important data structures (e.g. clusters) and is considered a preferred method to visualize 
document maps compared to conventional ways e.g. multidimensional scaling (MDS). 
On the other hand, document clusters and topics are considered the most interesting 
patterns (properties) of a document collection, and related work enables users to make 
interactive explorations for that. Cao 2010 [182] implemented a graph-based visualization 
150 
and let users explore the multifaceted relationships among documents and clusters. Heimerl 
2016 
[177] 
visualized 
a 
document 
map 
and 
used 
focus+context 
(dynamic 
lens) 
to 
characterize groups of documents with keywords and topics. Kim 2017 [183] visualized an 
augmented document map and computed topics on the fly via an interactive lens interface. 
In summary, visible document distribution, clusters, and topics are leveraged to 
assist in understanding and exploring of a document collection, as well as the applied 
document representations behind. 
7.2.3 Visual Analytics of Large Dataset 
This study is also related to visual analytics of large dataset with respect to the large text 
corpus in the real-world settings. As a reflection from related works, node-link diagrams 
and scatterplots are most used to visualize large datasets, due to the better intuitiveness and 
scalability [20]. Again, the semantic meanings lie in a large text corpus usually attract the 
most attention in a visualization to help understand the topics and guide the explorations. 
Therefore, related works utilize topic modeling or text clustering techniques to obtain an 
overview of a large text corpus, and the most compelling position channel in visual 
analytics is utilized to encode and reveal the cluster and topic patterns. 
Below we summarize some helpful hints from related works in visual analytics of 
large datasets. Specifically: 1) A visual budget should always be considered, which is an 
upper limit for the number of visual entities that can be displayed due to the limited screen 
space, computational power, and human perception [185]. 2) A visual analytics session can 
consist of a sequence of different tasks. Therefore, a single visualization (view) won’t be 
enough or competent, and users would switch among different visualizations (views) for 
151 
different analytics tasks [169], [185]. 3) Existing visual analytics has been trying to find a 
compromise for what to show at what level of detail [185]. A good way to explore a large 
dataset is interactively filtering and aggregating the large dataset to obtain a more 
manageable subset which is relevant to a task [170]. Specifically, reducing the amount of 
data by selecting a smaller subset of interest (preserve local detail but lose overview), or, 
abstracting the data so that multiple data items are mapped onto a single visual entity 
(preserve overall characteristics at the cost of details). Abstraction can be realized with 
clustering techniques, i.e. displaying only the super-nodes representing clusters and their 
relationships. 4) In addition, for a dataset with multiple variables or multiple analytic facets 
[169], we could use hierarchical layout to generate rich views of the multivariate data. 
Different hierarchical levels can have different layouts, which should be informed by the 
type of variable and the analytic facet being explored. 
We also summarize transition techniques (e.g. focus, context, and navigation) with 
respect to the multiple views for large datasets. For the focus+context, existing works first 
identify and select focal nodes, which can be high-level nodes e.g. representing clusters as 
well, and then take associated nodes to form a subset for display and exploration. A related 
concept is Degree of Interest (DOI) [186], where information items have different levels 
of importance, and only items above a certain interestingness threshold are displayed. DOI 
can be computed based on prior interests, user interests, and distances to the focal point. 
𝐷𝑂𝐼(𝑥
│
𝑦, 𝑧) = 𝑎𝑙𝑝ℎ𝑎 ∗ 𝐴𝑃𝐼(𝑥) + 𝑏𝑒𝑡𝑎 ∗ 𝑈𝐼(𝑥, 𝑧) + 𝑔𝑎𝑚𝑚𝑎 ∗ 𝐷(𝑥, 𝑦)
For the hierarchical navigation upon a visualization with a multilevel structure e.g. 
hierarchical clustering patterns, existing works usually maintain both the overview (with 
152 
efficient high-level layouts) and detailed views (with higher-quality layouts). Clusters can 
be expanded or collapsed per requests, and cluster boundaries can be highlighted e.g. by 
the convex hull method [170]. In some works, users can choose the layout for a detailed 
view, and apply additional constraints to refine the layout, such as non-overlapping cluster 
membership and paths cannot pass nodes. [170] Some auxiliary information, such as 
statistical properties and visual properties, can be presented as details on demand. For the 
interactive navigation, some works have been addressing the challenges lie in the space 
(RAM and screen) and the time (dynamic computation of layouts and clusters) [187]. 
There are a set of criteria to define a “good transition” [170] among different views, 
including: 
1) 
the 
scalability, 
especially 
for 
large 
networks, 
2) 
the 
stability 
during 
interactions (e.g. switches of views), such that nodes don’t move unnecessarily, and the 
current horizontal and vertical orders between nodes are preserved, and 3) the continuity, 
such that transitions to new layouts are smooth. For instance, the animation is very useful 
in providing smooth transitions and supporting users’ mental map during transitions. The 
most recognized transition methods can be categorized as: 1) Multiple coordinates views 
e.g. placing different visualizations inside a matrix visualization. 2) Focus+context, such 
as fisheye lens, which preserves the contexts during transitions. 3) In-situ visualization, 
which selects multiple subsets of data, chooses a desired visual representation for each 
selection, and embeds or imposes these representations right in the place where the 
selection was performed (or in a specified place). For example, portals and sub-windows 
imposed to the data canvas in the specified place. 4) Overview+detail or transient overlay, 
which superimposes one visualization upon the others. 
153 
7.2.4 Visual Analytics of Neural Networks 
Our visual analytics of neural document embeddings is inspired by previous work in visual 
analytics of neural networks with respect to their intermediate states or final outputs. 
Specifically, CNNs in pattern recognition have been first studied to gain insights into the 
behaviors of internal neurons and layers, thus help understand the superior performances 
and potential improvements. Zeiler 2013 [188] visualized feature maps for the contribution 
of different layers and used heatmaps for the final predictions. Simonyan 2013 [189] 
visualized gradients of class scores and presented saliency maps for classes. Liu 2017 [190] 
used a DAG metaphor and a hybrid visualization to understand the multi-facets of neurons, 
neuron interactions, and learned features. Rauber 2017 [191] used 2D projection of learned 
representations and analyzed contributing neurons based on cluster patterns. Alsallakh 
2018 [174] visualized a confusion matrix to reveal relations between class hierarchy and 
the learning behaviors of CNNs. 
Meanwhile, RNNs and their variants (e.g. LSTM and GRU) are widely exploited 
in NLP for language and sequence modeling. Related work has focused on analyzing the 
activities and behaviors of hidden states over time, exploring different linguistic aspects 
being captured, and understanding semantic properties of RNNs. Karpathy 2015 [192] used 
heatmaps to visualize cell activations and their contributions to the linguistic aspect of 
long-term dependencies. Li 2015 [193] used heatmaps to visualize hidden state values for 
encoding linguistics aspects (e.g. negation, intensification, and clause), and used salient 
heatmaps on derivatives to visualize the contribution of neuron units to the final meanings. 
Cirik 2016 [126] used several visual plots and diagrams to characterize the behavior of 
154 
internal states in curriculum learning. Strobelt 2018 [175] developed a visual analytics tool 
using parallel coordinates for users to examine hidden state dynamics and the associated 
semantics. 
Visual analytics of neural embedding models attracts increasing interests in recent 
years. Kiros 2015 [175] used 2D t-SNE to visualize and qualitatively evaluate sentence 
representations learned by an RNN-GRU model. Smilkov 2016 [194] developed an 
embedding projector (e.g. PCA and t-SNE) to help users understand neural embeddings 
and their performance. Palangi 2015 [115] used heatmaps to visualize activation behaviors 
of internal states and sentence embeddings for an RNN-LSTM model, and disclosed the 
encoding of keywords and topics. Lin 2017 [121] used heatmaps to reveal how important 
sentence components were captured by an RNN-LSTM model. Dai 2015 [99] used 2D t-
SNE to visualize document representations from the PV model and qualitatively evaluated 
clusters of benchmark documents. Liu 2018 [173] used analogy projection to visualize 
semantic analogies in word2vec, and augmented t-SNE to examine the overall structure. 
In summary, visual analytics of neural networks has been focusing on the relations 
among neuron units (states, dimensions), learned features or representations, and the 
associated performance. [195] More specifically, in NLP, visual analytics intends to 
understand semantic properties and contributions of different neuron units. 
7.3 A Visual Analytics System Design 
7.3.1 Article Collection Visualization 
We build our visual analytics system upon the visualization of an article collection using 
the established article representations and similarities, which have been demonstrated to 
155 
be effective in facilitating the identification of high relevant articles with compelling visual 
patterns e.g. article distributions and densities (clusters). The visualization performs as a 
fundamental 
mind 
map 
providing 
references 
and 
guidance 
for 
further 
analytics, 
explorations, and evaluations. Consider the multiple visualization schemes described in the 
previous chapters, we provide users with the option to choose a 2D layout (drawing method) 
for 
a 
given 
article 
collection. 
Because 
of 
the 
space 
limitation, 
here 
we 
use 
two 
representative visualization schemes for demonstration, including: 1) the AS sparsified 
article network based on lexical article representations and similarities, and is visualized 
with the force-directed graph drawing algorithm, and (2) the article map based on semantic 
article representations learned with the PV (or PV-MT) model, and is visualized with t-
SNE. In Figure 7.1, we illustrate the basic visualizations with the aforementioned two 
schemes. We use the ADHD dataset from DERP [26] and highlight the benchmark high 
relevant articles (points) with bold borders. At the current stage, we would observe that the 
two visualizations establish similar visual patterns (e.g. distribution and aggregation) for 
the high relevant articles, and the AS network performs slightly better. 
(a) Article network (AS Sparsification) (b) Article map (PV and t-SNE) 
Figure 7.1: Multiple 2D visualization schemes 
156 
The visualizations are also configurable. Specifically, with n-dimensional (n=200) article 
representations (embeddings) generated by the PV model, we use t-SNE for dimensionality 
reduction and visualize the article map in a 2D space. As shown in Figure 7.2 (b), we allow 
users to configure t-SNE parameters such as the perplexity and learning rate and decide the 
neural dimensions to be used in the embedding space, which is similar to the concept of 
feature selection. Although t-SNE and the force-directed algorithm are considered high-
quality methods that can preserve some important data structures (e.g. clusters), their nature 
of approximation can result in information loss and distortion in the 2D visualization. To 
better access to the actual clusters in the original feature space, we integrate additional 
functions of hierarchical clustering and dynamic topic synthesis to the visualizations. 
Figure 7.2: Illustrated 2D visualization with clusters and topics 
7.3.2 Adjustable Hierarchical Clustering - Multilevel Structure 
Users can invoke hierarchical clustering on both article networks (Louvain method) and 
article maps (the ward-mode [196] that minimizes inner cluster variances during merges). 
(b) 
(c) 
(d) 
(a) 
(e) 
157 
Under this notion, the agglomerative hierarchical clustering generates a dendrogram 
(hierarchical tree) with multilevel clustering results. Figure 7.2 (c) and (d) shows a selected 
clustering level of 4, where 8 clusters are formed. Users can decrease the clustering level 
to form fewer and larger clusters (e.g. one cluster at level 1) or increase the clustering level 
to form more and smaller clusters (e.g. 16 clusters at level 5). Therefore, the major 
advantage of hierarchical clustering is to enable users to adjust the granularity or resolution 
based on their needs and obtain coarse or fine-grain clusters on different levels. Specifically: 
1) higher resolution = less and larger clusters, where relaxed clustering rules are applied 
for ambiguous information need or general interests. 2) lower resolution = more and 
smaller clusters, where strict clustering rules are applied for more precise information need 
or more specific interests. In this sense, clustering levels with a higher resolution can assist 
in a rapid overview of an article collection; users can adjust the resolution and switch to 
fine-grain clusters for detailed examinations. In summary, the multilevel structure would 
be corresponding to a set of views supporting multiples analytics task (scales and scopes). 
However, because of the information loss in the 2D visualizations, articles from the 
same cluster (same color) are not necessarily placed close together in a visualized article 
network or article map. To alleviate the visual distortion, users can select to aggregate 
articles from the same cluster towards the direction 
𝑐𝑒𝑛𝑡𝑒𝑟
r†—‰ƒˆ•
− 𝑐𝑒𝑛𝑡𝑒𝑟
”†„Ð…†
, with a 
multiplier α controlling magnitude. This simple method is also able to preserve relative 
positions among clusters and articles in the same cluster. Figure 7.3 shows an article map 
before 
and 
after 
the 
aggregation. 
Besides, 
we 
highlight 
the 
cluster 
region 
using 
a 
dynamically calculated convex hull, which appears like a lens that captures all articles in 
158 
the cluster under mouse-over. We consider these schemes as visual enhancement of the 
clustering patterns in the original feature space. 
(a) Before enhancement (b) After enhancement 
Figure 7.3: Visual enhancement of clustering patterns 
7.3.3 Dynamic Topic Synthesis 
As shown in Figure 7.2 (e), topics of clusters on a user selected level are dynamically 
synthesized based on keywords of enclosed articles. In particular, article keywords are pre-
generated by an advanced RAKE [197] method in text processing. For each individual 
article, the RAKE method chunks the text into noun phrases, and scores and ranks the 
phrases to reflect their importance in the content. Therefore, in preliminary text analytics, 
phrases with high ranks are considered the article keywords; and in interactive visual 
analytics, keywords with high document (article) frequencies are considered the cluster 
topics. By leveraging the pre-generated article keywords, our dynamic topic synthesis is 
highly efficient, and can generate results comparable to traditional topic modeling methods 
(e.g. LDA [103] and NMF [198]). As an example on the ADHD dataset [26], for a cluster 
159 
that covers a large number of high relevant articles about drug effectiveness on ADHD, the 
cluster topics generated by different methods are: 
•
Rake: adult attention deficit hyperactivity disorder, child attention deficit hyperactivity 
disorder, comorbid attention deficit hyperactivity disorder, adhd treatment, stimulate 
medication, methylphenidate treatment, disorder received placebo, reduced symptom, 
adhd rating scale, etc. 
•
LDA: risperidone, clozapine, olanzapine, adhd, placebo, child, symptom, haloperidol, 
methylphenidate, disorder, etc. 
•
NMF: symptom, diagnostics, trial, adhd, adult, treatment, control, medication, etc. 
Moreover, comparing to related works that generate topics or keywords for 
documents captured by a dynamic lens [177], [183] our scheme of hierarchical clustering 
with dynamic topic synthesis can serve for a similar purpose in a more advisable way: 
capture a group of similar articles (i.e. clusters) with an adjustable granularity (i.e. 
clustering level) and generate topics in real-time. Together, as shown in Figure 7.2 (a), the 
2D article map with enhanced cluster patterns visually reveals the important properties of 
the applied article representations. The customizable hierarchical clustering and dynamic 
cluster topic synthesis further facilitate the interpretation and exploration in a user 
adjustable way. The essential semantics of articles can be reflected by topics and keywords. 
7.3.4 Hierarchical Navigation 
With a multilevel visualization realized with hierarchical clustering, we enable multiple 
views to support analytics task at different scales. For smooth transitions among views 
160 
during an analytics session consisting of multiple tasks, we implement Pan and Zoom to 
realize a hierarchical navigation on the visualization with a multilevel structure. 
Pan is utilized to explore a selected view that is either fit-in or beyond a screen 
space. Users can drag a view to any directions and concentrate on different portions of the 
view. The mechanism of Zoom in our application can be explained from two aspects: 
Zoom-out and Zoom-in. By zooming out a visualization, the visual densities representing 
clusters are collapsed (converged) into several super points, which have been reinforced 
by the applied position and color channels. With the distribution of super points, we can 
obtain an overview of an entire article collection on the top of a multilevel structure. 
Relationships among clusters, which can be referred to as a global structure, are also 
revealed by the relative positions of super nodes (Figure 7.4). 
On the contrary, selected clusters can be gradually expanded by zooming into the 
corresponding super points, so that the enclosed child clusters as well as article points are 
displayed with a full-scale for closer examinations. In this sense, we can approach a detail 
view of the distribution of low-level cluster points or article points of interests. Furthermore, 
during a Pan and Zoom process, views can be smoothly switched without any distortions 
[170]. And users can still maintain their mental maps through such a transition, for example, 
preserving the contexts around a focal point. Therefore, in our application scenario, Pan 
and Zoom is considered to bring comparable or even better scalability, stability, and 
continuity when compared to other methods, e.g. fisheye lens, portals, in-situ zooming, and 
transient overlays. In addition, both the abstract views with super points and the detail 
161 
views with article points are of a more manageable size that is scalable to large datasets 
and can be interpreted by users. 
Figure 7.4: A global structure with super points representing clusters 
7.3.5 Other Interactive Features 
Besides leveraging the clustering patterns to gain a rapid overview of an article collection 
as well as navigate into clusters for detailed examinations, we also enable a direct 
identification of individual articles as focal points. For doing so, we provide search 
functions to help locate individual articles of interests based on a matching with article IDs 
or a set of keywords. Article satisfying a search are then highlighted in the visualization, 
so users can have an immediate comprehension of their distribution and associations. Such 
articles can also perform as focal points, based on which users can explore surrounding 
regions 
or 
corresponding 
clusters, 
which 
are 
also 
referred 
to 
as 
relevant 
semantic 
contextual sets, to identify other articles of interests in a more advisable manner. Figure 
7.5 illustrates an identification of focal points and contextual sets via search. 
162 
Figure 7.5: Identification of focal points and contextual sets of potential interests 
We also integrate literature bibliometric information to support queries towards publication 
years and citations, which have been widely used for literature retrieval. Specifically, we 
enable 
the 
filter 
and 
selection 
of 
articles 
based 
on 
their 
publication 
years. 
Worth 
mentioning, this feature also brings about a valuable benefit in investigating the dynamic 
evolution or development of a research topic or a research community. By gradually 
including a sequence of publication years, i.e. through the past 28 years from 1990 to 2018, 
users can observe the constructive process and the enrichment of a research area while 
article points belonging to the years are progressively added and placed (distributed) onto 
the visualization. For citation information, we leverage it to help users expand their focal 
points or contexts as well. By clicking a recognized article A, other articles within the 
article collection citing A (citations) or being cited by A (references) will be automatically 
Search Term 
Contextual 
Set 
Focal 
Points 
1 
2 
3 
163 
highlighted. Users can also customize whether they want to approach citations, references, 
or both via a set of checkboxes. Besides, to highlight pilot articles or milestone articles that 
have gained lots of citations, we use the size channel to make them more distinguishable 
from 
the 
others. 
The 
size 
of 
article 
points 
is 
computed 
as 
size = size
ÒÓÔÕÖ
∙ (1 + 𝑟 ∙
𝑛𝑢𝑚𝑏𝑒𝑟
r1ƒ…ƒ1„-
)
, we took 
r = 0.2
in this work. In Figure 7.6, we show a clicked article along 
with its citations and reference in the Beta Blocker dataset from DERP. 
Figure 7.6: Identification and highlight of citations and references 
7.3.6 Use Case 
In the previous sections, we have integrated a series of interactive features into an effective 
visualization with reinforced visual patterns, aiming to achieve a better scalability and 
interpretability. Here we would like to have a case study on the ADHD dataset [26] and 
raking its task scope to validate our hypothesis that this interactive visual analytics system 
can increase human researchers’ capability and confidence in conducting biomedical 
Click an article to highlight its 
citations and references 
Highlighted 
citations and references 
164 
literature review and knowledge exploration. In specific, the high-level scope of this 
ADHD task is to review drug effectiveness towards ADHD, and its information need is 
described 
by 
several 
research 
questions, 
including: 
1) 
What 
is 
the 
comparative 
effectiveness of different pharmacologic treatments for ADHD? 2) What is the comparative 
tolerability and safety of different pharmacologic treatments for ADHD? 3) Are there 
subgroups 
of 
patients 
based 
on 
demographics 
(age, 
racial 
groups, 
gender), 
other 
medications, or co-morbidities for which one pharmacologic treatment is more effective or 
associated with fewer adverse events? 
More 
specifically, 
human 
researcheres 
would 
be 
interested 
in 
identifying 
biomedical articles about any recognizable treatments on children or adult with ADHD, as 
well as comparative evaluations on treatment effectiveness and safety. While different 
literature retrieval tasks might have diverse procedures in specific for task accomplishment, 
there are shared interests among them in general. Under this notion, we summarize a 
sequence 
of 
analytics 
tasks 
from 
literature retrieval 
activities, 
such 
as 
(systematic) 
literature reviews and explorations. We consider these analytics tasks, which are listed as 
below, as the most important compositions of the analytics session towards the ADHD task. 
1)
Obtain a high-level overview of an entire article collection (task corpus). 
2)
Identify or narrow down to subsets (clusters) of potential articles of interests. 
3)
Identify highly relevant articles according to the precise information need. 
4)
Optionally, generate new knowledge or identify new associations of interests. 
In this sense, users (human researchers) can carry out a literature retrieval task following 
the sequence of the analytics tasks. They are allowed to select different visualization types, 
165 
such as the AS sparsified network drawn by the Force Atlas algorithm, and they will be 
provided with a visual article distribution in the 2D space. The users can then select a 
clustering level with a high resolution for an overview and reinforce the clustering patterns 
by setting clustering aggregation. After this quick setup, the main window of the system 
will present a global distribution of clusters of similar articles, and promote a rapid 
overview of the clusters with dynamically synthesized topics, as shown in Figure 7.7. 
Therefore, the first analytics task of obtaining rapid high-level overview is achieved. 
Figure 7.7: A global overview of clusters and topics on the ADHD dataset 
With this enriched mind map about clusters, topics and their distributions, users can 
identify the green cluster, with the topic of 
“adhd, treatment, medication, children, adult”
, 
as an interesting candidate article subset to explore. Some closer clusters would be the ones 
adhd, treatment, 
medication, children, adult 
modafinil, 
sleepiness 
bupropion, 
patient, treatment 
clonidine, 
group, patient, 
clonidine, shivering 
apraclonidine, 
iop control 
atypical antipsychotics, 
adverse effect 
extrapyramidal 
symptom, risperidone 
olanzapine, 
antipsychotics treatment 
Clozapine, 
weight gain 
166 
talking about treatments for antipsychotics, daytime sleepiness, etc., but are not directly 
related to the information need. Other clusters, which are about specific drugs or symptom, 
are also out of the scope. Therefore, the second analytics task of identifying (narrowing 
down to) a subset of articles of potential interests is also achieved. 
With the identified cluster of potential interests, which can be named as 
adhd
, the 
users then zoom into this 
adhd cluster 
for closer examinations. Meanwhile, the users can 
also adjust the clustering level
to a lower level
and reveal sub-clusters and potential sub-
topics enclosed in the 
adhd cluster
. As shown in Figure 7.8, the 
adhd cluster
has 4 sub-
clusters, and the associated sub-topics are: 
treatment for adult with adhd
, 
treatment for 
children with adhd
, 
parent and teacher
rating scale
, and 
mental health treatment and 
research
. 
Figure 7.8: A detailed view of sub-clusters and sub-topics 
adult, 
adhd, treatment 
parent, 
teacher, 
rating scale 
Mental health, 
treatment, 
research 
children, 
adhd, treatment 
167 
The users can then review a more precise information need, and find that both 
children
and 
adult
can be included as sub-groups to study the ADHD treatments. On the other hand, 
rating scale
is not necessarily related to the information need, and 
mental health
treatment 
could be a potential direction to explore. Under this notion, the user is encouraged to 
contribute most efforts to the 
children
cluster and the 
adult
cluster, but quickly screen the 
mental health
cluster and ignore the 
rating scale
cluster. With a reference to the benchmark 
labels of articles, in fact, 19 out 20 high relevant articles lie in the 
children
or 
adult
cluster, 
and the remaining 1 lies in the 
mental health
cluster. Therefore, we conclude that the users 
are able to identify high relevant articles with improved effectiveness and efficiency, which 
is the third analytics task. 
In addition, knowledge discovery (the fourth analytics task) is made available 
throughout the interactive visual literature retrieval process. As an example, the users can 
explore connections between ADHD treatments and atypical antipsychotics treatments 
with inspirations from the global overview; they can also investigate associations between 
ADHD treatments with mental health researches as inspired by the detailed view. Such 
advantageous exploitations of abstract knowledge (e.g. backend article representations and 
similarities) and explorations towards new knowledge won’t be easily available without 
the visual mind maps or the interactive features provided by a visual analytics system. 
7.4 Visual Evaluation of Neural Document Embeddings 
In the last section, we have presented a series of features to design and build a usable visual 
analytics system to exploit text analytics results and enable human researchers, e.g. 
biomedical and clinical personnel, to conduct literature retrieval as well as text corpus 
168 
exploration. Besides, we are also interested in applying visual analytics to explore the text 
analytics results, especially to evaluate (this section) and interpret (next section) article 
representations 
generated 
by 
neural 
embedding 
models, 
which 
have 
advantageous 
performances but a black-box nature. In this section, we will utilize visual evaluation to 
compare the classic PV model and our proposed PV-MT model to more comprehensively 
understand their performances in the information retrieval application. 
7.4.1 Internal and External Validation 
Firstly, we evaluate document (article) representations based on the resulting clustering 
patterns as an internal validation. For doing so, we use a visual diagram to display the 
clusters along with high-level statistics information. In this visual diagram, super nodes 
representing clusters are placed in correspondence with the basic visualization, and are 
sized by the cluster size. Besides the primary coloring scheme for cluster identification, we 
also calculate the Silhouette coefficient [199] for each cluster and use the Silhouette 
coefficient to assign a color intensity. As an internal validation, the Silhouette coefficient 
is a combined measure of both cluster separation and cohesion, reflecting the quality of 
clustering. While the silhouette coefficient of a cluster can be either negative or positive, a 
higher value indicates a better quality with both good inner cohesion and external 
separation. We also provide a brief summary of cluster information per moreover a cluster 
node, using the cluster size, silhouette coefficient, etc. 
Secondly, we evaluate article representations based on the visual patterns (e.g. 
distribution, aggregation, and clustering) of high relevant articles with their benchmark 
labels as external validation. For doing so, we use a similar visual diagram to display the 
169 
super nodes representing clusters, and color the nodes based on the number of enclosed 
high relevant articles. Per mouseover a cluster node, we also provide the summary 
information, including the number, recall, and precision towards the high relevant articles 
in the specific cluster. 
7.4.2 Use Case 
Figure 7.9 shows a pairwise comparison between PV and PV-MT on the ADHD dataset, 
using silhouette coefficient for node coloring, where a higher intensity represents a higher 
value. Although there exist differences on cluster distribution (partially due to the 
visualization algorithms and the indeterministic nature), we cannot observe obvious 
differences on the cluster quality as reflected by the colors of individual nodes and the 
overall color range. PV and PV-MT might have comparable capabilities to capture and 
encode the similarities and dissimilarities among articles. 
Figure 7.10 shows a pairwise comparison between PV and PV-MT on the ADHD 
dataset, using the number of high relevant articles for node coloring, where a higher 
intensity represents a higher value, and the grey color represents a value of zero. As shown 
in Figure 7.10, we could rapidly understand the distribution of benchmark high relevant 
articles, especially for their aggregations into certain clusters. In particular, PV allocates 
the high relevant articles into four clusters (e.g. #0, #5, #4, #6) with cluster #0 as the most 
dominant one; while cluster #0, #5, and #6 are adjacent to each other, cluster #4 tends to 
be an outlier, which would affect the recall. On the other hand, PV-MT allocates the high 
relevant articles into two clusters only (e.g. #2 and #5), which are adjacent to each other. 
170 
Such a pattern can accelerate the identification of high relevant articles with a high recall 
and precision. 
Please note that PV and PV-MT are not deterministic methods and the results from 
different rounds can alter, however, in general PV-MT tends to ease the situation of outliers 
with the integration of task specification. This case demonstration is also consistent with 
the experimental results we have obtained in Section 4.4.2, such that PV has an averaged 
WSS95 of 0.7117 while PV-MT has a higher value of 0.7413 on the ADHD dataset. 
Figure 7.9: Internal validation using silhouette coefficient 
Figure 7.10: External validation using the distribution of benchmark high-quality articles 
PV 
PV-MT 
PV 
PV-MT 
171 
7.5 Visual Interpretation of Neural Document Embeddings 
In the last section, we have utilized visual analytics to evaluate the performance of neural 
document embeddings in a more comprehensive and intuitive manner. In this section, we 
further interpret the underlying neural embeddings space to gain insights into how the 
performance is achieved and how the semantic properties are encoded. As a pilot study, we 
decide to use the PV model, and our study design and approaches should be extensible to 
other neural embedding models e.g. PV-MT, CNNs, and RNNs. 
Neural document embeddings are widely used by many user groups for research 
and application purposes. Before formulating our analytic goals, we identify user groups 
who might benefit from the improved analytics. Consider the prevalent applications of 
information retrieval, we connect with a computational information retrieval group, who 
has 
long-term 
collaborations 
in 
biomedical/clinical 
informatics, 
aiming 
to 
facilitate 
biomedical literature retrieval with effective text and visual analytics. The members are 
characterized as application designers and application evaluators: 1) Application designer 
(trainers), who are typically computer science engineers, employ and configure proper 
neural embeddings to generate effective document representations to resolve problems in 
the 
application 
domain, 
2) 
Application 
evaluators 
(end-users), 
who 
are 
biomedical 
informaticians, utilize established document representations, provide insight into the 
performance, and reinforce application goals with their domain knowledge. 
In meetings and discussions with the users, we identify their views of analyzing 
neural document embeddings, and synthesize their interests as: understanding how the 
performance of neural document embeddings is achieved, the mechanisms behind, and how 
172 
to achieve more favorable performance towards their application goals. The intermediate 
results of our earlier prototypes and the iterative user feedback point to investigate how the 
established semantics (e.g. reflected by cluster topics) are built-up and encoded in neural 
dimensions (neuron states or units) in the embedding space, and how different neural 
dimensions contribute to the overall or desired performance. By breaking the black-box 
with improved analytics, users consider that they will not only recognize effective 
document 
embeddings 
that 
are 
suitable 
to 
their 
applications, 
but 
also 
exploit 
the 
embeddings to generate customizable or more favorable results with the obtained insights. 
This consideration aligns with the ideas of semantic analysis and feature selection. 
Therefore, our objective is to utilize visual analytics to help users explore neural document 
embeddings and understand the embedding space with respect to the neural dimensions. 
We formulate three analytic goals (G1, G2, and G3), which are generalizable to document 
embeddings learned by different models. 
•
G1: Understand the overall behaviors of dimensions in the neural embedding space. 
Dimensions’ behaviors can be reflected by their value variances and distributions 
across a document corpus. 
•
G2: Understand the semantics associated with or encoded in particular (salient) 
dimensions. This would be approached by allowing users to identify and explore 
important/interesting facets. 
•
G3: Understand the contributions of different dimensions to the overall (or desired) 
performance. This would be approached by subspace analysis e.g. to characterize 
dominant dimensions. [200] 
173 
7.5.1 Visualization of Dimension Behaviors 
Behaviors of neuron states or units are usually reflected by their values, and the change or 
dynamics of values. Considering our scenario of analyzing neural dimensions in the 
embedding 
space, 
we 
use 
variances 
and 
distributions 
of 
dimension 
values 
across 
documents to characterize dimension behaviors. For example, a dimension might have 
distinctive (high variance) or similar (low variance) values across documents; some 
dimensions might have similar value distributions across documents. With established 
patterns of clusters and topics reflecting the semantic properties, we can also examine 
dimension behaviors across clusters, and speculate dimensions that are likely to associate 
with certain semantics or contribute to the resulting patterns. 
We first use a bar chart to briefly reveal the overall value ranges of all dimensions 
across all documents. As shown in Figure 7.11 (a), each dimension is represented by a bar, 
its max/min values are encoded as the top/bottom of the bar, its mean value is indicated by 
a dark dot on the bar, and the range (max-min) is encoded by the bar length. Because the 
order of the dimensions is meaningless, we sort the bars by the dimension variances, which 
are calculated as the standard deviation across all documents, from the highest to the lowest. 
Because of the number of neural dimensions (e.g. n=200) and the limited scalability of this 
bar chart, it is used to provide a quick sense of dimension behaviors and let users spot focal 
parts (ranges) for detailed analysis. 
For more detailed analysis of a range of dimensions, related work typically uses 
heatmaps. However, the color channel is not effective enough to encode extensive 
dimension values across a large document collection. In this work, we use parallel 
174 
coordinates to visualize the dimension behaviors by taking each dimension as a vertical 
axis (coordinate) and taking each document as a data item (polyline). The polylines are 
colored by documents’ cluster assignments on a user selected level (Figure 7.2). The 
horizontal axis is labeled with dimension indexes, e.g. 0 to n-1. Under this notion, we 
encode the dimension values using the more effective position channel [17], [201]. 
Variances and distributions are also intuitively revealed by the interactions between 
polylines and dimension axes. To make different dimensions with diverse value ranges 
more comparable, we normalize values of each dimension to [0, 1]. As shown in Figure 
7.11 (b)(c), users can select the top (e.g. 30) dimensions with the highest variances and the 
bottom 
dimensions 
with 
the 
lowest 
variances 
respectively, 
to 
examine 
their 
closer 
behaviors i.e. value distributions across documents and clusters. 
As shown in Figure 7.11 (b)(c), instant and interpretable patterns of dimension 
behaviors can be obtained from the selected dimensions. For example, users might find 
that dimensions with higher variances tend to have more distinct values across different 
clusters and vice versa, by comparing Figure 7.11 (b) and (c). Users might further speculate 
that 
the 
dimensions 
with 
high 
variances 
would 
contribute 
more to the 
composited 
performance, due to their higher capability in differentiating clusters. At the same time, 
users might identify some “salient” dimensions of importance or of interests. For instances, 
the dimension D42 has evidently and exclusively high values in cluster 0 (green), while 
the dimension D87 has exclusively low values in this cluster. Therefore, users might 
speculate that D42 is associated the unique semantics of cluster 0 i.e. ADHD and treatment. 
We also allow users to brush the vertical axes and select data items based on their criteria. 
175 
With the rightest axis representing the clusters, users can select one or multiple clusters for 
closer analysis. 
In summary, we present a high-level overview of all dimensions, and allow users 
to select a range of dimensions for closer examinations of their variances and distributions 
across documents and clusters. Users can observe instant patterns of dimension behaviors 
and identify dimensions of potential importance or of interests, with the guidance of 
established document clusters and topics. 
Figure 7.11: Visualization of neural dimension behaviors 
7.5.2 Exploration of Dimension Semantics 
What is encoded in or captured by a neuron unit? It is a hot topic in both image processing 
and language modelling. More specifically, the interdisciplinary of NLP and visual 
analytics intends to explore certain linguistic aspects encoded in particular neuron units 
thus understand the semantic properties of a neural network model. In this sense and with 
respect to the application goals, we aim to explore important or interesting semantics 
associated with salient dimensions in the embedding space. We use a 2-stage process that 
176 
allows users to: 1) generate a hypothesis about dimension(s) and its associated semantics 
using identifiable information, in line with the idea of selecting potentially useful features, 
and 2) refine the hypothesis with evidence from exploring particular global patterns across 
the dataset. 
Hypothesis Generation
. For the first stage of hypothesis generation, a hypothesis 
can be in a format of 
dimension A is associated with semantics x
. We provide different 
ways for users to identify dimensions of importance or of interests as the analytics facets. 
Directly, users can refer to the overall dimension behaviors described in Section 7.5.1 and 
identify prospective dimensions. For example, dimensions with distinctively high values 
in a particular cluster with interesting topics. Recall that in Figure 7.11 (b), D42 tends to 
have exclusively high values in cluster 0 (green), thus users might hypothesize that this 
dimension is associated with the semantics of 
ADHD and treatment
as reflected by the 
cluster topics. On the other hand, users can also look for prospective dimensions based on 
individual documents that are sampled by prior knowledge (e.g. application interests, 
benchmark dataset) and established patterns (e.g. clusters and topics). This idea aligns with 
the previous studies [173], [175] that asked users to specify a certain linguistic aspect or 
event to explore based on their prior knowledge or external annotations. The advantage of 
doing so is to allow users to specify their interests and accommodate desired details on 
demand. In our work, users are allowed to employ benchmark documents with already-
known meanings (semantics) or select representative documents that might carry unique 
characteristics (semantics) of certain clusters. As shown in Figure 7.12 (a), users can take 
one benchmark document A with known semantics (
ADHD and treatment
). They can also 
177 
sample additional documents (B, C) from other clusters which are likely to have different 
semantics. With the seed documents, we provide a new set of parallel coordinates for users 
to explore all dimensions across the seed documents on the plate. This time, we have the 
seed documents as dynamically generated vertical axes and all dimensions as data items 
(polylines), so that users can brush and identify dimensions satisfying their criteria. For 
example, in Figure 7.12 (b), the polylines are colored based on the normalized dimension 
values in document A; users can observe bundles of yellow polylines (high values in A) 
tend to go down in B and C, while the blue polylines (low values in B) tend to go up in B 
and C. Users can then brush the axes and set up explicit thresholds to select dimensions 
with high values in A but low values in B and C. The resulting dimensions (e.g. D42 and 
D154) can be hypothesized to be associated with the unique semantics of A. 
As a supplementary view, we present aligned bar (box) charts to make pairwise 
comparisons between any pairs of seed documents using their original dimension values. 
Bars representing dimensions are sorted by the values in the first (top) document; bar length 
encodes the exact dimension values; bar color/intensity encodes categorical dimension 
values with a differentiation between positive and negative values. Although the aligned 
bar chart is limited to two documents, it takes advantage of human’s high sensitivity to 
symmetry, 
thus 
significant 
differences 
between 
two 
document 
embeddings 
and 
the 
responsible dimensions can be rapidly identified. Figure 7.12 (c) shows comparisons 
between documents from different clusters (A vs. B, A vs. C) and the same cluster (A vs. 
D). Users can observe a better symmetry exhibited between document A and D which are 
178 
from the same cluster. This would give users a quick sense of the document embeddings 
and their quality, and motivate further explorations. 
Figure 7.12: Exploration of neural dimension semantics – hypothesis generation 
Hypothesis Refinement
. For the second stage of hypothesis refinement, we allow users to 
validate or reject their hypothesis by gaining further insights into the identified dimensions. 
While we have presented visual analytics of dimension behaviors at an overview (Figure 
7.11 (a)) and intermediate (Figure 7.11 (b)(c)) level, users are not yet able to approach a 
complete view of a specific dimension at a detailed level i.e. the exact value distribution 
across documents and clusters. In this part, we utilize the document map presented in 
Figure 7.2, and leverage the compelling position channel and the color/intensity of 
document points to reveal the complete and detailed pattern of a specific dimension. Under 
this notion, document points are colored by their assigned values in the specified dimension: 
positive values are in red, negative values are in blue, and the magnitude of values are 
179 
represented by the categorical color intensity (Figure 7.13). In this sense, we generate a 
special heatmap for a selected dimension, which intuitively reveals the dimension’s value 
distribution across all documents and clusters to human eyes. 
We utilize such a heatmap to approach the global patterns of an identified 
dimension and substantiate a hypothesis on it. As we have keywords and topics indicate 
meanings and semantics of documents, conceptually, a dimension that is highly associated 
with certain semantics is expected to have exclusively high values in the cluster(s) with the 
corresponding (or relevant) topics. Meanwhile, a dimension that is partly associated with 
certain semantics, or is associated with mixed semantics, is expected to have a hybrid value 
distribution across multiple clusters. In Figure 7.13 we present heatmaps for the dimensions 
D42 and D154, which have been above-identified and hypothesized to be associated with 
the semantics of 
ADHD treatment
. As shown in Figure 7.13 (a), with high values precisely 
and exclusively allocated to the dense region of cluster 0 (Topic: 
ADHD, treatment
, etc.) 
on the top-left, users can validate their hypothesis that D42 is associated with this semantics. 
However, to reject or suspend a hypothesis, there can be many different situations. For 
example, a dimension has consistently high values across all clusters, consistently low 
values across all cluster, hybrid values across all clusters, high values in adjacent clusters, 
high values in non-adjacent clusters, and so on. As shown in Figure 7.13 (b), although 
D154 has high values in the top-left cluster, it also has high values in other clusters. This 
would help users reject the original hypothesis. At the same time, such heatmap patterns 
can also enable insights into those dimensions encoding general, higher-level, or mixed 
semantics. For instance, by synthesizing keywords from documents with high values in 
180 
D154 (e.g. red dots in Figure 7.13 (b)), we obtain a topic 
Placebo Group
, which is likely 
to be the semantics of D154. Without visualizations, the diverse situations or patterns won’t 
be easily approachable even with a mathematical or statistical tool. In addition, we also 
allow users to explore the heatmap of a combination of multiple dimensions, which will be 
illustrated in a use case later. 
Figure 7.13: Exploration of neural dimension semantics – hypothesis refinement 
7.5.3 Exploration of Dimension Contributions 
The contribution of different dimensions to the overall performance is another perspective 
to understand the neural dimensions, regarding their saliency and effectiveness. Although 
this issue hasn’t been commonly addressed by related work (Section 7.2.4), we are inspired 
by studies [200], [202] in subspace analysis of high-dimensional data and association 
analysis of multivariate data. Specifically, dimensions in the embedding space can be 
characterized as dominant or conforming dimensions, which can be reflected by data 
patterns 
such 
as 
clusters 
and 
distributions 
in 
the 
space 
[203]. 
Consider 
the 
high 
181 
dimensionality (e.g. n=200) of a neural embedding space, users might need to select 
potentially important or interesting dimensions for subspace analysis. Recall that in Section 
7.5.1, users can speculate the contributions of dimensions based on their behaviors. 
Particularly, dimensions with a high variance across clusters, or encoding more explicit 
semantics, are likely to contribute more to the overall performance, e.g. to capture 
documents with similar semantics and differentiate them from the others. In this section, 
we will take these dimensions to demonstrate subspace analysis and refine the hypothesis 
that the dimensions with higher variances have dominant contributions to the performance. 
Besides taking the overall performance as the reference, users can also customize desired 
performance towards their application goals based on the semantic features (dimensions) 
described in Section 7.5.2. Users can then explore subspaces with respect to the desired 
performance. 
For our approach, we leverage the augmented document map to disclose the visual 
patterns of document distribution and clusters for intuitive comparisons. Under this notion, 
users can select any subset of dimensions from the embedding space and generate a 
document map and corresponding clusters. Users can then visually compare such a 
distribution and clusters to the references, e.g. generated by all dimensions. Cluster topics 
and benchmark documents can be used as guidance to assist in an easier and more 
convenient comparison. For instance, Figure 7.14 (a) presents a document map generated 
with only the top 10 dimensions of the highest variances, and Figure 7.14 (b) presents the 
document map generated by all dimensions as a reference to the overall performance. With 
the top 10 dimensions, 15 out of the 20 benchmark documents with similar semantics 
182 
(
ADHD treatment
) are aggregated into the top-left cluster. According to the reference 
document map generated by all dimensions (e.g. n=200), 19 out of the 20 benchmark 
documents are allocated into the same cluster. It is encouraging to achieve the almost-
comparable performance with only a small portion of dimensions (10/200). This would be 
an indicator that dimensions with high variances are likely to have dominant contributions 
to the overall performance, e.g. capture the similarities among benchmark documents. This 
visual analytics component can be flexibly employed to explore an effective and efficient 
(low dimensionality) subspace that can produce desired performance. 
Figure 7.14: Exploration of neural dimension contributions 
183 
7.5.4 Use Case 
We present a use case on the ADHD dataset to demonstrate the usefulness of our visual 
analytics, 
i.e. 
to 
help 
users 
explore 
neural 
document 
embeddings, 
understand 
the 
underlying embedding space, and benefit from the insights of neural dimensions with 
respect to their behaviors, semantic properties, and contributions. 
Figure 7.15: Use case for neural dimension exploration on the ADHD dataset 
184 
The ADHD dataset consists of 851 clinical trials, among them, 20 are labeled as high 
relevant articles on 
ADHD treatment
by human experts, and are considered benchmark 
documents. In the previous sections, we have presented several demonstrations using this 
dataset. Here we arrange some interesting findings when working on the application goals 
to: 1) retrieve high relevant articles, 2) explore how the ADHD semantics as well as other 
biomedical/clinical concepts are encoded into neural dimensions, and identify some salient 
dimensions of interests, and 3) explore how the identified dimensions contribute to the 
desired performance of clustering benchmark documents. 
First of all, the clusters and topics (Figure 7.15 (a)) generated by our approach (PV 
and t-SNE are used for text analytics) are confirmed to be meaningful and useful by the 
expert in clinical informatics. We then examine dimension behaviors and identify several 
dimensions of interests with the analytics components illustrated in Figure 7.11 and Figure 
7.12. The identified dimensions either have exclusively high values in cluster 0 
ADHD 
Treatment
, or notably high values in most of the benchmark documents. In Figure 7.15 (b), 
we 
present 
complete 
global 
value 
distributions 
(heatmaps) 
for 
5 
of 
the 
identified 
dimensions. As we have explained earlier, D42 is associated with the semantics of 
ADHD 
Treatment
. The other dimensions also exhibit distinguishable values across clusters, 
however, they tend to have high values allocated in two to three clusters, including cluster 
0. To understand the associated semantics of these dimensions, we use dynamic topic 
synthesis on the documents which have high values in a selected dimension (i.e. red 
document points in a heatmap). Interestingly, we identify some hidden semantics that can 
explain the generalizability over multiple clusters. Specifically, D73 is associated with 
Side 
185 
Effect and Placebo
, D157 is associated with 
Disorder Treatment
, D94 is associated with 
Simulant Medication
, and D2 is associated with 
Placebo Group and Depression
. Although 
these semantics are not unique for cluster 0, they are important concepts in abundant 
clinical trials, and are properly captured in the embedding space. We also conduct subspace 
analysis using these 5 dimensions encoding interesting semantics, and generate a new 
document map and clusters. As shown in Figure 7.15 (c), with only 5 dimensions, 17 out 
of the 20 benchmark documents are aggregated into the same cluster (green). The high 
recall demonstrates the contributions of these dimensions and the usefulness of the applied 
semantics. The precision would be further improved by including additional semantics and 
dimensions with a similar feature recognition and selection process. 
7.5 Summary 
In this chapter, we leverage our previously established effective visualizations on article 
representations 
and 
similarities, 
and 
further 
promote 
them 
to 
meet 
the 
real-world 
expectations of scalability, interpretability, and interactivity. Under this notion, we design 
and develop a usable visual analytics system by integrating a series of interactive features 
and reinforced visual patterns. In particular, with a multilevel structure and customizable 
hierarchical clustering, our visualizations have gained improved scalability to largescale 
datasets, meeting the visual budgets while satisfying specific analytics tasks. Besides, the 
reinforced visual patterns and other interactive features, including dynamic topic synthesis 
and hierarchical navigation, also expedite human cognition and analytic reasoning, thus 
make 
our 
visualizations 
more 
interpretable 
and 
usable 
for 
human 
researchers. 
We 
formulate a sequence of analytics tasks and demonstrate a use case on the benchmark 
186 
dataset, and validate our hypothesis that the visual analytics system is able to provide end-
users (e.g. clinical and biomedical personnel) with increased capability and confidence in 
conducting literature retrieval as well as text corpus exploration. 
We also extend our visual analytics approaches to mutually support text analytics 
and benefit application designers (e.g. computer engineers and informaticians), especially 
for the neural embeddings which have achieved superior performance but are usually used 
as a black-box. In this sense, we utilize visual analytics to explore neural document 
embeddings and gain insights into the underlying embedding space, with respect to the 
behaviors, semantics, and contributions of neural dimensions. We formulate domain-
specific goals and develop a series of visual analytics components that bridge neural 
dimensions with the associated performance in information retrieval applications. For a 
guided exploration, we leverage established document (article) maps with adjustable 
clusters and topics (semantics), and visualize the overall behaviors of neural dimensions 
across documents and clusters. Under this notion, users are allowed to identify analytic 
facets of importance or of interests, and explore the semantics and contributions associated 
with salient dimensions. Throughout the process, we provide interactive visual components 
for users to recognize and discover dimensions (features) that are useful to their application 
domains and the desired results. With demonstrated use cases, designers and evaluators 
from application domains can benefit to recognize and obtain insights into effective 
embeddings, exploit the embeddings in further analytics (e.g. feature selection), and 
promote more favorable performance in an advisable manner. 
Highlight of Contribution 
187 
We develop a visual analytics system which exploits text analytics results (e.g. neural 
document embeddings) and integrates interactive features to assist in literature retrieval 
and text corpus exploration in general, with respect to the real-world expectations of high 
scalability, interpretability, and interactivity, benefitting end-users (e.g. biomedical and 
clinical personnel). We also extend this system to implement visual exploration of neural 
document embeddings to understand how the performance is achieved and gain insights 
into the semantic properties, benefitting application designers (e.g. computer engineers and 
informaticians) in promoting more favorable performance in an advisable manner. 
This work has been submitted to IEEE VAST 2018: 
Xiaonan Ji, Han-Wei Shen, Raghu Machiraju, Alan Ritter, Po-Yin Yen. Visual Exploration 
of Neural Document Embeddings: What are Behind the Neural Dimensions? IEEE VAST 
2018.
188 
Chapter 8. Conclusion 
8.1 Conclusion 
In this dissertation, we focus on a domain problem of biomedical literature retrieval, which 
has significant impacts in Evidence-based Medicine and clinical decision making, and is 
expecting for high effectiveness and efficiency with respect to the increasing volume of 
information resources and the critical information needs. To approach a comprehensive 
solution, we present a framework of biomedical literature mining by integrating and 
extending state-of-the-art techniques of text analytics and visual analytics. This framework 
is demonstrated to bridge the gaps among unstructured and extensive literature, structured 
and high-quality representations, and advantageous exploitation and exploration in real-
world applications. Under this notion, this dissertation makes two primary contributions: 
Firstly, for the biomedical and clinical domain, this integrated framework of 
biomedical literature mining can perform as an effective, affordable, and generalizable 
solution to facilitate the significant problem of biomedical literature retrieval. It can benefit 
widespread applications in Evidence-based Medicine, knowledge synthetization, and 
clinical decision making, thus promote quality healthcare and its delivery. 
Secondly, for the computer science and information domain, we extend state-of-
the-art techniques of text analytics and visual analytics, to promote their effectiveness and 
efficiency in resolving real-world problems with respect to: 1) scientific literature, a special 
189 
family of text documents, 2) domain knowledge such as UMLS ontologies, 3) advanced 
computational power such as neural embedding and task specification, 4) effective 
visualization schemes of text corpus, 5) human cognition, interaction, and exploration, 6) 
visual exploration and feedback to text analytics. Figure 8.1 presents a conclusive overview 
of this dissertation. 
Figure 8.1: A conclusive dissertation overview 
8.2 Contributions 
Here we present a list of the more specific contributions, which are in correspondence with 
the previous chapters of this dissertation. 
190 
•
We 
investigate 
the 
effectiveness 
(usefulness) 
of 
different 
article 
elements 
from 
biomedical 
articles 
(scientific 
publications), 
including 
both 
single 
elements 
and 
combinations of multiple elements, in indicating article relevancies in information 
retrieval. 
We 
also 
develop 
an 
active 
learning 
model 
for 
interactive 
article 
recommendation to employ and evaluate the lexical article features, representations, 
and similarities. Together, we demonstrate an effective, efficient, and generalizable 
solution using lexical article features for biomedical literature retrieval. 
•
We develop a novel semantic concept modelling process (e.g. a pipelined process with 
three stages) to derive optimized and enriched concepts as semantic features to 
effectively represent a biomedical article, by levering relevant concepts and concept 
relations 
encoded 
in 
UMLS 
ontologies. 
This 
ontology-based 
semantic 
approach 
demonstrates to outperform not only lexical approaches but also some existing corpus-
based semantic approaches
.
•
We advance an effective and efficient neural embedding model to learn semantic 
representations of biomedical articles. The Multi-Task Paragraph Vector (PV-MT) 
model advances the general-purpose Paragraph Vector (PV) model with the task-
specific interest, which is realized via minimal supervision and multi-task learning. 
This corpus-based semantic approach demonstrates to outperform lexical baselines and 
can be a strong alternative to the ontology-based semantic approach. 
•
We 
demonstrate 
the 
usefulness 
of 
visualizations 
of 
article 
relationships 
(representations) in assisting with the information retrieval towards biomedical articles. 
Specifically, for the concept proof using a visualized article similarity network, the 
191 
revealed 2D visual patterns, including the centralization (aggregation) and clustering 
of high relevant articles, can facilitate the identification of high relevant articles by 
leveraging human cognition and analytic reasoning. 
•
We 
promote 
effective 
visualizations 
of 
article 
collections 
(text 
corpuses) 
for 
biomedical 
literature 
retrieval 
(information 
retrieval) 
with 
multiple 
visualization 
approaches, including sparsified article networks and article maps. We develop a novel 
and configurable article network sparsification scheme which is shown to preserve 
important 
article 
relationships 
and 
result 
in 
effective 
visual 
patterns, 
e.g. 
the 
distribution and compelling clustering of high relevant articles. 
•
We develop a visual analytics system which exploits text analytics results (e.g. neural 
document embeddings) and integrates interactive features to assist in literature retrieval 
and text corpus exploration in general, with respect to the real-world expectations of 
high 
scalability, 
interpretability, 
and 
interactivity, 
benefitting 
end-users 
(e.g. 
biomedical and clinical personnel). We also extend this system to implement visual 
exploration of neural document embeddings to understand how the performance is 
achieved 
and 
gain 
insights 
into 
the 
semantic 
properties, 
benefitting 
application 
designers (e.g. computer engineers and informaticians) in promoting more favorable 
performance in an advisable manner. 
8.3 Discussions and Future Works 
Based on the work we have conducted so far, we could extend it to several directions in 
further work, and we believe that will strengthen biomedical literature mining in both its 
breadth and depth. The first possible direction is to enable article annotations and integrate 
192 
approachable human supervisions to iteratively refine the text analytics models towards a 
desired direction. Secondly, we could include more article elements or aspects from 
biomedical literature to derive additional features, which can also contribute to the value 
of article relevancy in information retrieval. Meanwhile, we can refine some noisy article 
elements with NLP approaches, such as extracting entities or sentences of interests. Thirdly, 
we could strengthen our semantic text analytics by leveraging additional knowledge 
resources or computation powers. In particular, deep neural networks e.g. CNNs and RNNs 
(LISM) will be able to encode more complicated linguistic aspects in generating semantic 
representations. For doing so, we could further improve the effectiveness of downstream 
applications of information retrieval and information visualization, especially when diverse 
types of semantics are expected. Fourthly, we can release our visual analytics system for 
literature review and document exploration to the public and receive iterative and long-
term feedback from public users, including not only clinical and biomedical personnel, but 
also general academic researchers. This could help us continually improve our visual 
analytics system to a state-of-the-art level. 
8.3.1 Article Annotations 
In this dissertation, we promote efficient and generalizable text analytics to capture the 
underlying meanings of biomedical articles, and we have been mainly focusing on 
unsupervised 
approaches 
leveraging 
advantageous 
knowledge 
and 
computational 
resources. While full-supervisors on article relevancies are considered expensive and with 
low generalization, annotations from humans can be a preferable tradeoff, which bridges 
the task-specific interests in an effective and approachable manner. In particular, humans 
193 
can annotate recognized articles of importance or interests, and highlight keywords (key 
terms and sentences) that can best indicate their interests. With such annotated keywords, 
an analytics model can be fine-tuned towards the task-specific interests. 
Therefore, one future direction is to enable humans to annotate articles and 
highlight keywords of interests before or during a model tuning process. To make the 
annotations more effective and approachable, we could provide an initial list of keywords 
which are automatically identified from an information need (such as key questions), users 
are then allowed to modify the keyword list (add, change, or delete keywords) on the fly 
in an advisable and convenient way. In this sense, humans and their supervisions are 
closely kept in-the-loop and an analytics model can be iteratively refined. For example, our 
Multi-Task Paragraph Vector (PV-MT) model for semantic text analytics can be further 
retrofitted with such annotations to capture continuously refined task interests. Article 
annotations are also beneficial to visual analytics models to advance the interactive features. 
8.3.2 Additional Article Elements 
In our text and visual analytics, we mainly utilize the title and abstract of biomedical 
articles, which have been considered the most concise and informative article elements 
(aspects) in delivering the essential article meanings. We also use authors, Medical Subject 
Headings (keywords), and publication types as they have also shown to contribute to the 
article relevancy in biomedical literature retrieval. Considering the special property and 
format of scientific literature, one future direction is to include additional article elements, 
such as the citation (reference) information and full-text content, in our literature mining. 
194 
Citations
. Citations of scientific publications are typically used to connect related 
publications and identify corresponding research topics and groups. With the assumption 
that articles with a 
citing
, 
being-cited, 
or
co-cited
relationship are likely to have similar 
topics and contribute to similar information needs, integrating citation information can 
increase the possibility of identifying relevant articles even they don’t share noticeable 
similarities in other article elements. However, this can also introduce many noises as 
articles with a 
citing
, 
being-cited, 
or
co-cited
relationship are not necessarily similar to 
each other, or, are similar in limited facets which are out of an information retrieval scope. 
Therefore, instead of taking citations in an all-inclusive manner, we should screen an 
article’s reference list and identify truly relevant citations based on the information need. 
This can also inspire future studies in conducting text analytics and information extraction 
on the reference list of scientific publications. 
Full-Texts
. Comparing to article titles and abstracts, article full-texts contain much 
more abundant information written in natural language, for example, the introduction, 
background, methodologies, results, conclusion, etc. By including article full-texts in our 
literature mining, we can have enriched corpus to identify plentiful features and derive 
more comprehensive article meanings. Under this notion, article full-texts are specifically 
advantageous when the abstract is of low-quality or unable to cover all essential meanings 
of the article. Besides, the all-embracing full-texts are also necessary when an information 
need doesn’t directly connect to the essential article meanings, instead, some auxiliary 
details are of higher interests, e.g., the protocol applied to placebo groups across a wide 
range of diseases or treatments. However, the abundant information, diverse aspects, 
195 
complicated structure, and narrative nature of article full-texts inevitably introduce many 
noises to text analytics. For example, many clinical trials have extensive descriptions of 
background information or the patient recruitment procedure, which wouldn’t be directly 
related to an information need such as drug effectiveness review. Besides, the larger 
volume of article full-texts also places challenges on efficient task accomplishment. Future 
work will need to address the extraction of important parts (components) from extensive 
full-texts before conducting further analytics for article representations and relationships. 
8.3.3 Information Extraction 
Article titles, abstracts, as well as full-texts (as we described in the previous Section 8.3.2) 
are written in natural language and used to derive text article features that indicate article 
meanings and relevancies with respect to an information need. Because of the narrative 
and multifaceted nature of article abstracts and full-texts, different terms and sentences 
across the texts contribute to the essential article meanings and relevancies differently. To 
further improve the effectiveness and accuracy of article features and the resulting article 
representations, one further direction is to conduct information extraction to target and 
utilize important terms and sentences which have salient or positive contributions to the 
information needs. 
As a motivating example, systematic reviews in the biomedical and clinical 
domains have been targeting the identification of high relevant articles for knowledge 
synthetization and decision making, and their protocols (information needs) are typically 
arranged in the PICO format, representing patient, intervention (treatment), comparison, 
and output. Therefore, the focal parts of articles are terms or sentences related or contribute 
196 
to 
the 
sense 
of 
patient 
population
, 
symptom
, 
disease
, 
treatment
, 
side 
effect
, 
and 
effectiveness/efficiency
. The identification of these terms/sentences not only reduces the 
noises lie in extensive article features, and but only helps organize the salient features in a 
structured way e.g. with a template or table. In this sense, the effectiveness of article 
representations and relationships can be further enhanced towards the information needs. 
The information extraction can occur at the entity, relation, or sentence level. Future 
work can implement named entity recognition (NER) to identify the mentions of patients, 
disease, symptom, etc.; and relation extraction to identify some interesting triplets such as 
(treatment, treat, disease), (disease, cause, symptom), etc. We can also refer to the UMLS 
Metathesaurus for the standard semantic relationships (terminologies) commonly utilized 
in the biomedical and clinical domain. Sentence level information extraction can perform 
as an extension to entity/relation extraction, with the advantages of including enriched 
context information to derive more comprehensive article representations and relationships. 
To ease the limitation and expensiveness of training data, we can utilize weakly 
supervised 
(semi-supervised) 
machine 
learning 
approach 
for 
information 
extraction. 
Specifically, we can generate a coarse and large training dataset by leveraging gold-
standard knowledge to automatically annotate sentences in existing corpus (e.g. PubMed 
Snapshots, google news, Wikipedia articles, tweets, etc.). With additional NLP syntactic 
annotations on all training and testing (task) sentences, we can then utilize sequence 
modelling methods, such as conditional random fields (CRF), to assign labels to terms, and 
obtain corresponding sentence labels at a post-stage. 
197 
While information extraction has achieved superior performance and impressive 
success in web contents and social media, it is of interests to apply and extend the state-of-
the-art techniques to the biomedical and clinical domain. This will assist in many of the 
significant and real-world issues, including Evidence-based Medicine, precise analytics of 
electronical health records and clinical notes, and so on. The future work can also be 
supported 
by 
the 
advantageous 
biomedical 
knowledge 
repository, 
the 
UMLS 
Metathesaurus. 
In 
a 
mutually 
beneficial 
manner, 
the 
information 
extraction 
upon 
biomedical texts converts unstructured extensive information to structured high-quality 
information which would further enrich the knowledge repository, and the knowledge 
repository provides valuable guidance (e.g. gold-standard semantics and relations) to 
facilitate effective information extraction, 
8.2.4 Deep Neural Document Embeddings 
Document (article) representation is an important topic in text analytics and language 
modeling, 
expecting 
to 
encode 
essential 
features 
and 
meanings 
of 
documents 
in 
a 
structured and machine understandable way. Neural document embedding, as a successful 
extension to neural word embedding has shown to leverage superior computational 
capability of neural networks, and generate effective document representations in dense 
and fixed-length feature vectors. In this dissertation, we have utilized the well-known 
Paragraph Vector (PV) model, which is a 2-layer neural network, to generate semantic 
article representations. As a general-purpose model trained in an unsupervised manner, the 
PV model has demonstrated effective performance in our downstream application of 
biomedical information retrieval. While the PV model shows to capture the semantics of 
198 
local 
context 
(e.g. 
n-gram) 
and 
benefit 
widespread 
information 
retrieval 
scenarios 
especially for long texts, it is unable to handle more sophisticated linguistic aspects, such 
long-term dependencies or sentence structures, due to its shallow architecture. 
Therefore, one future direction is to utilize deep neural networks to learn semantic 
article representations and account for more sophisticated linguistic aspects and more 
comprehensive semantics in the embeddings. In particular, RNNs with LSTM have shown 
to encode sequential information of sentences and thus capture the long-term compositions. 
While RNN-LSTM has been commonly used in sentence (sequence) modeling, as well as 
downstream 
applications 
of 
sentiment 
analysis 
and 
machine 
translation, 
it 
also 
demonstrates to generate universal sentence embeddings that are applicable to many 
scenarios. In our future work, we can extend RNN from the conventional sentence 
embedding to the more challenging document embedding, with respect to documents 
consisting of multiple sentences and with more complicated structures. 
Meanwhile, CNNs have been commonly used in pattern recognition and image 
classification. In the interdisciplinary of NLP, CNNs have demonstrated to capture position 
invariant 
salient 
features 
(e.g. 
n-grams), 
and 
result 
in 
favorable 
performance 
in 
classification tasks. In our future work, we can utilize CNNs to highlight and encode the 
salient features which dominantly related to the information need of biomedical literature 
retrieval, such as the patient, disease, treatment, etc. 
However, deep neural networks are generally well-trained by supervised tasks. 
How to ease the limitation and expensiveness of training dataset in biomedical literature 
199 
retrieval? This will also be an important issue to address in future work, as the task on 
which embeddings are trained can significantly impact the quality. 
Besides, we are also interested in integrating attention mechanism to the neural 
models. With the heuristics that not every term (component) is equally important for a task, 
an attention mechanism can help emphasize the task specification (task) and highlight the 
important and relevant terms (components), thus produce more favorable embeddings for 
our information retrieval tasks. There have been different attention-based schemes imposed 
on neural models, including weighting, alignment and matching, additional components 
(neurons or dimensions) for attentions/aspects, etc. With the intuition of emphasizing 
attentions related terms, we are motivated to integrate the aforementioned information 
extraction with neural embeddings to achieve the attention-based goals. 
8.2.5 UMLS Metathesaurus with Neural Embedding 
In this dissertation, we have leveraged the gold-standard domain knowledge maintained in 
the UMLS Metathesaurus to support our ontology-based semantic analytics of biomedical 
articles. Specifically, we utilized two ontologies, SNOMED-CT and MeSH, and derived 
effective concept relations as our background semantics. In this sense, the performance of 
our ontology-based approach largely relies on the coverage and quality of the applied 
ontologies. While SNOMED-CT has been recognized as one of the most useful and 
comprehensive ontologies in clinical settings, and MeSH is one of the most widely used 
ontologies in biomedical information indexing and retrieval, one future direction is to 
include additional ontologies to capture additional semantic that might be out of the scope 
of SNOMED-CT and MeSH. This also aligns with our finding that a joint utilization (e.g. 
200 
SNOMED-CT + MeSH) can result in more favorable performance than using a single 
ontology alone (e.g. SNOMED-CT or MeSH). In fact, UMLS Metathesaurus has integrated 
over 100 ontologies and vocabularies, it is of interests to dive into these resources and 
identify the ones that can benefit biomedical literature retrieval, especially for Evidence-
based Medicine. 
Besides taking advantages of the established knowledge resources in UMLS, 
another future direction is to help promote utilizations of the valuable resource of UMLS 
concepts. UMLS concepts, as the standard and unified terminologies in the biomedical 
domain, have been widely used to index and encode biomedical literature, clinical notes, 
patient 
records, 
etc. 
Existing 
researches 
have 
also 
attempted 
to 
capture 
semantic 
similarities among UMLS concepts to promote the effectiveness (accuracy) of downstream 
applications. In order to generate more usable semantic representations of UMLS concepts, 
in our future work, we can utilize neural embedding upon the directed acyclic graphs (DAG) 
of UMLS ontologies, which encode gold-standard concept relationships (e.g. hypernym, 
hyponym, and sibling) as the semantic resources. Under this notion, we can conduct 
truncated random walks on the DAGs and covert graphic concept relations (e.g. nodes and 
links) into special 
context information
that can perform as inputs to neural embedding 
models. The generated semantic representations of UMLS concepts would be readily 
applicable to any downstream models which expect vector representations, thus promote 
the utilization of UMLS concepts. 
201 
8.2.6 Long-term Feedback of Visual Analytics System 
The current version of our visual analytics system has been used within our research group, 
as well as some students and researchers from our institutes and collaborative institutes. 
One future direction is to release this system to the public. For doing so, we can receive 
iterative and long-term feedback from users, and analyze system logs of some eligible user 
actions. This will help us obtain insights into users' expectations, identify limitations, and 
make more advisable improvements for the visual analytics system. We consider the 
improvements can be made from both the backend computations and frontend interactions. 
8.2.7 Visual Analytics of Dynamic Neural Embeddings 
We have implemented visual exploration of semantic document (article) representations 
learned 
with 
the 
Paragraph 
Vector 
(PV) 
model, 
and 
the 
document 
representations 
(embeddings) are the final outputs of the PV model after the training process is completed. 
Future work will include explorations of embeddings produced by other models or with 
different 
parameters 
e.g. 
the 
windows 
size 
and 
embedding 
size; 
explorations 
of 
intermediate embeddings and their dynamics during a training process; collaboration with 
neural model designers and design visual analytics that can reveal the limitation and advice 
future design and improvement of neural embedding models. 
202 
Bibliography 
[1] 
R. Khare, R. Leaman, and Z. Lu, “Accessing biomedical literature in the current 
information landscape.,” 
Methods Mol. Biol.
, vol. 1159, pp. 11–31, 2014. 
[2] 
M. B. McClellan, 
Evidence-based medicine and the changing nature of health care: 
2007 IOM annual meeting summary
. Washington, D.C.: National Academies Press, 
2008. 
[3] 
W. Hersh, 
Information Retrieval: A Health and Biomedical Perspective
. Springer 
Science & Business Media, 2008. 
[4] 
A. Manconi, E. Vargiu, G. Armano, and L. Milanesi, “Literature Retrieval and 
Mining in Bioinformatics: State of the Art and Challenges,” 
Adv. Bioinformatics
, 
vol. 2012, pp. 1–10, 2012. 
[5] 
W. 
Hersh, 
“Biomedical 
Information 
Retrieval,” 
2009. 
[Online]. 
Available: 
https://dmice.ohsu.edu/hersh/BiomedicalIR.pdf. [Accessed: 02-Jan-2018]. 
[6] 
P. P. Glasziou, “Information overload: what’s behind it, what’s beyond it?,” 
Med. J. 
Aust.
, vol. 189, no. 2, pp. 84–5, Jul. 2008. 
[7] 
K. Canese and S. Weis, “PubMed: The Bibliographic Database,” 2013. 
[8] 
J. H. Elliott 
et al.
, “Living Systematic Reviews: An Emerging Opportunity to 
Narrow the Evidence-Practice Gap,” 
PLoS Med.
, vol. 11, no. 2, p. e1001603, Feb. 
2014. 
[9] 
G. Zuccon and B. Koopman, “SIGIR 2017 Tutorial on Health Search (HS2017): : A 
Full-day from Consumers to Clinicians,” in 
Proceedings of the 40th International 
ACM SIGIR Conference on Research and Development in Information Retrieval - 
SIGIR ’17
, 2017, pp. 1415–1418. 
[10] 
I. Shemilt 
et al.
, “Pinpointing needles in giant haystacks: use of text mining to reduce 
impractical screening workload in extremely large scoping reviews,” 
Res. Synth. 
Methods
, vol. 5, no. 1, pp. 31–49, Mar. 2014. 
[11] 
K. L. Sumathy and M. Chidambaram, “Text Mining: Concepts, Applications, Tools 
and Issues – An Overview,” 
Int. J. Comput. Appl.
, vol. 80, no. 4, pp. 975–8887, 
2013. 
[12] 
V. 
K. 
A 
and 
G. 
Aghila, 
“Text 
Mining 
Process, 
Techniques 
and 
Tools : 
an 
Overview,” 
Int. J. Inf. Technol. Knowl. Manag.
, vol. 2, no. 2, pp. 613–622, 2010. 
[13] 
C.-C. Huang and Z. Lu, “Community challenges in biomedical text mining over 10 
years: success, failure and the future,” 
Brief. Bioinform.
, vol. 17, no. 1, pp. 132–144, 
Jan. 2016. 
[14] 
G. Zhong, L.-N. Wang, X. Ling, and J. Dong, “An overview on data representation 
learning: From traditional feature learning to recent deep learning,” 
J. Financ. Data 
Sci.
, vol. 2, no. 4, pp. 265–278, Dec. 2016. 
[15] 
Y. Bengio, A. Courville, and P. Vincent, “Representation Learning: A Review and 
New Perspectives,” 
IEEE Trans. Pattern Anal. Mach. Intell.
, vol. 35, no. 8, pp. 
1798–1828, Aug. 2013. 
[16] 
B. Mitra and N. Craswell, “Neural Models for Information Retrieval,” 
arXiv Prepr. 
arXiv
, vol. 1705, no. 1509, May 2017. 
203 
[17] 
T. Munzner, 
Visualization Analysis and Design
. CRC Press, 2014. 
[18] 
C. Ware, 
Visual thinking for design
. Morgan Kaufmann, 2008. 
[19] 
K. W. Boyack, B. N. Wylie, and G. S. Davidson, “Information Visualization, 
Human-Computer Interaction, and Cognitive Psychology: Domain Visualizations,” 
in 
Visual Interfaces to Digital Libraries
, Springer, Berlin, Heidelberg, 2002, pp. 
145–158. 
[20] 
S. Liu, W. Cui, Y. Wu, and M. Liu, “A survey on information visualization: recent 
advances and challenges,” 
Vis. Comput.
, vol. 30, no. 12, pp. 1373–1393, Dec. 2014. 
[21] 
Z. Jordan, Z. Munn, E. Aromataris, and C. Lockwood, “Now that we’re here, where 
are we? The JBI approach to evidence-based healthcare 20 years on,” 
Int. J. Evid. 
Based. Healthc.
, vol. 13, no. 3, pp. 117–120, Sep. 2015. 
[22] 
Institute 
of 
Medicine 
(US) 
Roundtable 
on 
Evidence-Based, 
Leadership 
Commitments 
to 
Improve 
Value 
in 
Health 
Care: 
Finding 
Common 
Ground
. 
Washington, D.C.: National Academies Press, 2009. 
[23] 
S.-C. Chow and J. Liu, 
Design and analysis of clinical trials : concept and 
methodologies
. Wiley-Interscience, 2004. 
[24] 
“MEDLINE/PubMed Data Element (Field) Descriptions.” [Online]. Available: 
http://www.nlm.nih.gov/bsd/mms/medlineelements.html. 
[25] 
Z. Lu, “PubMed and beyond: a survey of web tools for searching biomedical 
literature,” 
Database
, vol. 2011, no. 0, p. baq036-baq036, Jan. 2011. 
[26] 
“Drug Effectiveness Review Project (DERP) Systematic Drug Class Review Gold 
Standard Data.” [Online]. Available: http://skynet.ohsu.edu/~cohenaa/systematic-
drug-class-review-data.html. 
[27] 
“Cochrane 
Database 
of 
Systematic 
Reviews 
(CDSR).” 
[Online]. 
Available: 
http://www.cochranelibrary.com/cochrane-database-of-systematic-reviews/. 
[28] 
“PICO 
Framework.” 
[Online]. 
Available: 
https://www.ncbi.nlm.nih.gov/pubmedhealth/PMHT0029906/. [Accessed: 02-Jan-
2018]. 
[29] 
A. M. Cohen, W. R. Hersh, K. Peterson, and P.-Y. Yen, “Reducing workload in 
systematic review preparation using automated citation classification,” 
J. Am. Med. 
Inform. Assoc.
, vol. 13, no. 2, pp. 206–19, 2006. 
[30] 
A. Liberati 
et al.
, “The PRISMA Statement for Reporting Systematic Reviews and 
Meta-Analyses of Studies That Evaluate Health Care Interventions: Explanation and 
Elaboration,” 
PLoS Med.
, vol. 6, no. 7, p. e1000100, Jul. 2009. 
[31] 
L. Ng 
et al.
, “Title and Abstract Screening and Evaluation in Systematic Reviews 
(TASER): a pilot randomised controlled trial of title and abstract screening by 
medical students.,” 
Syst. Rev.
, vol. 3, p. 121, Oct. 2014. 
[32] 
A. M. Cohen, “Optimizing Feature Representation for Automated Systematic 
Review Work Prioritization,” in 
AMIA Annu Symp Proc
, 2008. 
[33] 
S. Matwin, A. Kouznetsov, D. Inkpen, O. Frunza, and P. O’Blenis, “A new 
algorithm for reducing the workload of experts in performing systematic reviews,” 
J. Am. Med. Informatics Assoc.
, vol. 17, no. 4, pp. 446–453, Jul. 2010. 
[34] 
B. C. Wallace, T. A. Trikalinos, J. Lau, C. Brodley, and C. H. Schmid, “Semi-
automated 
screening 
of 
biomedical 
citations 
for 
systematic 
reviews,” 
BMC 
204 
Bioinformatics
, vol. 11, no. 1, p. 55, 2010. 
[35] 
B. C. Wallace, K. Small, C. E. Brodley, J. Lau, and T. A. Trikalinos, “Deploying an 
Interactive Machine Learning System in an Evidence-Based Practice Center.” 
[36] 
S. Jonnalagadda and D. Petitti, “A new iterative method to reduce workload in 
systematic review process,” 
Int. J. Comput. Biol. Drug Des.
, vol. 6, no. 1/2, p. 5, 
2013. 
[37] 
M. Khabsa, A. Elmagarmid, I. Ilyas, H. Hammady, and M. Ouzzani, “Learning to 
identify relevant studies for systematic reviews using random forest and external 
information,” 
Mach. Learn.
, vol. 102, no. 3, pp. 465–482, Mar. 2016. 
[38] 
M. Miwa, J. Thomas, A. O’Mara-Eves, and S. Ananiadou, “Reducing systematic 
review workload through certainty-based screening,” 
J. Biomed. Inform.
, vol. 51, 
pp. 242–253, Oct. 2014. 
[39] 
M. Grobelnik, D. Mladenic, and N. Milic-Frayling, “Text mining as integration of 
several related research areas: report on KDD’s workshop on text mining 2000,” 
ACM SIGKDD Explor. Newsl.
, vol. 2, no. 2, pp. 99–102, Dec. 2000. 
[40] 
S. Grimes, “A Brief History of Text Analytics.” [Online]. Available: http://www.b-
eye-network.com/view/6311. [Accessed: 01-Feb-2018]. 
[41] 
M. Inzalkar and J. Sharma, “A Survey on Text Mining-techniques and application,” 
Int. J. Res. Sci. Eng.
, vol. 24, pp. 1–14, 2015. 
[42] 
A. Holzinger, J. Schantl, M. Schroettner, C. Seifert, and K. Verspoor, “Biomedical 
Text Mining: State-of-the-Art, Open Problems and Future Challenges,” Springer 
Berlin Heidelberg, 2014, pp. 271–300. 
[43] 
G. H. Gonzalez, T. Tahsin, B. C. Goodale, A. C. Greene, and C. S. Greene, “Recent 
Advances and Emerging Applications in Text and Data Mining for Biomedical 
Discovery.,” 
Brief. Bioinform.
, vol. 17, no. 1, pp. 33–42, Jan. 2016. 
[44] 
R. Rodriguez-Esteban, A. Laegreid, J. Komorowski, E. Hovig, and J. Wang, 
“Biomedical Text Mining and Its Applications,” 
PLoS Comput. Biol.
, vol. 5, no. 12, 
p. e1000597, Dec. 2009. 
[45] 
V. D. Kumar, 
Biomedical literature mining
. Humana Press, 2016. 
[46] 
A. O’Mara-Eves, J. Thomas, J. McNaught, M. Miwa, and S. Ananiadou, “Using text 
mining for study identification in systematic reviews: a systematic review of current 
approaches,” 
Syst. Rev.
, vol. 4, no. 1, p. 5, Dec. 2015. 
[47] 
R. Lengler and M. J. Eppler, “Towards A Periodic Table of Visualization Methods 
for Management The Realm of Visualization Methods,” in 
IASTED Proceedings of 
the Conference on Graphics and Visualization in Engineering (GVE 2007)
, 2007. 
[48] 
J. Leskovec and R. Sosic, “SNAP: A General Purpose Network Analysis and Graph 
Mining Library,” 
ACM Trans. Intell. Syst. Technol.
, vol. 8, no. 1, Jun. 2016. 
[49] 
R. E. Patterson 
et al.
, “A human cognition framework for information visualization,” 
Comput. Graph.
, vol. 42, pp. 42–58, Aug. 2014. 
[50] 
J. L. Cybulski, S. Keller, L. Nguyen, and D. Saundage, “Creative problem solving 
in digital space using visual analytics,” 
Comput. Human Behav.
, vol. 42, pp. 20–35, 
Jan. 2015. 
[51] 
A. 
Holzinger 
et 
al.
, 
“Knowledge 
Discovery 
and 
interactive 
Data 
Mining 
in 
Bioinformatics - State-of-the-Art, future challenges and research directions,” 
BMC 
205 
Bioinformatics
, vol. 15, no. Suppl 6, p. I1, 2014. 
[52] 
I. J. Marshall, J. Kuiper, and B. C. Wallace, “RobotReviewer: evaluation of a system 
for automatically assessing bias in clinical trials,” 
J. Am. Med. Informatics Assoc.
, 
vol. 23, no. 1, pp. 193–201, Jan. 2016. 
[53] 
S. Kim, D. Martinez, L. Cavedon, and L. Yencken, “Automatic classification of 
sentences to support Evidence Based Medicine,” 
BMC Bioinformatics
, vol. 12, no. 
Suppl 2, p. S5, 2011. 
[54] 
C. Belter, “Visualizing Networks of Scientific Research,” 
Online-Medford
, vol. 36, 
no. 3, p. 14, 2012. 
[55] 
N. J. van Eck and L. Waltman, “Visualizing Bibliometric Networks,” in 
Measuring 
Scholarly Impact
, Cham: Springer International Publishing, 2014, pp. 285–320. 
[56] 
“MEDLINE®/PubMed® 
Resources 
Guide.” 
[Online]. 
Available: 
https://www.nlm.nih.gov/bsd/pmresources.html. [Accessed: 02-Jan-2018]. 
[57] 
S. Kim and J. Choi, “Improving the Performance of Text Categorization Models 
used for the Selection of High Quality Articles,” 
Healthc. Inform. Res.
, vol. 18, no. 
1, p. 18, Mar. 2012. 
[58] 
I. K. Dhammi and S. Kumar, “Medical subject headings (MeSH) terms.,” 
Indian J. 
Orthop.
, vol. 48, no. 5, pp. 443–4, Sep. 2014. 
[59] 
H. J. Lowe and G. O. Barnett, “Understanding and using the medical subject 
headings (MeSH) vocabulary to perform literature searches.,” 
JAMA
, vol. 271, no. 
14, pp. 1103–8, Apr. 1994. 
[60] 
C. A. Martins, M. C. Monard, and E. T. Matsubara, “Reducing the Dimensionality 
of Bag-of-Words Text Representation Used by Learning Algorithms,” in 
Proc of 
3rd IASTED International Conference on Artificial Intelligence and Applications
, 
2003, pp. 228–233. 
[61] 
“PubMed 
Stopwords 
List.” 
[Online]. 
Available: 
https://www.ncbi.nlm.nih.gov/books/NBK3827/table/pubmedhelp.T.stopwords/. 
[Accessed: 02-Jan-2018]. 
[62] 
M. F. Porter, “An algorithm for suffix stripping,” 
Program
, vol. 14, no. 3, pp. 130–
137, 1980. 
[63] 
M. Ouzzani, H. Hammady, Z. Fedorowicz, and A. Elmagarmid, “Rayyan—a web 
and mobile app for systematic reviews,” 
Syst. Rev.
, vol. 5, no. 1, p. 210, Dec. 2016. 
[64] 
I. Ruthven and M. Lalmas, “A survey on the use of relevance feedback for 
information access systems,” 
Knowl. Eng. Rev.
, vol. 18, no. 2, pp. 95–145, 2003. 
[65] 
B. Settles, “Active Learning Literature Survey,” 
Comput. Sci. Tech. Rep.
, 2010. 
[66] 
F. Olsson, “A literature survey of active machine learning in the context of natural 
language processing,” 2009. 
[67] 
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and J. F. Hurdle, “Extracting 
Information from Textual Documents in the Electronic Health Record: A Review of 
Recent Research,” 
IMIA Yearb. Med. Informatics Methods Inf Med
, vol. 47, no. 1, 
pp. 128–44, 2008. 
[68] 
O. Bodenreider, “The Unified Medical Language System (UMLS): integrating 
biomedical terminology.,” 
Nucleic Acids Res.
, vol. 32, no. Database issue, pp. D267-
70, Jan. 2004. 
206 
[69] 
K. Donnelly, “SNOMED-CT: The advanced terminology and coding system for 
eHealth.,” 
Stud. Health Technol. Inform.
, vol. 121, pp. 279–90, 2006. 
[70] 
A. R. Aronson, “Effective mapping of biomedical text to the UMLS Metathesaurus: 
the MetaMap program.,” in 
Proceedings of the AMIA Symposium
, 2001, pp. 17–21. 
[71] 
A. R. Aronson and F.-M. Lang, “An overview of MetaMap: historical perspective 
and recent advances,” 
J. Am. Med. Informatics Assoc.
, vol. 17, no. 3, pp. 229–236, 
May 2010. 
[72] 
“MetaMap 
- 
A 
Tool 
For 
Recognizing 
UMLS 
Concepts 
in 
Text.” 
[Online]. 
Available: https://metamap.nlm.nih.gov/. [Accessed: 02-Jan-2018]. 
[73] 
M. Zare, C. Pahl, M. Nilashi, N. Salim, and O. Ibrahim, “A Review of Semantic 
Similarity Measures in Biomedical Domain Using SNOMED-CT,” 
Journal of Soft 
Computing and Decision Support Systems
, vol. 2, no. 6. pp. 1–13, 14-Sep-2015. 
[74] 
“Relationships 
in 
Medical 
Subject 
Headings 
(MeSH).” 
[Online]. 
Available: 
https://www.nlm.nih.gov/mesh/meshrels.html. [Accessed: 02-Jan-2018]. 
[75] 
H. Al-Mubaid and H. A. Nguyen, “A cluster-based approach for semantic similarity 
in the biomedical domain,” in 
Annual International Conference of the IEEE 
Engineering in Medicine and Biology - Proceedings
, 2006, vol. 1, pp. 2713–2717. 
[76] 
H. 
Al-Mubaid 
and 
H. 
A. 
Nguyen, 
“Measuring 
Semantic 
Similarity 
Between 
Biomedical Concepts Within Multiple Ontologies,” 
IEEE Trans. Syst. Man, Cybern. 
Part C (Applications Rev.
, vol. 39, no. 4, pp. 389–398, Jul. 2009. 
[77] 
M. Batet, S. Harispe, S. Ranwez, D. Sánchez, and V. Ranwez, “An information 
theoretic approach to improve semantic similarity assessments across multiple 
ontologies,” 
Inf. Sci. (Ny).
, vol. 283, pp. 197–210, Nov. 2014. 
[78] 
M. Batet, D. Sánchez, A. Valls, and K. Gibert, “Semantic similarity estimation from 
multiple ontologies,” 
Appl. Intell.
, vol. 38, no. 1, pp. 29–44, Jan. 2013. 
[79] 
K. Saruladha, G. Aghila, and A. Bhuvaneswary, “COSS: Cross Ontology Semantic 
Similarity 
measure 
— 
An 
information 
content 
based 
approach,” 
in 
2011 
International Conference on Recent Trends in Information Technology (ICRTIT)
, 
2011, pp. 485–490. 
[80] 
V. N. Garla and C. Brandt, “Semantic similarity in the biomedical domain: An 
evaluation across knowledge sources,” 
BMC Bioinformatics
, vol. 13, no. 1, p. 261, 
Oct. 2012. 
[81] 
“UMLS Reference Manual. Metathesaurus - Rich Release Format (RRF).” [Online]. 
Available: 
https://www.ncbi.nlm.nih.gov/books/NBK9685/. 
[Accessed: 
02-Jan-
2018]. 
[82] 
“SPECIALIST 
Lexicon 
Fact 
Sheet.” 
[Online]. 
Available: 
https://www.nlm.nih.gov/pubs/factsheets/umlslex.html. [Accessed: 02-Jan-2018]. 
[83] 
D. Sánchez and M. Batet, “Semantic similarity estimation in the biomedical domain: 
An ontology-based information-theoretic perspective,” 
J. Biomed. Inform.
, vol. 44, 
no. 5, pp. 749–759, Oct. 2011. 
[84] 
M. Batet, D. Sánchez, and A. Valls, “An ontology-based measure to compute 
semantic similarity in biomedicine,” 
J. Biomed. Inform.
, vol. 44, no. 1, pp. 118–125, 
Feb. 2011. 
[85] 
T. Mabotuwana, M. C. Lee, and E. V. Cohen-Solal, “An ontology-based similarity 
207 
measure for biomedical data - Application to radiology reports,” 
J. Biomed. Inform.
, 
vol. 46, no. 5, pp. 857–868, Oct. 2013. 
[86] 
V. N. Garla and C. Brandt, “Ontology-guided feature engineering for clinical text 
classification,” 
J. Biomed. Inform.
, vol. 45, no. 5, pp. 992–998, Oct. 2012. 
[87] 
B. T. McInnes, T. Pedersen, and S. V. S. Pakhomov, “UMLS-Interface and UMLS-
Similarity : open source software for measuring paths and semantic similarity.,” 
AMIA Annu. Symp. Proc.
, vol. 2009, pp. 431–5, Jan. 2009. 
[88] 
D. Lin, “An Information-Theoretic Definition of Similarity,” 
Icml
, vol. 98, no. 1998, 
pp. 296–304, 1998. 
[89] 
T. 
Mikolov, 
I. 
Sutskever, 
K. 
Chen, 
G. 
Corrado, 
and 
J. 
Dean, 
“Distributed 
Representations of Words and Phrases and their Compositionality,” in 
Advances in 
neural information processing systems
, 2013, pp. 3111–3119. 
[90] 
Q. Le and T. Mikolov, “Distributed Representations of Sentences and Documents,” 
in 
International Conference on Machine Learning
, 2014, pp. 1188–1196. 
[91] 
J. J. Jiang and D. W. Conrath, “Semantic Similarity Based on Corpus Statistics and 
Lexical Taxonomy,” Sep. 1997. 
[92] 
P. F. Brown, V. J. Della Pietra, P. V DeSouza, J. C. Lai, and R. L. Mercer, “Class-
based n-gram models of natural language,” 
Comput. Linguist.
, vol. 18, no. 4, pp. 
467–479, 1992. 
[93] 
J. Pennington, R. Socher, and C. D. Manning, “GloVe: Global Vectors for Word 
Representation,” in 
Proceedings of the 2014 conference on empirical methods in 
natural language processing (EMNLP)
, 2014, pp. 1532–1543. 
[94] 
T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient Estimation of Word 
Representations in Vector Space,” 
arXiv Prepr. arXiv
, vol. 1301, no. 3781, 2013. 
[95] 
V. Oleshchuk and A. Pedersen, “Ontology based semantic similarity comparison of 
documents,” in 
14th International Workshop on Database and Expert Systems 
Applications, 2003. Proceedings.
, 2003, pp. 735–738. 
[96] 
A. Madylova and S. G. Oguducu, “A taxonomy based semantic similarity of 
documents using the cosine measure,” in 
Computer and Information Sciences, 2009. 
ISCIS 2009. 24th International Symposium on
, 2009, pp. 129–134. 
[97] 
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, 
“Natural Language Processing (Almost) from Scratch,” 
J. Mach. Learn. Res.
, vol. 
12, no. Aug, pp. 2493–2537, 2011. 
[98] 
A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, “Supervised 
Learning of Universal Sentence Representations from Natural Language Inference 
Data,” 
arXiv Prepr. arXiv
, vol. 1705, no. 2364, May 2017. 
[99] 
A. M. Dai, C. Olah, and Q. V. Le, “Document Embedding with Paragraph Vectors,” 
arXiv Prepr. arXiv
, vol. 1507, no. 7998, Jul. 2015. 
[100] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed 
Representations of Words and Phrases and their Compositionality,” in 
Advances in 
neural information processing systems
, 2013, pp. 3111–3119. 
[101] B. Mitra and N. Craswell, “Neural Text Embeddings for Information Retrieval,” in 
Proceedings of the Tenth ACM International Conference on Web Search and Data 
Mining - WSDM ’17
, 2017, pp. 813–814. 
208 
[102] J. A. Bullinaria and J. P. Levy, “Extracting semantic representations from word co-
occurrence statistics: stop-lists, stemming, and SVD,” 
Behav. Res. Methods
, vol. 44, 
no. 3, pp. 890–907, Sep. 2012. 
[103] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet Allocation,” 
J. Mach. 
Learn. Res.
, vol. 3, no. Jan, pp. 993–1022, 2003. 
[104] T. Hofmann and Thomas, “Probabilistic Latent Semantic Indexing,” 
ACM SIGIR 
Forum
, vol. 51, no. 2, pp. 211–218, Aug. 2017. 
[105] L. van der Maaten and G. Hinton, “Visualizing Data using t-SNE,” 
J. Mach. Learn. 
Res.
, vol. 9, no. Nov, pp. 2579–2605, 2008. 
[106] F. Hill, K. Cho, and A. Korhonen, “Learning Distributed Representations of 
Sentences from Unlabelled Data,” 
arXiv Prepr. arXiv
, vol. 1602, no. 3483, Feb. 
2016. 
[107] W. Yin, K. Kann, M. Yu, and H. Schütze, “Comparative Study of CNN and RNN 
for Natural Language Processing,” 
arXiv Prepr. arXiv
, vol. 1702, no. 1923, Feb. 
2017. 
[108] J. Tang, M. Qu, and Q. Mei, “PTE: Predictive Text Embedding through Large-scale 
Heterogeneous 
Text 
Networks,” 
in 
Proceedings 
of 
the 
21th 
ACM 
SIGKDD 
International Conference on Knowledge Discovery and Data Mining
, 2015, pp. 
1165–1174. 
[109] Y. Kim, “Convolutional Neural Networks for Sentence Classification,” 
arXiv Prepr. 
arXiv
, vol. 1408, no. 5882, Aug. 2014. 
[110] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov, “Bag of Tricks for Efficient 
Text Classification,” 
arXiv Prepr. arXiv
, vol. 1607, no. 1759, Jul. 2016. 
[111] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent Trends in Deep Learning 
Based Natural Language Processing,” 
arXiv Prepr. arXiv
, vol. 1708, no. 2709, Aug. 
2017. 
[112] J. Wieting, M. Bansal, K. Gimpel, and K. Livescu, “Towards Universal Paraphrastic 
Sentence Embeddings,” 
arXiv Prepr. arXiv
, vol. 1511, no. 8198, Nov. 2015. 
[113] R. 
Kiros 
et 
al.
, 
“Skip-Thought 
Vectors,” 
in 
Advances 
in 
neural 
information 
processing systems
, 2015, pp. 3294–3302. 
[114] M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger, “From Word Embeddings 
To Document Distances,” in 
International Conference on Machine Learning
, 2015, 
pp. 957–966. 
[115] H. Palangi 
et al.
, “Deep Sentence Embedding Using Long Short-Term Memory 
Networks: Analysis and Application to Information Retrieval,” 
IEEE/ACM Trans. 
Audio, Speech Lang. Process.
, vol. 24, no. 4, pp. 694–707, Feb. 2015. 
[116] X. Liu, J. Gao, X. He, L. Deng, K. Duh, and Y.-Y. Wang, “Representation Learning 
Using 
Multi-Task 
Deep 
Neural 
Networks 
for 
Semantic 
Classification 
and 
Information Retrieval,” pp. 912–921, 2015. 
[117] M. Ma, L. Huang, B. Xiang, and B. Zhou, “Dependency-based Convolutional 
Neural Networks for Sentence Embedding,” 
arXiv Prepr. arXiv
, vol. 1507, no. 1839, 
Jul. 2015. 
[118] H. Zhao, Z. Lu, and P. Poupart, “Self-Adaptive Hierarchical Sentence Model,” in 
IJCAI
, 2015, pp. 4069–4076. 
209 
[119] W. Wang, S. J. Pan, D. Dahlmeier, and X. Xiao, “Recursive Neural Conditional 
Random Fields for Aspect-based Sentiment Analysis,” 
arXiv Prepr. arXiv
, vol. 
1603, no. 6679, Mar. 2016. 
[120] W. 
Yin, 
H. 
Schütze, 
B. 
Xiang, 
and 
B. 
Zhou, 
“ABCNN: 
Attention-Based 
Convolutional Neural Network for Modeling Sentence Pairs,” 
arXiv Prepr. arXiv
, 
vol. 1512, no. 5193, Dec. 2016. 
[121] Z. Lin 
et al.
, “A Structured Self-attentive Sentence Embedding,” 
arXiv Prepr. arXiv
, 
vol. 1703, no. 3130, Mar. 2017. 
[122] R. Johnson and T. Zhang, “Semi-supervised Convolutional Neural Networks for 
Text Categorization via Region Embedding,” in 
Advances in neural information 
processing systems
, 2015, pp. 919–927. 
[123] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A Convolutional Neural 
Network for Modelling Sentences,” 
arXiv Prepr. arXiv
, vol. 1404, no. 2188, pp. 
655–665, 2014. 
[124] Y. Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event Extraction via Dynamic Multi-
Pooling Convolutional Neural Networks,” 
Proc. 53rd Annu. Meet. Assoc. Comput. 
Linguist. 7th Int. Jt. Conf. Nat. Lang. Process.
, vol. 1, no. Long Papers, pp. 167–
176, 2015. 
[125] O. Irsoy and C. Cardie, “Deep Recursive Neural Networks for Compositionality in 
Language,” in 
Advances in neural information processing systems
, 2014, pp. 2096–
2104. 
[126] V. Cirik, E. Hovy, and L.-P. Morency, “Visualizing and Understanding Curriculum 
Learning for Long Short-Term Memory Networks,” 
arXiv Prepr. arXiv
, vol. 1611, 
no. 6204, Nov. 2016. 
[127] R. Rehurek, R. Rehurek, and P. Sojka, “Software Framework for Topic Modelling 
with Large Corpora,” 
Proc. Lr. 2010 Work. New Challenges NLP Fram.
, pp. 45--
50, 2010. 
[128] “Gensim.” [Online]. Available: https://radimrehurek.com/gensim/. [Accessed: 02-
Jan-2018]. 
[129] A. Kerren, J. Stasko, J.-D. Fekete, and C. North, 
Information Visualization: Human-
Centered Issues and Perspectives
. Springer, 2008. 
[130] C. Ware, 
Information visualization: perception for design
. Elsevier, 2012. 
[131] T. M. J. Fruchterman and E. M. Reingold, “Graph drawing by force-directed 
placement,” 
Softw. Pract. Exp.
, vol. 21, no. 11, pp. 1129–1164, Nov. 1991. 
[132] M. Jacomy, T. Venturini, S. Heymann, and M. Bastian, “ForceAtlas2, a Continuous 
Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi 
Software,” 
PLoS One
, vol. 9, no. 6, p. e98679, Jun. 2014. 
[133] H. Ding, C. Wang, K. Huang, and R. Machiraju, “GRAPHIE: graph based histology 
image explorer,” 
BMC Bioinformatics
, vol. 16, no. Suppl 11, p. S10, Aug. 2015. 
[134] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre, “Fast unfolding of 
communities in large networks,” 2008. 
[135] D. B. West, 
Introduction to graph theory
. Upper Saddle River: Prentice hall, 2001. 
[136] M. E. Newman, “Modularity and community structure in networks,” 
Proc. Natl. 
Acad. Sci.
, vol. 103, no. 23, pp. 8577–8582, Jun. 2006. 
210 
[137] M. Girvan and M. E. Newman, “Community structure in social and biological 
networks,” 
Proc. Natl. Acad. Sci.
, vol. 99, no. 12, pp. 7821–7826, Jun. 2002. 
[138] A. Lancichinetti and S. Fortunato, “Community detection algorithms: a comparative 
analysis,” Aug. 2009. 
[139] S. Fortunato and D. Hric, “Community detection in networks: A user guide,” Jul. 
2016. 
[140] Z. Yang 
et al.
, “A Comparative Analysis of Community Detection Algorithms on 
Artificial Networks,” 
Sci. Rep.
, vol. 6, p. 30750, Aug. 2016. 
[141] P. Pons and M. Latapy, “Computing Communities in Large Networks Using 
Random 
Walks,” 
in 
International 
symposium 
on 
computer 
and 
information 
sciences
, 2005, pp. 284–293. 
[142] A. Clauset, M. E. J. Newman, and C. Moore, “Finding community structure in very 
large networks,” 
Phys. Rev. E
, vol. 70, no. 6, p. 66111, Dec. 2004. 
[143] G. Su, A. Kuchinsky, J. H. Morris, D. J. States, and F. Meng, “GLay: community 
structure analysis of biological networks,” 
Bioinformatics
, vol. 26, no. 24, pp. 3135–
3137, Dec. 2010. 
[144] M. Rosvall and C. T. Bergstrom, “Maps of random walks on complex networks 
reveal community structure.,” 
Proc. Natl. Acad. Sci. U. S. A.
, vol. 105, no. 4, pp. 
1118–23, Jan. 2008. 
[145] J. M. Pujol, V. Erramilli, and P. Rodriguez, “Divide and Conquer: Partitioning 
Online Social Networks,” 
arXiv Prepr. arXiv
, vol. 905, no. 4918, May 2009. 
[146] L. Waltman and N. J. van Eck, “A smart local moving algorithm for large-scale 
modularity-based community detection,” Aug. 2013. 
[147] M. Rosvall and C. T. Bergstrom, “An information-theoretic framework for resolving 
community 
structure 
in 
complex 
networks,” 
in 
Proceedings 
of 
the 
National 
Academy of Sciences
, 2007, pp. 7327–7331. 
[148] U. N. Raghavan, R. Albert, and S. Kumara, “Near linear time algorithm to detect 
community structures in large-scale networks,” 
Phys. Rev. E - Stat. Nonlinear, Soft 
Matter Phys.
, vol. 76, no. 3, p. 36106, Sep. 2007. 
[149] P. Hu and W. C. Lau, “A Survey and Taxonomy of Graph Sampling,” 
arXiv Prepr. 
arXiv
, vol. 1308, no. 5886, Aug. 2013. 
[150] M. Bastian, S. Heymann, and M. Jacomy, “Gephi : An Open Source Software for 
Exploring and Manipulating Networks Visualization and Exploration of Large 
Graphs,” 
Icwsm
, vol. 8, no. 2009, pp. 361–362, 2009. 
[151] R. A. Schoot, E. C. van Dalen, C. H. van Ommen, and M. D. van de Wetering, 
“Antibiotic and other lock treatments for tunnelled central venous catheter related 
infections in children with cancer,” in 
Cochrane Database of Systematic Reviews
, 
R. A. Schoot, Ed. Chichester, UK: John Wiley & Sons, Ltd, 2011. 
[152] R. L. Mulder 
et al.
, “Hepatic late adverse effects after antineoplastic treatment for 
childhood cancer,” 
Cochrane Database Syst. Rev.
, Jul. 2011. 
[153] N. Siegfried, L. van der Merwe, P. Brocklehurst, and T. T. Sint, “Antiretrovirals for 
reducing the risk of mother-to-child transmission of HIV infection,” 
Cochrane 
Database Syst. Rev.
, Jul. 2011. 
[154] J. W. van As, H. van den Berg, and E. C. van Dalen, “Medical interventions for the 
211 
prevention of platinum-induced hearing loss in children with cancer,” in 
Cochrane 
Database of Systematic Reviews
, E. C. van Dalen, Ed. Chichester, UK: John Wiley 
& Sons, Ltd, 2014. 
[155] M. S. Gordijn, R. J. Gemke, E. C. van Dalen, J. Rotteveel, and G. J. Kaspers, 
“Hypothalamic-pituitary-adrenal 
(HPA) 
axis 
suppression 
after 
treatment 
with 
glucocorticoid therapy for childhood acute lymphoblastic leukaemia,” in 
Cochrane 
Database of Systematic Reviews
, M. S. Gordijn, Ed. Chichester, UK: John Wiley & 
Sons, Ltd, 2012. 
[156] Y. Liu, A. Dighe, T. Safavi, and D. Koutra, “A Graph Summarization: A Survey,” 
arXiv Prepr. arXiv
, vol. 1612, no. 4883, Dec. 2016. 
[157] J. Chen and I. Safro, “Algebraic Distance on Graphs,” 
SIAM J. Sci. Comput.
, vol. 
33, no. 6, pp. 3468–3490, 2011. 
[158] E. John and I. Safro, “Single- and Multi-level Network Sparsification by Algebraic 
Distance,” 
J. Complex Networks
, vol. 5, no. 3, pp. 352–388, Jan. 2016. 
[159] G. Lindner, C. L. Staudt, M. Hamann, H. Meyerhenke, and D. Wagner, “Structure-
Preserving 
Sparsification 
of 
Social 
Networks,” 
in 
Proceedings 
of 
the 
2015 
IEEE/ACM International Conference on Advances in Social Networks Analysis and 
Mining
, 2015, pp. 448–454. 
[160] Y. Hu, “Efficient, high-quality force-directed graph drawing,” 
Math. J.
, vol. 10, no. 
1, pp. 37–71, 2005. 
[161] J. Leskovec and C. Faloutsos, “Sampling from Large Graphs,” in 
Proceedings of the 
12th ACM SIGKDD international conference on Knowledge discovery and data 
mining
, 2006, pp. 613–636. 
[162] M. F. De Oliveira and H. Levkowitz, “From visual data exploration to visual data 
mining: A survey,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 9, no. 3, pp. 378–394, 
Jul. 2003. 
[163] H. 
Hotelling, 
“Analysis 
of 
a 
complex 
of 
statistical 
variables 
into 
principal 
components.,” 
J. Educ. Psychol.
, vol. 24, no. 6, pp. 417–441, 1933. 
[164] W. 
S. 
Torgerson, 
“Multidimensional 
scaling: 
I. 
Theory 
and 
method,” 
Psychometrika
, vol. 17, no. 4, pp. 401–419, Dec. 1952. 
[165] I. Borg and P. J. F. Groenen, 
Modern multidimensional scaling : theory and 
applications
. Springer, 2005. 
[166] G. E. Hinton and S. T. Roweis, “Stochastic Neighbor Embedding,” in 
Advances in 
neural information processing systems
, 2003, pp. 857–864. 
[167] J. B. Tenenbaum, V. de Silva, and J. C. Langford, “A global geometric framework 
for nonlinear dimensionality reduction.,” 
Science (80-. ).
, vol. 290, no. 5500, pp. 
2319–23, Dec. 2000. 
[168] D. 
A. 
Keim, 
F. 
Mansmann, 
and 
J. 
Thomas, 
“Visual 
analytics: 
how 
much 
visualization and how much analytics?,” 
ACM SIGKDD Explor. Newsl.
, vol. 11, no. 
2, pp. 5–8, May 2010. 
[169] A. Slingsby, J. Dykes, and J. Wood, “Configuring Hierarchical Layouts to Address 
Research Questions,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 15, no. 6, pp. 977–
984, Nov. 2009. 
[170] T. Dwyer, K. Marriott, F. Schreiber, P. Stuckey, M. Woodward, and M. Wybrow, 
212 
“Exploration of Networks using overview+detail with Constraint-based cooperative 
layout,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 14, no. 6, pp. 1293–1300, Nov. 
2008. 
[171] D. A. Keim, F. Mansmann, J. Schneidewind, and H. Ziegler, “Challenges in Visual 
Data Analysis,” in 
Tenth International Conference on Information Visualisation 
(IV’06)
, 2006, pp. 9–16. 
[172] S. Teller, 
Data Visualization with D3. js
. Packt Publishing Ltd, 2013. 
[173] S. Liu 
et al.
, “Visual Exploration of Semantic Relationships in Neural Word 
Embeddings,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 24, no. 1, pp. 553–562, 2018. 
[174] B. Alsallakh, A. Jourabloo, M. Ye, X. Liu, and L. Ren, “Do Convolutional Neural 
Networks Learn Class Hierarchy?,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 24, no. 
1, pp. 152–162, 2018. 
[175] H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush, “LSTMVis: A Tool for Visual 
Analysis of Hidden State Dynamics in Recurrent Neural Networks,” 
IEEE Trans. 
Vis. Comput. Graph.
, vol. 24, no. 1, pp. 667–676, Jun. 2018. 
[176] P. 
Berkhin, 
“Survey 
of 
Clustering 
Data 
Mining 
Techniques,” 
in 
Grouping 
multidimensional data
, 2006, pp. 25–71. 
[177] F. Heimerl, M. John, Qi Han, S. Koch, and T. Ertl, “DocuCompass: Effective 
exploration of document landscapes,” in 
2016 IEEE Conference on Visual Analytics 
Science and Technology (VAST)
, 2016, pp. 11–20. 
[178] F. Wei 
et al.
, “TIARA: A Visual Exploratory Text Analytic System,” in 
Proceedings 
of the 16th ACM SIGKDD international conference on Knowledge discovery and 
data mining
, 2010, pp. 153–162. 
[179] F. Heimerl, Q. Han, S. Koch, and T. Ertl, “CiteRivers: Visual Analytics of Citation 
Patterns,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 22, no. 1, pp. 190–199, Jan. 2016. 
[180] S. Liu, J. Yin, X. Wang, W. Cui, K. Cao, and J. Pei, “Online Visual Analytics of 
Text Streams,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 22, no. 11, pp. 2451–2466, 
Nov. 2016. 
[181] X. Ji, R. Machiraju, A. Ritter, and P.-Y. Yen, “Examining the Distribution, 
Modularity, 
and 
Community 
Structure 
in 
Article 
Networks 
for 
Systematic 
Reviews.,” 
AMIA ... Annu. Symp. proceedings. AMIA Symp.
, vol. 2015, pp. 1927–
36, 2015. 
[182] Nan 
Cao, 
Jimeng 
Sun, 
Yu-Ru 
Lin, 
D. 
Gotz, 
Shixia 
Liu, 
and 
Huamin 
Qu, 
“FacetAtlas: Multifaceted Visualization for Rich Text Corpora,” 
IEEE Trans. Vis. 
Comput. Graph.
, vol. 16, no. 6, pp. 1172–1181, Nov. 2010. 
[183] M. Kim, K. Kang, D. Park, J. Choo, and N. Elmqvist, “TopicLens: Efficient Multi-
Level Visual Topic Exploration of Large-Scale Document Collections,” 
IEEE 
Trans. Vis. Comput. Graph.
, vol. 23, no. 1, pp. 151–160, Jan. 2017. 
[184] X. Ji, R. Machiraju, A. Ritter, and P.-Y. Yen, “Visualizing Article Similarities via 
Sparsified Article Network and Map Projection for Systematic Reviews.,” 
Stud. 
Health Technol. Inform.
, vol. 245, pp. 422–426, 2017. 
[185] S. Hadlak, H.-J. Schulz, and H. Schumann, “In Situ Exploration of Large Dynamic 
Networks,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 17, no. 12, pp. 2334–2343, Dec. 
2011. 
213 
[186] F. van Ham and A. Perer, “‘Search, Show Context, Expand on Demand’: Supporting 
Large Graph Exploration with Degree-of-Interest,” 
IEEE Trans. Vis. Comput. 
Graph.
, vol. 15, no. 6, pp. 953–960, Nov. 2009. 
[187] J. Abello, F. Van Ham, and N. Krishnan, “ASK-GraphView: A Large Scale Graph 
Visualization System,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 12, no. 5, pp. 669–
676, Sep. 2006. 
[188] M. 
D. 
Zeiler 
and 
R. 
Fergus, 
“Visualizing 
and 
Understanding 
Convolutional 
Networks,” in 
European conference on computer vision
, 2013, pp. 818–833. 
[189] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep Inside Convolutional Networks: 
Visualising Image Classification Models and Saliency Maps,” 
arXiv Prepr. arXiv
, 
vol. 1312, no. 6034, Dec. 2013. 
[190] M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu, “Towards Better Analysis of Deep 
Convolutional Neural Networks,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 23, no. 1, 
pp. 91–100, Jan. 2017. 
[191] P. E. Rauber, S. G. Fadel, A. X. Falcao, and A. C. Telea, “Visualizing the Hidden 
Activity of Artificial Neural Networks,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 23, 
no. 1, pp. 101–110, Jan. 2017. 
[192] A. Karpathy, J. Johnson, and L. Fei-Fei, “Visualizing and Understanding Recurrent 
Networks,” 
arXiv Prepr. arXiv
, vol. 1506, no. 2078, Jun. 2015. 
[193] J. Li, X. Chen, E. Hovy, and D. Jurafsky, “Visualizing and Understanding Neural 
Models in NLP,” 
arXiv Prepr. arXiv
, vol. 1506, no. 1066, 2015. 
[194] D. Smilkov, N. Thorat, C. Nicholson, E. Reif, F. B. Viégas, and M. Wattenberg, 
“Embedding Projector: Interactive visualization and interpretation of embeddings,” 
arXiv Prepr. arXiv
, vol. 1611, no. 5469, 2016. 
[195] D. 
Smilkov 
and 
S. 
Carter, 
“TensorFlow 
Playground.” 
[Online]. 
Available: 
http://playground.tensorflow.org/. 
[196] F. Murtagh and P. Legendre, “Ward’s Hierarchical Clustering Method: Clustering 
Criterion and Agglomerative Algorithm,” 
arXiv Prepr. arXiv
, vol. 1111, no. 6285, 
Nov. 2011. 
[197] S. Rose, D. Engel, N. Cramer, and W. Cowley, “Automatic keyword extraction from 
individual documents,” 
Text Min. Appl. Theory
, vol. 1, no. 20, 2010. 
[198] D. D. Lee and H. S. Seung, “Algorithms for Non-negative Matrix Factorization,” in 
Advances in neural information processing systems
, 2001, pp. 556–562. 
[199] P. J. Rousseeuw, “Silhouettes: A graphical aid to the interpretation and validation 
of cluster analysis,” 
J. Comput. Appl. Math.
, vol. 20, pp. 53–65, Nov. 1987. 
[200] J. Wang, X. Liu, and H.-W. Shen, “High-dimensional data analysis with subspace 
comparison using matrix visualization,” 
Inf. Vis.
, 2017. 
[201] J. Bertin, 
Semiology of graphics: diagrams, networks, maps
. 1983. 
[202] X. 
Liu 
and 
H.-W. 
Shen, 
“Association 
Analysis 
for 
Visual 
Exploration 
of 
Multivariate Scientific Data Sets,” 
IEEE Trans. Vis. Comput. Graph.
, vol. 22, no. 1, 
pp. 955–964, Jan. 2016. 
[203] H.-P. Kriegel, P. Kröger, and A. Zimek, “Clustering high-dimensional data: A 
survey on subspace clustering, pattern-based clustering, and correlation clustering,” 
ACM Trans. Knowl. Discov. Data
, vol. 3, no. 1, pp. 1–58, Mar. 2009. 
