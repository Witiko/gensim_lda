"AuTopEx": Automated Topic Extraction Techniques 
Applied in the Software Engineering Domain 
The design and evaluation of an approach for Automated Topic 
Extraction
Bachelor of Science Thesis Software Engineering & Management
JONATHAN KLEMETZ 
MAGNUS JOHANSSON 
University of Gothenburg 
Chalmers University of Technology 
Department of Computer Science and Engineering 
Göteborg, Sweden, June 2016 
The Author grants to Chalmers University of Technology and University of Gothenburg 
the non-exclusive right to publish the Work electronically and in a non-commercial 
purpose make it accessible on the Internet. 
The Author warrants that he/she is the author to the Work, and warrants that the Work does 
not contain text, pictures or other material that violates copyright law. 
The Author shall, when transferring the rights of the Work to a third party (for example a 
publisher or a company), acknowledge the third party about this agreement. If the Author 
has signed a copyright agreement with a third party regarding the Work, the Author 
warrants hereby that he/she has obtained any necessary permission from this third party to 
let Chalmers University of Technology and University of Gothenburg store the Work 
electronically and make it accessible on the Internet. 
"AuTopEx": Automated Topic Extraction Techniques 
Applied in the Software Engineering Domain
The design and evaluation of an approach for Automated Topic Extraction 
Jonathan Klemetz
Magnus Johansson
© Jonathan Klemetz, 
June 2016
.
© Magnus Johansson, 
June 2016
.
Examiner: Christian Berger
Supervisor: Alessia Knauss
Supervisor: Hang Yin
University of Gothenburg
Chalmers University of Technology
Department of Computer Science and Engineering
SE-412 96 Göteborg
Sweden
Telephone + 46 (0)31-772 1000
Department of Computer Science and Engineering
Göteborg, Sweden June 2016 
”AuTopEx”:
Automated Topic Extraction Techniques Applied
in the Software Engineering Domain
Magnus Johansson
Gothenburg University
Software Engineering & Management
Lindholmsplatsen 1, 412 96 Gothenburg, Sweden
gusjmagn03@student.gu.se
Jonathan Klemetz
Gothenburg University
Software Engineering & Management
gusklejoa@student.gu.se
Abstract
Automatically extracting topics from scien-
tific papers can be very beneficial
when a re-
searcher needs
to classify a large number of
such papers.
In this
thesis
we develop and evaluate an
approach for Automatic Topic Extraction, Au-
TopEx.
The approach is
comprised of
four
parts:
1) Text pre-processing.
2) Training a Latent Dirichlet Allocation model
on part of a corpus.
3)
Manually identifying relevant
topics
from
the model.
4) Querying the model using the rest of the cor-
pus.
We show that it is possible to automatically
extract topics by applying AuTopEx on a corpus
of scientific papers on autonomous vehicles.
According to our evaluation AuTopEx works
better on full-text articles than texts consisting
of just title, abstract and key-words.
Finally we show that this approach is vastly
faster than human annotators, although not as
accurate.
The source code used to build AuTopEx can be
found at:
(https://github.com/Klemetz/TopicExtraction).
1
Introduction
In this thesis we design and evaluate an approach
for Automated Topic Extraction.
Which is evaluated
on papers in the Software Engineering domain,
more
specifically on autonomous vehicles.
1.1
Background
Automated Topic Analysis and Automated Topic
Extraction allow researchers to extract the potential
topics that are contained in a large text corpus.
This
has been tried in other scientific domains but (to the
best of our knowledge) not in the field of Software En-
gineering.
1.2
Problem Domain & Motivation
In order to find relevant information, researchers of-
ten need to read a large number of published articles.
This is especially true when conducting work like map-
ping studies or Systematic Literature Reviews and can
be a very time-consuming process.
There is a lack of
automated approaches to topic
extraction that could support activities such as Sys-
tematic Mapping Studies,
especially in the Software
Engineering domain.
1.3
Research Goal & Research Questions
Our research goal
is to investigate the automation
of
Topic Extraction from scientific papers in order to
support time-consuming activities such as Systematic
Mapping Studies [18].
We investigate extraction from
both full-text articles and texts containing only title,
abstract and keywords using a topic model
called La-
tent Dirichlet Allocation (LDA).
The research goal
has been divided into three re-
search questions.
RQ 1:
How can we support Automatic Topic Ex-
traction for scientific papers in the Software Engineer-
ing domain?
RQ 2:
Which approach is
better
for
Automatic
Topic Extraction:
a) Extraction from title,
abstract
and keywords or b) Extraction from full text paper?
RQ 3:
How well does the approach of using Latent
Dirichlet Allocation (with suitable pre-processing) per-
form compared to a manual method?
1.4
Contributions
In this paper we present an approach (which we call
”AuTopEx”) for applying Automated Topic Extraction
on a large number of scientific papers.
From the extracted data, relevant topics are identi-
fied and labeled.
Researchers can then also automate
the process of finding which papers in the corpus that
are most likely to deal with the relevant topics.
Researchers will
benefit from AuTopEx as it shows
the
applicability
of
Natural
Language
Processing
(NLP) techniques in the Software Engineering Domain.
1.5
Scope
We construct and evaluate an approach for Auto-
matic Topic Extraction using Latent
Dirichlet
Allo-
cation (LDA).
This could contribute in making Au-
tomatic Topic Extraction a viable approach in Soft-
ware Engineering research.
We do not evaluate other
statistical
models such as the n-gram model
or term
frequency-inverse document frequency.
However, their
potential use in our approach is discussed in the Con-
clusions and Future Work section.
1.6
Structure of the Article
Section 2 presents related work on Automated Topic
Extraction.
Section 3 covers our Research Strategy.
In
Section 4 we answer our research questions.
First we
describe AuTopEx and go into detail about it’s imple-
mentation.
Then we evaluate the results from imple-
menting AuTopEx compared to human performance.
Evaluations are made on two corpuses, one corpus con-
taining articles in full-text and the other only title, ab-
stracts and keywords.
Section 5 contains analysis and
discussion of the results.
In this section we also discuss
the validity threats to our findings.
Section 6 concludes
our findings and discusses what implications they may
have for future research.
2
Related Work
We have not found any articles dealing with tools
specifically tailored towards automation of Systematic
Mapping Studies.
These do however share some sim-
ilarities
with Systematic Literature Reviews
(SLR).
Hence we can discuss tools that support the latter.
According to Marshall
and Brereton [16],
the two
most popular frameworks for tools that support SLR:s
are the Projection Explorer Pex and ReVis which both
make use of
Visual
Text Mining techniques.
Projec-
tion Explorer Pex [6]
can create a visualization from
a set of
textual
documents either by building a vec-
tor representation of the text corpus (which is handled
as table data to derive similarity information).
It can
also compute similarities by directly comparing text
against text.
It could possibly be used in helping with
document classification during a mapping study.
ReVis [5] supports primary studies selection during
SLRs.
Among it’s tools is the possibility of
visualiz-
ing the relationships of
potential
primary studies.
A
2D document map shows content and similarities of
di↵erent documents.
This is based on converting the
documents into multi-dimensional
vectors which can
be reduced using stemming, by eliminating stop words
and using projection techniques.
ReVis only uses title,
abstract and keywords for this document map however,
and we ideally want to use full-text articles to discover
topics.
CitNetExplorer analyzes citation patterns in scien-
tific literature.
The tool
collects bibliographical
data
and constructs a citation network which can then be
analyzed and visualized[11].
This
could be
useful
for mapping a specific research topic,
since key word
searches could miss out on papers that do not contain
these key words.
VOSviewer [10]
is a tool
for creating and visualiz-
ing bibliographical networks.
It can also use text min-
ing to create term maps from a text corpus.
Part of
speech-tagging is used to identify noun phrases and a
technique for choosing the most relevant noun phrases
is applied.
Maps and clusters can then be created and
visualized.
In
[12]
the
creators
of
CitNetExplorer
and
VOSviewer discuss the limitations of both tools.
They
argue that the loss of information occurring when ap-
plying these techniques is very hard to measure and
that they should be used as a complement rather than
substitute to expert judgment.
One approach for speeding up topic extraction could
be
automatic
summarization of
articles.
The
ab-
stract of a scientific paper is meant to provide a quick
overview, but does not necessarily provide enough key
information for the researcher.
Automatic summariza-
tion techniques can capture scientific concepts such as
Hypotheses,
Method and Background on a sentence
level [14] and thus provide more information than just
an abstract.
However, this work builds on having many
domain experts manually annotate a large number of
scientific papers used for training the machine learning
classifiers [15].
Such an undertaking is out of scope for
this thesis.
The ”Latent Dirichlet Allocation” method is widely
used and applicable in the discipline of
Natural
Lan-
guage Processing (NLP) [2].
As Blei puts it ”The sim-
ple LDA model provides a powerful tool for discovering
and exploiting the hidden thematic structure in large
archives of text” [2].
When attempting to extract top-
ics from a large corpus as the purpose is for the Au-
TopEx approach,
a tool
like the LDA method is very
compelling.
Blei
has also provided some comparisons
with other models which makes the choice of applying
LDA an attractive option.
He shows that even though
LDA is meant to perform ”in the spirit of LSI”(Latent
semantic indexing) [3],
the LDA method outperforms
the LSI method regarding perplexity measures [3].
But can we be sure that NLP tools such as the LDA
method are fitting for the Software Engineering do-
main? Studies such as the one performed by Hindle et
al [9], shows us that they can be applied but might not
always be suitable.
Hindle presents in his paper that in
the domain of
software engineering,
neither LDA nor
the n-gram analysis approach may be suitable if
the
intended goal
is to extract topics from files contain-
ing computer code.
However,
we do not expect that
code snippets will make up anything but a very small
portion of the scientific articles that we want to apply
Automatic Topic Extraction on.
3
Research Strategy
We have chosen Design Science as our research strat-
egy.
Hevner et.
al [8] present seven guidelines for con-
ducting,
evaluating and presenting Design Science re-
search.
These address design as an artifact,
problem
relevance, design evaluation, research contributions, re-
search rigor,
design as a search process,
and research
communication.
The artifact is in our case an approach based on Nat-
ural Language Processing techniques.
In this approach
we apply a number of pre-processing steps on a large
corpus of texts.
Then we automatically extract topics
from the corpus.
Finally we automatically classify the
papers based on the extracted topics.
The problem relevance is the fact that doing this
manually is a very time-consuming process.
Evaluation will
be done by comparing the topics
that
the machine learning algorithm produces
with
annotation made manually by humans.
Both of
the
authors will
first do manual
annotation of
the same
papers separately and then confirm that there exists
an inter-annotator agreement.
Basically that both au-
thors have identified the same topics in each paper.
As
for
research contributions
we are transferring
knowledge from the domain of language technology to
the area of Software Engineering.
We also believe the
resulting approach can be helpful
in future research
where speeding up topic extraction can be beneficial.
When it comes to research rigor we are actively se-
lecting and applying appropriate theories and methods
both when constructing and evaluating the resulting
artifact.
Design as a research process has been a must from
the start,
since this
approach has
not
been applied
on scientific articles about Software Engineering be-
fore.
Investigating existing tools and techniques that
are already being used for similar purposes and how to
implement them properly,
is the whole foundation of
constructing our approach.
We have worked in itera-
tions during the whole process,
constantly improving
our approach by trying out di↵erent ways of
working
with existing tools and developing our own tools where
needed.
Finally research communication must be taken into
account.
Since
this
thesis
is
concerned with re-
search,
we ensure that
we explain our
methods
as
thoroughly as possible so that other researchers can
evaluate
the
approach.
All
of
our
code
is
open
source and available under
the Gnu General
Public
License Version 2 at (https://github.com/Klemetz/
TopicExtraction) along with proper documentation
so that others can apply the approach themselves.
4
Results
4.1
AuTopEx
In order
to answer
RQ1:
”How can we sup-
port Automatic Topic Extraction for scientific
papers in the Software Engineering domain?”,
we have developed the approach AuTopEx, which sup-
ports Automatic Topic Extraction.
AuTopEx can be broken down into four steps:
1.
Pre-processing
the
articles
of
a
large
corpus
of scientific paper.
2.
Training a Latent Dirichlet Allocation (LDA) model
using 10 percent of the pre-processed papers.
3.
Manual identification and labeling of relevant topics
returned by the model.
4.
Automatic classification (extracting the topics) of
the rest of the corpus by querying the LDA model.
Some of the papers will be annotated manually be-
fore automatic classification.
We can evaluate the accu-
racy of the model by comparing this manual annotation
with the automatic classification of the same papers.
We have chosen existing tools
to help answering
our
research questions
and to construct
AuTopEx.
Calibre (https://calibre-ebook.com/) is used for
pdf-to
-text
conversion.
The
Natural
Language
ToolKit(NLTK) (http://www.nltk.org/) is used for
pre-processing individual
texts and Gensim (https:
//radimrehurek.com/gensim/) is used for applying
the machine learning algorithms.
Complementary tools
in the form of Python scripts have been developed by
the authors when needed.
4.1.1
Text Pre-Processing
Pre-processing the scientific papers follows a pipe-line
of six consecutive steps:
1.
Pdf-to-text conversion
2.
Converting all text to lower-case
3.
Tokenization
4.
Removal of stop words, numbers and punctuation
5.
Lemmatization
6.
Removal of references section
Pdf-to-text conversion
The first step of preparing the individual scientific pa-
pers is to convert them from pdf
to text format.
We
use the free and open-source Calibre software.
The
reasons are two-fold:
a) Unlike other tools we tried,
Calibre handles ligatures well
and b) dehyphenation.
If a word is cut o↵ with a hyphen at the end of a col-
umn, the program checks if the hyphenated word exists
elsewhere in the document without the hyphen and de-
hyphenates it if that is the case.
This ensures that more
accurate words remain in the document.
Converting all text into lower-case
All
text is transformed into lower-case format,
to en-
sure correct multiplicity of words even if they appear at
the start of a sentence.
It is also important that the list
of stop words (introduced below) are all in lowercase.
Tokenization
Tokenization means we break down the stream of text
into meaningful elements.
In our case this means indi-
vidual words (all contiguous alphabetic characters be-
come part of a token) which are then separated from
symbols such as punctuation.
The Natural
Language
Toolkit(NLTK) has a number of tokenizers, we recom-
mend their ”regexptokenizer” for this.
Removal
of stop words,
numbers and punctua-
tion
Latent Dirichlet Allocation uses a Bag-of-words model
[3].
This means that neither grammar or word order is
important, only the multiplicity of the words.
Thus we
can now safely remove all
punctuation and also stop
words (such as ”a”, ”and”, ”if”, ”or” etcetera).
Greek letters are often used as mathematical
nota-
tion in scientific articles.
Such a symbol on it’s own has
little to no semantic value for an annotator examining
our results.
Neither do we expect numbers from the
articles to hold any semantic importance in the topic
extraction so these are removed as well.
This is eas-
ily solved by only allowing alphabetical
words in the
tokenizer.
Punctuation is also removed using regular expres-
sions.
Stop words are removed by using a stop word list.
We use the default stop words list from NLTK,
and
supplement it with more words that we deem have no
semantic value.
For example the word ”fig.” very com-
monly appears next to images and graphs in research
papers.
This word pollutes the results rather than give
the topics any semantic meaning.
Lemmatization
Within a document a word can use several forms (such
as ”organize”,
”organizing” or ”organizes”) while all
referring to the same concept,
and we are interested
in the multiplicity of
this concept.
Stemming is the
process of reducing inflected words to their word stem.
A stemmer only operates on the word at hand by cut-
ting of
the word stem.
”Organize”,
”organizing” and
”organizes” would all
be reduced to ”organ” using a
stemmer,
which is not what we want since the word
now has an entirely new meaning.
Lemmatizing is closely related to stemming in that
it reduces inflected words, however it reduces the word
form to linguistically valid lemmas,
using algorithms
that deal with grammar and a built-in dictionary.
For
this purpose we use the WordNet Lemmatizer included
with NLTK.
Here are a few example sentences.
”They walk down
the road.
She walked by him.
The elephant walks on
four legs while we are used to walking on two.”
Using the most common stemmer (Porter) we get:
”They walk down the road .
She walk by him .
The
eleph walk on four leg while we are use to walk on two
.”
Using
the
WordNet
lemmatizer
we
instead get:
”They walk down the road .
She walked by him .
The
Pdf-to-text 
conversion
Make text 
lowercase
Tokenization
Removal of stop 
words, numbers 
& punctuation
Lemmatization
Removal of 
references
Figure 1:
The steps involved in text pre-processing.
elephant walk on four leg while we are used to walking
on two .”
The di↵erences between stemmers and the basic dif-
ferences between stemmers and lemmatizers are dis-
cussed in [13].
Removal of References section
Finally, all of the papers have a ”references” section at
the end.
We do not want the words in this section to
pollute the article at hand.
This section is removed by
finding the last occurrence of the word ”reference” (re-
member we have lemmatized all words) and removing
all remaining words in the document including ”refer-
ence”.
In the rare event that a reference has the word
”reference” in it,
some references might remain in the
document.
On the whole however we do not expect
this to have a major impact on the results.
Pre-processing Title, Abstract, Keyword texts
Pre-processing these texts uses the same pipe-line as
the full
text method outlined at the beginning of this
section but with the first and last step removed.
This
is because we already have the abstracts available in
text format and they don’t contain any references.
Extracting title, abstracts and keywords from a full-
text paper can be difficult,
since not all
articles are
formatted in the same way.
Some papers do not even
include the keywords in the article document.
Therefor
we chose to extract this information using meta data
stored in the software Endnote used by the researchers
who provided us with the data used for the evaluation.
Endnote can produce a single text file that contains
author names,
publishing year,
publication,
title,
ab-
stract and keyword for all
articles that you want to
perform topic extraction on.
A Python script
extracts
the relevant
meta data
(title,
abstract
and keywords)
and saves
a separate
text file for each article (the model
needs an entire
corpus of
papers to work with) naming them in the
format author-publication-year-title for identification
purposes when doing the evaluation.
Then we clean each document the same way as we
did for the full-text articles (tokenization,
removal
of
stop-words, lemmatization).
4.1.2
Training the model
Latent Dirichlet Allocation
The intent of using LDA in this study is to get topics
from the documents in the supplied data sets.
LDA
can do this through its probabilistic,
generative func-
tionalities.
So a trained LDA model
will
be able to
point out topics for documents[19].
There are however
quite a few ways of training an LDA model to achieve
the queryable functionalities [19].
Most of the ways of training a model boils down to
a guessing game.
This guessing game begins when for
every document every word has been assigned to a ran-
dom topic.
A topic is a list of
words and how many
instances of them there are.
A word can reoccur sev-
eral times in a topic, this gives it an increased chance to
be dominant within this topic, and get more instances
of itself within this topic.
Then the algorithm, for ev-
ery word in every document,
looks for what topic the
current word could fit in as well,
then moves it there.
Depending on how many instances of the current word
there are in that topic already,
the chance that the
word will
be moved there varies.
Now when the cur-
rent word is moved, the current words new topic which
has received the current word will
have an increased
chance of receiving another instance of this word.
In short the guessing game can be described as that
the LDA model
gets better at guessing as it keeps at
it, and a measurement of measuring how well a model
guesses is it’s perplexity value [3].
Using the Gensim Framework
Gensim is a framework that is accessible through the
programming language Python.
The Gensim frame-
work allows
the user
to build and train their
own
unique LDA model
based on the users own corpora.
Gensim also o↵ers other kinds of machine learning al-
gorithms outside the scope of LDA [19].
When creating an LDA model
with Gensim,
it re-
quires a corpus that has been tokenized.
In the case
of
AuTopEx,
the models trained are handed a num-
ber of the pre-processed text files.
AuTopEx can then
through the Gensim framework train an LDA model
given a sample from the whole corpus.
Throughout the training process, the Gensim frame-
work tells the user whether or not the model is improv-
ing by printing out what is called a perplexity measure
[2].
An indication of whether the model
is improving
is, if the perplexity measure is decreasing for each iter-
ation [3].
Then, when the perplexity measure is down to a pre-
determined value,
the LDA model
can be saved down
on the hard drive of
the users system and reused on
the entire corpus.
This is where AuTopEx can return
which topics are deemed most relevant for each docu-
ment.
To find measures that act as good examples when
training a model to perform as well as possible one can
observe Bleis experiments[3].
When asking for more
than a hundred topics in these experiments,
the per-
plexity measure is not improving as much anymore un-
like when the number of
topics approach a hundred.
Blei also presents in his paper that when training mod-
els, if training a model with a larger sample of ten per-
cent,
of the entire corpus,
the gain in accuracy is not
significant.
However,
approaching ten percent of
the
entire corpus for a training sample the gain in accu-
racy is certainly appealing [3].
4.1.3
Identifying and labeling relevant topics
The trained model
provides us with up to 100 topics,
each topic consisting of a set number of words.
A script
exports this data to a spreadsheet for easy access by
the annotators.
An example of
a complex topic (10 words)
out-
putted after training could look like this:
(0.005):
0.012*communication
+
0.009*channel
+ 0.008*packet
+ 0.008*velocity + 0.007*protocol
+ 0.006*follower + 0.006*platoon + 0.006*leader +
0.006*transmission + 0.006*controller.
What can be observed from this topic is that the
words that follow a number and a star is related to the
topic.
Inside this topic there are several
expressions,
for example ”0.012*communication”.
This expression
and all other expressions that follows inside this topic
will combined provide the interpreter guidance towards
labeling the topic.
At a first glance it seems like the topic could be
labeled as one of
the words that it already contains,
”Communication”.
The way this could be argued is be-
cause the topic also contains ”packet” and ”protocol”.
These words are tightly related with communication
solutions/properties in software and computers in gen-
eral.
At a closer look there are other options for the
label.
Since the topic contains ”platoon”,
”follower”
and ”leader” which all
probably refer to a platoon of
vehicles (the corpus being related to autonomous ve-
hicles).
The label
could then arguably be something
like ”Networked vehicles” or ”Cooperating Vehicles”.
Then again ”Transmission” might be related to com-
munication but could also refer to gearbox and we also
have ”velocity” and ”controller” in the topic.
As you can see the labeling phase can prove quite
difficult based on the number of
words and their se-
mantic relations.
We chose 7 words per topic but we encourage those
who want to try this approach to experiment with the
number of
words per topic.
In our experience,
with
fewer words the topics became more general (e.g.
”Net-
work”) and with more words the topics became more
specific (”Networked vehicles
grouped in platoon”).
Seven to us seemed like a good compromise because for
this specific corpus of papers it gave us a large amount
of varied topics.
It’s important to note that not all
topics from the
trained model
will
be interpret-able by humans.
This
is due to the generative and probabilistic nature of the
LDA model.
A model
will
produce a number of
bad
topics with low scores.
From our experience these top-
ics will never be assigned to papers during the classifi-
cation, so this is not a problem.
A very large majority of the topics from our corpus
were however indeed interpret-able and covered a wide
variety of
areas (please refer to the Appendix for ex-
amples of
the topics we got from the model
and how
we labeled them).
4.1.4
Querying the model
When an LDA model
is
finished and saved onto
the hard drive,
one can query this model
with pre-
processed documents in order to get the models opinion
of what topics might exist in each specific document.
As
an example,
when we
ask the
model
to re-
turn three possible topics for the paper ”A Real-Time
Multi-Sensor Fusion Platform for Automated Driving
Application Development”,
the model
outputs:
”(37,
0.81872937773255205),
(55,
0.078783842923631039),
(78, 0.034186934006349756)”
This means that according to the model,
topic 37
is the most probable topic for this document, followed
by topics 55 and 78.
These results are exported to
a spreadsheet,
where the researcher can look up the
Corpus of 2000 
research papers
on autonomous 
vehicles
50 papers chosen
at random for 
evaluation purposes
Each paper is pre-
processed 
(tokenization, 
lemmatization etc.)
Machine learning 
algorithm (Latent 
Dirichlet Allocation) 
is trained on 200 
of the papers
Results in trained model 
containing 100 topics:
1: [car, autonomous, drive]
2: [pattern, architecture, style]
… and so on ….
Categories manually labeled:
1 = “Autonomous driving”
2 = “Software architecture”
… and so on ….
Annotator #1 labels
all 50 papers using 
only labels from (E)
Inter-Annotator Agreement
(making sure both annotators agree 
about which topics are in which papers)
The three most dominant topics for each 
paper are chosen as the result.
Automatic classification of the 
50 random papers:
Paper 1 is about topic 4, 37, 78 
Paper 2 is about topic 0, 16, 54
...and so on ….
Final Evaluation:
How close was the results of the 
manual annotation to the automatic 
classification from LDA?
After pre-processing 
the papers, we ask 
the trained model (D) 
which topics they are 
about
Annotator #2 labels
all 50 papers using 
only labels from (E)
A
B
C
D
E
F
G
H
I
J
K
Evaluating automatic topic extraction technique AuTopEx
Figure 2:
A simplified overview of how we evaluate the AuTopEx approach.
labels corresponding to these numbers.
4.2
Evaluating AuTopEx
4.2.1
Setting up the evaluation
The data set consists of 425 scientific articles related to
autonomous vehicles.
These papers had been screened
based on certain inclusion and exclusion criteria for an
actual Systematic Mapping Study being performed by
researchers at Chalmers University of Technology, thus
we deemed it an excellent data set for performing our
evaluation.
For each of our two evaluations 200 of the 425 sci-
entific papers were selected at random for training the
LDA model.
This number was chosen because we ex-
pect the final
mapping study to include at least 2000
articles,
and it is considered good practice to use ten
percent of the data set for training purposes when im-
plementing LDA.
The 100 topics (containing 7 words each) from the
model are now manually labeled by the authors.
First
each author labels all
of
the topics on their own and
then check whether they disagree on any topic label.
Any disagreements are solved by discussing the topic
at hand.
The labeling phase is arguably the most dif-
ficult part of the entire process because it requires the
annotators to have very good language skills as well
as domain expertise.
More on that in the ”Threats to
Validity” section of this thesis.
4.2.2
Evaluation method
From the remaining 225 papers 50 are chosen at ran-
dom for evaluation purposes.
We use a Python script
for random selection as well, in order to eliminate any
potential
bias where an annotator could choose docu-
ments with very clear titles that were similar to the
topics we already knew existed in the corpus.
All
of
the 50 documents are now read and anno-
tated by each of the authors, if a document talks about
a topic labeled in the previous step,
it gets the same
label.
After the human labeling is completed we process
the same 50 documents using our trained LDA model.
A Python script exports the most probable topics for
each paper
to a spreadsheet.
We chose a two-fold
approach for both full-text and title-abstract-keyword
evaluations here:
First we export the three topics with
the highest probability weight according to the algo-
rithm.
Then we separately export all probable topics,
no matter how low the probability is.
This might give
us insights into both how the Gensim implementation
of
LDA works as well
as tell
us something about the
documents being analyzed (mainly the number of prob-
able topics per paper and their respective probability
weight according to the algorithm).
For the purpose of
supporting tasks such as docu-
ment classification in Systematic Mapping Studies we
are interested in knowing whether AuTopEx performs
better with a data set consisting of
full-text articles
or a set where the articles only contain titles, abstract
and keywords.
In order to evaluate this, as well as get-
ting a measure on how well the human annotators and
the system agree with each other, we use an evaluation
technique called precision and recall [1].
Before one can calculate the values for precision and
recall one must first collect the required data.
Rather
than just presenting this data in tabular form, it helps
to produce a confusion matrix, consisting of four fields.
See the model below as an example.
The four fields are
labeled true positive,
false positive,
true negative and
false negative.
In this
study,
true
positives
are
the
topics
that
are deemed by both the machine and the annotator
as
relevant
for
the
given articles.
False
negatives
are
topics
that
have
not
been deemed relevant
by
the machine but
have been deemed relevant
by the
human annotator.
False positives are topics that the
machine is returning as relevant topics but have not
been deemed relevant by a human annotator.
Lastly
true negatives, are basically just the rest of the topics
that
have
not
been returned by the
machine
and
that should not have been returned according to the
human.
Figure 3:
Example of the confusion matrix
Relevant?
Returned by LDA?
Yes
No
Yes
True
Positive
False
Negative
No
False
Positive
True
Negative
These boxes
would be filled with the values that has been described
previously in the respective box.
So to show an ex-
ample of how this would be performed, please refer to
the data supplied in the first appendix.
When look-
ing at the first sheet in this spreadsheet, there are four
columns, true positive, false negative, false positive and
true negative that are of importance.
The papers are
listed on the left and for each papers corresponding row
the values for each of
these elements are represented.
Since this study is focusing on how the di↵erent data
sets (full-text vs title, abstract and keywords) perform
against each other, one can observe at the bottom part
of
the sheets,
the sums of
all
the precision and recall
values are stored.
Here the values from the entire data
set are added together and presented.
It is these sums
of the true positives, false negatives, false positives and
true negatives for each data set that are used and later
presented inside these confusion matrices that is exem-
plified above.
When this data has been collected,
the following
equations can be applied to get the values of precision
and recall.
P recision =
T P
T P + F P
1
Recall =
T P
T P + F N
2
To bring a bit more clarity to what these values will
indicate in the case of this study, lets quickly summa-
rize.
Precision serves as an indication of
how many
of
the topics that are returned as relevant,
are truly
relevant.
Recall represents how many relevant topics
were returned by the system.
This study investigates if there is any preference for
what type of
documents to use when performing Au-
tomatic Topic Extraction.
Thus,
a value called an F-
measure,
which is a harmonic mean of
precision and
recall will be used in comparing the di↵erent results [1]
.
The F-measure can be a number between 0 and 1
and measures the accuracy of the test.
The closer the
result is to 1 the better.
F Measure = 2 ⇤
P recision ⇤ Recall
P recision + Recall
3
The harmonic mean from precision and recall gives
us a good measure of which method is better:
Apply-
ing LDA on full-text papers or on title,
abstract and
keywords.
With every query executed in the two LDA models
(one for full-text,
another for title,
abstract and key-
words) and all the human annotated data collected, we
will now outline what the confusion matrices looks like
with the corresponding values.
4.2.3
Evaluation 1:
All
LDA topics,
full-text
articles vs title, abstract & keywords
Figure 4:
Full text, all topics
Relevant?
Returned by LDA?
Yes
No
Yes
102
36
No
813
3198
The values generated by the table above is:
P recision =
102
102 + 813
= 0, 111
4
Recall =
102
102 + 36
= 0, 739
5
F Measure = 2 ⇤
0, 111 ⇤ 0, 739
0, 111 + 0, 739
= 0,193
6
Figure 5:
Title, abstract and keywords, all topics
Relevant?
Returned by LDA?
Yes
No
Yes
83
54
No
591
2273
The values generated by the table above is:
P recision =
83
83 + 591
= 0, 123
7
Recall =
83
83 + 54
= 0, 606
8
F Measure = 2 ⇤
0, 123 ⇤ 0, 606
0, 123 + 0, 606
= 0,204
9
Regarding ”RQ 2:
Which approach is better
for Automatic Topic Extraction:
a) Extraction
from title,
abstract
and keywords
or
b)
Ex-
traction from full
text paper?” The F-Measure is
slightly higher for title,
abstract and keywords.
How-
ever with such a small
di↵erence we can’t safely say
that one type is better than the other.
To answer ”RQ 3:
How well does the approach
of using Latent Dirichlet Allocation (with suit-
able
pre-processing)
perform compared to a
manual
method?” We assume that the human per-
formance is perfect, since that is what is accepted and
applied today in the Software Engineering domain.
So
when looking at the amount of
false negatives stored
(the amount of topics that should have been returned
by the machine,
but were not) in these two confusion
matrices.
The full-text gives us 36 and the title,
ab-
stract and keyword set 54.
So that tells us that full-text
data set returns the relevant topics more often than the
title,
abstract and keywords data set.
So the full-text
missed 36 topics that the humans had deemed relevant
and the title, abstract and keyword missed 54.
This is
the indication of how much the humans and the algo-
rithm disagree
4.2.4
Evaluation 2:
Top 3 LDA topics,
full-
text
articles
vs
title,
abstract
& key-
words
Figure 6:
Full text, top three topics
Relevant?
Returned by LDA?
Yes
No
Yes
46
103
No
103
3897
The values generated by the table above is:
P recision =
46
46 + 103
= 0, 309
10
Recall =
46
46 + 103
= 0, 309
11
F Measure = 2 ⇤
0, 309 ⇤ 0, 309
0, 309 + 0, 309
= 0,309
12
Figure 7:
Title, abstract and keywords, top three topics
Relevant?
Returned by LDA?
Yes
No
Yes
35
115
No
115
2735
The values generated by the table above is:
P recision =
35
35 + 115
= 0, 233
13
Recall =
35
35 + 115
= 0, 233
14
F Measure = 2 ⇤
0, 233 ⇤ 0, 233
0, 233 + 0, 233
= 0,233
15
In regards of ”RQ 2:
Which approach is better
for Automatic Topic Extraction:
a) Extraction
from title, abstract and keywords or b) Extrac-
tion from full text paper?” When we ask the model
to only return the three most probable topics per paper
we get a higher F-measure for the full-text articles and
the title,
abstract and keywords,
than when we asked
it to return all
topics.
This is probably because when
the Gensim framework only returns three topics, it re-
turns fewer false positives,
thus the value of precision
is higher.
Though due to probability, there is a smaller
chance for the annotators to agree with the machine
with only three returned topics.
So the recall
value is
smaller, due to the higher value of false negatives.
Full-text also performs somewhat better than title,
abstract and keywords when looking at the top 3 most
probable topics.
Regarding ”RQ 3:
How well does the approach
of using Latent Dirichlet Allocation (with suit-
able
pre-processing)
perform compared to a
manual
method?”
The
topics
that
the
machine
should have returned.
When only using the three most
likely topics, there are a lot more topics in the false neg-
ative boxes than when returning all topics.
There is a
bigger chance when returning all topics that the topic
the annotator deemed relevant will
show up.
How-
ever,
between the two data sets when only returning
the three most likely topics,
yet again,
the full-texts
model
returns more relevant topics than the title,
ab-
stract and keywords.
This is since the full-texts confu-
sion matrices only contains 103 false negatives and the
other 115.
4.2.5
Evaluation 3:
Most probable LDA topic,
full-text articles vs title, abstract & key-
words
However we are also interested in looking at the most
probable topic for each paper (the topic with the high-
est probability weight according to the algorithm) and
comparing this to the human evaluation.
Therefor (for each paper) we also do a simple binary
comparison to see if the most probable topic according
to the machine is among the three topics identified by
the human annotators.
Figure 8 provides a simplified overview of how this
evaluation was performed.
First we compared the la-
beling made by human annotators with the machines
categorization for the full-text articles and secondly we
compared the same results for title,
keyword and ab-
stracts.
For RQ 2:
Which approach is better for Au-
tomatic Topic Extraction:
a) Extraction from
title,
abstract and keywords or b) Extraction
from full text paper? its a bit difficult to motivate
using precision and recall
since if
the machine would
correctly return a relevant topic,
there would still
be
two false negatives left.
So a more simple approach is
applied for this evaluation.
One where if the machine
returned a topic that was among the three the humans
had deemed relevant it is labeled as a hit.
The data
Most probable topic according to the Model
Documents with a hit
Missed documents
Hit-ratio
Full-text articles
17
33
0,34
Title/abstract/key-words
13
37
0,26
Figure 8:
Only the top favorable topic returned from the queries
This is the result of a comparison of how often the most favorable topic returned from a query was among the three topics assigned
from the annotators
sets model with most hits should therefor have returned
the most relevant topic as their most probable topic.
In the case of this study, please refer to figure 8 to ob-
serve that the full-text has a hit rate of 0.34 and title,
abstract and keywords only have 0.26.
So in this case
it seems that the full-text data set has out performed
the title, abstract and keywords.
This
is
our
final
evaluation in regards
to ”RQ
3:
How well
does the approach of
using La-
tent
Dirichlet
Allocation (with suitable
pre-
processing)
perform compared to
a
manual
method?”.
We simply check if the most probable topic accord-
ing to LDA is among the three topics chosen by the
human annotators for each article (see figure 8).
Here
the model also performs slightly better on full-text ar-
ticles than on title, abstract and keywords.
For 17 out
of 50 documents, the most probable topic according to
the model
is also among the topics chosen by the an-
notators.
For title,
abstract and keywords.
the same
number is 14 out of 50.
This gives a hit-ratio of 0.34
for full-text and 0.26 for title, abstract and keywords.
5
Analysis & Discussion
5.1
Analysis
With the result from the human annotators com-
pared to the model, it seems fair to argue that the ma-
chine and humans agree more when both are supplied
the articles in their entirety.
From the evaluation results using Precision And Re-
call we can see that the algorithm performs better when
evaluating full-text articles rather than title,
abstract
and keywords and only looking at the top 3 topics.
When comparing the full-text, all topics result with
the Abstract and keywords,
all
topics result,
the F-
measure of
the lastly mentioned is however actually
0.011 higher than the F-measure of the full-text evalu-
ation.
The reason why it still seems fair to argue that the
full-text evaluation outperforms the Title, abstract and
keywords, is because of when the machine presents its
most probable choices of topics.
Then the F-measure is
much higher in the full-text evaluation.
Just to add to
this reasoning, another comparison was made with the
singular most probable topic according to the machines
and the annotators topics,
as shown in figure 8.
Yet
again (with other measurements however) it is clear
that when supplying full-text data sets to the machine,
it performs better.
Worth mentioning is that when the model for title,
abstract and keywords had been trained,
it generated
far fewer interpret-able topics when the time came to
label
them.
In fact,
for the full
text model,
83 clear
and usable topics were generated as for the abstract
and keywords model,
only 60 clear and usable topics
were generated.
So that explains the lower values of
the true negatives in the abstract and keywords data
sets.
Another reason why we wanted to compare the dif-
ferences between the results of asking the model for all
topics with the model’s top 3 topics was to show how
the Gensim implementation of LDA produces a lot of
topics for some documents with this data set.
A lot
of these topics get very low probability scores (see ap-
pendix) which is why there are a lot less false positives
when we just look at the top 3 topics.
5.2
Discussion
Using AuTopEx for Topic Extraction
With all
the tools in place a researcher only needs to
do the following in order to perform automatic topic
extraction:
1.
Batch-convert all desired pdf:s.
2.
Run the pre-processing script.
3.
Train the LDA model using part of the corpus.
4.
Query
the
model
with the
desired number
of
remaining documents from the corpus.
From our experience document conversion and text-
cleaning takes the longest time.
For a large corpus
(> 2000 scientific papers for example) each of
these
steps can take several
hours.
The researcher however
does not need to be present while the programs are run-
ning.
Training a model on 200 full-text papers took 40
minutes using a cheap laptop with a Celeron proces-
sor clocked at 2.0 GHz (utilizing two of the processor
cores).
Querying the trained model with 50 papers us-
ing the same computer is done in a couple of minutes.
Seeing as how it takes a human reader many hours
to read and annotate 50 scientific articles, using an ap-
proach such as AuTopEx can greatly speed up topic
extraction.
Especially during tasks that require a re-
searcher to read a large amount of
articles,
(such as
when doing document
classification in a Systematic
Mapping Study).
Of course this requires that the model classifies the
papers accurately enough,
and there is room for im-
proving AuTopEx here.
General Discussion
For the full
text evaluation,
the most probable topic
identified by the algorithm was indeed a topic in the
paper in 34 % of
the cases according to the human
annotators.
This might not sound as a huge percent-
age, but seeing as this was the very first evaluation of
the AuTopEx approach it seems very promising.
Es-
pecially when one compares the many hours it takes
for a human to read 50 scientific papers compared to
the mere minutes it took the algorithm to produce this
result.
It can be a good idea to perform word analysis on
the corpus using NLTK after text pre-processing,
for
example checking a lot of the most popular words in the
corpus.
While time-consuming it can give insights into
if some of the pre-processing steps might need adjusted.
For example, perhaps there are still words in the corpus
that could be considered stop words.
If batch-converting a large number of documents we
recommend that
the file sizes of
the documents are
checked afterwards.
If any of the text-files have a size
of 0 kilobytes the conversion has failed.
Discussion on Topics and their labeling
Labeling topics manually when performing evaluation
can be a very difficult task.
It requires both language
skills as well
as domain knowledge.
Sometimes the
words in a topic are acronyms or words that have no
meaning to those not familiar with the domain.
Mak-
ing sure that you found the correct meaning of
the
acronym (often an acronym has a number of meanings
in a multitude of fields) or finding an explanation of a
very niche word can be quite time consuming.
Interestingly enough adjectives
were very uncom-
mon in the results
from our
corpus.
Besides
”au-
tonomous”,
which came up in 17 topics,
the results
were dominated by nouns,
followed by verbs.
For
the full-text experiment only two other adjectives ap-
peared,
”intelligent” and ”content”,
and the latter is
also a noun.
For title/abstracts/keywords the adjec-
tives were more varied:
”Intelligent”, ”dynamic”, ”gen-
eralized”,
”industrial”,
”artificial”,
”automatic” and
”natural” appeared.
The word ”real” appeared three
times (and was always accompanied by ”time” in the
same topic).
The dominance of nouns was quite helpful when la-
beling the technology-oriented topics often found in the
software engineering domain.
This is especially true
when doing classification that does not take positives
and negatives into account (we don’t need colorful ad-
jectives criticizing or praising something in a topic).
Words like ”car”,
”architecture” or ”network” tells us
a great deal
on their own.
Verbs are helpful
in a sup-
porting role (such as ”driving” appearing in a topic
with ”autonomous” and ”vehicle”).
Another interesting note was that even though the
entire corpus consisted of scientific papers, none of the
topics produced in either of the two evaluation experi-
ments were about scientific methodology.
This is useful
data to extract when performing tasks like Systematic
Mapping Studies.
We found it interesting how the LDA model
pro-
duced a lot of
potential
topics with low probabilities
on the corpus on autonomous vehicle research.
This
could be due to how the scientific articles are written,
but requires further study before any conclusions can
be drawn.
5.3
Threats to Validity
It’s important to remember that LDA is a proba-
bilistic topic model, thus we are dealing with probabil-
ities.
If a human claims that a paper is about a certain
topic and the machine claims that this probability is
high,
we only argue that the likelihood of
this to be
true is very high.
Properly labeling topics
and scientific papers
re-
quires a lot from the human annotators.
They must
have excellent language skills as well as domain exper-
tise in order to interpret each topic supplied by the
model.
One misunderstanding of
a word could result
in an improper label, and this could impact the results
of the evaluation.
We mitigated this
by reading about
concepts
we
were not familiar with before finishing the topic la-
beling,
and looking up the meaning of
any acronyms
that appeared in the results.
Both authors are soft-
ware engineering students.
Having previously studied
concepts such as image processing or lane following for
autonomous vehicles meant that we had a good under-
standing of a large majority of the topics produced by
the model.
Then again,
labeling 100 topics and reading 50 sci-
entific articles (for two separate evaluations) can be dif-
ficult for humans.
Stress, fatigue or just having a bad
day can impact the accuracy both when performing
topic labeling and when manually assigning topics to
documents.
We tried to mitigate this by taking breaks
regularly during the evaluation.
However if
other re-
searchers would redo our evaluation,
using the same
articles, we can not say for certain that they would la-
bel
every single topic or classify the papers in exactly
the same way.
Our main mitigation strategy for human error was
that there were two of us doing the same work in par-
allel.
We continuously compared the results between
ourselves and where there were any disagreements re-
garding topic labels or which topics belong to a certain
paper, we tried to reason with each other until we came
to a result we could both agree on.
Another thing to consider when performing this kind
of automatic topic extraction is that there is no way of
handling positives and negatives.
A paper that deals
with a certain topic may actually reject the idea behind
that topic.
We mitigate this by not making any spe-
cific claims regarding the documents.
We only state
that in the results where human and machine agree
that a topic exists in a document, that topic is indeed
discussed in that specific document.
AuTopEx has only been evaluated on a corpus in
the scientific domain of
Autonomous
Vehicles.
We
can’t say with certainty how di↵erent the evaluation
results would be if
applying the approach on corpora
from other domains.
However,
steps have been taken
to make AuTopEx as generally applicable as possible.
Especially by only limiting the tokenization to alpha-
betical
words and using a very general
stop word list.
We recommend that anyone who uses this approach
carefully consider if there are any special
measures to
be taken in the text pre-processing stage (e.g.
adding
words to the stop words list).
During the testing phase,
we noticed that on some
occasions several words would appear together as a sin-
gle token after the texts had been cleaned and we sus-
pect this is due to bad quality of some of the original
pdf:s.
While it would be far too time-consuming to
check the entire corpus manually for this we believe
that this should very seldom occur in the data set we
used for evaluation.
This is because this data has been
screened by researchers and only contains pdf:s pub-
lished in 2005 or later.
6
Conclusions and Future Work
In this thesis we presented an approach for Auto-
matic Topic Extraction which we call AuTopEx.
This
approach uses Natural Language Processing tools and
techniques to pre-process the scientific articles of a cor-
pus.
Topics from this corpus are then extracted by
training and querying a Latent
Dirichlet
Allocation
(LDA) model.
This model
can be used to automati-
cally classify the documents of the corpus (identifying
which topics exist in which articles).
According to our results,
Automatic Topic Extrac-
tion with Latent Dirichlet Allocation works better on
full-text scientific articles than documents that consist
of title, abstract and keywords.
This is true both when
querying the model for the most probable topic per ar-
ticle as well
as when asking the model
for the three
most probable topics per article.
In our evaluation,
the model’s most probable topic
was among the three relevant topics (according to the
human annotators) in 34 % of the full-text documents
evaluated.
While the model is not as accurate than the
human annotators it is important to note that this was
the first evaluation of AuTopEx and perhaps most im-
portant of all:
The model does this work in a couple of
minutes while it takes humans many hours to perform
the same task.
We believe that
by refining this
approach it
will
be possible to speed up topic extraction tremendously
compared to manually reading and annotating papers.
Future work
One
possible
future
experiment
could be
to allow
the use of
n-grams in the data set before performing
the machine learning algorithm.
If
for example ”au-
tonomous vehicle” was considered a single word it could
free up more space for other words to occur together
with it in topics, possibly allowing for more meaningful
interpretations by human readers.
This process could
also easily be automated.
NLTK for example has the
tool
Collocations which performs n-gram analysis on
documents.
Another idea that could possibly improve the results
of
our approach is to apply tf-idf
on the text corpus
before training the model.
Tfidf is the product of two
statistics,
term frequency and inverse document fre-
quency.
Term Frequency is the number of times a term
occurs in a document.
Inverse Document Frequency is
a factor that diminishes the weight of terms that occur
very frequently in the document set and increases the
weight of
terms that occur rarely.
Thus a word like
”the” will have a very low weight in tf-idf.
A high weight in tfidf is reached by a high term fre-
quency (in the given document) and a low document
frequency of
the term in the whole collection of
doc-
uments;
the weights hence tend to filter out common
terms.
This could potentially be used for stop word
removal.
The results would depend on how focused the lan-
guage is
in the di↵erent
articles.
An article which
uses very broad language (using many synonyms for
the same word) will
produce di↵erent results than an
article with very focused language.
One idea could
also be to duplicate the title of the paper a couple of
times in each document before applying tf-idf.
Seeing
as how the title should reflect what the text is about
this would help ensure that the most important words
of the papers get a higher weight.
Another experiment
with tf-idf could be to give all nouns and verbs higher
weight since they convey a lot of
information about
technologically-oriented topics.
A
domain-specific
lemmatizer
for
text
pre-
processing
could be
useful.
This
would however
require a lot of
work by several
domain experts for
a gold standard to be
achieved and might
be
an
unrealistic thing to wish for.
Automatization of the labeling stage could make the
threat towards validity smaller while making the entire
process quicker and easier to use,
since there is less
required input from the user.
Such tools are already
being applied[17].
Acknowledgements
We would like to thank Peter Ljungl¨of of Chalmers
University, Simon Dobnik of the University of Gothen-
burg and Victor Botev from Iris AI for their kind as-
sistance.
We would also like to thank our supervisors
Alessia Knauss and Hang Yin of Chalmers University
for their invaluable feedback.
References
[1]
S.
Bird,
E.
Klein,
and L.
Edward.
Analyzing Text
with the Natural
Language Toolkit.
O’Reilly Media,
Sebastopol, California, United States, 2009.
[2]
M. D. Blei. Introduction to probabilistic topic models.
Prinston University, pages 1–16, 2011.
[3]
M. D. Blei and et.al. Latent dirichlet allocation. Jour-
nal
of Machine Learning Research, (3), 2003.
[4]
J.
Chang,
J.
Boyd-Graber,
and et.al.
Reading tea
leaves:
How humans interpret topic models.
Neural
Information Processing Systems, pages 1–9, 2009.
[5]
R.
Felizardo,
Katia,
N.
Salleh,
M.
Martins,
Rafael,
E.
Mendes,
G.
MacDonell,
Stephen,
Maldonado,
and
C. Jose. Using visual text mining to support the study
selection activity in systematic literature reviews.
In-
ternational
Symposium on Empirical
Software Engi-
neering and Measurement, pages 77–86, 2011.
[6]
P.
Fernando,
V,
C.
Maria,
O.
F,
and M.
Rosane.
The projection explorer:
A flexible tool for projection-
based multidimensional
visualization.
Analytical
and
Bioanalytical Chemistry Volume 400, Number 4, pages
1153 – 1159, 2011.
[7]
R.
Giuseppe and et.al.
Semantic enrichment for rec-
ommendation of
primary studies in a systematic lit-
erature review.
Digital
Scholarship in the Humanities
Advance Access, pages 1–14, 2015.
[8]
Henver, S. T. March, J. Park, and S. Ram. Design sci-
ence in information systems research.
MIS Quarterly,
pages 1–14, 2004.
[9]
A.
Hindle
and et.al.
On the
naturalness
of
soft-
ware. International Conference on Software Engineer-
ing (ICSE), pages 837–847, 2012.
[10]
N.
Jan van Eck and L.
Waltman.
Text mining and
visualization using vosviewer.
ISSI newsletter,
pages
50–54, 2011.
[11]
N.
Jan van Eck and L.
Waltman.
Citnetexplorer:
A
new software tool
for analyzing and visualizing cita-
tion networks. Journal of Informetrics, pages 802–823,
2014.
[12]
N.
Jan van Eck and L.
Waltman.
Visualizing biblio-
metric networks. In Y. Ding, R. Rousseau, & D. Wol-
fram (Eds.), Measuring scholarly impact:
Methods and
practice.
Springer Publishing Company, 11 West 42nd
Street, 15th Floor New York, NY 10036, 2014.
[13]
A. G. Jivani.
A comparative study of stemming algo-
rithms. International Journal of Computer Technology
and Applications, (Vol 2:
Issue 6), 2011.
[14]
M.
Liakata,
S.
Dobnik,
S.
Saha,
C.
Batchelor,
and
D.
Rebholz-Schuhmann.
A discourse-driven content
model
for summarising scientific articles evaluated in
a complex question answering task.
Proceedings of
the 2013 Conference on Empirical
Methods in Natu-
ral
Language Processing, page 747757, 2013.
[15]
M.
Liakata,
S.
Saha,
S.
Dobnik,
B.
Colin,
and
D.
Rebholz-Schuhmann.
Automatic
recognition of
conceptualization zones in scientific articles and two
life science applications.
Bioinformatics,
pages 991–
1000, 2012.
[16]
C.
Marshall
and P.
Brereton.
Tools to support sys-
tematic literature reviews in software engineering:
A
mapping study. 2013 ACM / IEEE International Sym-
posium on Empirical
Software Engineering and Mea-
surement, pages 296 – 299, 2013.
[17]
Q. Mei, X. Shen, and C. Zhai.
Automatic labeling of
multinomial topic models.
pages 1–10, 2007.
[18]
K. Petersen,
R. Feldt,
S. Mujtaba,
and M. Mattsson.
Systematic mapping studies in software engineering.
pages 1–10, 2008.
[19]
R.
Rehurek and P.
Sojka.
Software framework for
topic modelling with large corpora.
Natural
Language
Processing Laboratory Masaryk University, pages 1–5,
2010.
title
true positive
false negative
false positive
true negative
total number of t 17 skräp
83 bra topics
Fisheye optics for omnidirectional perception
2
1
16
64
18
Data age based retransmission scheme for reliable
control data exchange in platooning applications
2
0
19
62
21
89, 4, 16, 52,
Obstacle Avoidance in Real Time with Nonlinear
Model Predictive Control of Autonomous Vehicles
3
0
15
65
18
Intelligent Cruise Control
Stop and Go with and without Communication
3
0
13
67
16
Autonomous Navigation: Achievements in Complex Enviro
1
0
25
57
26
Bayesian Network Based Collision Avoidance
2
1
17
63
19
Experience, Results and
Lessons Learned from
Automated Driving on
Germany’s Highways
3
0
19
61
22
Multi-Objective Path Planning using Spline Represent
3
0
20
60
23
A Study on Autonomous Vehicle Development Process at
University*
3
0
13
67
16
Road Surface Recognition Using Laser Radar for
Automatic Platooning
1
2
20
60
21
Building a Prototype for Power-Aware Automatic
Parking System
2
0
19
62
21
A Computer Vision System for Detection and
Avoidance for Automotive Vehicles
3
0
11
68
15
Path Tracking of Autonomous Ground Vehicle Based on
Fractional Order PID Controller Optimized by PSO
2
0
12
69
14
Off-road Path Following using Region Classification and G
Constraints∗
3
0
19
61
22
Self-Tuning PID Controller for
Autonomous Car Tracking in Urban Traffic
2
1
18
62
20
Shared Control of Autonomous Vehicles
based on Velocity Space Optimization
2
1
19
61
21
A 13,000 km Intercontinental Trip with Driverless Vehicles:
2
1
11
69
13
Real-Time Coordination of Autonomous Vehicles
1
1
17
64
18
Accurate and Efficient Traffic Sign Detection Using Discrim
3
16
64
19
DeepDriving: Learning Affordance for Direct Perception in
2
1
16
64
18
A Robust Algorithm for the Detection of Vehicle Turn Signa
2
0
5
76
7
Constrained Global Path Optimization for Articulated Steeri
3
0
19
61
22
360◦ detection and tracking algorithm of both pedestrian an
using fisheye images
3
0
15
64
19
State your position
2
0
18
63
20
A robotic platform to evalute autonomous driving systems
3
0
17
61
22
Coordinated control of multiple vehicles with
discrete-time periodic communications
1
3
17
63
17
89, 4, 16, 52,
Real-time Implementation of a Novel Safety Function for Pr
3
0
12
68
15
Coordinated Path Following Control for a Group of Car-like
1
2
9
71
10
A Combined Model- and Learning-Based Framework for In
2
1
17
63
19
Towards a Framework for Testing Drivers’ Interaction with
1
1
16
65
17
Adopting WirelessHART for In-Vehicle-Networking
2
0
18
63
20
Terrain Mapping for Off-road Autonomous Ground Vehicle
3
0
18
62
21
Incremental Sampling-based Algorithm for
Minimum-violation Motion Planning
1
2
21
58
23
Vision-based Nighttime Vehicle Detection and Range Esti
3
0
19
62
21
Design and Comparative Analysis of a Driveless LED light
0
3
4
76
4
Local Path Planning for Off-Road Autonomous
Driving With Avoidance of Static Obstacles
3
0
24
56
27
HOG Based Multi-object Detection for Urban Navigation
2
1
17
63
19
Genetic Algorithm Approach for Locating Automatic Vehicl
Identification Readers
0
1
17
65
17
Reliable Intersection Protocols Using Vehicular Networks
3
0
11
69
14
INTELLIGENT TRAFFIC WITH CONNECTED
VEHICLES
2
1
18
62
20
MCMC Particle Filter for Real-Time Visual Tracking of Vehi
2
1
22
58
24
Globally Asymptotically Stable Filter for Navigation aided b
and Depth Measurements
0
3
22
58
22
A Real-Time Multi-Sensor Fusion Platform for Automated
1
2
2
78
3
A full-3D Voxel-based Dynamic Obstacle Detection for
Urban Scenario using Stereo Vision
3
0
11
69
14
A Real-Time Trajectory Control of Two Driving Mobile Rob
1
2
18
64
19
Vehicle Automation in Cooperation with V2I and
Nomadic Devices Communication
2
1
19
61
21
Automatic vehicle classification and tracking method for ve
movements at signalized intersections
3
0
19
61
22
Multi-Target Tracking using a 3D-Lidar Sensor for Autono
1
2
19
61
20
Traffic Sign Representation using Sparse-Representations
1
1
20
61
21
Speed Profile Optimization for Vehicles Crossing an
Intersection Under a Safety Constraint
3
0
14
66
17
Sum
102
36
813
3198
918
true positive
false negative
false positive
true negative
Precision =
0,111
Recall =
0,739
F =
0,1930094118
total number of topics
title
true positive
false negative
false positive
true negative
total number of topics 17 skräp
83 bra topics
Fisheye optics for omnidirectional perception
1
2
2
78
3
Data age based retransmission scheme for reliable
control data exchange in platooning applications
2
0
0
80
3
Obstacle Avoidance in Real Time with Nonlinear
Model Predictive Control of Autonomous Vehicles
0
3
3
77
3
Intelligent Cruise Control
Stop and Go with and without Communication
0
3
3
77
3
Autonomous Navigation: Achievements in Complex Enviro
0
3
3
77
3
Bayesian Network Based Collision Avoidance
1
2
2
78
3
Experience, Results and
Lessons Learned from
Automated Driving on
Germany’s Highways
1
2
2
78
3
Multi-Objective Path Planning using Spline Represent
1
2
2
78
3
A Study on Autonomous Vehicle Development Process at
University*
0
3
3
77
3
Road Surface Recognition Using Laser Radar for
Automatic Platooning
1
2
2
78
3
Building a Prototype for Power-Aware Automatic
Parking System
1
2
2
78
3
A Computer Vision System for Detection and
Avoidance for Automotive Vehicles
1
2
2
78
3
Path Tracking of Autonomous Ground Vehicle Based on
Fractional Order PID Controller Optimized by PSO
0
3
3
77
3
Off-road Path Following using Region Classification and G
Constraints∗
1
2
2
78
3
Self-Tuning PID Controller for
Autonomous Car Tracking in Urban Traffic
1
2
2
78
3
Shared Control of Autonomous Vehicles
based on Velocity Space Optimization
1
2
2
78
3
A 13,000 km Intercontinental Trip with Driverless Vehicles:
0
3
3
77
3
Real-Time Coordination of Autonomous Vehicles
0
3
3
77
3
Accurate and Efficient Traffic Sign Detection Using Discrim
0
3
3
77
3
DeepDriving: Learning Affordance for Direct Perception in
1
2
2
78
3
A Robust Algorithm for the Detection of Vehicle Turn Signa
2
1
1
79
3
Constrained Global Path Optimization for Articulated Steeri
2
1
1
79
3
360◦ detection and tracking algorithm of both pedestrian an
using fisheye images
1
2
2
78
3
State your position
1
2
2
78
3
A robotic platform to evalute autonomous driving systems
1
2
2
78
3
Coordinated control of multiple vehicles with
discrete-time periodic communications
2
1
1
79
3
Real-time Implementation of a Novel Safety Function for Pr
1
2
2
78
3
Coordinated Path Following Control for a Group of Car-like
1
2
2
78
3
A Combined Model- and Learning-Based Framework for In
1
2
2
78
3
Towards a Framework for Testing Drivers’ Interaction with
0
3
3
77
3
Adopting WirelessHART for In-Vehicle-Networking
3
0
0
80
3
Terrain Mapping for Off-road Autonomous Ground Vehicle
0
3
3
77
3
Incremental Sampling-based Algorithm for
Minimum-violation Motion Planning
1
2
2
78
3
Vision-based Nighttime Vehicle Detection and Range Esti
2
1
1
79
3
Design and Comparative Analysis of a Driveless LED light
0
3
3
77
3
Local Path Planning for Off-Road Autonomous
Driving With Avoidance of Static Obstacles
2
1
1
79
3
HOG Based Multi-object Detection for Urban Navigation
0
3
3
77
3
Genetic Algorithm Approach for Locating Automatic Vehicl
Identification Readers
0
3
3
77
3
Reliable Intersection Protocols Using Vehicular Networks
2
1
1
79
3
INTELLIGENT TRAFFIC WITH CONNECTED
VEHICLES
1
2
2
78
3
MCMC Particle Filter for Real-Time Visual Tracking of Vehi
0
3
3
77
3
Globally Asymptotically Stable Filter for Navigation aided b
and Depth Measurements
0
3
3
77
3
A Real-Time Multi-Sensor Fusion Platform for Automated
1
2
2
78
3
A full-3D Voxel-based Dynamic Obstacle Detection for
Urban Scenario using Stereo Vision
1
2
2
78
3
A Real-Time Trajectory Control of Two Driving Mobile Rob
2
1
1
79
3
Vehicle Automation in Cooperation with V2I and
Nomadic Devices Communication
2
1
1
79
3
Automatic vehicle classification and tracking method for ve
movements at signalized intersections
1
2
2
78
3
Multi-Target Tracking using a 3D-Lidar Sensor for Autono
1
2
2
78
3
Traffic Sign Representation using Sparse-Representations
1
2
2
78
3
Speed Profile Optimization for Vehicles Crossing an
Intersection Under a Safety Constraint
1
2
2
78
3
Sum
46
103
103
3897
true positive
false negative
false positive
true negative
total number of topics
Precision =
0,309
Recall =
0,309
F =
0,309
title
true positive
false negative
false positive
true negative
40 skräp
60 bra topics
A._B._P._C._S._D._M._C._
1
2
14
43
15
A._B._S._D._M._P._P._P._
2
1
9
48
11
A._B.-N._C._Grand_2012_
3
0
11
46
14
A._C._C._D._Gillet_2014_S
3
0
12
45
15
A._C._L._N._S._M._M._N._
0
3
8
49
8
B._B._H._Giese_2008_Incr
1
0
15
44
16
B._W._A._K._M._P._T._A._
0
2
16
42
16
B.-M._S._Chung,_Jin-Woo;
0
3
12
45
12
C._C._J._Liu_2010_A_Rein
3
0
13
44
16
C._C._Y._H._F._G._C._B._
2
1
12
45
14
C._L._B._N._T._M._C._S._
2
1
14
43
16
C._W._Axelrod_2015_Enfor
0
1
16
43
16
D._B._W._M._I._Posner_20
3
0
13
44
16
D._C._S._D._B._P._Stone_
3
0
8
49
11
G._A._J._I._N._E._M._Neb
2
1
13
44
15
H._x._E._C,_;ne,;T._Sattler;
2
1
14
43
16
J._A._C._S._A._Pascoal_2
3
0
10
47
13
J._C._S._U._B._L._M._Mau
3
0
11
46
14
J._S._B._P._H._H._Chen_2
3
0
12
45
15
K._B._H._M._A._Zell_2012
1
1
9
49
10
K._C._F._J._T._R._S._J._B
2
1
14
43
16
L._C._A._F._L._Pallottino_2
1
1
15
43
16
L._x._F._J._M._Alvarez;F._
2
1
10
47
12
M._A._A._R._M._J._M._Ekl
2
1
9
48
11
M._A._J._M._Dolan_2011_
1
2
11
46
12
M._A._P._F._C._O._J._Sjo
1
2
13
44
14
M._A.-M._W._S._M._Y._W
1
2
9
48
10
M._B._C._H._A._L._M._A._
2
1
15
43
16
M._B._Z._G._P._Z._M._B._
0
3
11
46
11
M._C._D._P._M._Pasquier_
0
3
15
42
15
M._H._Ang_2015_Achievin
0
0
14
46
14
M._J._B._C._M._Veth_201
1
1
11
47
12
M._J._H._Berg;R._Olsson;
1
2
10
47
11
M._x._E._A,_;yr,;x00E,;M._
0
1
5
54
5
N._C.-B._A._M._J._R._M._
3
0
13
44
16
N._T._Atsuhiro,_Yamaguchi
2
1
10
47
12
P._B._D._K._C._B._J._Dick
3
0
13
44
16
P._V._K._B._S._Vidas_201
2
1
14
43
16
P._V._M._E._O._J._R._d._
2
1
11
46
13
Q._B._M._P._C._Laugier_2
1
2
5
52
6
S._A._G._B._R._R._P._Mu
2
1
11
46
13
S._A._S._C._Y._S._Alj_201
2
1
9
48
11
S._B._M._M.-P._R._M.-P._
2
1
14
43
16
S._D._B._B._E._A._Speran
1
2
15
42
16
S._J._A._S._B._K._K._I._J.
1
2
6
51
7
S._P._B._R._W._Sadowski
1
2
15
42
16
T._A._M._M._M._Ali_2015_
2
1
14
43
16
Y._A._P._P._F._P._A._Burr
2
1
14
43
16
Z._B._J._J._N._Y._S._Linc
3
0
10
47
13
Z._K._x._E._T._Akg;x00Fc,
3
0
13
44
16
Sum
83
54
591
2273
673
true positive
false negative
false positive
true negative
Precision =
0,123
Recall =
0,606
F =
0,2044938272
total number of topics
total number of topics
title
true positive
false negative
false positive
true negative
40 skräp
60 bra topics
A._B._P._C._S._D._M._C.
0
3
3
54
3
A._B._S._D._M._P._P._P.
1
2
2
55
3
A._B.-N._C._Grand_2012
1
2
2
55
3
A._C._C._D._Gillet_2014_
0
3
3
54
3
A._C._L._N._S._M._M._N.
1
2
2
55
3
B._B._H._Giese_2008_Inc
0
3
3
54
3
B._W._A._K._M._P._T._A.
1
2
2
55
3
B.-M._S._Chung,_Jin-Woo
1
2
2
55
3
C._C._J._Liu_2010_A_Rei
2
1
1
56
3
C._C._Y._H._F._G._C._B.
1
2
2
55
3
C._L._B._N._T._M._C._S.
0
3
3
54
3
C._W._Axelrod_2015_Enf
0
3
3
54
3
D._B._W._M._I._Posner_2
1
2
2
55
3
D._C._S._D._B._P._Stone
0
3
3
54
3
G._A._J._I._N._E._M._Ne
1
2
2
55
3
H._x._E._C,_;ne,;T._Sattle
2
1
1
56
3
J._A._C._S._A._Pascoal_
0
3
3
54
3
J._C._S._U._B._L._M._Ma
3
0
0
57
3
J._S._B._P._H._H._Chen_
0
3
3
54
3
K._B._H._M._A._Zell_201
1
2
2
55
3
K._C._F._J._T._R._S._J._
1
2
2
55
3
L._C._A._F._L._Pallottino
1
2
2
55
3
L._x._F._J._M._Alvarez;F.
1
2
2
55
3
M._A._A._R._M._J._M._E
0
3
3
54
3
M._A._J._M._Dolan_2011
0
3
3
54
3
M._A._P._F._C._O._J._Sj
0
3
3
54
3
M._A.-M._W._S._M._Y._
1
2
2
55
3
M._B._C._H._A._L._M._A.
0
3
3
54
3
M._B._Z._G._P._Z._M._B.
0
3
3
54
3
M._C._D._P._M._Pasquier
0
3
3
54
3
M._H._Ang_2015_Achievi
1
2
2
55
3
M._J._B._C._M._Veth_20
1
2
2
55
3
M._J._H._Berg;R._Olsson;
0
3
3
54
3
M._x._E._A,_;yr,;x00E,;M.
2
1
1
56
3
N._C.-B._A._M._J._R._M.
2
1
1
56
3
N._T._Atsuhiro,_Yamaguc
0
3
3
54
3
P._B._D._K._C._B._J._Dic
1
2
2
55
3
P._V._K._B._S._Vidas_20
1
2
2
55
3
P._V._M._E._O._J._R._d.
0
3
3
54
3
Q._B._M._P._C._Laugier_
0
3
3
54
3
S._A._G._B._R._R._P._M
1
2
2
55
3
S._A._S._C._Y._S._Alj_20
1
2
2
55
3
S._B._M._M.-P._R._M.-P.
0
3
3
54
3
S._D._B._B._E._A._Spera
0
3
3
54
3
S._J._A._S._B._K._K._I._
1
2
2
55
3
S._P._B._R._W._Sadowsk
0
3
3
54
3
T._A._M._M._M._Ali_2015
1
2
2
55
3
Y._A._P._P._F._P._A._Bu
0
3
3
54
3
Z._B._J._J._N._Y._S._Lin
2
1
1
56
3
Z._K._x._E._T._Akg;x00F
1
2
2
55
3
SUM:
35
115
115
2735
true positive
false negative
false positive
true negative
Precision =
0,233
Recall =
0,233
F =
0,233
total number of topics
total number of topics
Fisheye optics for omnidirectional p
96, 70, 99
[(4, 0.036466422706146216), (5, 0.0472016135189803), (11, 0.13900239476104639), (12, 0.029865706520154144), (27, 0.010409100887905174), (32, 0.084519107906727814), (45, 0.016817932625549693), (55,
0.013088153836531176), (74, 0.064007961547378395), (76, 0.018506583543245914), (82, 0.1411648197999629), (87, 0.011917356865013553), (88, 0.068743207945819576), (94, 0.019468683552120121), (95,
0.02711527543512194), (96, 0.017129065436825901), (98, 0.077472637445050868), (99, 0.10121410958494333)]
Data age based retransmission sch
control data exchange in platooning
86, 44
[(4, 0.028874553239211141), (17, 0.036700693541632176), (27, 0.051178480694453223), (28, 0.03323451345464553), (30, 0.026608255782015682), (32, 0.029104856259469347), (34, 0.01311551017166334), (38,
0.068027477351375265), (42, 0.015190498129589438), (44, 0.20299343908608297), (50, 0.010744316255323219), (55, 0.065263830575791854), (59, 0.028489688181890422), (74, 0.018485855323466419), (76,
0.027724536809738687), (79, 0.037221272041805899), (82, 0.036133837790049167), (86, 0.073215505908526407), (88, 0.035590403541733755), (94, 0.045106365491891669), (95, 0.028930605069243616)]
Obstacle Avoidance in Real Time w
Model Predictive Control of Autono
78, 82, 57
(5, 0.038386696472847086), (9, 0.010653013320034176), (11, 0.015705371277200043), (12, 0.074008878469029024), (18, 0.023320591029763964), (25, 0.012392437803801231), (28, 0.036679371745214052), (33,
0.016290558909931193), (34, 0.020750925931566799), (52, 0.16088388134972184), (55, 0.019212512339675324), (57, 0.044130775987559531), (60, 0.015441841073421137), (62, 0.1970401075172302), (78,
0.039305141948395975), (82, 0.042453862475432154), (88, 0.01197853204646558), (95, 0.025287535878553612)
Intelligent Cruise Control
Stop and Go with and without Com
52, 55, 44
(5, 0.027639745343676494), (12, 0.019188336292638938), (25, 0.022301148623795883), (34, 0.023653000333320826), (38, 0.062647574228685471), (42, 0.1047802285152695), (44, 0.24323481593723542), (45,
0.01105384319615201), (47, 0.014187994875867088), (52, 0.041044422178386171), (55, 0.096569676833372989), (62, 0.051286541539141001), (66, 0.044549370663455851), (74, 0.011289702315730761), (76,
0.023121592729004666), (82, 0.11078935942794398)
Autonomous Navigation: Achievem
87
(1, 0.041950717160334883), (5, 0.010300697610600502), (9, 0.040233786042984013), (11, 0.022626840040364003), (12, 0.021829330187039923), (16, 0.013685418088077422), (18, 0.02784126516350896), (23,
0.030808531599387644), (35, 0.05633561956051656), (37, 0.016398928036397028), (48, 0.076914728007271754), (49, 0.010440374939072246), (54, 0.015628965133728447), (56, 0.014868045871999306), (57,
0.011756251047147894), (60, 0.025950975042395259), (73, 0.023509069434499735), (74, 0.062377483155007782), (82, 0.093557421418064252), (87, 0.02629714459123652), (88, 0.057810169715968494), (94,
0.032306354359161744), (95, 0.036060637807091789), (96, 0.051801132357493763), (98, 0.10503959543129056)
Bayesian Network Based Collision
95, number 4, 5
(1, 0.048407697513722696), (4, 0.010054351700983614), (5, 0.037279421921883556), (8, 0.01580305830699711), (9, 0.052609799982792478), (11, 0.023002530350174518), (23, 0.047161989087160086), (36,
0.013184448577781963), (38, 0.1212474391329631), (44, 0.039765645788108109), (48, 0.0187636440801511), (62, 0.040718541062042923), (66, 0.017411913389307275), (70, 0.078966788461138457), (82,
0.22744132992600527), (88, 0.057750513253375095), (92, 0.020609681909379281), (94, 0.023946244138114268), (98, 0.033324428046976849)
Experience, Results and
Lessons Learned from
Automated Driving on
Germany’s Highways
37, 55, 62
(11, 0.07218370756186919), (12, 0.013884871424249684), (17, 0.040644224118018893), (18, 0.023565500075658212), (34, 0.01689896943614818), (35, 0.046380143698396721), (37, 0.033373689510276944), (44,
0.029536467066443986), (45, 0.018777593451539645), (52, 0.01282188336300338), (55, 0.084793539320267744), (60, 0.041352907907479801), (62, 0.047591211438436129), (66, 0.018452112211078339), (74,
0.027204513160765637), (76, 0.018902114192998896), (77, 0.019492068586233188), (79, 0.026628715039458694), (82, 0.18936225101487586), (88, 0.018747904509202124), (94, 0.016240093882527874), (98,
0.10634841700615484)
Multi-Objective Path Planning us
62 ,5, 57
(5, 0.10523515343964221), (10, 0.072753530879815079), (12, 0.024882581869001934), (15, 0.08971235819758408), (17, 0.01888687361381499), (18, 0.032832333968889779), (27, 0.029611794605538009), (32,
0.020759768561927006), (35, 0.047293298715519932), (41, 0.033179398237307846), (52, 0.025774215252116678), (55, 0.012972881813004444), (57, 0.022119010871223803), (59, 0.014932554296979379), (61,
0.010952102598066761), (62, 0.1230114569017088), (82, 0.012460472459386401), (83, 0.028034090216202408), (84, 0.095175123030142555), (87, 0.020971797634204901), (94, 0.018284404059980768), (98,
0.053529638797303457), (99, 0.015800611410707707)
A Study on Autonomous Vehicle De
University*
No relevant topic
(4, 0.072837073289600651), (24, 0.017187260728907628), (25, 0.01225208988886581), (28, 0.015495598786614924), (34, 0.083261555734200673), (36, 0.026265375104342212), (38, 0.013641826256632107), (55,
0.01173339813173975), (60, 0.02015461253715705), (61, 0.033878305370461784), (66, 0.078456585540429002), (74, 0.12712435224285087), (79, 0.02513178305002808), (82, 0.22852976265870359), (87,
0.066171330258980907), (95, 0.020710612189924357)
Road Surface Recognition Using La
Automatic Platooning
35, 18, 54
(2, 0.059117322211207163), (11, 0.0347108202594303), (18, 0.056425060112055531), (25, 0.11090768087018449), (32, 0.017877122052208899), (34, 0.025906382103189501), (44, 0.016614468571468893), (52,
0.010881852259250181), (55, 0.013669125415651988), (60, 0.01014530219615342), (63, 0.031916029496663521), (66, 0.011531389754702958), (73, 0.052812456775942117), (74, 0.030518260537764356), (76,
0.042078636016652911), (79, 0.06445522793546235), (82, 0.080785324877041167), (88, 0.024026083537429695), (94, 0.14401170820600007), (96, 0.071368184415365452), (98, 0.032701552916586621)
Building a Prototype for Power-Awa
Parking System
47, 62
(0, 0.038173267066336398), (9, 0.01565269533126467), (10, 0.023078062880224893), (11, 0.061975251821895193), (32, 0.03362947545462986), (38, 0.016658680357274216), (39, 0.018378435324124053), (42,
0.020354549471308724), (44, 0.019912052997539974), (47, 0.11439396799026109), (50, 0.014432397216141335), (55, 0.016446896339219932), (61, 0.038475125987387088), (62, 0.075491856794028339), (66,
0.13015139933796271), (73, 0.011753956105946539), (82, 0.096545128076018782), (87, 0.044415344896305019), (89, 0.069849633983348147), (94, 0.041590437962270473), (95, 0.022929938322254692)
A Computer Vision System for Dete
Avoidance for Automotive Vehicles
5, 2, 99
(2, 0.016176445555523666), (5, 0.03850008715455687), (8, 0.057780714260407103), (11, 0.07215830323468457), (24, 0.034671280075042797), (31, 0.043442753427955359), (52, 0.11087821978848775), (63,
0.012091807976632155), (73, 0.010706950137016974), (82, 0.21034250761433265), (87, 0.012211147834157575), (88, 0.021661816863657101), (94, 0.13021308787624267), (96, 0.017665816257917355), (99,
0.1686118185386129)
Path Tracking of Autonomous Grou
Fractional Order PID Controller Opti
62, vehicle control
(1, 0.031754223151067171), (8, 0.053428421903017262), (9, 0.041052663146845907), (10, 0.056242832158839032), (18, 0.029844196763067096), (23, 0.097985868794989009), (25, 0.029133524708788241), (52,
0.26004241494471947), (55, 0.013384618848121803), (57, 0.014226896227839072), (62, 0.018389669530164014), (82, 0.017939724225966052), (87, 0.036641906665378136), (94, 0.017133337640853036)
Off-road Path Following using Regi
Constraints∗
48, 28, 63
(2, 0.018366641864863094), (8, 0.024364649485370187), (10, 0.01856307989372102), (11, 0.030984366483324959), (12, 0.020416742129019925), (15, 0.016575982656180703), (18, 0.13381102708044132), (48,
0.016346543313800012), (52, 0.093999335969911266), (56, 0.026205735272533118), (62, 0.027088377878621183), (63, 0.012319598944342224), (71, 0.025970781985930877), (73, 0.02284522189107668), (74,
0.016450168444618329), (77, 0.02856165022255094), (82, 0.10878123597901036), (88, 0.081779817330326132), (94, 0.11068233022525438), (96, 0.014754822297985372), (97, 0.045721523698294284), (99,
0.01707501962865314)
Self-Tuning PID Controller for
Autonomous Car Tracking in Urban
37, 95, Vehicle control
(11, 0.014546830497920497), (12, 0.049952876630334576) (24, 0.11571307477508025), (25, 0.028420213014510538), (29, 0.047439225264603799), (34, 0.050512190761654127), (35, 0.025721907790998946), (42,
0.011100244916173794), (52, 0.13905556918544504), (55, 0.011213128228133092), (56, 0.016481814500748863), (60, 0.10808760689065738), (64, 0.021454985226620021), (66, 0.026073753604446522), (73,
0.039225754862767685), (79, 0.015884779707245392), (82, 0.02570210799783991), (92, 0.019064628557500376), (94, 0.029373893841786691), (95, 0.03367852966253515)
Shared Control of Autonomous Veh
based on Velocity Space Optimizati
45 , vehicle control, 78
(1, 0.015204256699643145), (5, 0.071321467621890761), (10, 0.040531862242341704), (12, 0.022000732733928031), (15, 0.037093056521356256), (17, 0.079213628653808765), (18, 0.014793974921074171), (29,
0.042261868240953314), (30, 0.017176251273451348), (35, 0.023309423624775644), (39, 0.026155602722795469), (41, 0.026821385861819241), (52, 0.073211696807987212), (57, 0.01269510515526508), (59,
0.040457319222009701), (62, 0.23565339508091404), (64, 0.013397066304495109), (78, 0.024328304402388583), (92, 0.01209836883686036), (96, 0.018092529478416941), (98, 0.025120100832033782)
A 13,000 km Intercontinental Trip wi
95, vehicle control, 82
(0, 0.01096961582830335), (4, 0.020953266188498066), (11, 0.031573857037845321), (34, 0.013940028062548447), (35, 0.018843466001068936), (38, 0.010981184875408086), (44, 0.039944583753307755), (55,
0.027750817221277844), (61, 0.013867550467024521), (66, 0.068128908765476767), (74, 0.40817247415538177), (76, 0.017834777410784715), (82, 0.18704383461036819)
Real-Time Coordination of Autonom
35, 86
(11, 0.016227433805240381), (17, 0.069973158028272503), (30, 0.065443834733193026), (32, 0.078323253435050047), (35, 0.044422433394877163), (38, 0.06435183790727779), (42, 0.020746713954058742), (44,
0.12822145200088794), (47, 0.058684911607568023), (52, 0.019797800472184299), (55, 0.085426293809304457), (56, 0.06881408435241973), (62, 0.043862204504906716), (66, 0.040934890159761921), (76,
0.028911081579274776), (82, 0.045089820289050793), (94, 0.012449078221558982), (95, 0.012749162076601631)
Accurate and Efficient Traffic Sign
96 ,85, 2
(2, 0.079236223886248097), (11, 0.027076803130624084), (12, 0.011594375298004167), (15, 0.011026158801238192), (18, 0.10821584373327547), (28, 0.035039932474895924), (29, 0.078217295081442512), (33,
0.077930035737221001), (45, 0.015608665668371739), (59, 0.01614320069282716), (63, 0.051645992819195435), (82, 0.081512453833865808), (85, 0.021248020420123425), (88, 0.10385231754616912), (91,
0.051748525166017703), (93, 0.082951350685758013), (94, 0.026232391867160386), (96, 0.025334530302784344), (97, 0.043231866737447819)
DeepDriving: Learning Affordance f
77, 88, 2
(11, 0.029367257778072992), (18, 0.12161960091533071), (35, 0.010880360786604181), (37, 0.025123409710687432), (42, 0.017010620006296513), (55, 0.022731619331414935), (60, 0.05312210668651296), (62,
0.014353612952571395), (63, 0.05634568239614636), (66, 0.10384934804339024), (77, 0.042317959390524677), (82, 0.18960915856709309), (84, 0.012043600200518239), (88, 0.03194348256398831), (89,
0.074146579090601503), (95, 0.025843744197240469), (97, 0.015576533439637696), (99, 0.031189293229949063)
A Robust Algorithm for the Detectio
33, 88
(11, 0.02083393267210169), (12, 0.02616204272313882), (33, 0.81436774856870142), (57, 0.01224364955826001), (88, 0.011850333092218246), (94, 0.012404798985117357), (95, 0.020885043865360702)
Constrained Global Path Optimizati
62, 78, 10
(5, 0.017020339841600996), (10, 0.14761779675287326), (12, 0.03973763689262181), (15, 0.062044795539110575), (17, 0.019769507518277638), (18, 0.038753971571825412), (25, 0.010656292632978955), (32,
0.034363935249538291), (35, 0.034766915074440671), (45, 0.012828267128109201), (47, 0.020103107631511963), (52, 0.064249833722814245), (57, 0.081393558330500534), (59, 0.015505801659005251), (62,
0.16430836004450927), (78, 0.018355699444785083), (83, 0.018968251608243669), (87, 0.028059195076228685), (88, 0.010283189879043465), (92, 0.017739227385739548), (94, 0.037701262678859745), (98,
0.019439891506384106)
360◦ detection and tracking algorith
using fisheye images
47, 96, 99
(11, 0.071634892888885124), (12, 0.055209935533392901), (18, 0.05457276377504592), (32, 0.012282896128847091), (33, 0.0339509545035801), (45, 0.026130647818693301), (47, 0.019432738975387311), (54,
0.035851673956480846), (66, 0.018273048180097375), (74, 0.023440248491116977), (79, 0.031050539509758113), (82, 0.11031070940093081), (88, 0.058501234826679557), (94, 0.011956839582928308), (95,
0.015318426170071732), (96, 0.07954766469629132), (97, 0.0628486655361324), (98, 0.069652763940015655), (99, 0.16454368461705091)
State your position
1, 48,
(1, 0.25972098026667584), (12, 0.010128326163672211), (15, 0.13325021033697218), (17, 0.023880986071608535), (23, 0.017519480117258651), (27, 0.017382195005369024), (29, 0.034452905434029249), (35,
0.033994549156452142), (37, 0.013339792099660106), (39, 0.012570553398729291), (48, 0.049131753407658914), (52, 0.077168595128774123), (54, 0.033212224308735473), (55, 0.011355463366751114), (70,
0.046834399414228005), (76, 0.021631418916718066), (87, 0.038738498709408105), (88, 0.03091309152210624), (92, 0.034724124164763177), (94, 0.019823590795099419)
A robotic platform to evalute autono
Vehicle control, 55, 60
(4, 0.018957330378932935), (9, 0.024875529971388061), (11, 0.10564387259839503), (12, 0.037973243472866812), (24, 0.012223341330636068), (25, 0.011349757431892966), (35, 0.020947440146654648), (44,
0.020914627362493301), (47, 0.047220764368677365), (55, 0.061885696965886181), (60, 0.017655064569741523), (62, 0.039822609827811266), (66, 0.081648855621690927), (74, 0.030133044272027447), (79,
0.01471369173619788), (82, 0.15926520049709811), (87, 0.068181635732502827), (88, 0.011749681567200123), (94, 0.05125548213392065), (95, 0.029115385244926968), (96, 0.010021124843198075)
Coordinated control of multiple vehi
discrete-time periodic communicatio
77, 82, 86
(5, 0.025302630848755579), (8, 0.033103788575734122), (16, 0.25895607465347326), (17, 0.077298257258813188), (24, 0.010888548077837531), (29, 0.048966157745231012), (34, 0.030573933928102679), (35,
0.038069775299401323), (39, 0.04555580570918575), (44, 0.034025903962571026), (56, 0.035237522865387991), (57, 0.017690978002178639), (59, 0.035454407967011693), (62, 0.065248220945653648), (64,
0.062330597594210412), (67, 0.018833848054000613), (76, 0.010349435297715801)
Real-time Implementation of a Nove
Vehicle control, 24, 25
(4, 0.042339089395347994), (6, 0.015083471416866267), (11, 0.076279744558860094), (12, 0.019899713804863321), (17, 0.043618069403195986), (24, 0.041064211757948985), (25, 0.11022231865411181), (27,
0.06656721742466809), (34, 0.022850166084199597), (35, 0.010297573627429654), (44, 0.010705037496439619), (52, 0.27489935471687632), (55, 0.034944881499416686), (62, 0.039485504159352092), (74,
0.034661632143435636), (89, 0.071670412031490077), (92, 0.013006988541053166)
Coordinated Path Following Control
82, 86, vehicle control
(5, 0.011221945168687164), (8, 0.52763602635487761), (14, 0.015528531338255996), (16, 0.091083476359100135), (25, 0.049612112156256619), (39, 0.015310009220606716), (55, 0.010828176389087245), (62,
0.075266478492064609), (64, 0.015452143018376928), (87, 0.032534290957658578)
A Combined Model- and Learning-B
35, 92, vehicle control
(6, 0.018027805074217338), (12, 0.020840607437469457), (17, 0.038764672591651173), (18, 0.02097575014903405), (24, 0.030955143416710188), (29, 0.010529867836960418), (35, 0.06051850721888441), (47,
0.023647242955980294), (55, 0.035591507028927494), (60, 0.020010250628855149), (62, 0.15525493141392713), (70, 0.031537239880394488), (76, 0.16840433276106981), (79, 0.060233979849932326), (82,
0.055025954499155802), (92, 0.052825142293323447), (95, 0.028253849990030812), (96, 0.048701056934568256)
Towards a Framework for Testing D
55, 45
(5, 0.016332732263354254), (16, 0.012750702265022396), (35, 0.02238528358958156), (37, 0.013706107902485734), (38, 0.015287991284115962), (44, 0.015664534470396528), (47, 0.019235055737134712), (49,
0.035197481785219885), (55, 0.53311597367673769), (60, 0.014996706882221905), (61, 0.013750321666992612), (62, 0.034912830452773054), (74, 0.021081432511125067), (79, 0.018435864284421087), (82,
0.11088175929887201), (94, 0.012708108328764768), (95, 0.016685593381328436)
Adopting WirelessHART for In-Vehi
86, 61
(4, 0.017536733467756707), (12, 0.02572142983515039), (14, 0.011793792716468582), (18, 0.011565642696780133), (28, 0.039861618911722352), (33, 0.030034953218170478), (35, 0.024198293316550167), (38,
0.032083116645764451), (44, 0.070844942707067218), (45, 0.010105497144419252), (50, 0.012529651611363275), (55, 0.040182997066299735), (59, 0.073216167605142338), (61, 0.018812500847759751), (66,
0.18516054221053074), (74, 0.08032950488363895), (76, 0.010329155406958232), (82, 0.086750851051705574), (86, 0.079345747432637628), (95, 0.037543604625724238)
Terrain Mapping for Off-road Auton
96, 99 18,
(5, 0.015746090276838442), (11, 0.08902361271059582), (12, 0.044538466217027999), (18, 0.13730328783108575), (25, 0.017483243439265758), (39, 0.021698022302209116), (41, 0.016487792506911039), (59,
0.028609675926009978), (63, 0.019255261917735168), (67, 0.010357151011660461), (74, 0.07222914759877419), (77, 0.026415115777152744), (79, 0.019219360036228587), (82, 0.032322518706575223), (84,
0.053747617357255952), (88, 0.056365890846956021), (94, 0.041139559562904118), (95, 0.011183882718784051), (96, 0.092873836836584345), (98, 0.046183892432013277), (99, 0.097444633362412547
Incremental Sampling-based Algorit
Minimum-violation Motion Planning
60, Vehicle control, 78
(5, 0.025401483947249309), (6, 0.011173619383878477), (8, 0.016646145100123157), (10, 0.023144339237246361), (11, 0.025592393206326215), (12, 0.05329003875266411), (15, 0.10165021474192439), (17,
0.060731933232027226), (32, 0.012878758611213294), (33, 0.017728738436248257), (39, 0.025384375629672117), (46, 0.044804386129223842), (47, 0.051142956815661535), (56, 0.1132531248766827), (59,
0.024514017326008924), (61, 0.013842311041343171), (62, 0.12246169949335441), (64, 0.018807693865969439), (76, 0.08166915727352074), (79, 0.010345016877107042), (82, 0.010448271543365011), (89,
0.026328350960326357), (92, 0.024399768030426889)
Vision-based Nighttime Vehicle Det
51, 96, 55
(2, 0.013036981643656699), (8, 0.027644342895958754), (11, 0.01336400301615118), (12, 0.044267849841683256), (18, 0.035985077945331088), (33, 0.20487778194549938), (35, 0.014548517953525063), (37,
0.019873529787189762), (42, 0.027734037119229192), (52, 0.036350124993224757), (54, 0.034938655479154861), (55, 0.038239347882395434), (74, 0.020037322023319846), (76, 0.025920951745930471), (78,
0.01476222088310431), (82, 0.18723811097421242), (88, 0.048235525949377432), (94, 0.032680356097388064), (96, 0.040213497041408738), (98, 0.034381042558632259), (99, 0.037996556341170502)
Design and Comparative Analysis o
0, 33, 9
(29, 0.034609853743017178), (37, 0.69863093814294153), (39, 0.014946475231906857), (55, 0.19485987878501795)
Local Path Planning for Off-Road A
Driving With Avoidance of Static Ob
62, 78, 99,
(4, 0.01078201769078089), (8, 0.010691373605565968), (10, 0.055296772486137021), (11, 0.024971176051391847), (12, 0.032598403265177288), (15, 0.012719513183787903), (16, 0.012251606263353056), (18,
0.023185195364828933), (25, 0.012199988397866401), (27, 0.011565894114171468), (28, 0.01345091124636778), (32, 0.043658831532659668), (33, 0.015603970199711039), (35, 0.046894955042565231), (45,
0.032458835571934894), (47, 0.010016898129083976), (52, 0.03574248439413269), (60, 0.014333894945835807), (62, 0.2118134242012488), (76, 0.01933589695481094), (78, 0.05422311030339045), (79,
0.011854292911894257), (82, 0.13203123773369541), (84, 0.020542214323562739), (85, 0.015243414444244591), (87, 0.019036537012413823), (94, 0.028313951501351204)
HOG Based Multi-object Detection f
98, 51, 87
(2, 0.061322742383333372), (11, 0.065492966474895553), (12, 0.015439255873376859), (18, 0.13172201515924228), (28, 0.015467111487384898), (32, 0.015545594360878792), (35, 0.025775443353483196), (47,
0.030368345564793446), (55, 0.020367777680026702), (66, 0.02247617731937113), (73, 0.020245241446485974), (82, 0.1025475940521899), (87, 0.014837949277320616), (88, 0.087898574711964467), (91,
0.014604912870642115), (93, 0.044459357216783686), (94, 0.13515502755201253), (97, 0.035759900664674979), (98, 0.087561700085189167)
Genetic Algorithm Approach for Loc
Identification Readers
36
(15, 0.14401737457092492), (18, 0.19537523540597748), (27, 0.10887306087309488), (28, 0.04572457491015721), (35, 0.056687314317095533), (39, 0.027161388652146908), (41, 0.067533606008884361), (45,
0.010941376554662539), (49, 0.013470755210001437), (51, 0.016768822554154558), (59, 0.0481569398689492), (61, 0.0484652002982447), (62, 0.020107089534269318), (78, 0.010470688963315824), (79,
0.01383851286941231), (82, 0.06129668798819473), (89, 0.038844135777045111)
Reliable Intersection Protocols Usin
38, 44, 42
(17, 0.022712307159050427), (30, 0.038152802707892515), (35, 0.030481190218260991), (37, 0.013400180240291736), (38, 0.39409381753496925), (42, 0.15184692653754589), (44, 0.063200172948802757), (59,
0.029945179906688404), (73, 0.018593811532963495), (76, 0.076593369408331127), (78, 0.014871134010681693), (82, 0.014462477562069656), (94, 0.017949452443718251), (95, 0.01511824844257785)
INTELLIGENT TRAFFIC WITH CO
VEHICLES
44, 49, 20,
(27, 0.014041461527332173), (28, 0.071207857475662509), (32, 0.01948579577917238), (35, 0.035275729617136105), (38, 0.017168746968245486), (42, 0.016269384445106793), (44, 0.11114653569558897), (47,
0.031475142624220229), (49, 0.010141804470988785), (55, 0.042239754399324661), (60, 0.028220939206474185), (61, 0.01516835775949766), (66, 0.083765856633204572), (67, 0.018239040583859768), (74,
0.039204103950946124), (76, 0.03213043005096871), (82, 0.20864493379938656), (86, 0.033520831154179691), (87, 0.046184905874866233), (94, 0.041375878714960845)
MCMC Particle Filter for Real-Time
33, 88, 57
(6, 0.034899618716382294), (8, 0.04643812866492588), (11, 0.011122157673042705), (12, 0.084215754205146334), (17, 0.010493917346896765), (18, 0.12137547648943883), (23, 0.051364757631350809), (33,
0.038174538946347557), (35, 0.07764074642411696), (37, 0.030477045677101047), (42, 0.013503541221752995), (47, 0.019286150029598551), (52, 0.038634829743624657), (54, 0.025209822038269242), (62,
0.013124343064303453), (63, 0.017369275902318723), (66, 0.019349977625581383), (76, 0.021659226141050373), (82, 0.020093723582610488), (88, 0.012421863353859522), (92, 0.047871357612447499), (96,
0.058631392241418383), (97, 0.017887125226960603), (98, 0.083557085210943058)
Globally Asymptotically Stable Filter
and Depth Measurements
91, 87, 61
(9, 0.016150806975657073), (12, 0.012775731372536459), (15, 0.064045065984235233), (16, 0.18304670757585845), (17, 0.018547171122719017), (23, 0.025359233982424144), (24, 0.0355690335572387), (25,
0.055900779171438805), (27, 0.022195208618635411), (28, 0.013556122581072102), (29, 0.057359961290937733), (46, 0.030916975151806237), (48, 0.014721581307834514), (64, 0.028177841489578883), (67,
0.041486560704905924), (70, 0.06206132784724458), (74, 0.056532623377460268), (76, 0.039303407352360348), (83, 0.029318434001235846), (88, 0.02005128595939356), (94, 0.013459375796142013), (99,
0.014219801468060142)
A Real-Time Multi-Sensor Fusion Pl
37, 71, 70
(37, 0.81872937773255205), (55, 0.078783842923631039), (78, 0.034186934006349756)
A full-3D Voxel-based Dynamic Obs
Urban Scenario using Stereo Vision
99, 98, 28
(11, 0.11276210000428584), (12, 0.2129591221670441), (18, 0.073856718778386315), (28, 0.032968501484352222), (33, 0.018399084540318387), (35, 0.050147946025806397), (48, 0.012582377649569045), (62,
0.012606018487604056), (74, 0.016643664875164554), (82, 0.059598278765224777), (88, 0.076640066688028155), (96, 0.10500542994600785), (98, 0.090003133835002302), (99, 0.062035259941164038)
A Real-Time Trajectory Control of T
vehicle control, 78, 99
(5, 0.041744293514104462), (8, 0.027327110438853115), (11, 0.11313252788061186), (17, 0.022540913689733783), (29, 0.026362067516344585), (35, 0.014906421982601019), (37, 0.032009496388595342), (41,
0.011017485956797364), (52, 0.18522500773455677), (54, 0.015119123460219189), (55, 0.010985738695737641), (60, 0.046327297288017544), (62, 0.070621953188921213), (66, 0.021024544765307133), (82,
0.043026951527911222), (83, 0.011413279371115206), (87, 0.1906831953200861), (95, 0.049817304910760321), (98, 0.034275797930158648)
Vehicle Automation in Cooperation
Nomadic Devices Communication
55,99,76
(11, 0.086897027974576421), (17, 0.016035002191313406), (35, 0.023693328605991111), (38, 0.011551601660147112), (42, 0.0189606131681531), (44, 0.0337040094970441), (45, 0.018362922426253734), (47,
0.05919898357699966), (50, 0.023640552555759909), (52, 0.032306924084259038), (55, 0.099178272720567853), (62, 0.066989642713265851), (66, 0.052789015138285794), (74, 0.06451198974279683), (79,
0.011558605538622818), (82, 0.21023791869611239), (86, 0.010414820619839543), (87, 0.020662289231491111), (88, 0.013829392668713499), (89, 0.015140541001457246), (98, 0.039504004851171969)
Automatic vehicle classification and
movements at signalized intersectio
8, 42, 54
(4, 0.017501547572986963), (6, 0.012873709683197411), (8, 0.072490828816116334), (12, 0.010660770794638752), (24, 0.012539836707240187), (33, 0.060807092180063638), (34, 0.010890153468553501), (38,
0.028190413380469726), (42, 0.10567671105505069), (45, 0.015731953840449706), (47, 0.03082732993838283), (54, 0.013337185959005977), (55, 0.023523021444388451), (60, 0.011770928332716042), (62,
0.056160119407105052), (79, 0.058150316399712819), (82, 0.17185107072267314), (88, 0.061780654459708581), (94, 0.071540369671543758), (95, 0.038406541644510191), (96, 0.03179587197844784), (98,
0.027210260300943857)
Multi-Target Tracking using a 3D-Li
15, 77, 98
(6, 0.095987936801262386), (11, 0.034785123294199465), (12, 0.070769251434393332), (18, 0.071518061405754424), (23, 0.046203642433257107), (33, 0.025267742415417561), (35, 0.010493647923547322), (47,
0.010596874613683074), (52, 0.077381568375406357), (54, 0.036682116141475013), (70, 0.015711334691369379), (76, 0.012566726410911801), (78, 0.010991160602340086), (82, 0.023242955522464939), (84,
0.028105572737760882), (85, 0.01882960851564193), (88, 0.028501602161177444), (95, 0.061820846402345246), (96, 0.019253404169267348), (98, 0.27334997661984639)
Traffic Sign Representation using S
2, number 20
(2, 0.10946451178223582), (11, 0.052278300742839102), (12, 0.01031388764821964), (15, 0.017058519123586566), (18, 0.14080450033234806), (28, 0.011401107297863667), (35, 0.026056291926509198), (41,
0.024438634940475039), (59, 0.018097401626335819), (63, 0.1056367450161169), (64, 0.020850930034596423), (73, 0.016911118613878645), (79, 0.052863290583470703), (82, 0.062848062164756469), (88,
0.040759430136569491), (91, 0.049125721421668718), (92, 0.010673291925642165), (93, 0.030522111596631683), (94, 0.032577566115431875), (95, 0.021278557543011438), (97, 0.057835756454004073)
Speed Profile Optimization for Vehi
Intersection Under a Safety Constra
38, 42, vehicle control
(5, 0.018658412652346874), (11, 0.018214034625506848), (25, 0.041279180208719256), (35, 0.024229058366161248), (37, 0.010722275173047571), (38, 0.10009327970013882), (39, 0.03365829644212695), (42,
0.055112599883901457), (46, 0.066645048620358485), (52, 0.076020311006235866), (57, 0.031907751344818512), (59, 0.086906514537571061), (62, 0.16169786391830407), (76, 0.024317594320230718), (83,
0.057398467272538958), (94, 0.024910022699195553), (95, 0.020979120213654518)
Fisheye optics f
96, 70, 99
(11, 0.1390023
, (32, 0.084519
Data age based
control data exc
86, 44
(38, 0.0680274
(44, 0.2029934
Obstacle Avoid
Model Predictiv
78, 82, 57
(12, 0.0740088
(52, 0.1608838
Intelligent Cruis
Stop and Go wit
52, 55, 44
(42, 0.1047802
(44, 0.2432348
Autonomous Na
87
(48, 0.0769147 (82, 0.0935574
Bayesian Netwo
95, number 4, 5
(38, 0.1212474 (70, 0.0789667
Experience, Re
Lessons Learne
Automated Drivi
Germany’s High
37, 55, 62
(55, 0.0847935
(82, 0.1893622
Multi-Objective
62 ,5, 57
A Study on Auto
University*
No relevant topic
(34, 0.0832615 (74, 0.1271243
Road Surface R
Automatic Plato
35, 18, 54
(25, 0.1109076
(82, 0.0807853
Building a Proto
Parking System
47, 62
(47, 0.1143939
(66, 0.1301513
A Computer Visi
Avoidance for A
5, 2, 99
(82, 0.2103425
(94, 0.1302130
Path Tracking o
Fractional Order
62, vehicle control
(16, 0.1431923
(23, 0.0979858
Off-road Path F
Constraints∗
48, 28, 63
(18, 0.1338110
(82, 0.1087812
Self-Tuning PID
Autonomous Ca
37, 95, Vehicle control
(24, 0.1157130
(52, 0.1390555
Shared Control
based on Veloci
45 , vehicle control, 78
(17, 0.0792136
(52, 0.0732116
A 13,000 km Int
95, vehicle control, 82
(66, 0.0681289 (74, 0.4081724
Real-Time Coor
35, 86
(32, 0.0783232
(44, 0.1282214
Accurate and Ef
96 ,85, 2
(18, 0.1082158
(55, 0.0854262
DeepDriving: Le
77, 88, 2
(18, 0.1216196
(66, 0.1038493
A Robust Algorit
33, 88
(12, 0.0261620
(33, 0.8143677
Constrained Glo
62, 78, 10
(10, 0.1476177
(57, 0.0813935
(99, 0.10121410958494333)
(86, 0.073215505908526407),
(62, 0.1970401075172302)
(82, 0.11078935942794398)
(98, 0.10503959543129056)
(82, 0.22744132992600527)
(98, 0.10634841700615484)
(5, 0.10523515343964221), (62, 0.1230114569017088), (84, 0.095175123030142555)
(82, 0.22852976265870359),
, (94, 0.14401170820600007),
(82, 0.096545128076018782),
(99, 0.1686118185386129)
(89, 0.064092231910171715),
(94, 0.11068233022525438),
(60, 0.10808760689065738),
(62, 0.23565339508091404),
(82, 0.18704383461036819)
(55, 0.085426293809304457)
(93, 0.082951350685758013)
(82, 0.18960915856709309)
(95, 0.020885043865360702)
(62, 0.16430836004450927)
360◦ detection a
using fisheye im
47, 96, 99
(82, 0.1103107
(96, 0.0795476
State your positi
1, 48,
(1, 0.25972098
(15, 0.1332502
A robotic platfor
Vehicle control, 55, 60
(11, 0.1056438 (55, 0.0618856
Coordinated co
discrete-time pe
77, 82, 86
(16, 0.2589560
(17, 0.0772982
Real-time Imple
Vehicle control, 24, 25
(11, 0.0762797
(25, 0.1102223
Coordinated Pat
82, 86, vehicle control
(8, 0.52763602
(16, 0.0910834
A Combined Mo
35, 92, vehicle control
(35, 0.0605185
(62, 0.1552549
Towards a Fra
55, 45
(49, 0.0351974
(55, 0.5331159
Adopting Wirele
86, 61
(66, 0.1851605
(74, 0.0803295
Terrain Mappin
96, 99 18,
(18, 0.1373032
(96, 0.0928738
Incremental Sa
Minimum-violati
60, Vehicle control, 78
(15, 0.1016502
(56, 0.1132531
Vision-based Ni
51, 96, 55
(12, 0.0442678
(33, 0.2048777
Design and Co
0, 33, 9
(29, 0.0346098
(37, 0.6986309
Local Path Plan
Driving With Av
62, 78, 99,
(10, 0.0552967
(35, 0.0468949
HOG Based Mu
98, 51, 87
(18, 0.1317220 ( 82, 0.1025475
Genetic Algorith
Identification Re
36
(15, 0.1440173
(18, 0.1953752
Reliable Interse
38, 44, 42
(38, 0.3940938 (42, 0.1518469
INTELLIGENT
VEHICLES
44, 49, 20,
(44, 0.1111465
(66, 0.0837658
MCMC Particle
33, 88, 57
(12, 0.0842157 (18, 0.1213754
Globally Asympt
and Depth Mea
91, 87, 61
(15, 0.0640450
(16, 0.1830467
A Real-Time Mu
37, 71, 70
(37, 0.8187293
(55, 0.0787838
A full-3D Voxel-
Urban Scenario
99, 98, 28
(11, 0.1127621
(12, 0.2129591
A Real-Time Tr
vehicle control, 78, 99
(11, 0.1131325
(52, 0.1852250
Vehicle Automa
Nomadic Devic
55,99,76
(11, 0.0868970
(55, 0.0991782
Automatic vehic
movements at s
8, 42, 54
(8, 0.07249082
(82, 0.1718510
(99, 0.16454368461705091)
(52, 0.077168595128774123)
(66, 0.081648855621690927)
(62, 0.065248220945653648)
(52, 0.27489935471687632)
(62, 0.075266478492064609)
(76, 0.16840433276106981)
(82, 0.11088175929887201)
(82, 0.086750851051705574)
(99, 0.097444633362412547)
(62, 0.12246169949335441)
(82, 0.18723811097421242)
(55, 0.19485987878501795)
(78, 0.05422311030339045)
(94, 0.13515502755201253),
(27, 0.10887306087309488)
(76, 0.076593369408331127)
(82, 0.20864493379938656)
(98, 0.083557085210943058)
(70, 0.06206132784724458)
(78, 0.034186934006349756)
(96, 0.10500542994600785)
(87, 0.1906831953200861)
(82, 0.21023791869611239)
(94, 0.071540369671543758)
Multi-Target Tra
15, 77, 98
(6, 0.09598793
(52, 0.0773815
Traffic Sign Rep
2, number 20
(2, 0.10946451
(18, 0.1408045
Speed Profile O
Intersection Un
38, 42, vehicle control
(38, 0.1000932 (59, 0.0869065
(98, 0.27334997661984639)
(63, 0.1056367450161169)
(62, 0.16169786391830407)
Fisheye optics f
96, 70, 99
Data age based
control data exc
86, 44
Obstacle Avoid
Model Predictiv
78, 82, 57
Intelligent Cruis
Stop and Go wit
52, 55, 44
Autonomous Na
87
Bayesian Netwo 95, number 4, 5
Experience, Re
Lessons Learne
Automated Drivi
Germany’s High
37, 55, 62
Multi-Objective
62 ,5, 57
A Study on Auto
University*
No relevant topi
Road Surface R
Automatic Plato
35, 18, 54
Building a Proto
Parking System
47, 62
A Computer Visi
Avoidance for A
5, 2, 99
Path Tracking o
Fractional Order
62, vehicle cont
Off-road Path F
Constraints∗
48, 28, 63
Self-Tuning PID
Autonomous Ca
37, 95, Vehicle
Shared Control
based on Veloci
45 , vehicle cont
A 13,000 km Int
95, vehicle cont
Real-Time Coor
35, 86
Accurate and Ef
96 ,85, 2
DeepDriving: Le
77, 88, 2
A Robust Algorit
33, 88
(11, 0.13900239476104639)
(44, 0.20299343908608297)
(62, 0.1970401075172302)
(44, 0.24323481593723542),
(98, 0.10503959543129056)
(82, 0.22744132992600527)
(82, 0.18936225101487586)
(62, 0.1230114569017088)
(82, 0.22852976265870359),
, (94, 0.14401170820600007),
(66, 0.13015139933796271)
(82, 0.21034250761433265),
(16, 0.14319230305794198),
(18, 0.13381102708044132)
(52, 0.13905556918544504)
(62, 0.23565339508091404),
(74, 0.40817247415538177)
(44, 0.12822145200088794)
(18, 0.10821584373327547)
(82, 0.18960915856709309)
(33, 0.81436774856870142)
Constrained Glo
62, 78, 10
360◦ detection a
using fisheye im
47, 96, 99
State your positi
1, 48,
A robotic platfor
Vehicle control,
Coordinated co
discrete-time pe
77, 82, 86
Real-time Imple
Vehicle control,
Coordinated Pat
82, 86, vehicle
A Combined Mo
35, 92, vehicle
Towards a Fra
55, 45
Adopting Wirele
86, 61
Terrain Mappin
96, 99 18,
Incremental Sa
Minimum-violati
60, Vehicle cont
Vision-based Ni
51, 96, 55
Design and Co
0, 33, 9
Local Path Plan
Driving With Av
62, 78, 99,
HOG Based Mu
98, 51, 87
Genetic Algorith
Identification Re
36
Reliable Interse
38, 44, 42
INTELLIGENT
VEHICLES
44, 49, 20,
MCMC Particle
33, 88, 57
Globally Asympt
and Depth Mea
91, 87, 61
A Real-Time Mu
37, 71, 70
A full-3D Voxel-
Urban Scenario
99, 98, 28
A Real-Time Tr
vehicle control,
(62, 0.16430836004450927)
(99, 0.16454368461705091)
(1, 0.25972098026667584)
(11, 0.10564387259839503)
(16, 0.25895607465347326)
(52, 0.27489935471687632)
(8, 0.52763602635487761)
(76, 0.16840433276106981)
(55, 0.53311597367673769)
(66, 0.18516054221053074)
(18, 0.13730328783108575)
(62, 0.12246169949335441)
(33, 0.20487778194549938)
(37, 0.69863093814294153)
(10, 0.055296772486137021)
(94, 0.13515502755201253),
(18, 0.19537523540597748),
(38, 0.39409381753496925)
(82, 0.20864493379938656)
(18, 0.12137547648943883)
(16, 0.18304670757585845)
(37, 0.81872937773255205)
(12, 0.2129591221670441)
(87, 0.1906831953200861)
Vehicle Automa
Nomadic Devic
55,99,76
Automatic vehic
movements at s
8, 42, 54
Multi-Target Tra
15, 77, 98
Traffic Sign Rep
2, number 20
Speed Profile O
Intersection Un 38, 42, vehicle c
17 st
33 st
(82, 0.21023791869611239)
(82, 0.17185107072267314)
(98, 0.27334997661984639)
(18, 0.14080450033234806),
(62, 0.16169786391830407)
clean_A._B._P.
(36, 0.1848332
(29, 0.1298672
(76, 0.1187326
(15, 0.0879327
(37, 0.0812135
(27, 0.0742411
(44, 0.0666556
(67, 0.0490105
(42, 0.0439286
(14, 0.0350045
(34, 0.0335877
(63, 0.0326458
(98, 0.0269327
(94, 0.0170646
27
24
46
clean_A._B._S.
(72, 0.2559892
(61, 0.1131356
(76, 0.1072234
(3, 0.10315660
(78, 0.0796576
(20, 0.0786158
(63, 0.0751499
(87, 0.0684457
(36, 0.0357103
(44, 0.0237057
autonomous ve
3
37
0,40,76
clean_A._B.-N.
(2, 0.22733818
(8, 0.13336643
(98, 0.1151860
(0, 0.09980107
(6, 0.08954729
(59, 0.0605728
(90, 0.0491177
(18, 0.0485480
(15, 0.0469500
(10, 0.0329655
(16, 0.0304403
(85, 0.0304275
(56, 0.0186164
autonomous ve
8
18
16,78
clean_A._C._C.
(8, 0.18097194
(36, 0.1404788
(16, 0.1108507
(67, 0.0947545
(90, 0.0833808
(3, 0.06850182
(6, 0.06049974
(37, 0.0568441
(10, 0.0482495
(34, 0.0379784
(51, 0.0337652
(15, 0.0325365
(81, 0.0196686
(61, 0.0136593
8
81
10
54,58,66
clean_A._C._L.
(8, 0.52548262
(35, 0.1040605
(83, 0.1011615
(98, 0.0818036
(6, 0.07244673
(78, 0.0384183
(51, 0.0328057
autonomous ve
14
24
35,38
clean_B._B._H.
(95, 0.2259768
(85, 0.1084372
(36, 0.1027102
(63, 0.0929242
(83, 0.0906500
(86, 0.0706309
(0, 0.05276510
(41, 0.0485640
(34, 0.0362071
(27, 0.0349161
(78, 0.0316168
(31, 0.0209986
(52, 0.0207306
(91, 0.0194611
(16, 0.0183011
(80, 0.0104836
95
31,34
clean_B._W._A.
(2, 0.11201532
(0, 0.11037161
(78, 0.1098718
(44, 0.1065425
(37, 0.0930841
(14, 0.0618217
(15, 0.0604813
(72, 0.0550098
(16, 0.0538467
(95, 0.0532252
(41, 0.0338320
(86, 0.0271679
(71, 0.0263252
(35, 0.0180172
(39, 0.0165852
(23, 0.0140627
62
31
62,72,99
clean_B.-M._S.
(37, 0.1779281
(98, 0.1416671
(95, 0.1141763
(6, 0.11173812
(46, 0.0996277
(63, 0.0994831
(94, 0.0668342
(72, 0.0647038
(55, 0.0507047
(68, 0.0386981
(59, 0.0150594
3
2
37
road detection:
25,64
clean_C._C._J.
(64, 0.3036344
(61, 0.2105239
(23, 0.0917346
(71, 0.0381779
(90, 0.0381002
(68, 0.0362724
(37, 0.0361874
(35, 0.0338037
(54, 0.0312496
(6, 0.03102834
(55, 0.0265256
(78, 0.0246621
(63, 0.0243164
(94, 0.0242289
(98, 0.0217948
(46, 0.0208169
25
90
46
clean_C._C._Y.
(36, 0.1431102
(2, 0.13368098
(56, 0.1256067
(10, 0.0888821
(74, 0.0831769
(68, 0.0748363
(78, 0.0739692
(8, 0.06652429
(3, 0.06498196
(22, 0.0367551
(6, 0.02816350
(15, 0.0252004
(98, 0.0209641
49
2
56
clean_C._L._B.
(97, 0.1898303
(64, 0.1844834
(78, 0.0970447
(14, 0.0890471
(74, 0.0703246
(16, 0.0683517
(3, 0.06783980
(24, 0.0649523
(23, 0.0379740
(67, 0.0244115
(6, 0.02163216
(85, 0.0192533
(46, 0.0172676
(76, 0.0147327
(63, 0.0100260
(55, 0.0100012
2
97
46
clean_C._W._A (31, 0.3009491
(62, 0.0918091
(0, 0.08093508
(29, 0.0674028
(41, 0.0473584
(67, 0.0444138
(83, 0.0420481
(37, 0.0408359
(42, 0.0384776
(95, 0.0368339
(78, 0.0319376
(15, 0.0302549
(59, 0.0241452
(94, 0.0206537
(70, 0.0146945
(93, 0.0135124
39
clean_D._B._W. (64, 0.1244015
(10, 0.1023204
(35, 0.0841132
(32, 0.0755782
(46, 0.0731179
(37, 0.0705237
(29, 0.0697600
(91, 0.0669071
(85, 0.0627237
(2, 0.05562304
(56, 0.0544630
(22, 0.0326169
(36, 0.0305349
(94, 0.0279860
(23, 0.0191992
(3, 0.01801870
32
46
91
clean_D._C._S.
(0, 0.28784799
(14, 0.1804510
(67, 0.1609751
(37, 0.1187409
(3, 0.06255454
(31, 0.0491384
(54, 0.0472931
(72, 0.0286192
(35, 0.0253958
(2, 0.01814514
94 autonomous ve
27
clean_G._A._J.
(36, 0.2381233
(37, 0.1099040
(94, 0.0919301
(55, 0.0711347
(0, 0.06386727
(85, 0.0463860
(99, 0.0405868
(29, 0.0381238
(20, 0.0379839
(63, 0.0351393
(62, 0.0325047
(34, 0.0310356
(67, 0.0287670
(25, 0.0266637
3
85
clean_H._x._E.
(64, 0.1820410
(23, 0.1046004
(62, 0.1032573
(16, 0.0976474
(58, 0.0734623
(14, 0.0703622
(22, 0.0608163
(78, 0.0406319
(6, 0.03899274
(76, 0.0383016
(52, 0.0336817
(42, 0.0323516
(46, 0.0236606
(19, 0.0219050
(55, 0.0198637
(38, 0.0174714
16
2
23
clean_J._A._C.
(0, 0.16960891
(8, 0.16148190
(34, 0.1178658
(2, 0.10747126
(95, 0.0781148
(62, 0.0648085
(78, 0.0635932
(93, 0.0573990
(98, 0.0558509
(71, 0.0353839
(3, 0.03056499
(59, 0.0252523
31 autonomous ve
62
clean_J._C._S.
(14, 0.1697630
(16, 0.1557792
(61, 0.1321250
(32, 0.1029354
(42, 0.0911892
(95, 0.0885008
(64, 0.0503733
(46, 0.0412817
(68, 0.0396650
(66, 0.0373135
(52, 0.0256741
(94, 0.0237314
(36, 0.0215120
32
58
68
clean_J._S._B.
(86, 0.1693244
(34, 0.1211003
(62, 0.0986270
(95, 0.0697720
(78, 0.0656428
(0, 0.06544911
(2, 0.06274184
(16, 0.0601504
(71, 0.0446988
(41, 0.0426818
(40, 0.0300069
(35, 0.0280322
(59, 0.0244885
(37, 0.0219802
86
31
62
clean_K._B._H.
(14, 0.1679318
(16, 0.1518185
(2, 0.12430762
(42, 0.1039841
(6, 0.09018234
(62, 0.0887824
(22, 0.0859634
(0, 0.08512442
(46, 0.0759760
autonomous ve
ABOUT ROBOT
39
clean_K._C._F.
(14, 0.1600816
(54, 0.1509368
(32, 0.0748253
(24, 0.0659263
(35, 0.0572754
(61, 0.0545732
(42, 0.0452260
(2, 0.04319953
(46, 0.0383997
(68, 0.0371950
(3, 0.03614376
(49, 0.0341239
(44, 0.0322230
(64, 0.0318812
(0, 0.03015573
(93, 0.0300973
54
70
46
clean_L._C._A.
(3, 0.20711169
(78, 0.1675165
(52, 0.1085453
(16, 0.0894055
(67, 0.0722789
(34, 0.0512583
(44, 0.0454973
(43, 0.0431552
(14, 0.0422104
(86, 0.0414977
(56, 0.0358734
(63, 0.0200182
(68, 0.0175117
(95, 0.0159438
(27, 0.0142862
(39, 0.0126232
78
34
clean_L._x._F._ (91, 0.1984136
(64, 0.1295765
(61, 0.1256003
(23, 0.1105553
(46, 0.1078987
(14, 0.0901047
(3, 0.07324318
(76, 0.0477254
(68, 0.0457981
(25, 0.0243687
(31, 0.0189051
64
46
54
clean_M._A._A.
(8, 0.18233765
(78, 0.1422151
(14, 0.1040762
(40, 0.0812337
(2, 0.07756599
(36, 0.0754049
(93, 0.0722218
(58, 0.0716855
(16, 0.0669194
(87, 0.0645960
10 autonomous ve
14
clean_M._A._J.
(36, 0.2054616
(85, 0.1442392
(31, 0.1393064
(3, 0.13772922
(10, 0.0934645
(16, 0.0727969
(2, 0.07161588
(59, 0.0472226
(78, 0.0372751
(14, 0.0160768
(37, 0.0159838
16
81
clean_M._A._P.
(22, 0.2104320
(37, 0.2065421
(14, 0.0884570
(8, 0.08022760
(85, 0.0616219
(16, 0.0527599
(3, 0.05255327
(31, 0.0511956
(2, 0.05016996
(87, 0.0425793
(0, 0.04132405
(20, 0.0193233
(72, 0.0172744
81 autonomous ve
10
clean_M._A.-M.
(67, 0.3247214
(86, 0.2252329
(31, 0.1230190
(22, 0.1164246
(23, 0.0513312
(62, 0.0482710
(72, 0.0363643
(51, 0.0329214
(63, 0.0178345
29
51
94
clean_M._B._C.
(78, 0.2229050
(36, 0.1925538
(37, 0.1110608
(14, 0.0899208
(2, 0.05743418
(94, 0.0442463
(3, 0.04195056
(46, 0.0416650
(64, 0.0412450
(15, 0.0213728
(74, 0.0209990
(63, 0.0170060
(42, 0.0157907
(31, 0.0147819
(99, 0.0140082
(52, 0.0138141
36 autonomous ve
3
clean_M._B._Z.
(37, 0.1685113
(67, 0.1660120
(86, 0.1321063
(61, 0.1316656
(14, 0.1201769
(85, 0.0760359
(78, 0.0655742
(31, 0.0533479
(87, 0.0366508
(43, 0.0298655
3
81
24
clean_M._C._D.
(37, 0.1937008
(67, 0.1042430
(61, 0.0881204
(78, 0.0767087
(83, 0.0722061
(72, 0.0685707
(14, 0.0680548
(81, 0.0668991
(63, 0.0559347
(74, 0.0516330
(51, 0.0438057
(86, 0.0423050
(16, 0.0231577
(8, 0.02080304
51
81
3
clean_M._H._A (81, 0.1612589
(2, 0.14282397
(91, 0.1122323
(44, 0.1112996
(15, 0.1019960
(62, 0.0944139
(27, 0.0608212
(97, 0.0398686
(76, 0.0379315
(94, 0.0334008
(40, 0.0279023
(63, 0.0253300
(3, 0.02005423
clean_M._J._B.
(56, 0.1783399
(6, 0.17364498
(37, 0.1660415
(2, 0.16399419
(15, 0.0773080
(95, 0.0491186
(23, 0.0482749
(3, 0.03594614
(86, 0.0294965
(16, 0.0258904
(99, 0.0204536
2
46
clean_M._J._H.
(61, 0.3350818
(8, 0.24581696
(3, 0.10153867
(95, 0.0890293
(90, 0.0398670
(83, 0.0383637
(44, 0.0360285
(59, 0.0330463
(70, 0.0309609
(78, 0.0210298
autonomous ve
14
3
clean_M._x._E.
(76, 0.6514179
(64, 0.1343393
(80, 0.1103922
(23, 0.0628232
46
clean_N._C.-B.
(32, 0.1649809
(97, 0.1284980
(46, 0.1182114
(3, 0.09399305
(56, 0.0718359
(14, 0.0621802
(42, 0.0579016
(24, 0.0484068
(64, 0.0462743
(68, 0.0398392
(78, 0.0356620
(35, 0.0283078
(67, 0.0280645
(86, 0.0254055
(54, 0.0196094
(95, 0.0119122
97
68
46
clean_N._T._At
(81, 0.5435616
(20, 0.1444148
(38, 0.0723725
(59, 0.0462965
(14, 0.0401866
(99, 0.0333977
(52, 0.0303196
(0, 0.02399035
(51, 0.0172130
(98, 0.0114545
(23, 0.0113939
20
38
24
clean_P._B._D.
(64, 0.1508629
(3, 0.12069143
(16, 0.1189783
(68, 0.1031632
(78, 0.0633776
(17, 0.0618735
(42, 0.0597788
(32, 0.0596022
(98, 0.0444853
(22, 0.0418362
(36, 0.0361348
(40, 0.0264197
(2, 0.02360329
(8, 0.02173188
(52, 0.0165175
(58, 0.0132241
17
98
42
clean_P._V._K.
(46, 0.1440402
(31, 0.0866386
(6, 0.08090087
(0, 0.07888446
(35, 0.0718078
(55, 0.0587026
(78, 0.0554074
(64, 0.0529645
(22, 0.0527072
(25, 0.0492425
(32, 0.0468147
(42, 0.0378991
(52, 0.0352766
(86, 0.0324205
(59, 0.0312733
(23, 0.0251361
46
42
70
clean_P._V._M.
(8, 0.32049936
(6, 0.14211583
(3, 0.13474801
(22, 0.0925464
(63, 0.0721793
(37, 0.0695099
(55, 0.0392101
(10, 0.0360285
(76, 0.0260295
(31, 0.0221150
(61, 0.0137829
(18, 0.0120839
autonomous ve
8
14
clean_Q._B._M.
(64, 0.3092760
(14, 0.2418771
(94, 0.1694451
(98, 0.1411230
(46, 0.0818697
68
98
2
clean_S._A._G.
(67, 0.4891512
(37, 0.0883767
(29, 0.0582277
(94, 0.0578563
(72, 0.0530567
(91, 0.0497829
(62, 0.0481698
(81, 0.0465742
(86, 0.0336902
(36, 0.0220420
(35, 0.0179252
(0, 0.01626559
31
94
72
clean_S._A._S.
(38, 0.2684228
(46, 0.1710749
(72, 0.1260759
(37, 0.1117266
(8, 0.06626350
(29, 0.0559041
(78, 0.0542343
(81, 0.0512411
(2, 0.03838671
(95, 0.0343008
38 autonomous ve
16
clean_S._B._M.
(35, 0.1151783
(14, 0.0855717
(6, 0.08162071
(31, 0.0816080
(3, 0.06400195
(63, 0.0626852
(37, 0.0597022
(2, 0.05763397
(95, 0.0516309
(16, 0.0493779
(34, 0.0421244
(99, 0.0406811
(22, 0.0386142
(39, 0.0318285
(94, 0.0226686
(71, 0.0225984
31 autonomous ve
34
clean_S._D._B.
(10, 0.1621257
(86, 0.1059432
(22, 0.0745854
(6, 0.07362194
(23, 0.0728841
(2, 0.06763015
(42, 0.0669907
(14, 0.0661608
(36, 0.0556248
(46, 0.0554692
(16, 0.0496237
(29, 0.0439753
(59, 0.0292436
(35, 0.0268612
(55, 0.0222010
(37, 0.0119815
16
97
42
clean_S._J._A.
(78, 0.8132457
(2, 0.04271687
(8, 0.04136943
(3, 0.03297572
(59, 0.0264367
(32, 0.0133767
autonomous ve
81
78
clean_S._P._B.
(61, 0.1657975
(16, 0.1497854
(42, 0.1143952
(22, 0.0861378
(29, 0.0755208
(14, 0.0632106
(23, 0.0538681
(6, 0.05092820
(41, 0.0507432
(64, 0.0432399
(46, 0.0346472
(35, 0.0330778
(71, 0.0178680
(2, 0.01660323
(15, 0.0148087
(67, 0.0144630
46
70
clean_T._A._M.
(86, 0.1014169
(67, 0.0907007
(15, 0.0841750
(16, 0.0829978
(61, 0.0814972
(90, 0.0778078
(10, 0.0576244
(37, 0.0545590
(83, 0.0536309
(14, 0.0498823
(78, 0.0494930
(93, 0.0428179
(35, 0.0329805
(94, 0.0272723
(72, 0.0253321
(62, 0.0223435
86 autonomous ve
62
clean_Y._A._P.
(64, 0.1485641
(78, 0.1357398
(98, 0.0751312
(61, 0.0623166
(74, 0.0590923
(23, 0.0577777
(91, 0.0565877
(46, 0.0509807
(95, 0.0463537
(99, 0.0457865
(76, 0.0401288
(52, 0.0386581
(37, 0.0381580
(22, 0.0298268
(70, 0.0173934
(44, 0.0164889
91
32
clean_Z._B._J._ (54, 0.1497370
(91, 0.1219828
(37, 0.1132889
(64, 0.0951638
(36, 0.0840551
(0, 0.08122954
(29, 0.0585554
(97, 0.0396515
(32, 0.0338085
(14, 0.0323105
(46, 0.0294377
(58, 0.0194477
46
54
91
clean_Z._K._x._ (14, 0.1791914
(76, 0.1175761
(2, 0.11085380
(37, 0.0968642
(49, 0.0788293
(3, 0.07762989
(51, 0.0533364
(42, 0.0532999
(0, 0.03704890
(36, 0.0342273
(78, 0.0321005
(17, 0.0319909
(58, 0.0274747
(46, 0.0265536
(22, 0.0217748
(31, 0.0151606
3
37
(16, 0.011909540990210056)
Same topics (= green if present)
(39, 0.015528555433854917)
autonomous vehicle control:
(83, 0.011389083436556098)
vehicle path planning:
(31, 0.010892402353927796)
vehicle detection/tracking
(93, 0.031218164416049261)
automated parking:
networked vehicles:
vehicle communication:
(34, 0.013664542990161588)
(90, 0.019900185966621735)
ABOUT INTERNET OF THINGS
(27, 0.013165997485884592)
(42, 0.024293219895053165)
autonomous vehicle control
(94, 0.019156179644451415)
(85, 0.015296814717774537)
(94, 0.018184296733200003)
(55, 0.01916229414603756)
autonomous vehicle control
(2, 0.015520872833170435)
(44, 0.030509747502455286)
(23, 0.012005809254166498)
autonomous vehicle control
(25, 0.012777512625419499)
(27, 0.015225388005508661)
(93, 0.011962179678279811)
(38, 0.017697232268601131)
(50, 0.019199418504130011)
Very lightweight paper, should produce low probabilities
(87, 0.017064637977851408)
(68, 0.011148673899817859)
(15, 0.032693754677606525)
(58, 0.010199882296212947)
(59, 0.012077592792415107)
(59, 0.043532058260849736)
(95, 0.01459517896237166)
(23, 0.016145201922830577)
(66, 0.010109179841445656)
autonomous vehicle control
(22, 0.012203132802697051)
autonomous vehicle control
27
24
46 (36, 0.1848564
(29, 0.1298889
autonomous ve
3
37 (72, 0.2559601
(61, 0.1129838
0,40,76
autonomous ve
8
18 (2, 0.22751369
(8, 0.13359666
16,78
8
81
10 (8, 0.18093794
(36, 0.1405603
54,58,66
autonomous ve
14
24
(8, 0.52582454
(35, 0.1053866
35,38
95
(95, 0.2347570
(85, 0.1075159
31,34
62
31
(2, 0.11209784
(0, 0.10982743
62,72,99
3
2
37 (37, 0.1778671
(98, 0.1415837
road detection:
25,64
25
90
46 (64, 0.3030772
(61, 0.2103519
49
2
56 (36, 0.1507248
(2, 0.13043038
2
97
46 (97, 0.1898263
(64, 0.1845176
39
(31, 0.3009422
(62, 0.0918108
32
46
91 (64, 0.1208752
(10, 0.0981494
94 autonomous ve
27 (0, 0.28780847
(14, 0.1804275
3
85 autonomous ve
(36, 0.2382778
(37, 0.1099536
16
2
23 (64, 0.1820686
(23, 0.1043574
31 autonomous ve
62 (8, 0.16329881
(0, 0.15599647
32
58
68 (14, 0.1694132
(16, 0.1556911
86
31
62 (86, 0.1676208
(34, 0.1203641
autonomous ve
ABOUT ROBOT
39 (14, 0.1679848
(16, 0.1519493
54
70
46 (14, 0.1592115
(54, 0.1507850
78
34
autonomous ve
(3, 0.20696931
(78, 0.1675713
64
46
54 (91, 0.1982367
(64, 0.1295936
10 autonomous ve
14 (8, 0.18235012
(78, 0.1422352
16
81 autonomous ve
(36, 0.2004328
(85, 0.1446369
81 autonomous ve
10 (22, 0.2103915
(37, 0.2065201
29
51
94 (67, 0.3241753
(86, 0.2248693
36 autonomous ve
3 (78, 0.2228182
(36, 0.1871256
3
81
24 (37, 0.1686570
(67, 0.1659653
51
81
3 (37, 0.1987332
(67, 0.1021661
(81, 0.1769540
(2, 0.16601571
2
46
(56, 0.1906900
(6, 0.17118230
autonomous ve
14
3 (61, 0.3349081
(8, 0.24586743
46
(76, 0.6515046
(64, 0.1347841
97
68
46 (32, 0.1648620
(97, 0.1284760
20
38
24 (81, 0.5448869
(20, 0.1467503
17
98
42 (64, 0.1508724
(3, 0.12064299
46
42
70 (46, 0.1449212
(31, 0.0869137
autonomous ve
8
14 (8, 0.32051546
(6, 0.14205713
68
98
2 (64, 0.3092776
(14, 0.2421471
31
94
72 (67, 0.4859769
(37, 0.0879113
38 autonomous ve
16 (38, 0.2684245
(46, 0.1710797
31 autonomous ve
34 (35, 0.1151748
(14, 0.0856002
(76, 0.11872437549516186)
Same topics (= green if present)
(76, 0.10728386724429928)
autonomous vehicle control:
(98, 0.11482204054275112)
vehicle path planning:
(16, 0.11095466067959121)
vehicle detection/tracking
(83, 0.099992708227210195)
automated parking:
(36, 0.10197158258058026)
networked vehicles:
(78, 0.10974667443858584)
vehicle communication:
(95, 0.11439256411777463)
(23, 0.091408064252341062)
(56, 0.12716253801145527)
(78, 0.097066065651726055)
ABOUT INTERNET OF THINGS
(0, 0.080914136703779838)
(35, 0.085743075471516106)
(67, 0.16098908890118968)
(94, 0.091980057358372974)
(62, 0.10328175892908135)
(34, 0.11921252479198195)
(61, 0.13212397282221669)
(62, 0.098210500234343259)
(2, 0.12432099890414264)
(32, 0.074856650682069967)
(52, 0.10858727166328266)
(61, 0.12567340234222596)
(14, 0.10409280196304362)
(3, 0.13290219339100076)
(14, 0.08843138212265389)
(31, 0.12382616448151035)
(37, 0.10862600579458387)
(86, 0.13212604834057004)
(61, 0.092271868290538586)
Very lightweight paper, should produce low proba
(91, 0.11885317918223637)
(2, 0.14453992095615442)
(3, 0.10115871457693873)
(80, 0.11052799511617664)
(46, 0.1179388406423926)
(38, 0.066431417501460852)
(16, 0.11892074962130053)
(6, 0.081995218719289387)
(3, 0.13477592829968937)
(94, 0.16943680061267874)
(94, 0.052500121435727472)
(72, 0.12607618268822365)
(6, 0.081638863744275481)
16
97
42 (10, 0.1479399
(86, 0.1079217
autonomous ve
81
78 (78, 0.8130703
(2, 0.04279595
46
70 autonomous ve
(61, 0.1677124
(16, 0.1457492
86 autonomous ve
62 (86, 0.1014034
(67, 0.0901782
91
32
(64, 0.1485523
(78, 0.1357089
46
54
91 (54, 0.1524063
(91, 0.1294819
3
37 autonomous ve
(14, 0.1791856
(76, 0.1176699
(14, 0.08591718688928264)
(8, 0.041522056163905016)
(42, 0.089127651321651818)
(15, 0.08420761338234857)
(98, 0.075125971005098865)
(37, 0.10938122093028534)
(2, 0.1108253691003747)
clean_C._C._J.
(64, 0.3030772
(61, 0.2103519
27
24
46
clean_G._A._J.
(36, 0.2382778
(37, 0.1099536
autonomous ve
3
37
clean_Z._B._J._ (54, 0.1524063
(91, 0.1294819
autonomous ve
8
18
clean_M._A._P.
(22, 0.2103915
(37, 0.2065201
8
81
10
clean_S._B._M.
(35, 0.1151748
(14, 0.0856002
autonomous ve
14
24
clean_D._C._S.
(0, 0.28780847
(14, 0.1804275
95
clean_A._B.-N.
(2, 0.22751369
(8, 0.13359666
62
31
clean_D._B._W. (64, 0.1208752
(10, 0.0981494
3
2
37
clean_T._A._M.
(86, 0.1014034
(67, 0.0901782
25
90
46
clean_K._C._F.
(14, 0.1592115
(54, 0.1507850
49
2
56
clean_J._C._S.
(14, 0.1694132
(16, 0.1556911
2
97
46
clean_M._J._B.
(56, 0.1906900
(6, 0.17118230
39
clean_L._x._F._ (91, 0.1982367
(64, 0.1295936
32
46
91
clean_Z._K._x._ (14, 0.1791856
(76, 0.1176699
94 autonomous ve
27
clean_J._A._C.
(8, 0.16329881
(0, 0.15599647
3
85
clean_J._S._B.
(86, 0.1676208
(34, 0.1203641
16
2
23
clean_A._B._S.
(72, 0.2559601
(61, 0.1129838
31 autonomous ve
62
clean_M._B._Z.
(37, 0.1686570
(67, 0.1659653
32
58
68
clean_M._x._E.
(76, 0.6515046
(64, 0.1347841
86
31
62
clean_S._A._S.
(38, 0.2684245
(46, 0.1710797
autonomous ve
ABOUT ROBOT
39
clean_M._J._H.
(61, 0.3349081
(8, 0.24586743
54
70
46
clean_S._J._A.
(78, 0.8130703
(2, 0.04279595
78
34
clean_K._B._H.
(14, 0.1679848
(16, 0.1519493
64
46
54
clean_P._V._K.
(46, 0.1449212
(31, 0.0869137
10 autonomous ve
14
clean_B._B._H.
(95, 0.2347570
(85, 0.1075159
16
81
clean_P._B._D.
(64, 0.1508724
(3, 0.12064299
81 autonomous ve
10
clean_M._A._J.
(36, 0.2004328
(85, 0.1446369
29
51
94
clean_A._C._L.
(8, 0.52582454
(35, 0.1053866
36 autonomous ve
3
clean_S._P._B.
(61, 0.1677124
(16, 0.1457492
3
81
24
clean_M._H._A (81, 0.1769540
(2, 0.16601571
51
81
3
clean_P._V._M.
(8, 0.32051546
(6, 0.14205713
clean_S._D._B.
(10, 0.1479399
(86, 0.1079217
2
46
clean_N._T._At
(81, 0.5448869
(20, 0.1467503
autonomous ve
14
3
clean_S._A._G.
(67, 0.4859769
(37, 0.0879113
46
clean_C._C._Y.
(36, 0.1507248
(2, 0.13043038
97
68
46
clean_A._C._C.
(8, 0.18093794
(36, 0.1405603
20
38
24
clean_C._L._B.
(97, 0.1898263
(64, 0.1845176
17
98
42
clean_M._B._C.
(78, 0.2228182
(36, 0.1871256
46
42
70
clean_B.-M._S.
(37, 0.1778671
(98, 0.1415837
autonomous ve
8
14
(23, 0.091408064252341062)
(36, 0.18485644806255622)
(94, 0.091980057358372974)
(72, 0.25596012169338933)
(37, 0.10938122093028534)
(2, 0.2275136900047077)
(14, 0.08843138212265389)
(8, 0.18093794262797613)
(6, 0.081638863744275481)
(8, 0.52582454211157947)
(67, 0.16098908890118968)
(95, 0.23475704115663953)
(98, 0.11482204054275112)
(2, 0.11209784927366824)
(35, 0.085743075471516106)
(37, 0.17786710077087728)
(15, 0.08420761338234857)
(64, 0.30307722290019357)
(32, 0.074856650682069967)
(36, 0.15072481588128858)
(61, 0.13212397282221669)
(97, 0.18982639068292873)
(2, 0.14453992095615442)
(31, 0.30094220805326038)
ABOUT INTERNET OF THINGS
(61, 0.12567340234222596)
(64, 0.12087527306715326)
(2, 0.1108253691003747)
(0, 0.28780847536848242)
(34, 0.11921252479198195)
(36, 0.23827786532158918)
autonomous vehicle control
(62, 0.098210500234343259)
(64, 0.1820686842132069)
(76, 0.10728386724429928)
(8, 0.1632988121483552)
(86, 0.13212604834057004)
(14, 0.16941322421587424)
(80, 0.11052799511617664)
(86, 0.16762081225685338)
(72, 0.12607618268822365)
(14, 0.16798481714197125)
(3, 0.10115871457693873)
(14, 0.15921153738421318)
(8, 0.041522056163905016)
(3, 0.20696931793586407)
autonomous vehicle control
(2, 0.12432099890414264)
(91, 0.19823679216460691)
(6, 0.081995218719289387)
(8, 0.18235012515513513)
(36, 0.10197158258058026)
(36, 0.20043286904297364)
autonomous vehicle control
(16, 0.11892074962130053)
(22, 0.21039159482527384)
(3, 0.13290219339100076)
(67, 0.32417530572754122)
(83, 0.099992708227210195)
(78, 0.22281829026120953)
(42, 0.089127651321651818)
(37, 0.16865701426790394)
(91, 0.11885317918223637)
(37, 0.19873329096901887)
(3, 0.13477592829968937)
(81, 0.17695400107396794)
Very lightweight paper, should produce low probabilities
(14, 0.08591718688928264)
(56, 0.19069006158388141)
(38, 0.066431417501460852)
(61, 0.33490816986681493)
(94, 0.052500121435727472)
(76, 0.65150463233416078)
(56, 0.12716253801145527)
(32, 0.16486208522831469)
(16, 0.11095466067959121)
(81, 0.54488699580868405)
(78, 0.097066065651726055)
(64, 0.1508724259221636)
(37, 0.10862600579458387)
(46, 0.14492128773661103)
(95, 0.11439256411777463)
(8, 0.32051546792194785)
clean_H._x._E.
(64, 0.1820686
(23, 0.1043574
68
98
2
clean_B._W._A.
(2, 0.11209784
(0, 0.10982743
31
94
72
clean_M._C._D.
(37, 0.1987332
(67, 0.1021661
38 autonomous ve
16
clean_L._C._A.
(3, 0.20696931
(78, 0.1675713
31 autonomous ve
34
clean_Q._B._M.
(64, 0.3092776
(14, 0.2421471
16
97
42
clean_A._B._P.
(36, 0.1848564
(29, 0.1298889
autonomous ve
81
78
clean_M._A.-M.
(67, 0.3241753
(86, 0.2248693
46
70
clean_M._A._A.
(8, 0.18235012
(78, 0.1422352
86 autonomous ve
62
clean_C._W._A (31, 0.3009422
(62, 0.0918108
91
32
clean_N._C.-B.
(32, 0.1648620
(97, 0.1284760
46
54
91
clean_Y._A._P.
(64, 0.1485523
(78, 0.1357089
3
37
13 st
37 st
(62, 0.10328175892908135)
(64, 0.3092776500292902)
(78, 0.10974667443858584)
(67, 0.48597694012599374)
(61, 0.092271868290538586)
(38, 0.2684245399469421)
(52, 0.10858727166328266)
(35, 0.11517488141378281)
(94, 0.16943680061267874)
(10, 0.14793998316307688)
(76, 0.11872437549516186)
(78, 0.81307037047720154)
(31, 0.12382616448151035)
(61, 0.16771243527230448)
autonomous vehicle control
(14, 0.10409280196304362)
(86, 0.10140347723875423)
(0, 0.080914136703779838)
(64, 0.1485523219220494)
(46, 0.1179388406423926)
(54, 0.15240639953675808)
(98, 0.075125971005098865)
(14, 0.17918560023085695)
autonomous vehicle control
image
Word #1
Word #2
Word #3
Word #4
Word #5
Word #6
Word #7
Label
0 agent
coil
power
circuit
policy
aorta
turn
1 uncertainty
parameter
estimation
covariance
method
matrix
system
2 image
recognition
sample
vehicle
model
mask
logo
3 oscillation
natural
system
vo
velocity
locomotion
value
4 vehicle
speed
control
lateral
reference
profile
strategy
Vehicle Control
5 obstacle
vehicle
task
avoidance
control
path
velocity
6 vehicle
utility
target
feature
eye
assignment
based
Vehicle Utility
7 ve
olarak
filtre
için
bir
ekbho
bu
8 vehicle
task
method
control
background
detection
video
9 sensor
market
imu
system
cost
fusion
data
IMU market
10 path
curve
vehicle
bézier
point
Path prediction
11 system
vehicle
image
obstacle
used
decision
detection
12 vehicle
set
time
approach
algorithm
system
13 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
14 rate
ber
vocoder
channel
amr
ecall
mode
15 problem
bound
path
constraint
solution
state
approach
Pathfinding
16 control
vehicle
tracking
trajectory
controller
following
time
Vehicle Control
17 vehicle
set
trajectory
reachable
constraint
tk
system
18 road
model
color
algorithm
point
method
image
Road modelling
19 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
20 image
character
plate
network
neural
value
recognition
21 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
22 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
23 particle
measurement
fastslam
filter
problem
position
based
position measur
24 vehicle
threat
figure
position
landmark
avg
time
25 vehicle
control
wheel
tire
friction
road
longitudinal
Wheel control
26 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
27 attack
vehicle
risk
severity
problem
car
number
28 cost
disparity
cloud
time
census
pixel
weight
Image processi
29 agent
ri
source
signal
ci
formation
algorithm
30 process
group
request
pi
node
quorum
message
31 image
obstacle
top
used
system
view
path
32 system
data
obstacle
tentacle
mobility
information
vehicle
33 light
vehicle
detection
tracking
algorithm
frame
signal
34 control
brake
system
vehicle
design
stopping
model
35 vehicle
state
time
model
fleet
based
road
36 research
technology
curve
patent
stage
vehicle
37 vehicle
component
driving
architecture
platform
system
control
38 vehicle
intersection
figure
type
lane
system
output
39 av
ri
leader
time
follower
one
40 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
41 tag
reader
antenna
function
rra
position
positioning
42 intersection
vehicle
traffic
time
group
car
light
43 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
44 vehicle
communication
cooperative
attack
cacc
stream
Vehicle coopera
45 trajectory
passenger
based
test
boarding
alighting
tracking
System test with
46 demand
policy
vehicle
condition
tmhp
time
stability
47 parking
vehicle
space
pedestrian
task
state
system
Parking
48 image
based
figure
time
navigation
algorithm
environment
49 car
vehicle
algorithm
caravan
time
aid
three
50 information
platform
service
self
driving
tourist
content
51 vehicle
detection
cluster
algorithm
ve
time
FALSE
52 vehicle
control
angle
model
lateral
dynamic
system
Vehicle Control
53 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
54 task
vehicle
tracking
target
model
platoon
measurement
55 driving
driver
automated
vehicle
wa
speed
distance
Driving
56 graph
pattern
rule
system
time
set
model
57 trajectory
vehicle
trim
point
using
time
maneuver
Maneuvering
58 task
cbba
bundle
assignment
agent
bid
ij
59 agent
task
node
algorithm
cbba
assignment
time
60 system
driving
change
vehicle
development
software
simulation
61 vehicle
prt
system
wheel
consumption
fuel
campus
62 vehicle
path
planning
constraint
trajectory
problem
time
63 road
image
network
feature
learning
pixel
segmentation
64 oscillation
vo
system
natural
velocity
locomotion
control
??
65 ve
için
filtre
olarak
ekbho
bir
bu
66 car
system
model
control
driven
ha
vehicle
67 fault
bg
system
element
set
model
68 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
69 frequency
stimulus
signal
autonomous
car
ss
70 camera
state
imu
estimate
vehicle
model
behavior
Hardware beha
71 system
sensor
vehicle
control
team
rascal
figure
72 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
73 signal
turbulence
sequence
time
set
system
coherence
74 vehicle
system
data
wa
sensor
test
gps
75 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
76 vehicle
road
time
future
position
forwarding
driving
77 road
lane
semantic
node
map
based
image
Lane following
78 vehicle
trajectory
obstacle
car
driving
self
road
79 driving
event
feature
data
driver
speed
classification
80 plate
image
character
license
correction
value
neural
81 arduous
narikiyo
centred
parametrize
hiriko
emplified
pean
82 vehicle
system
lane
line
detection
sensor
wa
Lane following
83 potential
control
vehicle
field
sin
co
?
84 segment
point
track
tolerance
curvature
curve
arc
85 video
tracking
frame
system
tdt
sequence
proposed
Video tracking
86 uxv
node
flow
routing
network
packet
path
Network
87 robot
mobile
control
navigation
environment
obstacle
motion
Navigation
88 camera
image
vehicle
feature
map
track
method
89 car
driver
control
system
track
program
model
Vehicle Control
90 prt
vehicle
fuel
system
consumption
campus
amd
91 observer
fx
fy
longitudinal
sensor
sliding
fault
92 probability
input
traffic
markov
chain
state
participant
Traffic predictio
93 traffic
light
prior
detection
image
location
score
94 road
image
system
edge
method
detection
algorithm
Lane detection
95 follower
sensor
velocity
leader
vehicle
test
heading
96 image
camera
point
frame
estimation
motion
method
97 image
visual
database
localization
feature
solution
infrared
Acronyms/ word explanations
Power management
Probability estimation
Image recognition
Obstacle avoidance managemen
Vehicle detection (video feed?)
imu = inertial measurement unit, measures crafts velocity orientation and gravitational forces
Obstacle detection
Vehicle software
Vehicle limitations
Image input to machine learning
fastslam is an algoritm for localising a robot and mapping it's surroundings
Vehicle threat assesment
Security aspects
disparity could be related to image recognition
Obstacle management
Obstacle management
Obstacle management
Vehicle detection
Brake control/management
Model for vehicle fleets
Vehicle research
System architecture
Intersection handling
tag might be related to gps technology
Intersection handling
cacc could be correlated active clause coverage, A Logic Coverage Criterion from Software Testing
alighting might refer to getting off a vehicle
Image-based navigation
Vehicle cooperation
Information service for tourists?
Vehicle detection?
Vehicle tracking (platoon implies several vehicles)
Data representation
Software development for vehicles in simulation
Fuel Consumption
Path planning (Planera körning?)
Image recognition for roads
System for vehicle control
Signal interpretention from sensors?
(IMU is a chip that works along with cameras)
Hardware relation
Signal data behavior
Vehicle localization/vehicle navigation
Maneuver planning (Planera körning?)
Obstacle avoidance
Driving data handling
Image recognition of license plates
Curve assessment
Vehicle tracking through image processing
Fuel Consumption
Navigation failure management
markov = probability mathematician
Traffic Light detection
Follow other vehicle
Image processing
Image processing
98 object
grid
moving
detection
laser
motion
data
Object detection
99 obstacle
camera
depth
image
car
map
plane
Obstacle detection
0 system
vehicle
control
driver
controller
traffic
autonomous
autonomous vehicle c
40, 76
1 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
2 vehicle
system
autonomous
control
intelligent
path
navigation
3 vehicle
model
driving
road
autonomous
driver
based
autonomous driving
4 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
5 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
6 measurement
vehicle
inertial
noise
path
controller
ground
7 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
8 control
vehicle
lateral
speed
autonomous
cruise
based
9 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
10 vehicle
obstacle
uncertainty
probabilistic
planning
electric
linear
11 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
12 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
13 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
14 vehicle
autonomous
system
simulation
sensor
real
time
15 vehicle
dc
lane
battery
system
converter
control
16 vehicle
trajectory
path
optimization
control
planning
method
Vehicle path planning
Samma som 78
17 radar
automotive
sige
bicmos
technology
packaging
ghz
Radar technology
18 sliding
mode
vehicle
skid
control
observer
force
Sliding mode
19 vision
robot
road
vehicle
obstacle
dynamic
static
20 vehicle
interface
mobile
human
user
automotive
interaction
21 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
22 vehicle
system
autonomous
data
tracking
tire
signal
23 vehicle
detection
obstacle
algorithm
system
camera
autonomous
obstacle detection
24 pedestrian
feature
behavior
traffic
road
estimation
relevance
traffic behavior
25 road
cue
detection
level
method
low
vision
road detection
samma som 64
26 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
27 road
software
challenge
robot
urban
traffic
system
urban traffic software
28 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
29 system
lighting
autonomous
vehicle
car
intelligent
led
vehicle lights/lighting
30 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
31 vehicle
sensor
system
fleet
network
automated
management
Networked vehicles
32 object
detection
data
vehicle
sensor
fusion
classification
33 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
34 vehicle
tracking
distributed
network
system
topology
dynamic
35 vehicle
automated
parking
human
autonomous
system
detection
Automated parking
36 road
maneuver
autonomous
vehicle
driving
prediction
model
Maneuver prediction
37 control
system
driving
vehicle
road
gps
driver
Driving
38 car
system
parking
automated
android
existing
human
35
39 cell
sram
write
tfet
characteristic
circuit
noise
Hardware
40 control
system
vehicle
autonomous
integrator
robot
dynamic
0
41 software
component
algorithm
robotic
advanced
robot
system
robot software
42 sensor
system
market
calibration
autonomous
fusion
parameter
Sensor system
43 control
warehouse
system
vehicle
net
petri
generalized
44 transport
autonomous
vehicle
model
agent
based
future
Autonomous transport
45 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
46 camera
vehicle
image
road
vision
system
autonomous
47 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
48 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
49 system
vehicle
autonomous
forest
control
terrain
ground
50 tracking
driving
autonomous
activity
driver
recognition
classification
51 neural
network
system
artificial
braking
car
labview
artificial intelligence
52 robot
vehicle
autonomous
state
position
control
industrial
?
53 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
54 vehicle
video
detection
tracking
traffic
automatic
method
Vehicle detection/track lik 58, 66
55 vehicle
skew
plate
system
recognition
number
correction
Vehicle number plate
56 semantic
autonomous
mapping
bridge
scale
large
map
Map/mapping
57 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
58 vehicle
obstacle
lane
tracking
system
camera
vision
vehicle/obstacle tracki
lik 54, 66
59 locomotion
system
natural
oscillation
matrix
damping
body
60 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
61 system
road
lane
detection
vision
vehicle
based
lane detection
62 driving
network
self
communication
vehicular
car
vehicle
vehicle communication obs samma som 72 och 99
63 fuzzy
control
decision
logic
system
set
paper
64 image
detection
estimation
vision
stereo
road
based
Road detection
samma som 25
65 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
66 vehicle
tracking
particle
method
time
real
filter
vehicle tracking
lik 54,58
67 intersection
traffic
autonomous
vehicle
transportation
intelligent
road
?
68 fusion
sensor
track
filter
information
data
environment
69 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
70 vehicle
vision
road
algorithm
hough
lane
ransac
image analysis
71 graph
control
vehicle
theory
system
dimensional
rigid
?
72 vehicle
driving
automated
communication
sensor
technology
infrastructure
Vehicles communicati
samma som 62 och 99
73 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
74 learning
terrain
classification
mechanical
visual
supervision
automatic
Terrain classification
75 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
76 robot
vehicle
control
sensor
image
autonomous
mobile
0
77 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
78 vehicle
planning
path
control
autonomous
approach
based
path planning
samma som 16
79 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
80 control
vehicle
sensor
tool
nist
robot
guided
guided vehicle
81 vehicle
system
collision
safety
service
avoidance
sensor
safety management
82 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
83 vehicle
driving
personalization
autopilot
safety
automated
control
automated driving
84 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
85 traffic
safety
vehicle
dynamic
participant
verification
markov
safe decision making
86 autonomous
vehicle
system
car
decision
communication
algorithm
decision making
87 controller
hierarchical
intelligent
vehicle
control
model
level
88 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
89 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
90 trajectory
profile
generation
velocity
curvature
curve
vehicle
Vehicle curve handling
91 road
detection
representation
pedestrian
image
geometry
classification
92 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
93 control
vehicle
field
potential
aircraft
avoidance
spline
94 vehicle
information
behavior
tracking
intersection
position
system
intersection handling
95 vehicle
system
autonomous
time
real
utility
coordination
96 re
maximum
cubic
interpolation
manoeuvre
phase
parametrized
97 vehicle
localization
visual
method
feature
route
robot
localization/navigation
98 control
vehicle
motion
autonomous
system
dynamic
tracking
99 vehicle
communication
system
wireless
intelligent
highway
roadside
vehicle communication obs samma som 62 och 72
autonomous vehicle navigation
autonomous vehicle speed control
cruise lateral och speed refererar till speed
vehicle obstacle avoidance
autonomous vehicle simulation
vehicle power management
Human-vehicle interface/interaction
Object detection & classification
Networked vehicle mapping/tracking
vehicle-warehouse system
petri net used for graphical moddeling of formal system
Image processing for autonomous vehicles
Off road vehicle control
environment information
hough = smart computer guy who does computer vision stuff
intelligent vehicle controller
image classification in traffic
POTENTIAL AIRCRAFT AVOIDANCE
autonomous vehicle coordination
autonomous vehicle motion tracking
