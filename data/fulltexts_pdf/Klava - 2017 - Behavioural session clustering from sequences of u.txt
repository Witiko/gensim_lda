Behavioural session clustering
from sequences of user actions
Anton Klava
aklava@kth.se
May 29, 2017
Abstract
Website and application owners are constantly looking at improving their
service which can result in rapidly changing applications.
This creates the need
for dynamic and unsupervised methods of finding common shared user behavior
across sessions.
In this work an unsupervised method of session clustering is presented.
It
involves three steps from stored session events to clustered sessions;
labeling
and sequencing, feature extraction and clustering.
Comparison is also made to
a baseline method of manually engineered feature vectors.
It is found that the baseline feature extraction method show comparable
cluster strengths compared to earlier work based on silhouette scores.
Stronger
still
was the clusters of
a 3-gram method using one of
the proposed labeling
methods.
Gaussian mixture models with spherical
covariance restriction was
also shown to be slightly stronger compared to the other methods tested.
Sammanfattning
F
¨
oretag letar st
¨
andigt efter nya s
¨
att att f
¨
orb
¨
attra sina produkter.
Det kan
leda till snabba f
¨
or
¨
andringar i en produkt och ett behov av att kunna analysera
anv
¨
andarbeteende dynamiskt och utan handh˚allning.
I det h
¨
ar arbetet behandlas tre steg fr˚an lagrad sessionsdata till gruppering
av sessioner d
¨
ar liknande beteende observerats.
De tre stegen
¨
ar:
d
¨
opning och
sortering,
vektorsrepresentation och gruppering.
J
¨
amf
¨
orelse g
¨
ors ocks˚a mot en
basmetod som anv
¨
ander manuellt definierade m˚att f
¨
or vektorsrepresentation.
Basmetoden gav grupperingar med j
¨
amf
¨
orbar styrka till tidigare arbete som
gjorts baserat p˚a Silhouette v
¨
ardet.
¨
Annu starkare grupperingar gav en 3-gram
metod tillsammans med en av de f
¨
oreslagna d
¨
opningsmetoderna. Gaussian mix-
ture models var den mest framg˚angsrika grupperingsmetoden och framf
¨
or allt
n
¨
ar variansen begr
¨
ansades sf
¨
ariskt.
Acknowledgements and thanks
I would like to thank Magnus Petersson at Spotify for making this work possi-
ble.
All the support and feedback he has provided throughout has really been
invaluable.
Many thanks to Mario Romero for the supervision from KTH and his forth-
comingness making things as smooth as possible.
Ann Bengtsson also deserve
a mention because of
her prompt email
replies and patience with a confused
student.
Thanks also to the others at the Analytics Reaseach team at Spotify that
helped me with technical problems, feedback and discussions during my work.
Contents
1
Introduction
3
1.1
Background .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
1.2
Disposition
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
1.3
Spotify .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
1.4
Problem definition
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
2
Related work and theory
5
2.1
Vector space models and feature extraction
.
.
.
.
.
.
.
.
.
.
.
.
5
2.1.1
Latent semantic analysis .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
2.1.2
Principal component analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
2.1.3
Doc2vec .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
2.2
Clustering and visualization .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
2.2.1
K-means .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
2.2.2
Gaussian mixture model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
2.3
Cluster evaluation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
2.3.1
Mean squared error
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
2.3.2
BIC score
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
2.3.3
Silhouette score .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
2.4
Behavioral and user modeling .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
3
Method
14
3.1
Overview .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
3.2
Preprocessing and data set
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
3.2.1
Session definition .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
3.2.2
Session behavior similarity .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
3.3
Labeling and sequencing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
3.3.1
Raw labeler
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
3.3.2
Action labeler .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
3.3.3
Compact labeler
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
3.3.4
Compact time labeler
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
3.4
Feature extraction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
3.4.1
Baseline .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
3.4.2
N-gram .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19
3.4.3
Doc2vec .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
3.5
Clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
3.6
Evaluation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
4
Results
23
4.1
Labeling and sequencing methods .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
4.2
Feature extraction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
4.2.1
N-gram .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
4.2.2
Doc2vec .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
4.3
Clusterings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
4.3.1
K-means .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
1
4.3.2
Gaussian mixture models
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
4.3.3
Qualitative analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
5
Discussion
31
5.1
Labeling and sequencing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
31
5.2
Feature extraction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
31
5.3
Session clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
6
Conclusions
33
6.1
Future work .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
A Cluster table
37
2
1
Introduction
1.1
Background
Website and application owners are constantly looking to offer users a better
service.
Companies are increasingly leveraging collected user data to reach this
goal.
Analyzing user behavior is nothing new and has become big business.
Com-
panies such as Optimizely [1], Heap [2] and Mixpanel [3] work solely with offering
services to interpret and track the behavior of users.
They all, however, require
expert domain knowledge to set up page-flow funnels and metrics to track.
This
can be difficult and labor intensive for large,
changing applications when fine
grained behavior is sought.
The results may also be biased or limited by the ex-
pert in question, sometimes called the knowledge bottleneck problem [4].
These
issues can motivate using an unsupervised approach to identifying recurring
usage patterns.
Spotify [5]
use their usage data for artist compensation,
focusing develop-
ment resources, build streaming features such as Discover Weekly [6] and more.
User behavior research is also nothing new for the company [7]–[9].
Earlier work
into clustering sessions at Spotify has been made either using music genre and
mood [7] or manually engineered feature vectors [9].
In this paper focus is on finding an unsupervised method of analyzing user
behavior on a session level by considering actions taken by a user.
The hypoth-
esis is that more nuance of the sessions can be incorporated by considering the
pages viewed, streaming features used and the order they appear compared to
the baseline method of manually engineered feature vectors.
Clustering sequences of actions require several operations to find which ses-
sions are similar to others and what attributes they share.
First each action
and event with corresponding meta data must be assigned a label and position
in a session sequence.
Secondly each sequence must be mapped to some fixed
length vector so that session to session comparisons can be made.
Third,
fi-
nally, sessions need to be clustered so that similar observed behavior is grouped
together.
1.2
Disposition
Background and problem statement is given here in section 1 on page 3.
Related work is discussed in section 2 on page 5 together with details of some
of the earlier work.
Vector space models,
clustering and visualization,
cluster
evaluation, behavioral- and user modeling are considered.
Method is addressed in section 3 on page 14.
Reasoning behind the chosen
methods,
definition of
training set and preprocessing will
be handled in this
section.
Results are presented in section 4 on page 23.
Plots and tables show various
findings.
3
Discussion of observations from the results and why they may can be found
in 5 on page 31.
Finally conclusions of
the work is presented together with suggestions for
future work in section 6 on page 33.
1.3
Spotify
This
work was
done in collaboration with Spotify,
which offered access
to
anonymized data and the tooling they use for research.
Spotify is a music streaming company that lets users access a large collection
of songs for instant listening.
They offer a free version,
supported by ads,
and
a paid version without ads for a fixed monthly fee.
Software clients are a available for most popular devices and operating sys-
tems including computers, phones, smart-TV’s and other digital media players.
1.4
Problem definition
An unsupervised method to get from stored session events to session vectors
and session clusters is sought.
The sessions considered are limited to the free
Spotify iOS client.
Three stages describe the process:
1.
Methods for sequencing and labeling to convert high-dimensional session
events to a labeled sequence.
2.
Methods of
feature extraction to represent variable length sequences as
fixed length vectors.
3.
Clustering approaches of session vectors to find common behavior.
Common cluster metrics will
be used to reason about the performance of
the various approaches taken.
The unsupervised nature of
the work makes
determining effectiveness of
specific methods difficult as no test data of
true
labeled clusters are available.
Other than the cluster metrics some heuristics
will be used to determine the best clustering method, in terms of cohesion and
descriptiveness.
Compared to earlier work into clustering sessions at Spotify [9]
emphasis
here is on the sequences of session events rather than aggregated metrics.
4
2
Related work and theory
2.1
Vector space models and feature extraction
Vector space models represent objects by points in space where position and
vicinity has meaning.
Clustering and classification can be accomplished by
partitioning the space and similarities between objects can be measured by their
closeness.
It is a model that is commonly used to represent text documents but
is not limited to that domain.
Using vector space models for predicting playlists was done in [10], where an
embedding was created so that song transition probabilities was proportional
to the distance in the space.
Several
other attributes were added to increase
fidelity;
song popularity,
penalization,
semantic tags and observable features
(derived from the audio signal for example).
The final embedding enabled cre-
ation of new playlists by walking to nearby datums in the space.
Personalized point of interest (POI) recommendation was considered in [11]
where an embedding was created for POIs.
The euclidean distance between POIs
was defined to be proportional to the probability of observing such a transition
in the training data set.
Personalization is achieved by an additional embedding
space with distances proportional to user preference.
Final recommendation is
done ranking POI distances from the two models by linear combination.
2.1.1
Latent semantic analysis
Clearly vector space models can be applied to a wide variety of tasks.
Creating
vector representations of
text is commonly solved in information retrieval
by
treating documents as bag-of-words.
Each word is represented by a new dimen-
sion with its value related to the number of occurrences of that word.
As the
vocabulary can be in the hundreds of thousands the bag-of-words representation
become unwieldy and some form of dimensionality reduction that maintain the
semantic meaning of the documents become of interest.
One such method is the latent semantic analysis (LSA) [12], which uses the
singular value decomposition (SVD) [13] to find this reduction.
Given a matrix
M of k documents represented as bag-of-words in l
dimensions the SVD gives
a decomposition:
M = UΣV
?
where U is a k × k orthogonal
matrix,
Σ is a diagonal
k × l
matrix with a
non-negative diagonal and V
?
is an l × l orthogonal matrix.
An intuitive interpretation of SVD is sometimes given by liking the V
?
as a
rotation or reflection to be scaled by Σ in some new basis,
and finally another
post-scaling rotation or reflection by U .
The diagonal
values of Σ are by con-
vention positioned in a non-increasing order and known as the singular values
of M .
Truncation of
the SVD is used to create a low rank approximation of
M
and can also be used to reduce the column dimensionality of M .
To reduce M
from l to some given m using truncation only the m highest singular values of
5
Σ are kept, making Σ
m
a k × m matrix.
Since the basis of the reduced M
m
is
unimportant, as long as they sufficiently represent the vector space of M , there
is no need to rotate or reflect back to the old basis l so V
?
in the SVD can be
omitted.
It is now possible to construct M
m
of reduced dimensionality.
M
m
= UΣ
m
When M is large randomized SVD is often used,
which also can utilize
sparseness in M .
The interested reader is referred to [14] for details on how to
compute the randomized SVD.
One limitation of the LSA model is the bag-of-word representation that do
not capture the order of words and in what context they appeared.
This can be
alleviated by using sub-sequences of words for each dimensionality, also known
as n-grams.
For example a sub-sequence of length 1 is the same as the bag-of-
words representation while 2-gram consider each pair of words.
2.1.2
Principal component analysis
Another method of dimensionality reduction that also can be computed using
SVD is principal component analysis (PCA). The method is commonly used in
exploratory data analysis because of simplicity and non-parametric nature.
In short PCA can be described as constructing the orthogonal
basis that
best capture the variance of the data M .
We seek some projection P of M into
some new space Y
Y = PM
such that the covariance matrix C
Y
=
1
n
Y Y
T
is diagonal.
n is the number of
samples.
Then the rows of P are called the principal
components of M [15].
2.1.3
Doc2vec
Recently another method of
finding vector representations of
documents has
been gaining attention known as Doc2vec [16].
It
is closely related to the
Word2vec model [17] used to find vector representations of words.
Word2vec uses a simple neural network and has been found to enable training
on billions of words per hour with vocabularies in the hundreds of thousands.
Much of
this is because of
two alternative optimizations to the common final
softmax layer:
hierarchical softmax and negative sampling.
Hierarchical softmax
will be discussed further below.
Negative sampling uses two sets of context and
word tuples, one with true and observed tuples and the other, much larger set,
with random negative examples [18], [19].
Predicting future user actions using a combination of Word2vec with a tree-
based frequency model
was done in [20].
Word2vec capture the context of
which a word appear and was used to provide relevant suggestions.
Top perfor-
mance was achieved on all real-world evaluation tests showing the usefulness of
Word2vec in subjects other than language modeling.
6
Figure
1:
Structure
of
Distributed Memory model
of
Paragraph Vectors
(PV-DM).
The structure of the Word2vec and Doc2vec models is similar but for Doc2vec
with the addition of a document word in each context.
The document word is
unique and fixed for all
contexts of that document.
Word vectors trained us-
ing Word2vec can be reused during training of the document vectors.
Doc2vec
document vectors were found to perform well
in semantic tests,
and similarly
to the word vectors vector operations can produce useful semantic results [21].
Two configurations of Doc2vec exist which of one is the Distributed Mem-
ory model
of Paragraph Vectors (PV-DM).
Figure 1 shows an overview of the
structure of PV-DM where the document vector D
i
is shown in purple and the
word vectors V
j
in white.
Given a context, or sub-sequence, of word vectors V
1
, V
2
, . . . , V
w
and a doc-
ument vector D
i
, the context vector c is created by averaging or concatenating
the other vectors.
The context vector c is the input for the next word classifier,
which uses
multinomial logistic regression with softmax σ.
Logistic regression in turn uses
a weight matrix W and bias vector b.
With a dictionary of k words W has the
size d × k and b is of
length k.
Note that the multinomial
logistic regression
fulfills the same function as a layer in a feed-forward neural
network with d
inputs and k outputs.
The probability of next word V
w+1
taking some value j
with softmax:
p(V
w+1
= j|c) = σ(W · c + b)
j
where the softmax function σ is defined as:
σ(z)
j
=
exp(z
j
)
P
i=1
exp(z
i
)
(1)
7
During training the weights are updated relative only to the error of j so full
probabilities of the other words can be avoided for that context.
However the
denominator in softmax,
equation (1),
is still
dependent on iteration over the
whole vocabulary.
As the vocabulary and number of sessions grow this become
expensive.
One proposed solution is to use hierarchical softmax.
Hierarchical
softmax avoid iterating the vocabulary by constructing a bal-
anced binary tree where each leaf is a word of the vocabulary and each inner
node contains a weight vector w
i
and bias scalar b
i
.
The probability of V
w+1
is then given by following the path of the tree down to the word j,
the length
of the path is given by H(j).
Hierarchical
softmax thereby avoids visiting all
the other words and only have to walk down the depth of
the tree which is
logarithmic to the size of the vocabulary.
The probability of next word V
w+1
using hierarchical
softmax is expressed
as [18] (sometimes without the bias b):
p(V
w+1
= j|c) =
H(j)−1
Y
i=1
σ

J
n(j, i + 1) = ch(n(j, i))
K
· (w
i
T
c + b
i
)

j
(2)
where n(j, i) is the i’th unit on the path down the tree towards j, ch(n) is the
left child of n and
J
x
K
=
(
1
if x is true;
−1
otherwise
The
J
x
K
function and its input in equation (2) represent the branches chosen
when descending the tree to word j.
Choosing the left branch at a node does
not change the sign of the input to σ but for the right branch the negative value
is used.
One way of justifying the soundness of the hierarchical softmax is to consider
the following general
scenario [22],
where C is some clustering for Y with a
deterministic assignment function c(·):
p(Y |X) =
X
i
p(Y, C = i|X)
=
X
i
p(Y |C = i, X)p(C = i|X)
= p(Y |C = c(Y ), X)p(C = c(Y )|X)
In the case of hierarchical softmax the function c(·) is given by the path of the
tree which indeed is deterministic for any Y .
For example take a vocabulary of
only two words and a tree with only one inner node.
To find the probability of
the first word sitting on the left branch c(Y = 1) = left:
p(Y = 1|X) = p(Y = 1|C = left, X)p(C = left|X)
Computing the probability once a leaf is reached is trivial p(Y = 1|C = left, X) =
1 meaning only the probability of taking a left at the inner node needs to be
computed, which is done by the softmax σ in equation (2).
Using this reasoning
one may get a better understanding how the hierarchical softmax works.
8
Details on training Doc2vec will not be addressed here but can for example
be found in [23].
Training can briefly be explained as using stochastic gradient
descent where gradients are found using backpropagation [16].
2.2
Clustering and visualization
Research into clustering sessions based on behavior has been done before at
Spotify.
In the work of G¨other 21 various features were collected from sessions
where at least 30 seconds of a track was played [9].
Gaussian mixture models
and k-means was used to cluster the sessions.
6 clusters were found for 179 748
sessions.
Another case of clustering sessions at Spotify was done using genre and mood
of music [7].
Each session was represented as a document of mood and genres
and vectorized using bag of
words.
20991 sessions were analyzed and logistic
regression was used to determine dominant features from each of the clusters.
6 clusters were found of which one, labeled “Christmas”, surprised the author.
One example of a more uncommon clustering method was used in [24] where
clustering was based on model
parameters.
Website navigation patterns were
observed and a mixture of first-order Markov models was trained.
Clusters and
models were trained jointly using the Expectation-Maximization algorithm so
that cluster assignment was made such that the most likely Markov model was
assigned.
Visualization was done by mapping a color to each type of webpage
and showing each cluster in a grid.
An example of clustering using sequences of user events can be found in [25].
Hierarchical clustering was used to recursively partition users into smaller groups
based on their clickstream similarity which was found using 5-gram feature
vectors.
χ
2
metric was used to prune dominant features and iteratively work
to smaller and more fine-grained groups of
users sharing a series of
common
features.
Graph based clustering was done in [26]
to find groups of
documents and
query refinements likely to satisfy a users information retrieval
need based on
an initial query.
Edges of the graph were given transitioning probabilities pro-
portional
to the number of
transitions observed.
Documents were marked as
absorbing with a single edge leading back to the document itself.
Random
walks in the graph simulate users search and different refinements yield vari-
ous document absorption distributions, reminiscent of PageRank [27].
Full-link
clustering was then performed on the document absorption distributions.
2.2.1
K-means
When it comes to clustering there are a number of go-to algorithms.
One such
classic is the K-means clustering algorithm.
K-means aim to minimize the inter-
cluster variance.
The algorithm uses the following steps [28]:
1.
K cluster centers are randomly positioned in space.
2.
Each cluster k is assigned data that is nearest to its center.
9
3.
The k cluster centers are updated as the mean of their assigned data.
4.
Check for convergence:
if any change in cluster assignments was observed
in step 2 repeat from there.
K-means is fast and easy to understand but has a few shortcomings.
Since
the cluster centers are updated using the mean of their assigned data they are
sensitive to scale and outliers.
Just a few data points with extreme values may
affect the center and corresponding assigned data greatly.
Another thing to note
is that separations between clusters are always linear,
clusters within clusters
are not possible.
2.2.2
Gaussian mixture model
Gaussian mixture models (GMM) uses a number of Gaussian distributions and
fit them to the data.
Clustering is performed by assigning data to the Gaussian
distribution that deem the data most likely.
GMM can, unlike K-means, model
differences in variance over clusters.
The probability of observing some datum x using GMM of M models can
be expressed as:
p(x|µ, Σ) =
M
X
i=1
w
i
g(x|µ
i
, Σ
i
)
where g is the Gaussian function, w is the associated weight, µ its mean and Σ
its variance.
Training of GMM is commonly performed using the Expectation
Maximization algorithm [29].
To avoid overfitting, or if some specific type of Gaussian is sought, the vari-
ances may be restricted.
For the spherical restriction the same variance is used
for every dimension,
for example in the 2D case increasing the variance in the
X-axis would also increase it for the Y-axis.
For the diagonal
restriction each
dimension has its own variance, an increase of variance in the X-axis would not
affect the Y-axis variance.
2.3
Cluster evaluation
Once some cluster assignments have been found the problem of determining if
the were useful
remain.
Here some common metrics to aid in that assessment
is presented.
2.3.1
Mean squared error
The mean squared error (MSE) is similar to the target objective of
K-means
clustering for K clusters and N number of data points:
MSE =
1
N
K
X
k=1
X
x∈k
(x − µ
k
)
2
10
Silhouette score
Interpretation
0.71 – 1.0
A strong structure has been found.
0.51 – 0.70
A reasonable structure has been found.
0.26 – 0.50
The structure is weak and could be artificial.
Try addi-
tional methods of data analysis.
<0.25
No substantial structure has been found
Table 1:
Silhouette score ranges and how they may be interpreted [32].
Small values of MSE indicate compact clusters.
However the value also decrease
with the number of
clusters,
so when plotting MSE against K a break point,
also known as “knee” or “elbow”, is sought.
The break point hopefully indicate
some intrinsic number of clusters of the data.
2.3.2
BIC score
Bayesian information criterion (BIC)
is
a metric to explain how well
some
probalistic model describe its data.
BIC is expressed as:
BIC = −2 ln(
ˆ
L) + k ln(n)
where
ˆ
L is the likelihood of the model applied to the data, k number of model
parameters and n the number of data points.
BIC is intended to strike a balance
between model fit
ˆ
L and model complexity k so that it describes the data well
while not overfitting.
Another model criteria that is sometimes used instead of BIC is the Akaike
information criterion (AIC).
AIC differs in how model
complexity is penalized
[30].
For more than 8 data points BIC penalizes model complexity more.
Some
work has indicated that the BIC score still does not penalize model complexity
enough [31].
2.3.3
Silhouette score
Silhouette score measures the ratio of cohesion, how well a point compare to its
own cluster, and separation, how it compares to the nearest other cluster.
The silhouette score s of some point i can be expressed as follows:
s(i) =





1 − a(i)/b(i)
if a(i) < b(i)
0
if a(i) = b(i)
b(i)/a(i) − 1
if a(i) > b(i)
where a(i) is the average distance from i to points in the same cluster, and b(i)
the average distance from i to points of the nearest other cluster.
Data points right on the edge of two clusters will
have the silhouette score
of 0 and for data that is closer to some other cluster the value will be negative.
Table 1 show ranges of silhouette scores and how they may be interpreted
[32].
11
2.4
Behavioral and user modeling
One goal of this work is capturing the behavior within the sessions.
With that
in mind it is relevant to consider various approaches previously taken for this
problem.
Behavior and sentiment in sessions at Spotify was the topic of [8].
Various
overall metrics were analyzed and a clear behavior was found where users either
skip a song early or let it play to the end and rarely anything in between.
It
was found that if the previous song was skipped the next song was more likely
to be skipped too.
The correlation between sentiment and skip-likelihood was
assumed and sessions were given sentiment scores based on this behavior.
If a
user was unlikely to skip but went out of
their way to skip the song anyway
it was assumed to indicate negative sentiment.
Skip-likelihood was estimated
using logistic regression.
User attention during a session was also analyzed and
4 latent user states was found using a hidden Markov model
(HMM) roughly
representing various degrees of attention and engagement.
The idea of contextual Markov models is introduced in [33] where predictive
performance of
Markov models on user navigation behavior on the web was
improved.
Various clustering methods were employed to define contexts.
The
novice/expert contexts gave significant prediction improvements and random
contexts gave similar performance as a global
model.
Aiding in next action
prediction is an interesting example of usage of clusters.
User behavior is also a common topic of papers in information retrieval and
in [34] an elaborate dynamic Bayesian network (DBN) was created where actions
transitioned users across complex states.
Hidden Markov models were not used
because the number of states is not known and can be large to accommodate
annotated user actions (e.g.
queries a phrase).
It was argued that HMM can be
hard to interpret, validate and that it is difficult to incorporate available domain
expertise.
States of the DBN contain both observed and hidden attributes.
The
model
got top performance compared to baseline both on the task of
session
completion and post-session action prediction.
Manually annotated HTTP logs were used in [35]
to analyze user behavior
on several online social networks.
92% of user activity was found to be purely
browsing.
Session lengths were heavy tailed.
Detailed clickstreams from the
HTTP logs were concluded to be a powerful method for behavior analysis that
capture silent behavior such as browsing.
Even if
the method was deemed
powerful this approach is susceptible to the knowledge bottleneck problem and
bias by the experts that does the analysis.
Enabling intelligent user interfaces was discussed in [36]
by observing un-
modified applications and infer the state space which the user navigates.
This
state space is then used to make predictions for next user action.
Prediction
results were better than the baseline algorithm however their intended purposes
differ slightly.
12
Various predictive statistical models for user modeling was discussed in [4].
Two concepts related to prediction is introduced; content-based methods based
on users past behavior,
and collaborative methods based on the behavior of
other users.
There are many approaches to user- and behavior modeling.
All affected by
the knowledge bottleneck problem in various degrees.
Domain specific models
can often provide clearer answers but at the cost of
constructing said model,
unable to use something off the shelf.
13
Figure 2:
Overview of the method stages from session to evaluation.
3
Method
3.1
Overview
An unsupervised method to find clusters of sessions based on raw events must
involve three steps; sequencing and labeling the events, feature extraction from
sequences of labels and finally clustering based on these feature vectors.
Figure
2 shows an overview of the stages involved.
In this work we consider four labeling and sequencing methods,
three fea-
ture extraction methods and two clustering methods with hyper parameters
where appropriate.
Methods that give strong clusters in terms of cohesion and
descriptiveness are preferred.
3.2
Preprocessing and data set
Sessions from a random subset of newly registered users during the first week
of
May 2016 on the iOS version 9.x serve as the base data set for this work.
Number of users who’s sessions were used was arbitrarily limited to speed up
computations and hyper parameter search.
The training set used for clustering
contained 165 541 sessions.
Only sessions on the free product were considered in order to keep the ex-
perience consistent for all users and sessions.
No other artificial limits, such as
removing outliers, were put on the session selection.
14
3.2.1
Session definition
Sessions are defined by all interactions and events for a user in the app that are
no longer than 15 minutes apart and on the same device.
For example pressing
pause and putting down a device to return more than 15 minutes later and
resume would be considered as two separate sessions.
Similarly transitioning
from the desktop client to a mobile client is also considered two separate sessions
even if
interactions occur within 15 minutes.
H˚astad discusses more details
about sessionization at Spotify in [37].
3.2.2
Session behavior similarity
Determining if
two sessions are similar and by how much is a problem that
may have different answers depending on what one is looking for.
In this work
focus is on the behavior itself during the session.
That means two sessions are
considered similar if the streaming features used, skips and pages visited appear
at similar frequency and order.
This definition means that less importance is
given to aggregated session metrics such as session duration.
3.3
Labeling and sequencing
Labeling and sequencing address the problem of
converting sequences of
high
dimensional events into sequences of labels using some labeling method.
Labels
are created by concatenating events and various metadata fields using the +-
character.
In the data set considered there are three base types of
events:
streams,
ads and page views.
All base types share three attributes;
type,
time stamp
and duration.
Each event type has a number of additional important meta data
fields.
For streams this included:
• feature – The streaming feature used in the product, such as artist, search
or album.
• reason start – Why the stream started, for example actively selecting a
new song, the previous stream ended or the forward button was pressed.
• reason end – Why the stream ended, for example the song played to the
end, the application was terminated or the forward button was pressed.
For page view events the page visited was given such as album,
artist or
playlist.
Finally for ad events the content type was specified indicating a video or
audio ad.
How to represent these events and their attributes as a sequence of labels will
be addressed below.
Four labeling methods are tested to find the importance of
the labeling and if any method tend to give stronger structure.
15
Figure 3:
Session with the Raw la-
beler.
Figure 4:
Session with the Action la-
beler.
3.3.1
Raw labeler
The Raw labeler is the labeling method that most closely resembles the events
as they are stored.
Primary meta data fields for each event type are concate-
nated.
Ordering of the labels is unchanged relative to the ordering of the events.
View events are labeled by concatenating the word “view” and the path visited.
Stream events were represented as a concatenation of “stream”,
feature used,
reason start and reason end.
Ad events were represented by concatenating the
word “ad” and the type of ad.
Figure 3 shows an example of a session labeled
using the Raw labeler, view labels colored green, stream labels in blue and ads
in red.
In the training set the Raw labeler gave 809 unique labels and a median
sequence length of 9 and 90th percentile of 37.
3.3.2
Action labeler
The Action labeler uses the same ordering as the Raw labeler but explicitly
marks the beginning and end of each session with a “start” and “end” label.
It
also does not concatenate reason end as it is most often the same as reason start
for the next stream, except for a few cases such as an unexpected exit.
Finally
streams that do not change streaming feature used and that begins with the
previous track playing to the end gets labeled “trackdone”, so that a very idle
session may be filled with mostly “trackdone” labels.
Figure 4 shows the start
and end labels in white and the second stream labeled “trackdone”.
By not combining feature used, reason start and reason end, the number of
unique labels get greatly reduced compared to the Raw labeler.
In the training
set the Action labeler gave 111 unique labels and a median sequence length of
11 and 90th percentile of 43.
16
Figure 5:
Session with the Compact
labeler.
Figure
6:
Session
with
the
Compact time labeler.
3.3.3
Compact labeler
Since the Raw- and Action labeler represent each stream with a single label they
do not fully capture when and why a stream ended.
This is particular apparent
for the final stream of a session.
For example when a page view occurred after
the last stream started but before the stream and session ended.
The Compact
labeler address this by splitting stream events so that the end of
the stream
is handled separately.
Figure 5 shows an example of this where the “fwdbtn”
occurs at the very end, compared to figure 4 where the action is not present at
all.
The Compact labeler uses the same “trackdone” label for continous streams
from the same streaming feature.
Moreover the streaming feature is only part of
the label when the used streaming feature changes.
For example when a session
start to play from the radio the label is “click+radio”.
As a final operation each label is ordered by the corresponding time stamp.
For the training set the Compact labeler gave 50 unique labels and a median
sequence length of 10 and 90th percentile of 38.
3.3.4
Compact time labeler
The Compact time labeler uses the same labels and ordering as the Compact
labeler discussed in subsection 3.3.3 but with the addition of duration labels.
17
Duration labels indicate how long has passed between each label, a technique
also used in [25].
The duration between labels was bucketed and represented
as one of five labels;
“2s”,
“1m”,
“5m”,
“15m”,
“15m+”.
Figure 6 show these
duration labels in white.
The “15m+” label can occur for very long streams.
For the training set the Compact time labeler used 55 unique labels and a
median sequence length of 19 and 90th percentile of 75.
3.4
Feature extraction
Finding groups of similar data can be done by searching nearby neighbors using
some type of
distance metric.
In this work the euclidean distance was used.
Feature extraction is the task of positioning sessions with similar behavior,
as
discussed in subsection 3.2.2 on page 15,
closely to each other in some space.
In this section three approaches will
be discussed to represent variable length
session sequences as fixed length vectors.
3.4.1
Baseline
The baseline method is similar to previous work at Spotify by G¨other in [9].
Here 91 features are manually defined and computed for each session sequence.
The baseline method does not use the labeled session sequences discussed in
subsection 3.3 on page 15 but operates directly on the collection of
session
events.
A large portion of
the features represent specific pages viewed and
streaming features used.
Below is a selection of some of the features computed.
• duration – Session duration in seconds.
• full songs – Number of completed streams in session.
• time played – Total streaming duration.
• pageviews – Number of page views observed in the session.
• pageviews unique – Number of unique page views.
Finally the features were standardized, also known as Z-score normalization
[38].
This is a common operation since euclidean distances are sensitive to
feature scaling.
Without normalization some features like time played would
affect the distances a lot more than for example full songs.
The standardized
feature value Z is found by subtracting the mean E[X]
and dividing by the
variance Var(X) for each feature type X:
Z =
X − E[X]
Var(X)
18
Figure 7:
Two-gram features from a sequence of labels.
3.4.2
N-gram
The baseline method capture many attributes of each session but not the order
of events.
Consider for example a session with the first k events as page views
and the remaining k events as streams,
compared to a session with k page
view and stream pairs.
The baseline method represent the two sessions as very
similar, assuming it is the same pages visited and streaming features used.
One approach to capture the ordering in the sequences is to represent sessions
as a collection of continuous sub-sequences of some length n, also known as n-
grams.
The n-gram method presented here is strongly related to the LSA model
discussed in subsection 2.1.1 on page 5.
Figure 7 shows an example of
the 2-gram features found in a sequence of
labels.
The values of the features are scaled so that they all sum up to 1 within
each session.
In the example none of the 2-grams appeared more than once so
all features get the same value of 1/5 = 0.2.
As mentioned, one phenomenon of the n-gram and bag-of-words representa-
tions are that the number of unique features tend to be high and growing rapidly
as n increase.
For the training set the number of features is in the many thou-
sands, depending on labeler and n.
Finding nearby neighbors in space with the
euclidean distance metric is not feasible without some dimensionality reduction
[39].
Truncated and randomized single value decomposition (SVD) was used,
similarly to LSA and as randomized SVD can find approximate decompositions
from large amounts of sparse data by sampling and the truncation only keeps
the m most significant basis [13], [14].
To provide generalization and keep the number of dimensions low while not
removing all
detail
the 30 most significant singular values were kept for all
n-
gram representations.
Figure 8 shows the singular values of
the 4-gram SVD
using the Raw labeler.
Most of the energy of the singular values is maintained
in the first dimensions and good approximations can be made without the full
199 059 dimensions.
19
0
20
40
60
80
100
Singular value
0
20
40
60
80
100
Strength
Figure 8:
Singular values of the 4-gram SVD using the Raw labeler,
the trun-
cation threshold is marked with a dashed line.
After the SVD truncation each new vector was normalized so that its eu-
clidean distance was 1.
The sub-sequence length,
n in n-gram,
was allowed to vary between 1 and
7.
3.4.3
Doc2vec
Another method to create vector representations of
sequences based on next-
label prediction given some context is the Doc2vec model discussed in subsection
2.1.3 on page 6.
No major adaptations were made from the model discussed previously.
Ses-
sions are considered as documents and labels as words.
The PV-DM config-
uration was chosen as no vector representations of
the labels existed and the
PV-DM model can train session and label vectors jointly,
it is also the default
of the gensim library used [40].
Averaging of the session and label vectors was
used to create the context vectors as it also is the default of the library.
The size context window was tested at 3, 6 and 12, while the vector dimen-
sionality was tested at 10, 30, 50, 100 or 200.
3.5
Clustering
With the various session vectors from the feature extraction methods discussed
in subsection 3.4 here the task of finding similar sessions is addressed.
Several methods were considered but few handle the requirements of inter-
pretation and scale.
In terms of interpretation algorithms such as linkage-based
ones were ruled out since they risk spanning large distances in the space over
multiple behavior types.
20
For this work euclidean distance was used for clustering.
It has been shown to
behave unfavorably with high-dimensional data [39] but was used for efficiency
and to limit the scope.
Alternative metrics include cosine similarity which is
commonly used when working with high dimensional text documents.
However
cosine similarity does not fulfill the triangle inequality which makes it unfit for
efficient space-searching optimizations such as ball-trees or KD-trees.
Clustering
using cosine similarity requires,
in general
[41],
[42],
a full
datum to datum
similarity matrix.
Computing this matrix is often not possible for large data
sets such as the one used here.
K-means is a classic algorithm that aim to minimize the inter-cluster vari-
ance.
It was chosen as the structures of the clusters are interpretable and it is
one of the faster clustering methods available.
Values of k tested were 2, 5, 10,
15, 20, 25, 30, 40, 50, 70 and 100.
Gaussian mixture models cluster data by assuming it can be modeled by a
number of
Gaussian functions.
Compared to K-means it can model
different
variances and cluster within cluster is possible.
Spherical and diagonal variance
restrictions were tested,
as well
as setting the number of mixture components
to 2, 10, 15, 20, 25, 30, 40, 50, 60 and 100.
3.6
Evaluation
Methods exist to inspect and describe various properties of clusters but rarely is
there any ground truth available.
Much comes down to judgment and intuition
for the specific problem at hand.
Qualitative understanding of
what behavior a cluster of
sessions describe
across the various cluster and vector representations was found by using means
of the baseline feature vectors (section 3.4.1 on page 18).
For the baseline vectors
each feature had a distinct description, such as playlist page views or full tracks
listened to,
making means of baseline vectors interpretable.
The clusters were
ordered by the one dimensional
PCA 2.1.2 projection with the mean feature
vector of respective cluster.
This was done in an attempt to have similar cluster
types near each other.
For all
clusters the mean squared error,
silhouette scores and feature cen-
troids were computed.
Additionally for the probalistic GMM models the BIC
score was also computed.
Since computing the silhouette score is an operation of
O(n
2
) complexity
stratified sampling was applied.
Stratification ensures that the relative sizes
between clusters is maintained even after sampling.
A sampling size of 15 000
was chosen.
Figure 9 show the silhouette score changing only slightly after
15 000 samples.
Some thresholds will also be considered to help reason about what methods
were successful.
Silhouette scores above 0.51 are of particular interest, as shown
in table 1.
The number of clusters is expected to be no less than 10 to provide a
reasonable level of granularity.
Finally no single cluster is expected to be larger
than 50% of the samples.
21
5000 10000 15000 20000 25000
Number of samples
0.548
0.550
0.552
0.554
0.556
0.558
0.560
0.562
0.564
Silhouette score
5000 10000 15000 20000 25000
Number of samples
0.260
0.265
0.270
0.275
0.280
0.285
0.290
0.295
0.300
0.305
Silhouette score
Figure 9:
Silhouette score as the number of stratified samples increase.
Spherical
GMM of 40 components using the baseline features were used for the left plot
and Spherical GMM of 100 components using the 3-gram features was used for
the right plot.
22
Feature type
Cluster type
Count
Baseline
GMM
20
Baseline
K-means
11
Doc2vec
GMM
1200
Doc2vec
K-means
660
N-gram
GMM
560
N-gram
K-means
308
Table 2:
Number of
cluster attempts made for each feature type and cluster
method.
4
Results
For the range of combinations of feature types, cluster type and hyper param-
eters a total
of
2759 various cluster attempts were made.
Table 2 shows a
breakdown of the numbers for each feature type and cluster type.
Appendix A
shows a table of all
clusters within the thresholds mentioned in subsection 3.6
on page 21.
Results will
be presented in aggregate and successively hyper parameters
will be fixated.
Bands shown in the line plots are the 95% and 99% confidence
bands.
4.1
Labeling and sequencing methods
Figure 10 show distributions of silhouette scores for the four labeling methods
over all
feature types,
cluster types and hyper parameters.
Labeling method
appear to have little effect on the silhouette scores overall.
However,
when
considering the evaluation thresholds it is appears that the Raw labeling gave
stronger clusters.
Based on this results presented below will be with the Raw labeling.
4.2
Feature extraction
No hyper parameters were available for the baseline method, further evaluation
of this model will be in subsection 4.3 on page 26.
4.2.1
N-gram
Figure 11 show the silhouette score as n varies in the clusterings.
The structure
of the clustering appear stronger as n increase up to the value of 3,
at which
point the trend changes.
Based on this results below will have n in n-grams set to 3.
23
raw
action
compact
compact_time
Labeling
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Silhouette score
raw
action
compact
compact_time
Labeling
0.4
0.2
0.0
0.2
0.4
0.6
0.8
Silhouette score
Figure 10:
Violin plots of silhouette score distributions per labeling method.
To
the left is the overall
distributions and to the right distributions of silhouette
scores that pass the evaluation thresholds.
0
1
2
3
4
5
6
7
8
N
0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Silhouette score
Figure 11:
Silhouette score as the number of
n in N-gram changes using the
Raw labeling.
24
10
30
50
100
200
0.15
0.10
0.05
0.00
0.05
0.10
Silhouette score
Window = 3
10
30
50
100
200
0.15
0.10
0.05
0.00
0.05
0.10
Silhouette score
Window = 6
10
30
50
100
200
Dimensionality
0.15
0.10
0.05
0.00
0.05
0.10
Silhouette score
Window = 12
Figure 12:
Doc2vec with various window sizes and dimensionality using the
Raw labeling.
Each dot correspond to a clustering that pass the evaluation
thresholds except the silhouette score limitation.
25
2
20
40
60
80
100
K
0.000
0.005
0.010
0.015
0.020
0.025
0.030
MSE
Baseline
N-gram
Doc2vec
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
MSE (Baseline)
2
20
40
60
80
100
K
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Silhouette score
Figure 13:
Mean squared error and silhouette score as k varies in K-means
clustering.
4.2.2
Doc2vec
Figure 12 show the silhouette scores for various sizes of the context window and
dimensionality of
the session vector where the clusterings pass the evaluation
thresholds except the silhouette score limit.
All scores are low and no substantial
structure was found according to table 1.
Window size of 3 gave the highest number of clusters that fit the criteria.
Dimensionality appear to have had slightly larger impact than the window size.
The highest silhouette scores are for 10 dimensions.
Based on this a window size of 3 and session vector dimensionality of 10 will
be considered as the parameters giving the strongest clusters.
4.3
Clusterings
4.3.1
K-means
Figure 13 show mean squared error and silhouette score for various values of
k in K-means.
The models shown are the baseline,
3-gram and Doc2vec with
window size 3 and vector dimensionality of 10 all using the Raw labeling.
For Doc2vec the silhouette score hovers around 0.
Little change can be
observed for values of k larger than 40.
26
2
20
40
60
80
100
Components (K)
2.5
2.0
1.5
1.0
0.5
0.0
BIC
1e7
Covariance = diag
Baseline
N-gram
Doc2vec
6
5
4
3
2
1
0
BIC (Baseline)
1e7
2
20
40
60
80
100
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Silhouette score
Covariance = diag
2
20
40
60
80
100
Components (K)
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
BIC
1e7
Covariance = spherical
0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
BIC (Baseline)
1e7
2
20
40
60
80
100
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Silhouette score
Covariance = spherical
Figure 14:
Bayesian information criterion and silhouette score for varying num-
ber of components and covariance restrictions of Gaussian mixture models.
4.3.2
Gaussian mixture models
Figure 14 show BIC score and silhouette scores as the number of
Gaussian
mixtures change.
The Doc2vec silhouette score moves in the opposite direction
compared to the other two feature types as it did for K-means in figure 13.
Spherical
covariance restriction appear
to give slightly higher
silhouette
scores and BIC scores overall
compared to the diagonal
restriction,
in partic-
ular for the Baseline method.
Similar to K-means very little changes after 40
components.
The silhouette scores for the spherical covariance Gaussian mixture appear
similar to those of K-means.
27
4.3.3
Qualitative analysis
Figure 15 shows the scatter plot of sessions, colored by the assigned cluster and
corresponding silhouette plot.
The session projection was done using T-SNE
[43].
Figure 16 shows the feature centroids for each cluster.
Notice how certain
features and page views are very strong in some clusters.
The average silhouette
score for this clustering is 0.26,
according to [32]
this is a weak clustering.
The ordering by the one dimensional PCA projection does not appear to have
presented too much of a structure with a few exceptions such as clusters 0 and
28.
Figure 17 shows the scatter plot of 3-gram feature vectors projected using
T-SNE, colored by assigned cluster and with corresponding silhouette plot.
Fig-
ure 18 shows the baseline feature centroids for the clusters.
Compared to the
baseline the features and pages viewed per cluster seem to be less strict.
Some
features such as view playlist have a strong polar values in almost every clus-
ter.
The average silhouette score for this clustering is 0.55.
According to table 1
this can be considered a reasonable structure.
It appears that this method yield
stronger clusters compared to the baseline.
Compared to the feature centroids
of
the baseline method the ordering of
the one dimensional
PCA projection
seem to have presented some structure.
In the top-right part of
the image a
deep blue area can be seen, indicating that clusters with low listening ratio are
placed near each other in the feature space.
28
Figure 15:
Baseline features,
scatter plot and silhouette plot.
Clustered using
40 Gaussians with spherical constraint.
13 33 14 30 26 20 29 5 12 15 21 6 31 24 22 1 32 4 16 10 8
2
7
3 23 11 18 19 25 34 0 28
feature_zlink
feature_unknown
feature_sounds_of
feature_search
feature_running
feature_radio
feature_playlist-folder
feature_party
feature_own_playlist
feature_others_playlist-unpublished
feature_others_playlist-spotify
feature_others_playlist-published
feature_fresh_finds
feature_externalcontentprovider
feature_external
feature_discover_weekly
feature_com.spotify.gaia
feature_collection-songs
feature_collection-artist
feature_collection-album
feature_collection
feature_chart
feature_buddy-list
feature_artist
feature_app
feature_album
view_running/category
view_radio/station
view_profile/playlists
view_profile/following
view_profile/followers
view_profile/artists
view_profile
view_playlist
view_collection/artists/artist
view_collection/albums/album
view_charts/chart
view_artist/related
view_artist/playlists
view_artist/gallery
view_artist/albums
view_artist
view_album
stutters_mean
stutters
streams
latency_mean
skips
skip_ratio
shuffle_ratio
features_unique
ads
ad_ms_played
ad_ratio
pageviews_unique
pageviews
pageview_duration_median
ms_played_mean
ms_played
listening_ratio
full_songs
event_count
duration
Figure 16:
Baseline feature centroids.
Clustered using 40 Gaussians with spher-
ical constraint.
Clusters ordered by the one dimensional PCA projection of the
mean feature vector.
29
Figure 17:
3-gram features using the Raw labeling,
scatter plot and silhouette
plot.
Clustered with K-means and k set to 40.
2638 3 0 2 32 8 17373125221019 4 1 6 30 9 161335 5 113420153328 7 12213927241423361829
feature_sounds_of
feature_search
feature_running
feature_radio
feature_party
feature_own_playlist
feature_others_playlist-unpublished
feature_others_playlist-spotify
feature_others_playlist-published
feature_fresh_finds
feature_external
feature_discover_weekly
feature_com.spotify.gaia
feature_collection-songs
feature_collection-artist
feature_collection-album
feature_collection
feature_chart
feature_artist
feature_app
feature_album
view_running/category
view_radio/station
view_profile/playlists
view_profile/following
view_profile/followers
view_profile/artists
view_profile
view_playlist
view_collection/artists/artist
view_collection/albums/album
view_charts/chart
view_artist/related
view_artist/playlists
view_artist/gallery
view_artist/albums
view_artist
view_album
stutters_mean
stutters
streams
latency_mean
skips
skip_ratio
shuffle_ratio
features_unique
ads
ad_ms_played
ad_ratio
pageviews_unique
pageviews
pageview_duration_median
ms_played_mean
ms_played
listening_ratio
full_songs
event_count
duration
Figure 18:
Feature centroids, 3-gram features using the Raw labeling clustered
with K-means and k set to 40.
Clusters ordered by the one dimensional
PCA
projection of the mean feature vector.
30
5
Discussion
The validation of clustering structures is the most difficult and frus-
trating part of cluster analysis.
Without a strong effort in this direction,
cluster analysis will
re-
main a black art accessible only to those true believers who have
experience and great courage.
— Algorithms for Clustering Data, Jain and Dubes
5.1
Labeling and sequencing
The choice of labeling method appeared to have less impact than expected but
is still clearly of importance.
Labeling with the Compact methods appeared to
perform the worst and it may be because the streaming feature used was not
repeated in the same manner as for the Raw and Action labeling.
Adding inter-event time for the Compact time labeling likely smeared most of
the N-grams and contexts making it hard to find distinctiveness in the sessions.
That is probably the reason Compact time was the only labeling method that
had no clusters that passed the evaluation thresholds.
The success of the Raw labeling in combination with the n-gram method can
perhaps be explained by the explicitness of the labels, repeating the streaming
feature used for every new song.
5.2
Feature extraction
When it comes to building features from the sessions the classic baseline ap-
proach is powerful.
Very often when doing this kind of task we already know
what we want to find.
This makes advanced methods like Doc2vec unneces-
sary since we can go and measure what we were looking for straight away.
The
downside is the knowledge bottleneck problem that was mentioned in section 1,
what if we do not know or are measuring the wrong thing?
The N-gram approach is very similar to the proven LSA model and capture
much of
what is expressed in the label
sequence.
It does not suffer from the
knowledge bottleneck problem in terms of what can be represented in the session
vector.
However,
there are things that are difficult to represent as a sequence
of labels,
in particular aggregated metrics,
such as the duration of the session
or how much time was spent playing music.
Doc2vec is interesting and capture many nuances of a sequence by distribut-
ing the context knowledge over each of
the label
vectors and the topic of
the
session vector.
However,
because of all
these complex interactions the vectors
become very compact, they look almost random.
Figure 19 shows a comparison
of the various feature types.
It is possible that the Doc2vec results could improve by changing training
rate, number of epochs and size of the data set.
31
Figure 19:
Visualized feature vectors;
left:
Baseline;
center:
N-gram;
right:
Doc2vec.
The vectors are ordered according to respective clustering.
During development it was found that finding similar sessions using the
cosine similarity on the Doc2vec vectors appeared to work very well.
For that
type of task the Doc2vec vectors may work better.
They could perhaps be used
for detecting similar streams to prevent fraud or some type of action prediction
as was done in [20].
5.3
Session clustering
K-means appeared to be a bit more unstable compared to the Gaussian mixtures
which may be attributed to outliers and noisy data.
The Gaussian mixtures with
spherical variance restriction appeared to perform best overall.
For the baseline slightly stronger clusters were found compared to the earlier
work by G¨othner where the highest silhouette score was 0.205 [9].
However, this
can likely be explained by the different data sets and features used, even if the
approaches were similar.
Ordering the n-gram clusters by their one dimensional PCA projection seem
to have presented some structure.
Less active clusters were placed next to each
other while clusters with a high listening ratio also got positioned relatively
close.
This indicates that similar clusters were positioned closely in the feature
space as well.
Overall
the silhouette scores of the n-gram clusters were stronger than ex-
pected.
One could also argue that the n-gram clusters were more nuanced
compared to the baseline since the clusters found had a higher number of ex-
treme values.
Unfortunately, it is difficult to claim that those clusters are better
than the baseline.
It becomes a matter of preference and objective.
32
6
Conclusions
An unsupervised method to go from stored session events to session vectors
and clustered sessions based on three steps was presented.
First events were
sequenced and labeled, features were extracted from the sequences of labels and
finally clustering was made using the feature vectors.
Four event labeling approaches were tested and the Raw labeling method
that makes expressive labels by concatenating many fields of
meta data was
found to give the strongest clusters overall.
The baseline method of manually engineered features used in this work gave
clusters of
comparable strength in relation to previous work into behavioral
session clustering at Spotify [9].
Clustering sessions based on sequences of
events appear to have yielded
stronger and more nuanced clusters compared to the baseline method.
Silhou-
ette score of
0.55 was found for the n-gram method compared to a silhouette
score
0.24 for the baseline method.
The n-gram feature extraction method based on latent semantic analysis
(LSA) gave the strongest clusters when using sub-sequences of
length 3 and
Gaussian mixture models with spherical covariance.
Doc2vec was tested but did not yield any significant clustering structures.
The model can likely be used to find vector representations of sessions but the
compact vectors produced made clustering difficult.
Other tasks such as session
similarity or classification could be more suitable for the model.
6.1
Future work
For future work more distance metrics and other clustering methods should be
tested, in particular ones that should perform well with high dimensional data.
Other cluster evaluation metrics could also be considered.
One such metric
could for example be the spread of streaming features and unique pages viewed
in the clusters.
Finally,
future work using the results and methods here could be to look
into how a user moves through the various types of clusters over time.
Another
interesting area to explore is the creation of contextual Markov models [33] to
predict future actions in a session.
33
References
[1]
Optimizely, https://www.optimizely.com/, Accessed: 2016-10-13.
[2]
Heap, https://heapanalytics.com/, Accessed: 2016-10-13.
[3]
Mixpanel, https://mixpanel.com/, Accessed: 2016-10-13.
[4]
I.
Zukerman and D.
W.
Albrecht,
“Predictive statistical
models for user
modeling”, User Modeling and User-Adapted Interaction, vol. 11, no. 1-2,
pp. 5–18, 2001.
[5]
Spotify, https://www.spotify.com/, Accessed: 2016-10-13.
[6]
Introducing discover weekly:
Your ultimate personalised playlist,
https:
/ / press . spotify . com / it / 2015 / 07 / 20 / introducing - discover -
weekly-your-ultimate-personalised-playlist/,
Accessed:
2016-10-
13.
[7]
O. Carlsson, “Cluster user music sessions”, 2014.
[8]
K.
Ahmed,
“Analyzing user behavior and sentiment in music streaming
services”, 2016.
[9]
F.
G¨
othner,
“Identifying patterns in user behavior in a music streaming
service: A cluster analysis approach”, 2013.
[10]
S. Chen, J. L. Moore, D. Turnbull, and T. Joachims, “Playlist prediction
via metric embedding”,
in Proceedings of the 18th ACM SIGKDD inter-
national conference on Knowledge discovery and data mining, ACM, 2012,
pp. 714–722.
[11]
S. Feng, X. Li, Y. Zeng, G. Cong, Y. M. Chee, and Q. Yuan, “Personalized
ranking metric embedding for next new poi
recommendation”,
in Proc.
IJCAI, 2015.
[12]
T. K. Landauer, P. W. Foltz, and D. Laham, “An introduction to latent
semantic analysis”, Discourse processes, vol. 25, no. 2-3, pp. 259–284, 1998.
[13]
D. Kalman, “A singularly valuable decomposition: The svd of a matrix”,
The college mathematics journal, vol. 27, no. 1, pp. 2–23, 1996.
[14]
N.
Halko,
P.-G.
Martinsson,
and J.
A.
Tropp,
“Finding structure with
randomness: Probabilistic algorithms for constructing approximate matrix
decompositions”, SIAM review, vol. 53, no. 2, pp. 217–288, 2011.
[15]
J. Shlens, “A tutorial on principal component analysis”, CoRR, vol. abs/1404.1100,
2014. [Online]. Available: http://arxiv.org/abs/1404.1100.
[16]
Q.
V.
Le and T.
Mikolov,
“Distributed representations of sentences and
documents”, in ICML, vol. 14, 2014, pp. 1188–1196.
[17]
T.
Mikolov,
K.
Chen,
G.
Corrado,
and J.
Dean,
“Efficient estimation of
word representations in vector space”,
arXiv preprint
arXiv:1301.3781,
2013.
34
[18]
T.
Mikolov,
I.
Sutskever,
K.
Chen,
G.
S.
Corrado,
and J.
Dean,
“Dis-
tributed representations of words and phrases and their compositionality”,
in Advances in neural
information processing systems,
2013,
pp.
3111–
3119.
[19]
Y. Goldberg and O. Levy, “Word2vec explained: Deriving mikolov et al.’s
negative-sampling word-embedding method”, CoRR, vol. abs/1402.3722,
2014. [Online]. Available: http://arxiv.org/abs/1402.3722.
[20]
C. Moon, D Medd, P Jones, S Harenberg, W Oxbury, and N. F. Samatova,
“Online prediction of user actions through an ensemble vote from vector
representation and frequency analysis models”, in Proceedings of the 2016
SIAM International
Conference on Data Mining. SIAM, 2016.
[21]
A. M. Dai, C. Olah, and Q. V. Le, “Document embedding with paragraph
vectors”, arXiv preprint arXiv:1507.07998, 2015.
[22]
F.
Morin and Y.
Bengio,
“Hierarchical
probabilistic neural
network lan-
guage model.”, in Aistats, Citeseer, vol. 5, 2005, pp. 246–252.
[23]
X. Rong, “Word2vec parameter learning explained”, CoRR, vol. abs/1411.2738,
2014. [Online]. Available: http://arxiv.org/abs/1411.2738.
[24]
I.
Cadez,
D.
Heckerman,
C.
Meek,
P.
Smyth,
and S.
White,
“Visualiza-
tion of navigation patterns on a web site using model-based clustering”,
in Proceedings of
the sixth ACM SIGKDD international
conference on
Knowledge discovery and data mining, ACM, 2000, pp. 280–284.
[25]
G.
Wang,
X.
Zhang,
S.
Tang,
H.
Zheng,
and B.
Y.
Zhao,
“Unsupervised
clickstream clustering for user behavior analysis”, in SIGCHI Conference
on Human Factors in Computing Systems, 2016.
[26]
E.
Sadikov,
J.
Madhavan,
L.
Wang,
and A.
Halevy,
“Clustering query
refinements by user intent”, in Proceedings of the 19th international
con-
ference on World wide web, ACM, 2010, pp. 841–850.
[27]
L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation
ranking: Bringing order to the web.”, 1999.
[28]
A. K. Jain, “Data clustering: 50 years beyond k-means”, Pattern recogni-
tion letters, vol. 31, no. 8, pp. 651–666, 2010.
[29]
D.
Reynolds,
“Gaussian mixture models”,
Encyclopedia of
biometrics,
pp. 827–832, 2015.
[30]
J.
E.
Cavanaugh,
“171:
290 model
selection,
lecture vi:
The bayesian in-
formation criterion”,
[31]
G.
Hamerly and C.
Elkan,
“Learning the k in k-means”,
in Advances in
Neural
Information Processing Systems, 2004, pp. 281–288.
[32]
P.
Nagpaul,
“Guide to advanced data analysis using idams software”,
Retrived online from/www. unesco. org/webworld/idams/advguide/TOC.
htm S, 2001.
35
[33]
J. Kiseleva, H. T. Lam, M. Pechenizkiy, and T. Calders, “Predicting cur-
rent user intent with contextual markov models”, in 2013 IEEE 13th In-
ternational Conference on Data Mining Workshops, IEEE, 2013, pp. 391–
398.
[34]
K.
Punera and S.
Merugu,
“The anatomy of
a click:
Modeling user be-
havior on web information systems”, in Proceedings of the 19th ACM in-
ternational conference on Information and knowledge management, ACM,
2010, pp. 989–998.
[35]
F.
Benevenuto,
T.
Rodrigues,
M.
Cha,
and V.
Almeida,
“Characterizing
user behavior in online social
networks”,
in Proceedings of
the 9th ACM
SIGCOMM conference on Internet measurement conference, ACM, 2009,
pp. 49–62.
[36]
P.
Gorniak and D.
Poole,
“Predicting future user actions by observing
unmodified applications”, in AAAI/IAAI, 2000, pp. 217–222.
[37]
M. H˚astad, “Exploration and evaluation of different sessionization meth-
ods in a music streaming context”, 2016.
[38]
H. Abdi, “Normalizing data”,
[39]
C.
C.
Aggarwal,
A.
Hinneburg,
and D.
A.
Keim,
“On the surprising be-
havior of
distance metrics in high dimensional
space”,
in International
Conference on Database Theory, Springer, 2001, pp. 420–434.
[40]
R.
ˇ
Reh˚uˇrek and P. Sojka, “Software Framework for Topic Modelling with
Large Corpora”, English, in Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks, http://is.muni.cz/publication/
884893/en, Valletta, Malta: ELRA, May 2010, pp. 45–50.
[41]
X. Z. Fern and C. E. Brodley, “Random projection for high dimensional
data clustering:
A cluster ensemble approach”,
in ICML,
vol.
3,
2003,
pp. 186–193.
[42]
I.
S.
Dhillon,
J.
Fan,
and Y.
Guan,
“Efficient clustering of
very large
document collections”,
in Data mining for scientific and engineering ap-
plications, Springer, 2001, pp. 357–381.
[43]
L. v. d. Maaten and G. Hinton, “Visualizing data using t-sne”, Journal of
Machine Learning Research, vol. 9, no. Nov, pp. 2579–2605, 2008.
36
A
Cluster table
Table of clusters with strong structure, silhouette score above 0.51, at least 10
clusters and no single cluster larger than 50% of the data.
Silhouette
Largest
Description
0.561
12669
GMM(components=100, cov=spherical) 4gram raw
0.552
11928
KMeans(k=40) 3gram raw
0.550
9676
GMM(components=100, cov=spherical) 3gram raw
0.548
12564
KMeans(k=75) 4gram raw
0.547
10113
KMeans(k=75) 3gram raw
0.543
35136
GMM(components=20, cov=spherical) 6gram compact
0.541
9846
GMM(components=60, cov=spherical) 3gram raw
0.540
13118
GMM(components=60, cov=spherical) 4gram raw
0.538
26182
GMM(components=40, cov=spherical) 6gram compact
0.538
25713
GMM(components=50, cov=spherical) 6gram compact
0.535
28929
GMM(components=30, cov=spherical) 6gram compact
0.535
14068
GMM(components=50, cov=spherical) 4gram raw
0.531
15922
GMM(components=100, cov=spherical) 5gram raw
0.531
10394
GMM(components=40, cov=spherical) 3gram raw
0.531
14169
KMeans(k=50) 4gram raw
0.530
21358
KMeans(k=20) 4gram raw
0.530
9993
GMM(components=50, cov=spherical) 3gram raw
0.529
25110
KMeans(k=50) 6gram compact
0.528
15160
KMeans(k=40) 4gram raw
0.528
22003
GMM(components=50, cov=spherical) 5gram compact
0.528
11796
KMeans(k=50) 3gram raw
0.527
24181
GMM(components=60, cov=spherical) 6gram compact
0.527
14024
GMM(components=40, cov=spherical) 4gram raw
0.527
12066
GMM(components=100, cov=diag) 4gram raw
0.526
13594
KMeans(k=30) 4gram raw
0.524
12597
KMeans(k=30) 3gram raw
0.523
21472
GMM(components=60, cov=spherical) 5gram compact
0.523
37379
KMeans(k=15) 6gram compact
0.523
26155
GMM(components=30, cov=spherical) 5gram compact
0.522
13129
KMeans(k=100) 4gram raw
0.522
28519
GMM(components=25, cov=spherical) 6gram compact
0.521
12870
KMeans(k=25) 3gram raw
0.520
32162
GMM(components=30, cov=spherical) 7gram compact
0.520
22429
KMeans(k=15) 4gram raw
0.519
23273
GMM(components=40, cov=spherical) 5gram compact
0.518
10228
KMeans(k=100) 3gram raw
0.518
37354
GMM(components=40, cov=spherical) 7gram action
0.517
41392
KMeans(k=10) 6gram compact
Continued on next page
37
Silhouette
Largest
Description
0.516
14568
KMeans(k=20) 3gram raw
0.516
34132
KMeans(k=25) 7gram compact
0.515
42543
KMeans(k=15) 7gram compact
0.515
9595
GMM(components=100, cov=diag) 3gram raw
0.514
28233
GMM(components=50, cov=spherical) 7gram compact
0.514
39033
KMeans(k=20) 7gram compact
0.514
18561
KMeans(k=50) 5gram raw
0.513
16750
GMM(components=60, cov=spherical) 5gram raw
0.513
20160
GMM(components=100, cov=spherical) 6gram compact
0.512
9595
GMM(components=60, cov=diag) 3gram raw
0.512
19581
GMM(components=100, cov=spherical) 5gram compact
0.510
17413
GMM(components=30, cov=spherical) 4gram raw
0.510
32173
GMM(components=25, cov=spherical) 7gram compact
38
