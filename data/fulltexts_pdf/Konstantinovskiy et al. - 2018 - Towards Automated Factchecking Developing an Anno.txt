Towards Automated Factchecking:
Developing an Annotation
Schema and Benchmark for Consistent Automated Claim
Detection
Lev Konstantinovskiy
1
, Oliver Price
2
, Mevan Babakar
1
, Arkaitz
Zubiaga
2
1
Full Fact, London, UK
2
University of Warwick, Coventry, UK
Abstract
In an effort to assist factcheckers in the process of
factchecking,
we tackle
the claim detection task,
one of
the necessary stages prior to determining
the veracity of
a claim.
It consists of
identifying the set of
sentences,
out
of
a long text,
deemed capable of
being factchecked.
This paper is a col-
laborative work between Full Fact, an independent factchecking charity, and
academic partners.
Leveraging the expertise of professional factcheckers, we
develop an annotation schema and a benchmark for automated claim detec-
tion that is more consistent across time, topics and annotators than previous
approaches.
Our annotation schema has been used to crowdsource the anno-
tation of a dataset with sentences from UK political TV shows.
We introduce
an approach based on universal sentence representations to perform the clas-
sification, achieving an F1 score of 0.83, with over 5% relative improvement
over the state-of-the-art methods ClaimBuster and ClaimRank.
The system
was deployed in production and received positive user feedback.
Introduction
Misinformation has recently become more central
in public discourse (Shu,
Sliva,
Wang, Tang, & Liu, 2017; Zubiaga, Aker, Bontcheva, Liakata, & Procter, 2018; Ciampaglia,
2018).
As a consequence, interest has increased in the scientific community to further NLP
approaches that can help alleviate the burdensome and time-consuming human activity of
factchecking (Vlachos & Riedel, 2014; Thorne & Vlachos, 2018).
Factchecking is known as
the task of producing an informed assessment of the veracity of a claim (Graves,
Nyhan,
& Reifler, 2016; Graves, 2018).
When considered inside a factchecking organisation, it is a
series of tasks.
The vast majority of
the scientific research has focused on the part visible to the
general
public - the determination of veracity (Huynh & Papotti,
2018;
Thorne,
Vlachos,
Christodoulopoulos, & Mittal, 2018), often referred to as fake news detection (Wang, 2017;
Long,
Lu,
Xiang,
Li,
& Huang,
2017) or arguably rumour detection (Yang,
Liu,
Yu,
&
Yang, 2012; Ma, Gao, Wei, Lu, & Wong, 2015; Kwon, Cha, & Jung, 2017).
In the industrial
arXiv:1809.08193v1 [cs.CL] 21 Sep 2018
CONSISTENT AUTOMATED CLAIM DETECTION
2
setting of a factchecking organisation, we believe the demands for automatically establishing
veracity are too high to be satisfied by current methods.
High levels of
generality and
precision would be required for it to become a part of the pipeline.
However, there is a user
need for another component which we believed to be more tractable,
and for which there
is limited scientific literature.
It is the stage before veracity called ’claim detection’,
i.e.
monitoring news sources and identifying if
a particular sentence constitutes a claim that
could be factchecked.
In this work, we set out to present the outcome of development of the first stage in the
automated factchecking pipeline.
It is an automated claim detection system developed by
the UK’s independent factchecking charity, Full Fact
1
together with academic collaborators.
The contributions of our work are as follows:
•
We introduce the first annotation schema for claim detection,
iteratively developed
by experts at Full Fact, comprising 7 different labels.
•
We describe a crowdsourcing methodology that enabled us to collect a dataset with
5,571 sentences labelled according to this schema.
•
We develop a novel
claim detection system that leverages transfer learning and uni-
versal sentence representations, as opposed to previous work that was limited to word-
level representations.
Our experiments show that our claim detection system outper-
forms the state-of-the-art claim detection systems, ClaimBuster and ClaimRank.
•
With the annotation schema, crowdsourcing methodology and task definition, we set
forth a benchmark methodology for further development of claim detection systems.
The Factchecking Process
Most factcheckers say that their mission is to give citizens information to make po-
litical
choices,
improve the quality of public political
discourse and to hold politicians ac-
countable (Graves et al.,
2016).
Misinformation and misperceptions can undermine this
goal
(Fridkin,
Kenney,
& Wintersieck,
2015;
Flynn,
Nyhan,
& Reifler,
2017).
There is a
very small
number of
factchecking organisations in the world,
about 150 (Stencel,
2017),
compared to the volume of media items produced daily.
The speed at which information
now flows means there is less time to verify the claims made and myths spread more easily.
The task of factchecking has never been bigger or more challenging.
Hence, automating any
parts of the factchecking process would save factcheckers time to work on the more difficult
tasks needing human judgement.
The factchecking process has been formally defined by Full Fact as consisting of the
following four stages (Babakar & Moy, 2016):
1.
Monitor media, capturing content e.g.
articles, videos, images,
2.
Detect claims, spotting when an item contains a checkable claim,
3.
Check claims, doing the research to check and verify that claim, and
4.
Publish, creating a piece of content that encapsulates the results of the check.
1
http://fullfact.org/
CONSISTENT AUTOMATED CLAIM DETECTION
3
First, the media is recorded and the content extracted, e.g.
HTML markup is stripped
off online articles or speech is transcribed to text.
Second, the sentences containing factual
claims are highlighted.
This generally removes at least 70% (Hassan,
Zhang,
et al.,
2017;
Gencheva,
Nakov,
Màrquez,
Barrón-Cedeño,
& Koychev,
2017) of
sentences that do not
require further processing.
Third, the claims are checked manually against datasets and/or
by calling up the claimant.
And finally,
the results are published to the public if
they
are considered a meaningful
contribution to the debate.
A meaningful
contribution is not
necessarily one that ends in a ‘True’
or ‘False’
classification.
A factcheck may also clarify
a controversy,
unravel
the many interpretations of data,
or supply context as a means of
getting to a more nuanced answer.
There are activities that take part after publication too,
such as lobbying for improvements in data or getting publications to correct the record.
The timeframe of this process varies from minutes during live factchecking to possibly
weeks when writing a complex article.
At each step in the process there is editorial selection
due to finite human resources, impact goals and other considerations.
It involves deciding
which media to monitor, which claims to check and which findings to publish.
These are dif-
ficult tasks as they require knowledge of the current political and media landscape; thus, are
out of scope of automation in the near future.
However monitoring media, detecting claims,
clustering claims, automatically checking simple claims or surfacing relevant datasets, all aid
the factchecker, and all involve a level of automation.
Thus automation could help increase
the volume of factchecker output, or help select claims that are more valuable.
But there is
only a limited amount that automation can do to speed up factchecking.
Complex checking
remains the bottleneck and usually requires careful human judgement and expertise.
Claim detection, the stage we tackle in this paper, is a necessary pre-requisite for fu-
ture work on claim clustering.
It could maximise the impact of a small number of factcheck-
ers, as they will be able to see which claims are the most popular across a large scale media
landscape and therefore could be the best targets to intervene on.
Related Work
Most work around automatically factchecking claims has focused on the later stages of
determining the veracity of claims, usually by building knowledge graphs out of knowledge
bases,
such as Wikidata (Wu,
Agarwal,
Li,
Yang,
& Yu,
2014;
Ciampaglia et al.,
2015;
Shiralkar,
2017;
Shi
& Weninger,
2016;
Cao,
Li,
Luo,
& Yao,
2018).
Less work has been
reported on the preceding stage of claim detection, which is still in its infancy.
One of the best-known approaches to claim detection is ClaimBuster (Hassan, Zhang,
et al., 2017).
They collected a large annotated corpus of televised debates in the USA. Their
model
combines TF-IDF,
POS tags and NER features on an SVM classifier and produces
a score of
how important a claim is to factcheck.
This has the caveat of
then having to
choose a cut-off score to determine the claims that will be considered for factchecking.
Our
approach is instead to define an annotation schema that is binary,
and built on several
types of claims.
It better fits the use case in this factchecking pipeline - in a live stream of
subtitles we are unable to know in advance which sentence will make it to the top ranking
until the end of the entire programme.
In (Hanto & Tostrup,
2018) annotations were collected using ClaimBuster-inspired
annotation guidance from volunteers together with their age, gender and education.
With
possible labels being verifiable check-worthy (VCW), verifiable not check-worthy (VNCW),
CONSISTENT AUTOMATED CLAIM DETECTION
4
and not verifiable (NV), they obtained 2,100 labelled sentences (reduced to 264 high-quality
labelled sentences).
They find that annotators with a natural
sciences background agreed
internally about what constitutes a check-worthy claim,
whereas those with humanities,
medicine,
and ontology backgrounds saw more internal disagreement on check-worthiness.
Furthermore, using 35 labelled control claims to test annotator skill, they find that the age
group 40-49 obtains a higher average score, and label more claims, than that of 30-39, which
correspondingly scores higher than age group 20-29.
Another recent approach to claim detection is ClaimRank (Gencheva et al.,
2017).
They compiled a dataset by taking the outputs of factchecking of a political
debate,
pub-
lished by 9 organisations simultaneously.
Models were created to predict if the claim would
be highlighted by at least one or by a specific organisation.
The modelling is done with a
large variety of features from both the individual
sentence and the wider context around
it.
A subsequent version of the dataset (Jaradat,
Gencheva,
Barrón-Cedeño,
Màrquez,
&
Nakov,
2018;
Nakov et al.,
2018) includes a larger set of sentences in two languages,
En-
glish and Arabic.
These datasets are similar to ours in order of magnitude, however use a
different definition of claim, as we further elaborate in the next subsection.
A slightly different approach to claim detection is that of
context-dependent claim
detection (CDCD) (Levy, Bilu, Hershcovich, Aharoni, & Slonim, 2014).
This study proposes
identifying claims given a specific context.
Articles relevant to a topic are used to detect
claims on that topic.
Another piece of
work worth mentioning is the development of
the FEVER (Fact
Extraction and VERification) dataset (Thorne et al., 2018).
Whilst this is primarily aimed
at work regarding claim veracity,
the mere presence of
a vast quantity of
claims in the
dataset allow it to be extended for claim detection in the future.
Previous Attempts at Defining Claims
There is a body of work on claim detection that has not formalised the definition of
a claim e.
g.
(Gencheva et al.,
2017),
(Jaradat et al.,
2018).
Instead it directly relies on
what has been identified by external organisations.
The lack of a formal definition prevents
others from replicating or extending their work.
These studies used claims identified by
9 organisations in a political
speech as a proxy.
The annotations were sourced from pub-
licly available online articles.
This is different from our approach where we crowdsourced
annotations following our definition of the task.
The authors in (Gencheva et al., 2017) ac-
knowledge this limitation, which led to high number of false positives in their experiments.
For example,
an article consisting of a debate transcript with editorial
comments will
not
highlight repeated instances of claims.
This creates inconsistent annotation – during a TV
debate,
popular claims are discussed on repeated occasions.
This had to be re-annotated
by researchers in (Nakov et al.,
2018).
Another caveat is that only 3 of
the 9 annotator
organisations contributing to those online articles sign up to be neutral and transparent in
their selection of claims as verified signatories of the International Factchecking Network’s
Code of Principles.
2
ClaimBuster (Hassan,
Zhang,
et al.,
2017) provides a definition of
a claim which
revolves around the question:
“Will
the general
public be interested in knowing whether
2
https://perma.cc/BM43-SJ4N
CONSISTENT AUTOMATED CLAIM DETECTION
5
this sentence is true or false?”.
Claims are considered to be those sentences for which the
answer to this question is yes.
Their aim was for anyone to be able to feed in a source,
e.g.
a political speech, and for the system to produce a list of claims ranked by importance,
which could directly feed into the editorial
process.
This definition of
a claim includes
the judgement of
‘importance’
which we avoid in our work.
We believe it is an editorial
judgement best left to factcheckers.
ClaimBuster annotators were journalists,
students
and professors.
Annotations that agreed with the authors of
that study were selected to
ensure good agreement and shared understanding of
the assumptions.
Researchers from
the ClaimBuster team also defined an annotation schema called PolitiTax,
3
a taxonomy
of
political
claims which we considered.
However the categories were not useful
for the
downstream task of
checking the veracity of
the claim by routing it to the right dataset
or team at Full
Fact,
in part due to the level
of granularity in the taxonomy and in part
because the teams are split by topic.
There was also a taxonomy defined by factcheckers during the HeroX factchecking
challenge (Francis & Babakar,
2016),
which is less granular than PolitiTax.
It has four
claim types - numerical,
political
stance,
quotes,
objects.
During this work we discovered
that the latter three categories are rare and intersect with others,
so we did not use them
in our schema.
Dataset
Our Claim Definition and Process
Writing the annotation guidance was not straightforward.
During 2015 UK election
Full
Fact defined a claim as “an assertion about the world that can be checked”.
Media
monitoring volunteers were encouraged to ask a factchecker if they had doubts on whether
something was assessable.
We tried to codify some of this thinking in conversations with
the factcheckers.
After discussions with factcheckers, we chose to decouple the importance of the claim
from the claim itself.
We felt that importance was heavily subjective, reliant on context and
best left to factcheckers.
Importance is a subtle, and forever changing feature.
Even though
the most “important” issues in the view of
the UK public are often about the economy,
immigration and health, their relative positions change
4
.
In some cases new issues become
important, e.g.
in the UK, importance of claims about the EU increased significantly after
the 2016 EU referendum.
5
Different claims are important to different people.
There is empirical confirmation of
this link when claim annotations are contrasted with volunteer’s educational
background
in the crowdsourced annotations in (Hanto & Tostrup, 2018).
For example, “Norway has a
long coast, and it will take at least three days to sail it from one end to the other” was one
of
the sentences that volunteers agreed to be a claim but disagreed on check-worthiness.
Those with “natural
science” education thought it was not check-worthy while those with
“humanities” and “other” thought it was.
3
"PolitiTax A Taxonomy of Political Claims" by IDIR Lab
https://perma.cc/4RQF-FCPV
4
"Ipsos MORI Issues Index:
2017 in review"
https://perma.cc/9SMV-CQR8
5
"July 2016 Economist/Ipsos MORI Issues Index"
https://perma.cc/DPA5-4XV5
CONSISTENT AUTOMATED CLAIM DETECTION
6
We also chose to decouple the topic from the definition.
By making our definition
descriptive of the claim and not, by proxy, the topic, we would have a more consistent final
dataset.
In some cases selection of
topics is an inherently political
choice,
e.g.
it varies
across the population whether “drugs” relate to the topic of “crime” or “health”.
This kind
of classification was avoided.
Our goal was to come up with a claim detection system that is more consistent than
ClaimBuster and ClaimRank over time, across topics and across different annotators.
With factcheckers,
we identified what was definitely not a claim.
We iterated on
potential
rules and found examples that broke them.
We identified some constraints,
like
a claim has to be checkable.
6
We were most concretely able to exclude claims based on an
individual’s personal
experience,
as more often than not they were un-checkable.
This is
similar to ‘verifiable experiential’ statements (Park & Cardie, 2014).
We went through several
versions of
the guidance with different taxonomies.
We
trialled them within Full
Fact,
and then two versions with external
volunteers.
The first
version applied the 2015 thinking and was a binary accept/reject classification task, accom-
panied by a guidance.
It listed several types of qualities of claims and non-claims.
Claims,
for example,
may be explicit,
implicit,
or trivial.
Non-claims in this version were formed
of personal experience and opinion.
We decided against these categories in the end as they
sometimes involve explicit judgements from our annotators – these choices can sometimes
be highly political.
For example,
in the case of “The EU is made up of 27 [instead of 28]
countries” or “The NHS is there for everyone” some annotators could classify them as triv-
ial while others might consider them explicit legal claims.
The implicit/explicit categories
were also removed,
because whether the claim is implicit or explicit is not important for
the next downstream task in the factchecking process after claim detection – factchecking.
For the second version,
we looked at Full
Fact’s factchecks.
They mostly covered
statistical
claims.
We also identified claims around current laws or rules of operation and
correlation/causation claims e.g.
“there’s no clear correlation between prisons’ performance
ratings and whether they’re publicly-run or contracted out to the private sector.”.
This
became the basis of
our claim categories.
We believed if
we joined these categories,
and
removed personal
experience,
we would have a good proxy for claims.
There were many
other types of claims that we identified, such as definitions, voting records, and expressions
of support.
We limited our categories to 7 to make the task realistic for annotators.
We
also wanted to minimise the overlap between categories to make the task single-choice.
Annotation Guidance
Our annotation schema is the first to be created with a factchecking organisation,
building on years of experience manually detecting claims.
It comprises the following 7 categories,
only one of
which can be assigned to each
sentence:
1.
Personal
experience.
Claims that aren’t capable of being checked using publicly-
available information, e.g.
“I can’t save for a deposit.”
6
Although, it is difficult to preempt what data is publicly available - as an example, statistics on the relative
happiness of
cities actually do exist,
a topic whose claim might seem initially not checkable.
https://
fullfact.org/health/heartache-hertsmere-unhappiest-place-uk/
CONSISTENT AUTOMATED CLAIM DETECTION
7
2.
Quantity in the past or present.
Current value of
something e.g.
“1 in 4 wait
longer than 6 weeks to be seen by a doctor.” Changing quantity, e.g.
“The Coalition
Government has created 1,000 jobs for every day it’s been in office.” Comparison, e.g.
“Free schools are outperforming state schools.”.
Ranking, e.g.
“The UK’s the largest
importer from the Eurozone.”
3.
Correlation or causation,
Correlation e.g.
“GCSEs are a better predictor than
AS if
a student will
get a good degree.”
Causation,
e.g.
“Tetanus vaccine causes
infertility.” Absence of a link, e.g.
“Grammar schools don’t aid social mobility.”
4.
Current laws or rules of operation, e.g.
“The UK allows a single adult to care for
fewer children than other European countries.” Procedures of public institutions, e.g.
“Local
decisions about commissioning services are now taken by organisations that
are led by clinicians.” Rules and changes, e.g.
“EU residents cannot claim Jobseeker’s
Allowance if they have been in the country for 6 months and have not been able to
find work.”
5.
Prediction,
Hypothetical
claims about the future e.g.
“Indeed,
the IFS says that
school funding will have fallen by 5% in real terms by 2019 as a result of government
policies.”
6.
Other type of claim,
Voting records e.g “You voted to leave,
didn’t you?” Public
Opinion e.g “Public satisfaction with the NHS in Wales is lower than it is in England.”
Support e.g.
“The party promised free childcare” Definitions,
e.g.
“Illegal
killing of
people is what’s known as murder.” Any other sentence that you think is a claim.
7.
Not a claim, These are sentences that don’t fall into any categories and aren’t claims.
e.g.
“What do you think?.”, “Questions to the Prime Minister!”
These categories have proven to broadly cover sentences from political TV shows that
Full Fact has encountered over several years.
Categories have different levels of occurrence
(see Table 1).
As previously found (Jaradat et al.,
2018;
Hanto & Tostrup,
2018;
Hassan,
Zhang, et al., 2017), “Not a claim” is the most popular category, amounting to about 55%
of the annotations.
“Other” is the second largest category with 952 instances,
23% of the whole.
It can
be broken down into claims that are less well-defined,
with formal
sub-categories being:
‘Definitions’,
‘Voting records’,
‘Public opinion’,
‘Trivial
claim’,
‘Support’,
‘Quote’,
‘Other
other’.
We amalgamate these because they are likely to overlap and we wanted our an-
notators to only select one option.
For example,
“She said she voted to keep free school
meals.”
is both a quote and a voting record.
Furthermore,
high granularity of categories
allows one to perpetually think of rarer categories and a high number of categories slows
down annotation unnecessarily.
To verify if our level of granularity was correct, we split a
sample of 160 sentences in ‘Other’ into sub-categories.
The vast majority are in the ‘Other
other’ category (see Table 1), supporting our chosen level of granularity.
Crowdsourced Annotation
The annotations were done by 80 volunteers recruited through Full Fact’s newsletter
– this meant that volunteers were keen on factchecking.
28,100 annotations were collected
for a set of
6,304 sentences extracted from subtitles of
four UK political
TV shows,
14
CONSISTENT AUTOMATED CLAIM DETECTION
8
Category
Subcategory
Counts
Example
Not a claim
54.8%
“Give it all to them, I really don’t mind.”
Other
Other other
10.4%*
“Molly gives so much of who she is away
throughout the film.”
Support/policy
5.5%*
“He has advocated for a junk food tax.”
Quote
4.7%*
“The Brexit secretary said he would guarantee
free movement of bankers.”
Trivial claim
1.6%*
“It was a close call.”
Voting record
0.7%*
“She just lost a parliamentary vote.”
Public opinion
0.4%*
“A poll showed that most people who voted
Brexit were concerned with immigration.”
Definition
0.0%*
“The unemployed are only those actively
looking for work.”
Quantity
Current value
9.9%
“1 in 4 people wait longer than 6 weeks
to see a doctor.”
Changing quantity
Comparison
Ranking
Prediction
Hypothetical
statements
4.4%
“The IFS says that school funding will
have fallen by 5% by 2019.”
Claims about
the future
Personal experience
Uncheckable
3.0%
“I can’t save for a deposit”
Correlation/causation
Correlation
2.6%
“Tetanus vaccine causes infertility”
Causation
Absence of
a link
Laws/rules of operation
Public institution-
al procedures
1.9%
“The UK allows a single adult to care
for fewer children than other European
countries.”
Rules/rule
changes
Table 1
A breakdown of the 4,080 sentences where majority agreement achieved.
The "Other" cate-
gory is 23%.
*The proportions for ‘Other’ sub-categories are taken from a random sample
of 160 claims labelled as ‘Other’.
CONSISTENT AUTOMATED CLAIM DETECTION
9
episodes in total.
TV subtitles were chosen because 69% of UK population get their news
from TV.
7
The software used for collecting annotations was Prodigy
89
, a self-hosted annotation
platform.
It was customised to support multiple annotators – a login and password screen
routing via nginx to a specific pre-started instance of Prodigy running in a Docker container.
Sentences were shown in random order.
The preceding two sentences were also shown on
the screen to provide context and assist with potential co-references.
Once a sentence was
annotated 5 times by different annotators, it was not shown again.
Annotators were encouraged to contact us for any clarifications needed, with thought-
ful
questions such as:
“Where it appears that a claim is dressed up as rhetorical
question,
should we classify it as a claim? e.g.
‘Why should unelected officials in Brussels make rules
to stop bananas being sold in bunches of more than 2 or 3?”’
To answer this, questions are
classified as the claims that they implicitly contain.
Figure 1 .
Annotation UI in Prodigy
Agreement
At the level of all the 7 granular categories the inter-annotator agreement is moderate,
with a Krippendorff’s alpha (Krippendorff, 1980) of 0.46.
However, we attain higher values
of
alpha of
0.70 and 0.53 if
we convert the annotations into a binary claim/non-claim
annotation task, following the two methods shown in Table 3.
Most of the disagreement was between “Not a claim” and “Other claim”.
This showed
that it is hard to define the boundary and explicitly list all kinds of claims as we saw in the
7
"News consumption in the UK:2016" by Ofcom
https://perma.cc/5FDK-BRHD
8
https://prodi.gy/
9
https://fullfact.org/blog/2018/feb/how-we-customised-prodigy-ai/
CONSISTENT AUTOMATED CLAIM DETECTION
10
Qu
12
Corr
10
10
Law
2
19
11
Pred
3
42
27
25
Other
50
102
129
87
87
Not
114
58
90
69
103
668
Pers
Qu
Corr
Law
Pred
Other
Table 2
Annotation disagreements.
The most prominent disagreement is between “Other claim” and
“Not a claim”.
The labels are shortened versions of those in Figure 1 due to space limita-
tions.
(Qu:
quantity,
Corr:
correlation and causation,
Pred:
predictions,
Pers:
personal
experience.)
Claim
Non-claim Omitted
α
N
2
3, 4, 6, 7
1, 5
0.70
6,095
2, 3, 4, 5
1, 6, 7
–
0.53
4,777
Table 3
From 7 categories to binary claim vs “not a claim” classification.
N = number of sentences
annotated by majority.
(1) Personal
experience,
(2) Quantity in the past or present,
(3)
Correlation or causation, (4) Current laws or rules of operation, (5) Prediction, (6) Other
type of claim, (7) Not a claim.
process of creating the annotation guidance.
The disagreements across all
sentence types
can be seen in Table 2.
To solve the disagreements we reframe the task as binary classification by grouping
some categories together.
Another reason for the binary instead of multi-class classification
is that the end user application is binary,
i.e.
a sentence either makes it to the excerpted
list of claims or not.
The binary classification system still captures the expert factcheckers
input by using the granular positive claim categories during annotation.
Most importantly
it achieves dataset robustness by reducing the annotator disagreement.
See the two possible
binary re-formulations in Table 3.
The are no negative sides to this binary re-formulation
for this use case.
The claims are routed to the relevant factchecking team or a dataset based
on their topic (e.g.
healthcare or immigration) and not claim type.
The most agreement and easiest to model view of the data is in the first row of Table
3.
‘Quantity’ was selected as the positive class because it is the majority category of claims
(received most annotations).
It’s also the category that Full
Fact most frequently writes
about.
We established this after annotating 800 of their claims.
The categories of ‘Personal
experience’ and ‘Prediction’ were excluded for ease of modelling because they might contain
quantities outside of the ‘Quantity’ claim class.
Once we found models that perform with 92% precision and 88% recall on identifying
quantitative claims, we moved to evaluate them on more realistic data in the second row of
Table 3.
Here ‘Other type of claim’ is not in the positive class for two reasons.
First of all
there is a lot of disagreement between it and the ‘Not a claim’
class.
Secondly,
the kinds
of
claim in the ’Other’
section - voting records,
quotes,
statements about public opinion
CONSISTENT AUTOMATED CLAIM DETECTION
11
polls, are less frequently written about by Full Fact.
We are aware that this choice encodes
the peculiarity of
one organisation at one point in time into our model
and needs to be
re-considered if, say, an organisation wishing to use the model specialises in voting records.
The agreement of 60% is still low – that is a lot of sentences to throw away if we were
only to consider agreement among all the annotators.
So instead we choose a majority vote
where at least 3 annotators marked the sentence and more than half of them agree.
Out
of
the initial
6,304 sentences this filter selects 4,777 sentences,
3,973 not claims and 804
claims.
This is in line with previous studies where the proportion of claims is 10-30% in
political
TV (Hassan,
Zhang,
et al.,
2017;
Gencheva et al.,
2017).
As extra training data
we add 794 claims from the Full
Fact database.
Out of them 766 are annotated by us as
positive because they fall into our claim categories, for example “The courts have said that
the so-called ‘bedroom tax’
is illegal.” The remaining 28 are in the ‘Other type of claim’
category, for example “The British economy is not only getting better, it is healing.”
In the same way of
choosing the majority votes we can produce a dataset of
4080
sentences annotated across the 7 classes with majority voting, see Table 1.
Methods
To capture the diversity of sentences observed during political TV shows, we propose
to leverage universal sentence representations.
We use InferSent (Conneau, Kiela, Schwenk,
Barrault, & Bordes, 2017) as a method to achieve sentence embeddings.
These embeddings
are different from averaging word embeddings because they take word order into account
using a recurrent neural
network.
The method provided by InferSent involves words be-
ing converted to their common crawl GloVe implementations before being passed through
a bidirectional
long-short-term memory (BiLSTM) network (Hochreiter & Schmidhuber,
1997).
The sentence embeddings were pre-trained on a large dataset of Natural Language
Inference tasks
10
.
Additionally, we also tried concatenating POS and NER information to
the embeddings.
For each sentence,
the POS/NER feature vector was the count of
each
POS/NER tag in the corpus.
We input our sentence representations to a range of super-
vised classifiers implemented using scikit-learn (Pedregosa et al., 2011), with the classifiers
set to their default parameters.
The four classifiers we tested include Logistic Regression,
Linear SVM, Gaussian Naïve Bayes and Random Forests.
We use a number of other features as baselines:
1.
A number of variants of the state-of-the-art claim detection system by ClaimBuster,
using different combinations of TF-IDF, POS and NER features, as in (Hassan, Zhang,
et al., 2017).
2.
Averaging pre-trained word embedding vectors for all words in a sentence.
We evalu-
ate:
•
Word2vec (Mikolov,
Chen,
Corrado,
& Dean,
2013) via the Gensim implemen-
tation (Řehůřek & Sojka, 2010), using the GoogleNews embedding.
•
GloVe (Pennington,
Socher,
& Manning,
2014) trained on Common Crawl,
as
well as combining them with dimensionality reduction using principal component
analysis (PCA).
10
https://nlp.stanford.edu/projects/snli/
CONSISTENT AUTOMATED CLAIM DETECTION
12
3.
TF-IDF representations of sentences with logistic regression.
Numbers have a signif-
icant role in claims - the “Cardinal
Number” part-of-speech tag is the second most
discriminating feature in (Hassan, Arslan, Li, & Tremayne, 2017)), so we try a Spacy
NER to replace numbers with ‘*NUMBER*’ during preprocessing.
For our implementation of the ClaimBuster system, we use the Watson Natural Lan-
guage Understanding API,
the updated version of the Alchemy API used by the original
authors.
All other features were implemented as outlined in (Hassan, Zhang, et al., 2017).
The ClaimRank (Gencheva et al., 2017) model was harder to re-implement.
In order
to maintain impartiality Full
Fact can’t use the data on sentence speakers when selecting
which claims to check.
This special care is also encouraged in the computer science research
for systems that are integrated into the infrastructure of society by the ACM code of ethics
(ACM Code of
Ethics,
2018).
Additionally,
our dataset didn’t have data on applause,
laughing,
or speaker crossover.
Even though we unfortunately couldn’t use one third of
ClaimRank’s features
11
, we trained the FNN, SVM and logistic regression classifiers on the
remaining features in our dataset (Gencheva et al., 2017).
Experiment Settings
The dataset consists of
5,571 sentences (4,777 from annotations and 794 from the
Full
Fact database of
claims),
of
which 1,570 are claims and 4,001 are not claims,
which
gives a 30/70 class imbalance.
We use stratified 5-fold cross-validation to train and test our
models.
We use precision, recall and F
1
-score measures to assess classifier performance.
We show the best-performing classifier for any given feature set.
We also show 95%
confidence interval
for the precision and recall
using binomial
distributions.
This demon-
strates possible overlap in results between different models.
The interval is wide for recall
due to the small number of positive examples.
The next section will present the results of
applying these methods.
Claim Detection
Here we present results for the binary classification,
using the class grouping shown
in the second row of Table 3.
Analysis of Results
Table 4 shows the results of our experiments.
Interestingly,
the simple approach of
TF-IDF achieves high precision but low recall.
We call our new model ‘CNC’ which stands
for “Claim/No Claim”.
It achieves a better balance of precision and recall;
logistic regres-
sion classifier gives the highest overall F1 score of 0.83, outperforming all other techniques.
The use of POS and NER features in our model has no effect on the performance.
GloVe
embeddings achieve performance close to our method with F1 scores 2% lower and substan-
tially lower recall
scores.
Despite the overlap in precision scores between GloVe and our
method, the overlap is minimal in terms of recall.
Our CNC model also clearly outperforms
the state-of-the-art method by ClaimBuster at 0.79 F1; hence our method yields F1 scores
that improve ClaimBuster by over 5% in relative terms.
ClaimBuster performs similarly to
11
https://github.com/pgencheva/claim-rank
CONSISTENT AUTOMATED CLAIM DETECTION
13
Features
Classifier
P
R
F
1
P-interval
R-interval
TF-IDF
LogReg
.90
.59
.70
.89 - .91
.56 - .61
TF-IDF+spacy number preproc.
LogReg
.91
.59
.70
.90 - .92
.56 - .61
Word2Vec
SVM
.85
.75
.78
.84 - .86
.73 - .77
GloVe
LogReg
.89
.76
.81
.88 - .90
.74 - .78
GloVe+PCA
LogReg
.89
.75
.81
.88 - .90
.73 - .77
CB: TF-IDF
LogReg
.90
.59
.70
.89 - .91
.56 - .61
CB: TF-IDF+POS
LogReg
.88
.68
.76
.86 - .89
.66 - .71
CB: TF-IDF+NER
LogReg
.88
.60
.71
.87 - .89
.58 - .63
CB: TF-IDF+POS+NER
LogReg
.87
.71
.78
.86 - .88
.68 - .73
CB: TF-IDF
SVM
.84
.70
.76
.83 - .85
.69 - .73
CB: TF-IDF+POS
SVM
.86
.74
.79
.85 - .87
.72 -.76
CB: TF-IDF+NER
SVM
.84
.71
.77
.83 - .85
.69 - .73
CB: TF-IDF+POS+NER
SVM
.86
.75
.79
.85 - .87
.73 - .77
ClaimRank
LogReg
.93
.65
.77
.92 - .94
.63 - .67
ClaimRank
SVM
.93
.53
.67
.92 - .94
.51 - .55
ClaimRank
FNN
.89
.61
.72
.87 - .91
.58 - .62
CNC
LogReg
.88
.80
.83
.87 - .89
.78 - .82
CNC+POS
LogReg
.88
.80
.83
.87 - .89
.78 - .82
CNC+NER
LogReg
.88
.80
.83
.87 - .89
.78 - .82
CNC+POS+NER
LogReg
.88
.80
.83
.87 - .89
.78 - .82
Table 4
Results for the Claim/No claim experiments.
CB: ClaimBuster.
The 95% confidence inter-
vals are from binomial
distribution adapting (Kohavi et al., 1995).
CNC in terms of precision, albeit with substantially lower recall scores.
ClaimRank has the
best precision scores across the board, but with the lower recall scores.
CNC achieves a 6%
relative improvement in F1-score over ClaimRank.
Multi-class Classification
For the multi-class classification into 7 different labels, we use the subset with 4,080
sentences where there was enough agreement at this level of granularity.
We train logistic
regression on the features from CNC, the best performing binary classifier.
Analysis of Results
Table 5 shows the results for the multi-class classification experiments.
These results
reaffirm our expectations that,
beyond the binary classification of claims and non-claims,
classification at a finer granularity becomes more challenging.
This is especially true for the
categories with the smallest number of instances,
such as “Current laws” or “Correlation
or causation”.
We achieve low F1 score for these categories,
however the small
number of
instances may have a significant impact on this.
Unfortunately we couldn’t source a lot of
claims of these categories from the Full Fact’s database of claims.
This is an example of how
CONSISTENT AUTOMATED CLAIM DETECTION
14
Class
P
R
F1
N
Not a claim
.77
.90
.83
2235
Other type of claim
.59
.55
.57
952
Quantity (past/present)
.80
.79
.79
403
Prediction
.60
.27
.37
181
Personal experience
.72
.39
.50
124
Correlation/causation
.50
.13
.21
107
Current laws/rules
.27
.04
.07
78
microavg / total
.71
.73
.70
4080
macroavg / total
.61
.44
.48
4080
Table 5
Multi-class classifier performance.
CNC model.
the definition of a ‘usual claim’ is specific to an organisation - Full Fact hasn’t had a full-
time legal factchecker since 2016 but has plans to re-instate one.
To achieve more accurate
classification and less organisational
specificity at this level
of granularity,
we would need
to annotate more sentences to expand the dataset.
Looking at bigger classes,
“Quantity”
(relatively easy to identify by looking for numbers and quantitative words) and “Not a
claim” (the most popular category) yield the best
F
1
-scores.
The results for “Other type
of claim” are also good, which only tend to be confused with “Not a claim” – this is in line
with human annotator disagreement in Figure 1.
Overall results are reasonably good when
we measure with a microaveraged F1 score of 0.70,
however it shows significant room for
improvement when we measure it by macroaveraged F1 score of 0.48.
We aim to expand
our dataset in the near future to circumvent these issues.
Deployment and Impact
Model
deployment has impacted Full
Fact and the crowdsourcing exercise has edu-
cated the volunteer community.
Full
Fact’s factcheckers are provided with a UI that shows a live feed of transcripts
from television in a tool called "Live" which aids live factchecking.
When the claim detection
model identifies a claim in these sentences, it highlights it in bold.
In addition to this, the
factcheckers have the ability to manually highlight (in yellow) any claim they believe to be
of interest.
The integration of claim detection model has provided several benefits.
It has
saved time – factcheckers can quickly skim through the text looking for claims,
instead of
reading the entire text.
(Though,
to ensure 100% recall
there are still
designated people
who watch the entire programme or read the entire transcript.)
As a consequence,
some
factcheckers have started skimming just the claims in the transcripts in areas that are
outside of their immediate domain.
Going through the entire transcript was not viable for
them prior to this automation.
We analysed 4 live factchecking sessions and noted that all
claims manually high-
lighted in yellow had also been detected by the model, demonstrating the high recall.
The
precision however is harder to measure during user deployment.
The model
does detect
claims that aren’t highlighted in yellow,
but this is to be expected.
The reason for this is
that factcheckers rely on their domain expertise and awareness of current affairs to decide
CONSISTENT AUTOMATED CLAIM DETECTION
15
which claims are of
specific interest to them – as opposed to highlighting all
claims that
they notice.
These considerations are impossible to encode in a model as discussed in our
claim definition section.
A factchecker gave the following feedback on the system:
“Claim detection is very
useful after I have finished live fact-checking a show and reviewing it to decide what to write
a longer piece about.
I no longer have to read the whole transcript, just the highlighted bits.”
Outside of the Full Fact factcheckers, there has also been anecdotal evidence of impact
on the volunteer community.
The annotation exercise has been educational for them.
They
became more scrupulous media consumers.
This was also a notable side product in other
crowdsourced factchecking initiatives such as TruthSquad
12
.
.
Figure 2 .
Claims highlighted in bold in the "Live" factchecking tool during Prime Minister’s
Questions on 12 Sept 2018.
The transcript errors are from the closed captions broadcast.
Future Work
Our plans for future work include expanding our dataset to other languages and
collaborating with other factchecking organisations.
We also wish to collect more data
using the same annotation schema from other news sources,
such as social
media,
digital
and print outlets.
Another small potential improvement is using the counts of first/second
person pronouns to detect “Personal
Experience” category as proven useful
in (Park &
Cardie, 2014).
Conclusion
Through leveraging the professional factcheckers at Full Fact, and through academia-
industry collaboration,
we have developed the first annotation schema for claim detection
12
"Crowdsourced Fact-Checking? What We Learned from Truthsquad" 2010
https://perma.cc/J8AS-YU8E
CONSISTENT AUTOMATED CLAIM DETECTION
16
informed by experts.
This has enabled us to create an annotated dataset made of sentences
extracted from transcripts of political TV shows.
We have introduced and tested a classifier
that leverages universal
sentence representations which,
with an F1 score of 0.83,
outper-
forms a range of baseline classifiers, including a well-known method by ClaimBuster, by over
5% in relative terms.
While we achieve competitive results for binary claim classification,
there is room for improvement when we need finer granularity of classification into the 7
categories in the annotation schema.
Acknowledgments
We would like to thank all 80 volunteers who participated in the annotation task, in
particular Andreas Sampson Geroski.
Also,
thanks to Joseph O’Leary,
Amy Sippitt,
Will
Moy, Ed Ingold, Vigdis Hanto, Mats Tostrup, Heri Ramampiaro, Jari Bakken, Benj Petitt,
Michal Lopuszynski, Ines Montani, Gael Varoquaux.
This work has been partially funded by Open Society Foundation (OR2017-35395)
and Omidyar Network.
References
Acm code of ethics.
(2018).
Retrieved 2018-01-09, from
https://www.acm.org/code-of-ethics
Babakar, M., & Moy, W.
(2016).
The state of automated factchecking (Tech. Rep.).
Full Fact.
Cao, Y., Li, H., Luo, P., & Yao, J.
(2018).
Towards automatic numerical cross-checking:
Extracting
formulas from text.
In Proceedings of the 2018 world wide web conference on world wide web
(pp. 1795–1804).
Ciampaglia,
G.
L.
(2018).
The digital
misinformation pipeline.
In Positive learning in the age of
information (pp. 413–421).
Springer.
Ciampaglia,
G.
L.,
Shiralkar,
P.,
Rocha,
L.
M.,
Bollen,
J.,
Menczer,
F.,
& Flammini,
A.
(2015).
Computational fact checking from knowledge networks.
PloS one, 10 (6), e0128193.
Conneau,
A.,
Kiela,
D.,
Schwenk,
H.,
Barrault,
L.,
& Bordes,
A.
(2017).
Supervised learning of
universal sentence representations from natural language inference data.
In Proceedings of the
2017 conference on Empirical
Methods in Natural
Language Processing (pp. 670–680).
Flynn, D., Nyhan, B., & Reifler, J. (2017). The nature and origins of misperceptions:
Understanding
false and unsupported beliefs about politics.
Political
Psychology, 38 (S1), 127–150.
Francis,
D.,
& Babakar,
M.
(2016).
Herox factchecking challenge.
Retrieved 2017-01-11,
from
https://www.herox.com/factcheck/5-practise-claims
Fridkin,
K.,
Kenney,
P.
J.,
& Wintersieck,
A.
(2015).
Liar,
liar,
pants on fire:
How fact-checking
influences citizensâĂŹ reactions to negative advertising.
Political
Communication,
32 (1),
127–151.
Gencheva,
P.,
Nakov,
P.,
Màrquez,
L.,
Barrón-Cedeño,
A.,
& Koychev,
I.
(2017).
A context-
aware approach for detecting worth-checking claims in political
debates.
In Proceedings of
the international
conference recent
advances in natural
language processing (pp.
267–276).
INCOMA Ltd.
Retrieved from
https://doi.org/10.26615/978-954-452-049-6_037
doi:
10.26615/978-954-452-049-6_037
Graves,
L.
(2018).
Factsheet:
Understanding the promise and limits of
automated fact-checking
(Tech. Rep.).
Reuters Institute for the Study of Journalism, University of Oxford.
Graves,
L.,
Nyhan,
B., & Reifler,
J.
(2016).
Understanding innovations in journalistic practice:
A
field experiment examining motivations for fact-checking.
Journal
of Communication, 66 (1),
102–138.
CONSISTENT AUTOMATED CLAIM DETECTION
17
Hanto,
& Tostrup.
(2018).
Towards automated fake news classification - on building collections
for claim analysis research.
Retrieved 2018-01-09,
from
http://hdl.handle.net/11250/
2560779
Hassan,
N.,
Arslan,
F.,
Li,
C.,
& Tremayne,
M.
(2017).
Toward automated fact-checking:
De-
tecting check-worthy factual
claims by ClaimBuster.
In Proceedings of
the 23rd acm sigkdd
international
conference on knowledge discovery and data mining (pp. 1803–1812).
Hassan, N., Zhang, G., Arslan, F., Caraballo, J., Jimenez, D., Gawsane, S., . . .
others (2017). Claim-
Buster:
the first-ever end-to-end fact-checking system.
Proceedings of the VLDB Endowment,
10 (12), 1945–1948.
Hochreiter,
S.,
& Schmidhuber,
J.
(1997).
Long short-term memory.
Neural
computation,
9 (8),
1735–1780.
Huynh, V.-P., & Papotti, P.
(2018).
Towards a benchmark for fact checking with knowledge bases.
In Companion of the the web conference 2018 on the web conference 2018 (pp. 1595–1598).
Jaradat,
I.,
Gencheva,
P.,
Barrón-Cedeño,
A.,
Màrquez,
L.,
& Nakov,
P.
(2018).
Claimrank:
De-
tecting check-worthy claims in Arabic and English.
In Proceedings of the annual conference of
the north american chapter of the association for computational
linguistics:
Human language
technologies.
Kohavi,
R.,
et al.
(1995).
A study of cross-validation and bootstrap for accuracy estimation and
model selection.
In International
joint conference on artificial
intelligence (pp. 1137–1145).
Krippendorff, K.
(1980).
Reliability.
Wiley Online Library.
Kwon,
S.,
Cha,
M.,
& Jung,
K.
(2017).
Rumor detection over varying time windows.
PloS one,
12 (1), e0168344.
Levy,
R.,
Bilu,
Y.,
Hershcovich,
D.,
Aharoni,
E.,
& Slonim,
N.
(2014).
Context dependent claim
detection.
In Proceedings of coling 2014, the 25th international
conference on computational
linguistics:
Technical
papers (pp. 1489–1500).
Long,
Y.,
Lu,
Q.,
Xiang,
R.,
Li,
M.,
& Huang,
C.-R.
(2017).
Fake news detection through multi-
perspective speaker profiles.
In Proceedings of
the eighth international
joint
conference on
natural
language processing (volume 2:
Short papers) (Vol. 2, pp. 252–256).
Ma, J., Gao, W., Wei, Z., Lu, Y., & Wong, K.-F.
(2015).
Detect rumors using time series of social
context information on microblogging websites.
In Proceedings of the 24th acm international
on conference on information and knowledge management (pp. 1751–1754).
Mikolov, T., Chen, K., Corrado, G., & Dean, J.
(2013).
Efficient estimation of word representations
in vector space.
arXiv preprint arXiv:1301.3781 .
Nakov,
P.,
Barrón-Cedeño,
A.,
Elsayed,
T.,
Suwaileh,
R.,
Màrquez,
L.,
Zaghouani,
W.,
. . .
Da San Martino,
G.
(2018,
September).
CLEF-2018 lab on automatic identification and
verification of claims in political debates.
In Working notes of CLEF 2018 – conference and
labs of the evaluation forum.
Avignon, France.
Park,
J.,
& Cardie,
C.
(2014).
Identifying appropriate support for propositions in online user
comments.
In Proceedings of the first workshop on argumentation mining (pp. 29–38).
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . .
Duchesnay, E.
(2011).
Scikit-learn:
Machine learning in Python.
Journal of Machine Learning Research, 12 ,
2825–2830.
Pennington, J., Socher, R., & Manning, C.
(2014).
Glove:
Global vectors for word representation.
In Proceedings of the 2014 conference on Empirical
Methods in Natural
Language Processing
(EMNLP) (pp. 1532–1543).
Řehůřek,
R.,
& Sojka,
P.
(2010,
May 22).
Software Framework for Topic Modelling with Large
Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks
(pp. 45–50).
Valletta, Malta:
ELRA.
(
http://is.muni.cz/publication/884893/en
)
Shi, B., & Weninger, T. (2016). Discriminative predicate path mining for fact checking in knowledge
graphs.
Knowledge-Based Systems, 104 , 123–133.
CONSISTENT AUTOMATED CLAIM DETECTION
18
Shiralkar, P. (2017). Computational fact checking by mining knowledge graphs (Unpublished doctoral
dissertation).
Indiana University.
Shu,
K.,
Sliva,
A.,
Wang,
S.,
Tang,
J.,
& Liu,
H.
(2017).
Fake news detection on social
media:
A
data mining perspective.
ACM SIGKDD Explorations Newsletter, 19 (1), 22–36.
Stencel,
M.
(2017).
International
fact checking gains ground,
Duke census finds.
Duke Reporters’
Lab (Tech. Rep.).
Duke University, Durham, NC.
Thorne, J., & Vlachos, A.
(2018).
Automated fact checking:
Task formulations, methods and future
directions.
arXiv preprint arXiv:1806.07687 .
Thorne,
J.,
Vlachos,
A.,
Christodoulopoulos,
C., & Mittal,
A.
(2018).
Fever:
a large-scale dataset
for fact extraction and verification.
arXiv preprint arXiv:1803.05355 .
Vlachos, A., & Riedel, S.
(2014).
In Proceedings of
the acl
2014 workshop on language technologies and computational
social
science (pp. 18–22).
Wang, W. Y. (2017). " liar, liar pants on fire":
A new benchmark dataset for fake news detection. In
Proceedings of the 55th annual meeting of the association for computational linguistics (volume
2:
Short papers) (Vol. 2, pp. 422–426).
Wu,
Y.,
Agarwal,
P.
K.,
Li,
C.,
Yang,
J.,
& Yu,
C.
(2014).
Toward computational
fact-checking.
Proceedings of the VLDB Endowment, 7 (7), 589–600.
Yang,
F.,
Liu,
Y.,
Yu,
X.,
& Yang,
M.
(2012).
Automatic detection of rumor on Sina Weibo.
In
Proceedings of the acm sigkdd workshop on mining data semantics (p. 13).
Zubiaga, A., Aker, A., Bontcheva, K., Liakata, M., & Procter, R.
(2018).
Detection and resolution
of rumours in social media:
A survey.
ACM Computing Surveys (CSUR), 51 (2), 32.
