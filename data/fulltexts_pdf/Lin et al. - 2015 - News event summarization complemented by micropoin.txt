News Event Summarization Complemented by
Micropoints
Pingping Lin,
Rong Xiao and Yan Zhang
Department of Machine Intelligence,
Peking University
Key Laboratory on Machine Perception,
Ministry of Education
Beijing,
P.R.
China
linpingping12@pku.edu.cn
xrsmile@gmail.com
zhy@cis.pku.edu.cn
Abstract—In this paper,
we propose a framework to produce
comprehensive summarization of
news event
complemented by
microblog.
Given a basic summary extracted from traditional
news articles,
we extract
micropoints from microblog to com-
plement
it.
A micropoint
is
a micro-viewpoint
composed of
a
few sentences from several
relevant
microblog tweets.
We first
filter microblog tweets
and get
candidate ones
according to a
combination of
the degree of
relevance and complementarity.
Then we extract micropoints through tweet clustering, scattering
and micropoint
rebuilding.
The last
step -
micropoint
selection
is based on three criteria,
which guarantee high informativeness,
popularity and conciseness.
Experiments conducted on real data
demonstrate a good coverage and content coherence.
I.
I
NTRODUCTION
In the era of Web2.0, we are always returned to with enor-
mous news reports when searching a news event.
Researchers
have paid much effort
to the extraction of a brief and to-the-
point summary,
which can provides people with quick access
to the most important information with least time.
Traditional
news websites, such as Xinhua News, CNN, BBC, are still ma-
jor access to news events. They are mainstream and objective.
On the other hand, online social networks such as Sina Weibo
become more and more important
news sources nowadays.
Users post their comments about hot news events and may even
influence the development of news events.
Complementary to
traditional news reports, these tweets are subjective but diverse.
Thus it
may be valuable to use information from microblog
as complement
to traditional
summarization extracted from a
set of news articles.
So far,
most researches on microblog,
to
be specific,
Sina Weibo,
focus on its properties as a social
network,
such as the user
relations,
while we focus on its
contribution as a news media.
Microblog data has many defects itself.
There are a high
proportion of noise tweets which should not be selected into a
summary.
Besides,
tweets are limited to a certain length (just
140 Chinese characters or so),
so that
an existing tweet
may
not express a viewpoint clearly.
All these facts determine that
existing multi-document
summarization methods may not
be
applied to microblog data directly.
We propose a framework to produce comprehensive sum-
marization of news event complemented by micropoints given
a basic summary extracted from traditional
news articles.
A
micropoint is a micro-viewpoint composed of a few sentences
from several
relevant
microblog tweets.
We employ microp-
oints rather
than existing microblog tweets because a tweet
is
too short
and not
so informative,
and if
we gather
the
most
important
information that
is related to each other
in
a micropoint,
the complemented summary may show better
coherence as a whole as well
as informativeness than that
is
complemented by tweets.
The
contributions
of
our
work can be
summarized as
follows:
•
We filter microblog tweets and get
candidate tweets
according to a combination of the degree of relevance
and complementarity.
•
We propose the
concept
-
micropoint,
extract
mi-
cropoints
from microblog through tweet
clustering,
scattering and micropoint rebuilding, and employ them
to complement
basic summary for
the consideration
of coherence and informativeness of the summary as a
whole. We scatter a tweet into snippets, each of which
is
composed of
several
consecutive sentences
with
strong relations,
in order to retain coherence relations
between consecutive sentences in the original
tweet.
And we select several most representative snippets to
compose a micropoint that contains most information
of this cluster.
•
We select micropoints based on three important crite-
ria,
which guarantee high informativeness,
popularity
and conciseness.
II.
R
ELATED
W
ORK
A.
Multi-document Summarization
Multi-document summarization conveys the main and most
important information of a set of documents. Generally speak-
ing, multi-document summarization can be either extractive or
abstractive.
Most
researches so far
focus on the former
one
which extracts the existing text
segments that
are considered
as the most important information to form the final summary.
Both of these two types can be either supervised or unsuper-
vised.
Researchers have tried on many methods.
Mckeown et al.
[1],
Barzilay et
al.
[2] and Radev et
al.
[3] base their works
on clustering.
Carbonell
and Goldstein [4]
use the Maximal
978-1-4799-8442-8/15/$31.00 
©
2015 IEEE
ICDE Workshops 2015
190
Marginal
Relevance (MMR)
to select
relevant
but
concise
information.
Graph-based ranking methods,
such as LexRank
[5],
TextRank [6],
ClusterHITS [7],
use algorithms similar to
PageRank and HITS to compute sentence importance.
He et
al.
try on data reconstruction [8].
Li
et
al.
employ integer
linear
programming [9].
Gross
et
al.
[10]
think that
word
associations can reflect the key ideas of news, and news reports
are supposed to contain some novel
word associations.
It
is
worth mentioning that
some researchers have turned to the
state of art method - deep learning [11].
B.
Microblog Mining
With the booming of
online social
networks,
some re-
searchers have turned to data from these social networks. Lin et
al.
[12] generate event
storylines from microblogs.
Takamura
et
al.[13] summarize tweets in a certain period.
Sakaki
et
al.
[14] and Hua et al. [15] focus on event detection. Bermingham
and Smeaton [16]
and Tang et
al.
[17]
conduct
sentiment
classification on microblogs.
Zhao et
al.
[18]
detect
users
purchase intents from their microblogs and make product rec-
ommendation based on users demographic information.
Since
there is a lot of user demographic information on microblogs,
many researches extract user profiles [19] or employ microblog
user
information to personalize all
kinds of
text
processing
tasks,
such as searching result
[20],
news browsing [21],
and
so on.
Works on summarization are also motivated by microblog.
Inouye and Kalita [22] make comparison among eight popular
summarizers conducted on microblog tweets,
including ran-
dom, most recent, MEAD, LexRank, TextRank, cluster, Hybrid
TF-IDF and SumBasic.
They find that
all
these traditional
methods perform not very well on microblog dataset.
Some summarization methods
specialized on microblog
dataset
have been proposed.
Zhao et
al.
[23] employ PageR-
ank method for keyword ranking and a probabilistic scoring
function that
considers both relevance and interestingness of
keyphrases for keyphrase ranking. Phrase Reinforcement (PR)
algorithm proposed by Sharifi et
al.
[24]
builds
a
graph
representing the common sequences of words.
Some methods
also consider
event
time-line.
SHMMHMM [25]
segments
the event
time-line using Hidden Markov Model,
depending
on both the
burstiness
of
the
tweet-stream and the
word
distribution used in tweets. GESM [12] employs a model based
on graph, representing the similarities and chronological order
between sentences. Duan et al. focus on both the text content of
microblog posts and user’s social influence.
Chang et al.
[26]
employ a supervised learning method,
the features of which
include text
signals and user
influence signals.
Zhang et
al.
[27] and Nichols et al.
[28] focus on specific domain.
Previous researches on summarization only focus on either
traditional
news articles or microblog data,
and there is little
work about comprehensive summarization involving both news
websites and online social networks. ccTAM [29] matches each
sentence with a tweet
through the complementary measure
based on ccLDA,
and gradually selects sentence-tweet
pairs
to the final
summary.
However,
it
focuses more on the effec-
tiveness of topics and aspects for complementarity,
while we
focus more on the coherence and informativeness at the same
time.
III.
T
HE
F
RAMEWORK
O
VERVIEW
As mentioned earlier,
most researches on microblog focus
on its properties as a social
network,
while we focus on its
contribution as a news media,
employing it
as complement
to traditional summarization extracted from news articles. Our
method can be scheduled with the following 3 steps.
•
Given a
basic
summary extracted from traditional
news articles,
we first
filter the raw microblog tweet
set and get the candidate tweets based on a combina-
tion of the degree of relevance and complementarity.
•
Then we extract micropoints from microblog through
tweet
clustering,
scattering and micropoint
rebuild-
ing.
We scatter a tweet
into snippets,
each of which
is
composed of
several
consecutive sentences
with
strong relations, and select several most representative
snippets to compose a micropoint
that
contains most
information of
this cluster
but
is of
length no more
than a threshold.
•
Finally,
we select
micropoints based on three impor-
tant
criteria,
which guarantee high informativeness,
popularity and conciseness.
We will explain each step as follows, and use
mp
to denote
a micropoint and
t
to denote a tweet.
IV.
M
ICROBLOG
T
WEET
F
ILTERING
There are a large number
of
tweets about
a topic (news
event),
but
also a high proportion of
noise tweets thereinto,
for
instance,
ad tweets.
Besides,
we want
tweets containing
information distinct
from that
the basic summary contains.
Hence we filter
the raw microblog tweet
set
first
and get
most
relevant
tweets from a large dataset.
We consider about
2 criteria - the degree of relevance and complementarity.
A.
Relevance
As we know, topical keywords can represent the news event
well.
It
is worth mentioning that
viewpoints from microblog
are of diverse versions,
and we try to retain the diversity of
microblog information. We try to find the highly-differentiated
characteristic keywords of
different
versions using CDW,
a
graph-based ranking method we have proposed in previous
work [30].
Then we compute the cosine similarity between
the news event
keywords and a tweet
to assess the relevance
degree of a tweet to a news event
I
rel
.
B.
Complementarity
It
is intuitively acceptable that
if a tweet
is a good com-
plement
to a basic summary,
they should be similar.
At
the
same time, the information in the tweet should be distinct from
that
in the basic summary,
otherwise the final
complemented
summary may have too much redundant
information and not
so informative.
Thus we employ Gao et
al.’s measure [29] to
assess the degree of
complementarity of
a tweet
to a basic
summary
I
comp
.
We want to get a balance between the above 2 criteria,
so
we employ the harmonic mean of these 2 criteria according to
191
Equation 1,
just like
F
1
score in Information Retrieval.
α
is a
coefficient that is positive,
and we set it to 1.
F
α
(
I
rel
, I
comp
) =
(
α
2
+ 1
)
I
rel
I
comp
α
2
I
rel
+
I
comp
(1)
V.
M
ICROPOINT
E
XTRACTION
In this section,
we extract micropoints from the candidate
tweet
set
we get
in Section IV.
A micropoint
is
a micro-
viewpoint composed of a few sentences from several relevant
microblog tweets.
We first
divide microblog viewpoints into
different
topical
groups
through tweet
clustering,
and then
get
micropoints
by scattering tweets
and rebuilding them
into micropoints.
Fig.
1 shows the framework of micropoint
extraction. To be specific, there are 3 steps: topic mapping and
tweet clustering,
tweet scattering and micropoint rebuilding.
Fig.
1: The Framework of Micropoint Extraction
A.
Topic Mapping and Tweet Clustering
We map each tweet to different topics using LDA method,
and get the topic-tweet distribution:
p
(
topic
|
t
)
.
Based on
p
(
topic
|
t
)
we get
above,
we build a vector (as
shown in Equation 2) for each tweet,
and cluster them using
a density-based algorithm -
DBSCAN.
As
thus,
we divide
microblog viewpoints into different topical groups.
v
(
t
) = (
p
(
topic
1
|
t
)
, p
(
topic
2
|
t
)
, ..., p
(
topic
n
|
t
))
(2)
B.
Tweet Scattering
For each cluster, we scatter each tweet into several snippets,
each of which may be composed of one or more consecutive
sentences. As we know, there are coherence relations between
consecutive sentences in the original
tweet,
and they may be
of
significance to coherence as
well
as
informativeness
of
the final
summary.
So we should not
simply scatter a tweet
into single sentences,
instead,
we take the original
relations
between sentences into consideration.
We
adapt
the
Snippet
Growth Model
proposed in our
previous work [31] to scatter tweets into text snippets. Starting
from a head-sentence which describes the news event
very
well,
a
snippet
grows
up by adding relevant
neighboring
sentences into itself,
while the relevancy is decided by text
similarity,
distance function and influence function.
Since a
tweet is too short
to select
a head-sentence,
we simply select
the first sentence that hasn’t been processed in a tweet as the
head-sentence,
and expand the snippet
backwards.
Besides,
influence function is
left
out
because it
is
used to assess
the distance between the event-relevant head-sentence and the
sentence to be assessed, and then the ability of the sentence to
describe the news event,
but
we don’t
select
a event-relevant
head-sentence,
and we leave the relevance assessment
in the
fourth step.
Relation score between current snippet and its subsequent
sentence is computed according to Equation 3.
S
stands for a
snippet,
and
s
is a sentence.
R
=
∑
s
i
∈S
sim
(
s
i
, s
)
dis
(
s
i
, s
)
|
S
|
(3)
Text similarity is computed by Equation 4.
sim
(
s
i
, s
j
) =
|
s
i
∩
s
j
|
(4)
In distance function (see Equation 5),
d
ij
is the number of
sentences between
s
i
and
s
j
and
θ
is a coefficient.
dis
(
s
i
, s
j
) = log
θ
d
ij
(5)
C.
Micropoint Rebuilding
For each cluster, we select several most representative snip-
pets to compose a micropoint
that
contains most
information
of this cluster but is of length no more than a threshold.
We consider a sentence to be representative if it is similar
to all
other
sentences in the same cluster.
So we compute
the average similarity of a sentence with all
other sentences
in each cluster,
and employ it
as the reference of
sentence
ranking. Since a sentence from a tweet is too short to compare
with others,
we
first
represent
a
sentence
with a
bag of
words through word segmentation and stop word removing,
and then get
a word vector for each word usingword2vec of
gensim [32], a text processing tool. After that, we compute the
cosine similarity between the two sets of words from the two
sentences (denoted as
sim
cos
).
If the difference between word probability distribution of
current
selected sentences (denoted as
sents
) and that
of all
the sentences in the cluster
C
is lower than a threshold,
we
think the current
subset
of sentences retain most
information
of this cluster and represent
the whole cluster quite well.
We
assess
the difference between them using Kullback-Leibler
Divergence (as shown in Equation 6),
which is an important
concept in Information Theory.
KL
(
sents, C
) =
∑
w
p
(
w
|
sents
)
log
p
(
w
|
sents
)
p
(
w
|
C
)
(6)
In order to reduce redundance, when selecting sentences in
each cluster, we compute the cosine similarity between current
sentence and the selected sentences.
If they have too much in
common,
we skip to the next one.
The overall processing is formalized as Algorithm 1.
192
Algorithm 1 Micropoint Rebuilding
1:
procedure (C)
2:
for s
i
∈ C do
3:
for s
j
∈ C and i ̸= j
do
4:
score(s
i
) ← score(s
i
) + sim
cos
(s
i
, s
j
)
5:
end for
6:
end for
7:
sort(C, score)
◃ sort all the sentences
in C according to array score
8:
for s ∈ C do
9:
if sim
cos
(sents, s) < thres
sim
then
10:
sents ← sents
∪
s
11:
end if
12:
if KL(sents, C)
< thres
KL
or Len(sents)
>
thres
len
then
13:
return
14:
end if
◃ Len denotes the length of a
text
15:
end for
16:
end procedure
VI.
M
ICROPOINT
S
ELECTION
We get
a set
of micropoints in the previous section,
and
we will explain how to select micropoints as the complement
to the basic summary from it in this section.
We first propose
3 important criteria, then transform the problem into an elastic
0-1 Knapsack Problem,
and provide a solution.
We select micropoints based on the following 3 criteria.
A.
Informativeness
It is important that
the complement to the basic summary
should contain distinct information from that is in the summa-
ry.
We compute the Information Gain between the summary
before and after added into a micropoint according to Equation
7,
where
S
is the current summary,
S
+
is the summary after
adding
mp
into
S
,
and
w
denotes a word.
Inf oGain
(
mp
) =
E
(
S, mp
)
−
E
(
S
)
=
−
∑
w∈S+
p
S+
(
w
) log
p
S+
(
w
) +
∑
w∈S
p
S
(
w
) log
p
S
(
w
)
(7)
B.
Popularity
This is a distinct
criterion from those used in traditional
summarization in consideration of
the unique properties
of
microblog. As we know, there are many subjective viewpoints
in microblog,
and some of
them may be unpersuasive or
even go to extremes.
Thus we propose this criterion to select
micropoints that are accepted or spread by a part of users. We
assess popularity of a micropoint
based on users’ retweeting.
More often the initial
microblog tweets which the sentences
in the micropoint
are from are retweeted,
more popular
the
micropoint
is considered to be.
To be specific,
the popularity
of a micropoint
is defined by Equation 8,
where
s
denotes a
sentence, and
retweet
(
t
)
indicates how many times the tweet
t
is retweeted. Generally speaking, Equation 8 gives the average
frequency of being retweeted of sentences in a micropoint.
P op
(
mp
) = log
∑
s∈mp
∑
t
e
(
s, t
)
retweet
(
t
) + 1
∑
s∈mp
∑
t
e
(
s, t
)
e
(
s, t
) =
{
1
, if s
∈
t
0
, if s /
∈
t
(8)
C.
Conciseness
It
is
reasonable that
the complementary part
from mi-
croblog to a basic summary should be concise and not
usurp
the basic summary. It is important to restrain the length of the
complementary part. We assess conciseness based on Equation
9.
Con
=
Len
(
SupplementedP art
)
Len
(
BasicSummary
)
(9)
D.
Problem Formalization
Micropoint selection can be transformed into an elastic 0-
1 Knapsack Problem.
Given a length constraint
of
Con
(i.e.
conciseness) and a set of micropoints
MP
, each of which has
a value score (determined by informativeness and popularity
according to Equation 10) and a length of 1,
we try to find a
solution that maximizes the total value of selected micropoints.
In addition, if a solution has a length constraint of
Con
∗
(larger
than
Con
) and the increasement of total value is no less than
c
,
we will
adapt
length constraint
to
Con
∗
and choose the
solution with the length constraint of
Con
∗
.
Algorithm to solve this problem is given in Algorithm 2,
where Function
DP
(
int, doubt
[])
is to solve the 0-1 Knapsack
Problem with the length constraint
of
Con
∗
using Dynamic
Programming, and return solution and the maximum total value
of selected micropoints.
Algorithm 2 Elastic 0-1 Knapsack Problem
1:
procedure (MP, Con, Con∗, c)
2:
for mp ∈ MP do
3:
Compute InfoGain(mp)
and P op(mp)
4:
Compute value(mp)
according to Equation
10
5:
end for
6:
DP(Con∗, value[])
7:
if
V ∗−V
V
≥ c
then
8:
Return Solution 1 and V ∗
9:
else
10:
Return Solution 2 and V
11:
end if
◃ Solution1 and V ∗: solution under the
length constraint of Con∗. Solution2 and V :
solution under the constraint of Con. They
are contained in Solution1 and Con∗
12:
end procedure
In Equation 10,
we put more weight on Information Gain
because informativeness
assessment
is
more objective than
popularity.
Besides,
we employ piecewise function,
where
γ
is
a threshold.
If
popularity is
less
than threshold,
the
weight
difference between informativeness and popularity is
smaller than in another case.
That
is to say,
if a micropoint
193
TABLE I: News Event Datasets
News Subject
Domain
Time
#Articles
#Tweets
iPhone5s Release
Technology
Sep,
2013
1538
2839
Wendi Deng’s Divorce
Entertainment
Nov,
2013
1250
1428
Nelson Mandela’s Death
Politics
Dec,
2013
1397
1161
Zhouzi Fang’s Charge
against Yongyuan Cui
Society
Jan,
2014
1240
1414
Malaysia Flight
Disappearances
Disaster
Mar,
2014
1784
2235
is not
widely accepted or
spread,
it
should not
be assigned
a large weight
even though it
is informative.
Meanwhile,
if
a micropoint
is frequently retweeted,
we focus more on its
Information Gain.
value
(
mp
) =
{
α
1
Inf oGain
(
mp
) +
β
1
P op
(
mp
)
, P op
(
mp
)
≥
γ
α
2
Inf oGain
(
mp
) +
β
2
P op
(
mp
)
, otherwise
α
1
−
β
1
> α
2
−
β
2
>
0
(10)
VII.
E
XPERIMENTS
A.
Datasets
As
mentioned earlier,
there
is
little
work about
com-
prehensive summarization involving both news
articles
and
microblog, and few existing standard datasets we can employ.
Thus we collect
news articles and microblog tweets about
5
news events,
as shown in Table 1,
where #Articles denotes
the total
number of news articles and #Tweets is the number
of tweets in each dataset.
News articles are collected from 5
influential
websites in China -
Sina,
Sohu,
Tecent,
Netease,
and PhoenixNet,
and microblog tweets are collected from the
2 most influential microblog sites in China - Sina Weibo and
Tecent Weibo.
B.
System Setup and Baselines
In the experiments, we tentatively set these coefficients and
tune them later.
We set
θ
= 10 in Equation 5,
γ
= 100,
α
1
=
0.65,
β
1
= 0.35,
α
2
= 0.60, and
β
2
= 0.40 in Equation 10. We
set
the constraint
of length
Con
= 0.5,
Con
∗
= 0.6 and
c
=
0.05 in Section VI.
We extract
the basic summary from news articles using
the classic methods
mentioned in the Section II-A (named
as
no-MB).
And each volunteer
is
required to produce
a
reference summary based on relevant
texts
from Wikipedia
after scanning the recommended tweets and hot
tweets about
a news event
given by the microblog sites.
We use all
these
manual
summaries by different
volunteers in evaluation,
and
we believe the results to be not so subjective.
We compare our
proposed method with 2 methods spe-
cialized on microblog data - PR [24] and SHMMHMM [25],
as well as ccTAM [29] which involves information from both
news articles and microblog tweets. Besides, we compare with
the components of our method.
•
non-mp:
we get
this
method by skipping the third
step of
micropoint
selection and selecting the most
representative tweets in each cluster
using the same
method as that
is used to extract
micropoints in the
Section V. We set this method in order to evaluate the
micropoints we extracted.
•
non-Pop: our method without involvement of popular-
ity.
•
non-Info:
our
method without
involvement
of
infor-
mativeness.
•
half-half: a special case of our method with coefficient
alpha
1
, alpha
2
, beta
1
and
beta
2
in Equation 10 set to
0.5.
C.
Evaluation Metrics
1) ROUGE: One of the most important and popular evalua-
tions is on the coverage of important information in a summary.
It
is of
significance to the overall
quality of
the summary.
Most recent researches focus on the
ROUGE
evaluation [33]
[34].
We focus
on the information coverage of
the overall
summaries rather than the only microblog summaries because
the microblog summary is based on the basic summary and
the latter ones by different
methods are extracted differently.
And we compare the overall summaries by different methods
with no-MB, which can reflect the complementarity to the tra-
ditional summary to some extent.
To be specific,
we compute
ROUGE
−
1
and
ROUGE
−
2
for
the overall
summaries
by each method in our
experiments
according to Equation
11,
where
Count
match
(
gram
n
)
is the maximum number of
n-grams co-occurring in a candidate summary and a set
of
reference summaries.
ROUGE − N =
∑
summary∈ReferenceSummaries
∑
gram
n
∈summary
Count
match
(gram
n
)
∑
summary∈ReferenceSummaries
∑
gram
n
∈summary
Count(gram
n
)
(11)
2) nDCG:
We gather
related information from different
tweets into a micropoint,
so we expect
our
method a good
performance on content
coherence.
We consider a microblog
summary to be coherent
if its order of sentences corresponds
to manual
ordering.
So we disorder sentences in a microblog
summary and request
evaluators to reorder them.
We employ
nDCG
index (given in Equation 12)
used in Information
Retrieval
to evaluate
the
consistence
of
ordering between
reordered summary and candidate summary.
IDCG
is
DCG
value of the corresponding summary reordered by evaluators.
score
i
is the score of the
ith
sentence,
and a higher score is
assigned to a sentence with an earlier
position in reordered
summary.
nDCG
(
summary
) =
DCG
IDCG
(
I
)
DCG
=
score
1
+
∑
1≤i≤|(Reordered)summary|
score
i
log
i
(12)
3) Manual
Evaluation on Content
Diversity:
Apart
from
coverage and content
coherence,
we also conduct
a manual
evaluation on content
diversity,
since microblog is
a news
media
characterized by diversity of
viewpoints.
We
invite
evaluators who have a general
idea about
the news events to
194
grade the summaries from 1 to 4, and 4 denotes the best content
diversity.
We employ the average score of different evaluators
so as to reduce the grading difference between them.
D.
Overall Performance Comparison
Table
II
shows
the
ROUGE
scores
of
summaries
by
different
methods,
and Table
III
shows
nDCG
values
of
microblog summaries by different methods. We leave out non-
Pop, non-Info and half-half in
nDCG
evaluation because they
all employ micropoint as supplement.
Generally speaking,
our proposed method performs well.
To be specific:
•
The methods
involving both news
articles
and mi-
croblog
(i.e.
ccTAM,
half-half,
and
our
method)
outperform those specialized on microblog data on
ROUGE
score.
The result
indicates to some extent
that information from traditional news articles plays an
important role in summarization. Also, these methods
outperform no-MB,
so we believe there is valuable
information from microblog.
•
Both Informativeness and popularity contribute to the
ROUGE
values,
but
Informativeness
seems
more
important than popularity since non-Pop outperforms
non-Info.
Besides,
ccTAM comes close to non-Pop
because they both leave out popularity.
•
Our
method outperforms
those methods
employing
existing tweets to complement the basic summary on
ROUGE
,
and this
may because that
a micropoint
retains the most
important
in some aspect
of
news
event.
•
As we expected,
microblog summaries composed of
micropoints show good performance in coherence as
shown in Table III.
Performance of non-mp and our
proposed method shows that
micropoint
does help to
improve the content
coherence of
summary.
ccTAM
performs
not
very well
in
nDCG
,
and this
may
because that ccTAM matches each sentence from news
articles with another sentence from microblog tweets,
and causes a loss of summary coherence as a whole.
Fig.
2 shows the manual
grades on content
diversity of
summaries by different
methods.
We can find that
methods
involving both news articles and microblog tweets show better
performance than no-MB on content diversity. It confirms that
microblog information is good complement
to news articles,
and it is more diverse.
We also display basic summary and different
microblog
complementary summaries of “Fang’s Charge against Cui” in
Table IV. We can find that the tweet summary seems disperse
in content. On the contrast, summary composed of micropoints
is more coherent in content.
We can conclude to some extent
that our proposition of micropoint is meaningful.
VIII.
C
ONCLUSION
In this paper,
we propose a framework to produce com-
prehensive summarization of
news
event
complemented by
micropoints extracted from microblog.
Experiments conduct-
ed on real
data demonstrate a good coverage and content
TABLE II:
ROUGE
Scores of Overall Summaries by Differ-
ent Methods
iPhone5s Release
Wendi Deng’s Divorce
ROUGE-1
ROUGE-2
ROUGE-1
ROUGE-2
no-MB
0.330
0.040
0.305
0.032
PR
0.343
0.043
0.307
0.035
SUMMHMM
0.352
0.050
0.343
0.045
ccTAM
0.354
0.051
0.346
0.050
non-mp
0.365
0.072
0.349
0.052
non-Pop
0.357
0.053
0.347
0.049
non-Info
0.336
0.048
0.345
0.046
half-half
0.365
0.068
0.351
0.053
Our Method
0.371
0.074
0.355
0.059
Nelson Mandela’s Death
Fang’s Charge against Cui
ROUGE-1
ROUGE-2
ROUGE-1
ROUGE-2
no-MB
0.320
0.029
0.306
0.030
PR
0.321
0.032
0.303
0.031
SUMMHMM
0.325
0.041
0.334
0.050
ccTAM
0.337
0.051
0.334
0.052
non-mp
0.335
0.057
0.338
0.057
non-Pop
0.338
0.053
0.336
0.045
non-Info
0.337
0.046
0.331
0.042
half-half
0.340
0.049
0.343
0.056
Our Method
0.342
0.061
0.347
0.065
MF Disappearances
ROUGE-1
ROUGE-2
no-MB
0.319
0.040
PR
0.309
0.037
SUMMHMM
0.338
0.043
ccTAM
0.351
0.055
non-mp
0.356
0.062
non-Pop
0.353
0.054
non-Info
0.343
0.049
half-half
0.358
0.062
Our Method
0.363
0.067
TABLE III:
nDCG
of
Microblog Summaries
by Different
Methods
iPhone5s
Release
Wendi
Deng’s
Divorce
Nelson
Man-
dela’s
Death
Fang’s
Charge
against
Cui
MF Dis-
appear-
ances
PR
0.429
0.337
0.336
0.367
0.432
SUMMHMM
0.298
0.254
0.202
0.231
0.302
ccTAM
0.304
0.234
0.190
0.238
0.309
non-mp
0.292
0.230
0.197
0.304
0.295
Our Method
0.450
0.348
0.356
0.424
0.474
coherence of
the summary produced by our
approach.
We
can say that
the proposition of micropoint
is meaningful
and
may play an important
role in the future work of microblog
summarization.
As
our
future work,
we will
try on more
robust evaluations. Besides, we will make a step to incorporate
personal
information from online social
networks to produce
more personalized summarization for users.
A
CKNOWLEDGMENT
Yan Zhang is the corresponding author of this work.
And
we
sincerely thank all
the
anonymous
reviewers
for
their
valuable comments,
which have helped a lot
to improve this
paper.
This
work is
supported by NSFC with Grant
No.
61370054 and 973 Program with Grant No.
2014CB340405.
R
EFERENCES
[1]
K. McKeown and D. R. Radev, “Generating summaries of multiple news
articles,” in Proceedings of
the 18th annual
international
ACM SIGIR
conference
on Research and development
in information retrieval.
ACM,
1995,
pp.
74–82.
195
TABLE IV: Basic and Microblog Summaries of “Fang’s Charge against Cui”
Basic Summary Extracted from News Articles
In a taste test
activity in Sep,
2013,
Fang commented,
“Maybe it
(the taste test) is of no significance to scientific
research, but it is helpful in promoting the scientific knowledge of GM food. I think it is meaningful to make efforts
to enable us to eat GM food in our daily lives.” Cui retorted upon Fang,
“You choose to eat,
but I choose not.
You
say you understand science,
but I doubt whether what you understand is scientific or not.” ...
...
Haidian Court received Fang’s charge against Cui on reputation violation on Jan 21st,
2014.
Cui said, “... Fang has the right to question someone, while we have the right to question Fang. ...” Fang’s wife was
said to have misconduct in her master’s thesis before.
Fang responded,
“I don’t think it is worth questioning.
Even
if it is true,
I think I should avoid your suspicion.”
013
年
9
月，在一场转基因玉米试吃活动中，方舟子说了这样一句话：“品尝转基因玉米虽无科学研究价值，但有科普价
值，应当创造条件让国人可以天天吃转基因食品。”崔永元随即转发微博驳斥“你可以选择吃，我可以选择不吃，你可以
说你懂‘科学’，我有理由有权利质疑你懂的‘科学’到底科学不科学。”。……
……“经审查，海淀法院于
2014
年
1
月
21
日决定依法受理原告方是民（笔名方舟子）诉被告崔永元名誉权纠纷一案。”
2014
年
2
月，崔永元称：“……你可以打别人的假，别人也可以打你的假……”此前方舟子的妻子刘菊花被指硕士论文造
假，对此方舟子表示：“我不认为老婆的假值得打，即使真的是假的，我也有回避的权利。”
Tweet Summary
Micropoint Summary
#Zhouzi Fang’s Charge against Yongyuan Cui# I wonder
whether Cui’s reaction to Fang was “poisoned” by GM
food.
It
is Fang’s wrongdoing to put
up personal
attack
to Cui,
but
I’m strongly against
Cui’s counterattack in
the same way.
“There is no need of Fang’s charge against
Cui.”:
Both
of Fang and Cui are people with strong personalities, and
I appreciate both of them.
Cui questioned Fang with his professionalism as a media
personality.
Why must
he be criticized? According to
Changjiang Daily (Dec 31st,
2013).
GM research is not
a controversial
academic direction,
but GM product is a controversial social issue.
GM food is closely bound up with our daily lives,
and
I think it
is of meaning to promote Cui’s video.
I think
Fang’s and Nan Sima’s actions are for the money.
方舟子控告崔永元
#
原来不知转基因是否厉害，现在看来
真的让有些人基因突变的非常厉害，见谁咬谁，这是啥本
能？严厉谴责崔永元，方舟子可以问候你父亲，你也不应
该问候他母亲。
【方舟子没必要起诉崔永元】方舟子和崔永元都很有个
性，我都很欣赏。
2013
年
12
月
31
日，《长江日报》称“崔永元以其媒体职业
训练出来的素养，开展调查有何可指责？”
“转基因”在学界并不是一个有争议的话题，但确实是一
个有争议的社会话题。
崔永元自费采访的视频，我认为和自身生活相关，所以
应科普一下……个人看法，方舟子和司马南拼命推广转基
因，应该是为了钱……
We also conducted a survey on GM product
in USA,
and get
a completely different
conclusion from Cui’s,
according to “The Truth of GMP in USA” published in
“Biz & Economic” (Dec 24th,
2013).
Cui
questioned
Fang with his professionalism as a media personality.
Why must
he be criticized? According to Changjiang
Daily (Dec 31st,
2013).
“Cui is unreasonable”: Nan Sima said, Cui was humorous
but is unreasonable now. Hua Yu, who is a famous writer
in China,
ridiculed that
some magazine conducted an
“unreal” survey on “GMP in USA”,
but
was counterat-
tacked by Fang, “Yu should not only catch up in biology,
but also Chinese.”
013
年
12
月
24
日，《财经天下周刊》杂志发表《美国转基
因真相》一文，称“跟随崔永元的脚步，我们来到了大洋
彼岸，并得出了完全不一样的结论。”
2013
年
12
月
31
日，
《长江日报》称“崔永元以其媒体职业训练出来的素养，
开展调查有何可指责？”
【崔永元病得厉害】司马南说，过去小崔很幽默好玩，现
在小崔很让人同情
,
他病得厉害。著名作家余华因一时“口
舌之快”，调侃某媒体子虚乌有的“随崔永元赴美调查
转基因”，不幸卷入漩涡，引来方舟子一番“唇枪舌剑”
反击，称“小说家余华不仅该恶补生物学，也该恶补语
文”。
Fig. 2: Manual Grades on Content Diversity of Summaries by
Different Methods
[2]
R.
Barzilay,
K.
R.
McKeown,
and M.
Elhadad,
“Information fusion in
the context
of multi-document
summarization,” in Proceedings of
the
37th annual
meeting of
the Association for Computational
Linguistics
on Computational Linguistics.
Association for Computational Linguis-
tics,
1999,
pp.
550–557.
[3]
D.
R.
Radev,
H.
Jing,
and M.
Budzikowska,
“Centroid-based sum-
marization of
multiple documents:
sentence extraction,
utility-based
evaluation, and user studies,” in Proceedings of the 2000 NAACL-ANLP
Workshop on Automatic Summarization.
Association for Computational
Linguistics,
2000,
pp.
21–30.
[4]
J.
Carbonell
and J.
Goldstein,
“The
use
of
mmr,
diversity-based
reranking for
reordering documents
and producing summaries,”
in
Proceedings of
the 21st
annual
international
ACM SIGIR conference
on Research and development
in information retrieval.
ACM,
1998,
pp.
335–336.
[5]
G.
Erkan and D.
R.
Radev,
“Lexpagerank: Prestige in multi-document
text summarization.” in EMNLP,
vol.
4,
2004,
pp.
365–371.
[6]
R. Mihalcea and P. Tarau, “A language independent algorithm for single
and multiple document summarization,” 2005.
[7]
X.
Wan and J.
Yang,
“Multi-document
summarization using cluster-
based link analysis,” in Proceedings of
the 31st
annual
international
ACM SIGIR conference on Research and development
in information
retrieval.
ACM,
2008,
pp.
299–306.
196
[8]
Z.
He,
C.
Chen,
J.
Bu,
C.
Wang,
L.
Zhang,
D.
Cai,
and X.
He,
“Document
summarization based on data reconstruction.” in AAAI,
2012.
[9]
C.
Li,
X.
Qian,
and Y.
Liu,
“Using supervised bigram-based ilp for
extractive summarization.” in ACL (1).
Citeseer, 2013, pp. 1004–1013.
[10]
O.
Gross,
A.
Doucet,
and H.
Toivonen,
“Document
summarization
based on word associations,” in Proceedings of
the 37th international
ACM SIGIR conference on Research and Development
in Information
Retrieval.
ACM,
2014.
[11]
Y.
Liu,
S.-h.
Zhong,
and W.
Li,
“Query-oriented multi-document
summarization via unsupervised deep learning.” in AAAI,
2012.
[12]
C.
Lin,
C.
Lin,
J.
Li,
D.
Wang,
Y.
Chen,
and T.
Li,
“Generating
event
storylines from microblogs,” in Proceedings of
the 21st
ACM
international
conference on Information and knowledge management.
ACM,
2012,
pp.
175–184.
[13]
H. Takamura, H. Yokono, and M. Okumura, “Summarizing a document
stream,” in Advances in Information Retrieval.
Springer,
2011,
pp.
177–188.
[14]
T. Sakaki, M. Okazaki, and Y. Matsuo, “Earthquake shakes twitter users:
real-time event detection by social sensors,” in Proceedings of the 19th
international
conference on World wide web.
ACM,
2010,
pp.
851–
860.
[15]
T. Hua, F. Chen, L. Zhao, C.-T. Lu, and N. Ramakrishnan, “Sted: Semi-
supervised targeted event detection.”
[16]
A.
Bermingham and A.
F.
Smeaton,
“Classifying sentiment
in mi-
croblogs:
is brevity an advantage?” in Proceedings of
the 19th ACM
international
conference on Information and knowledge management.
ACM,
2010,
pp.
1833–1836.
[17]
D.
Tang,
F.
Wei,
N.
Yang,
M.
Zhou,
T.
Liu,
and B.
Qin,
“Learning
sentiment-specific word embedding for twitter sentiment classification,”
in Proceedings
of
the 52nd Annual
Meeting of
the Association for
Computational Linguistics,
2014,
pp.
1555–1565.
[18]
X.
W.
Zhao,
Y.
Guo,
Y.
He,
H.
Jiang,
Y.
Wu,
and X.
Li,
“We
know what you want to buy: a demographic-based system for product
recommendation on microblogs,” in Proceedings
of
the 20th ACM
SIGKDD international
conference on Knowledge discovery and data
mining.
ACM,
2014,
pp.
1935–1944.
[19]
J. Li, A. Ritter, and E. Hovy, “Weakly supervised user profile extraction
from twitter.”
ACL,
2014.
[20]
A. Younus, C. O’Riordan, and G. Pasi, “A language modeling approach
to personalized search based on users’ microblog behavior,” in Advances
in Information Retrieval.
Springer,
2014,
pp.
727–732.
[21]
C.
Liu and A.
K.
Tung,
“Twitter+:
Build personalized newspaper for
twitter,” in Data Engineering (ICDE),
2013 IEEE 29th International
Conference on.
IEEE,
2013,
pp.
1272–1275.
[22]
D.
Inouye and J.
K.
Kalita,
“Comparing twitter
summarization algo-
rithms for multiple post summaries,” in Privacy, security, risk and trust
(passat), 2011 ieee third international conference on and 2011 ieee third
international conference on social computing (socialcom).
IEEE, 2011,
pp.
298–306.
[23]
W.
X.
Zhao,
J.
Jiang,
J.
He,
Y.
Song,
P.
Achananuparp,
E.-P.
Lim,
and X.
Li,
“Topical keyphrase extraction from twitter,” in Proceedings
of
the 49th Annual
Meeting of
the Association for
Computational
Linguistics:
Human Language Technologies-Volume 1.
Association
for Computational Linguistics,
2011,
pp.
379–388.
[24]
B.
Sharifi,
M.-A.
Hutton,
and J.
Kalita,
“Summarizing microblogs
automatically,” in Human Language Technologies:
The 2010 Annual
Conference of
the North American Chapter
of
the Association for
Computational Linguistics.
Association for Computational Linguistics,
2010,
pp.
685–688.
[25]
D.
Chakrabarti and K.
Punera,
“Event summarization using tweets.” in
ICWSM,
2011.
[26]
Y.
Chang,
X.
Wang,
Q.
Mei,
and Y.
Liu,
“Towards
twitter
context
summarization with user influence models,” in Proceedings of the sixth
ACM international conference on Web search and data mining.
ACM,
2013,
pp.
527–536.
[27]
R. Zhang, W. Li, D. Gao, and Y. Ouyang, “Automatic twitter topic sum-
marization with speech acts,” Audio, Speech, and Language Processing,
IEEE Transactions on,
vol.
21,
no.
3,
pp.
649–658,
2013.
[28]
J.
Nichols,
J.
Mahmud,
and C.
Drews,
“Summarizing sporting events
using twitter,” in Proceedings of the 2012 ACM international conference
on Intelligent User Interfaces.
ACM,
2012,
pp.
189–198.
[29]
W.
Gao,
P.
Li,
and K.
Darwish,
“Joint
topic
modeling for
event
summarization across news and social media streams,” in Proceedings
of the 21st ACM international conference on Information and knowledge
management.
ACM,
2012,
pp.
1173–1182.
[30]
R.
Xiao,
L.
Kong,
Y.
Zhang,
and M.
Wang,
“Cdw:
a text
clustering
model for diverse versions discovery,” in Fuzzy Systems and Knowledge
Discovery (FSKD),
2011 Eighth International
Conference on,
vol.
2.
IEEE,
2011,
pp.
1113–1117.
[31]
Y. Wang, L. Zhao, and Y. Zhang, “Magiccube: choosing the best snippet
for each aspect of an entity,” in Proceedings of the 18th ACM conference
on Information and knowledge management.
ACM,
2009,
pp.
1705–
1708.
[32]
R.
ˇ
Reh
˚
u
ˇ
rek and P.
Sojka,
“Software Framework for Topic Modelling
with Large Corpora,” in Proceedings of
the LREC 2010 Workshop on
New Challenges for NLP Frameworks.
Valletta,
Malta:
ELRA,
May
2010,
pp.
45–50,
http://is.muni.cz/publication/884893/en.
[33]
J. Christensen, S. S. Mausam, and O. Etzioni, “Towards coherent multi-
document
summarization,” in Proceedings of
NAACL-HLT,
2013,
pp.
1163–1173.
[34]
C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,”
in Text
Summarization Branches
Out:
Proceedings
of
the
ACL-04
Workshop,
2004,
pp.
74–81.
197
