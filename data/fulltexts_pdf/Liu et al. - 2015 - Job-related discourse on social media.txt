Job-related discourse on social media
Tong Liu
Department of Computer
Science
Rochester Institute of
Technology
Rochester, NY
tl8313@rit.edu
Christopher M. Homan
Department of Computer
Science
Rochester Institute of
Technology
Rochester, NY
cmh@cs.rit.edu
Cecilia Ovesdotter Alm
Department of English
Rochester Institute of
Technology
Rochester, NY
coagla@rit.edu
Ann Marie White
Department of Psychiatry
University of Rochester
Medical Center
Rochester, NY
Megan C. Lytle-Flint
Department of Psychiatry
University of Rochester
Medical Center
Rochester, NY
Henry A. Kautz
Goergen Institute for Data
Science
University of Rochester
Rochester, NY
ABSTRACT
Working adults spend nearly one third of their daily time at
their jobs.
In this paper, we study job-related social media
discourse from a community of users.
We use both crowd-
sourcing and local
expertise to train a classifier to detect
job-related messages on Twitter.
Additionally,
we analyze
the linguistic differences in a job-related corpus of
tweets
between individual users vs.
commercial accounts.
The vol-
umes of
job-related tweets from individual
users indicate
that people use Twitter with distinct monthly,
daily,
and
hourly patterns.
We further show that the moods associ-
ated with jobs,
positive and negative,
have unique diurnal
rhythms.
Categories and Subject Descriptors
H.4 [Information Systems Applications]:
Collaborative
and social computing systems and tools; H.4 [Information
systems applications]:
Data mining—Collaborative filter-
ing
General Terms
Application; Measurement
Keywords
social
media;
job;
employment;
crowdsourcing;
sentiment
analysis; Twitter; behavior pattern; linguistic
1.
INTRODUCTION
In this paper,
we build a robust language-based classifier
that can automatically and accurately identify Twitter mes-
sages about job-related topics.
The classifier is trained by
a supervised learning pipeline that uses humans-in-the-loop
to boost model performance.
We discover and analyze tem-
poral patterns in the volume of job-related tweets and inves-
tigate how positive and negative affect in job-related tweets
vary over time.
The obvious importance of this research is that working-age
Americans spend on average 8.7 hours per day in job-related
activities [4],
which is more time than any other single ac-
tivity.
So any attempt to understand a working individual’s
experiences,
state of
mind,
or motivations must take into
account their life at work.
Social
media has become a rich
source of in situ data for research on social
issues and be-
haviors, yet to our knowledge, little of this work has focused
on how individuals talk there about their jobs.
Conversely,
a better understanding of
how people discuss
work informally through social
media can potentially shed
light on behavioral problems that impact work.
70% of US
workers are disengaged at work [13].
This hurts companies
and costs between 450 and 550 billion dollars each year in
lost productivity.
Disengaged workers are 87% more likely to
leave their jobs than their more satisfied counterparts [13].
Job dissatisfaction poses serious health risks and even leads
to suicide [15].
The deaths by suicide among working age
people (25-64 years old) costs more than $44 billion annually
[6].
The need for machine learning,
rather than simple heuris-
tics,
to discover work-related social
media posts is due to
the inherent ambiguity of language related to jobs.
For in-
stance,
a tweet like “@SOMEONE @SOMEONE shit man-
ager shit players shit everything” contains the work-related
word “manager,” yet the presence of “player” ultimately sug-
gests this tweet is about a sport team.
The tweet “@SOME-
ONE anytime for you boss lol ” might seem job-related, but
“boss” here could also simply mean “friend” in an informal
and familiar register.
Aggregated job-related information from Twitter
can be
valuable to a range of
stakeholders.
For instance,
public
health specialists, psychologists and psychiatrists could use
such first-hand reportage of
work experiences to monitor
job-related stress at a community level.
Employers might
arXiv:1511.04805v1 [cs.SI] 16 Nov 2015
use it
to improve how they manage their
business,
cost,
and entrepreneurial
energy.
It could also help employees
to maintain better online reputations for potential
job re-
cruiters.
We also recognize that there are ethical
consider-
ations involved for analyzing job-related distress of individ-
uals (e.g.,
supervisors monitoring particular employees’ job
satisfaction).
2.
BACKGROUND AND RELATED WORK
Social media accounts for about 20% of the time spent on-
line [8].
Online communication has a faceless nature that
can embolden people to reveal their cognitive state in a nat-
ural,
unselfconscious manner [16].
Mobile phone platforms
help social media to capture personal behavior in situ, when-
ever and wherever possible [10, 28].
These signals are often
temporal, and can reveal how phenomena change over time.
Thus,
aspects about
individuals or groups,
such as pref-
erences and perspectives,
affective states and experiences,
communicative patterns, and socialization behaviors can, to
some degree, be analyzed and computationally modeled con-
tinuously and unobtrusively [10].
Previous research shows that social
media can predict po-
litical
inclination [35],
or performance of
stock market [3],
which reflect the social and economic situations.
The spread
of infectious diseases, like flu, can also be predicted through
online social media [29].
Syndromic surveillance system for
multiple ailments also can be established with social
me-
dia data [22].
Smoking and drinking,
depression,
domestic
abuse,
and other behavioral
and public wellness problems
can also be analyzed [33, 10, 31].
In contrast to such prior studies,
we focus on a broad dis-
course and narrative theme that touches most adults.
Mea-
sures of
volume,
content,
affect of
job-related discourse on
social
media may help understand the behavioral
patterns
of
working people,
predict labor market changes,
monitor
and control satisfaction/dissatisfaction with respect to their
workplaces or colleagues, and help people strive for positive
change [9].
The language differences exposed in social
me-
dia have been observed and analyzed in relation to location
[7],
gender,
age,
regional
origin,
and political
orientation
[25].
With the help of social media, researchers have identi-
fied individual-level
diurnal
and seasonal
mood rhythms in
cross-culture comparisons [14].
We are interested to know
the affective rhythms hidden in job-related discourse on so-
cial media specifically.
Twitter has drawn much attention from researchers in vari-
ous disciplines in large part because of the volume and gran-
ularity of publicly available social data.
This micro-blogging
website,
which was launched in 2006,
has attracted more
than 500 million registered users by 2012,
with 340 million
tweets posted every day.
Twitter supports directional
con-
nections (followers and followees) in its social network, and
allows for geographic information about where a tweet was
posted if
a user enables location services.
In our context,
this nature of the data allows us to study job-related topics
in multidimensional ways.
LIWC
1
,
has proved useful
for extracting the psychological
1
Linguistic Inquiry and Word Count, http://www.liwc.net/
dimensions of
language [34]
and address many challenging
problems,
such as classifying depression and paranoia suf-
ferers [20],
monitoring emotion expression under stress in
instant messaging [23],
characterizing sentiment in tweets
[21,
19],
revealing cues about neurotic tendencies and psy-
chiatric disorders [27], and estimating the risk of suicide from
unstructured clinical records [24].
3.
RESEARCH QUESTIONS
Here, we pursue the following questions:
RQ 1:
Can we identify linguistic characteristics that differ-
entiate groups of
users who talked about job-related
topics?
RQ 2:
What are job-related topics usually about?
RQ 3-1:
How do posts of job-related messages change over
the course of a year?
RQ 3-2:
From Monday to Sunday in each week, what pat-
terns emerge?
For instance,
on which day do people
talk the most vs.
the least about job-related topics?
RQ 3-3:
When is the most active period in a day when
people tweet about jobs?
RQ 4:
How do tweeters’ affective tone change over time in
job-related tweets? Are there observable variations by
seasonal or diurnal factors?
4.
DATA AND METHODS
In this section,
we first review our method of
building an
iterative humans-in-the-loop supervised learning framework
to automatically detect the job-related messages from Twit-
ter [18].
Then we leverage the pipeline and labeled tweets
and developed a series of descriptive analysis.
Figure 1 sum-
marizes our workflow.
Using the DataSift
2
Firehose, we collected tweets from pub-
lic accounts with geographical
coordinates located in a 15-
counties region surrounding a mid-sized US city from July
2013 to June 2014.
This data set contains over 7 million
geo-tagged tweets (approximately 90% written in English)
from around 85,000 unique Twitter accounts.
We fix our
data to this particular locality because it is geographically
diverse, covering both urban and rural areas and providing
mixed and balanced demographics.
Also, due to the nature
of the subject matter,
it is helpful
to use knowledge about
the local job scene in the modeling and analysis.
Then, to preprocess the tweets, we remove punctuation and
special
characters,
and heuristically map informal
terms to
standard ones using the Internet Slang Dictionary
3
.
We also
remove special characters, like emoticons, before conducting
a crowdsourcing study.
4.1
Data filtering
Words
such as “job” have multiple meanings.
In order
to identify likely job-related tweets while excluding others
2
http://datasift.com/
3
http://www.noslang.com/dictionary
Figure 1:
Overview of our detection and analysis framework
(such as those discussing homework or other school-related
activities) we filtered the tweets using the inclusion and ex-
clusion terms shown in Table 1.
This yielded over 40,000
tweets having at least five tokens each.
These tweets were
labeled Job-Likely.
Include
job, jobless, manager, boss
my/your/his/her/their/at work
Exclude
school, class, homework, student, course
good/nice/great job
Table 1:
Filters used to extract the Job-Likely set.
4.2
Crowdsourced annotation
We randomly chose around 2,000 Job-Likely tweets and split
them equally into 50 subsets of 40 tweets each.
To measure
both inter- and intra-annotator agreement,
we additionally
randomly duplicated five tweets in each subset.
We then
constructed Amazon Mechanical
Turk (AMT)
4
Human In-
telligence Tasks (HITs) to collect reference annotations.
For
each tweet,
we asked workers, “Is this tweet about employ-
ment or job? ”.
The answer “Y” means “job-related” and “N”
means “not job-related”.
We assigned five crowdworkers to each HIT – this is an
empirical scale for crowdsourced linguistic annotation tasks
suggested by [5,
11].
Crowdworkers were required to live
in the United States and have an approval
rating of
90%
or better.
They were paid $1.00 per HIT.
Workers were
allowed to work on as many distinct HITs as they liked,
and bonuses were given to those who completed multiple
HITs.
To evaluate annotation quality, we examined whether
each worker provided identical answers to the five duplicate
tweets.
Among the annotators of each HIT,
we calculated
Fleiss’
kappa [12]
and Krippendorff’s alpha [17]
measures,
using the tool
5
to assess inter-annotator reliability.
Our con-
jecture, borrowed from [32], is that labeled tweets with high
inter-annotator agreement among crowdworkers can be used
to build a robust model.
The above measures also help us
decide whether to reward an annotator in full or partially.
Before publishing the HITs,
we also consulted with Turker
4
https://www.mturk.com/mturk/welcome
5
Inter-Rater Agreement with multiple raters and variables,
https://mlnl.net/jg/software/ira/
Nation
6
to ensure that the workers were treated and com-
pensated fairly for their tasks.
4.3
Classification model
The aforementioned labeling task yielded 1,297 tweets where
all five annotators agreed on the labels.
1,027 of these were
labeled “job-related” (and the rest “not job-related”).
To
construct a balanced training set,
we added another 757
tweets chosen randomly from tweets outside the Job-Likely
set.
After converting text to lower case,
text features were ex-
tracted as unigrams,
bigrams,
and trigrams.
For example,
the tweet “I really hate my job” is represented as {i,
really,
hate, my, job, i really, really hate, hate my, my job, i really
hate, really hate my, hate my job}.
SVM
light7
was used to
train the classification model, which was then used to clas-
sify the rest of the dataset.
Excluding tweets with less than
five tokens,
the model
labeled a total
of
535,646 tweets as
job-related and 4,465,616 tweets as not.
4.4
Second crowdsourced annotation
To generate better training data and evaluate the effective-
ness of the aforementioned model, a second round of labeling
was conducted.
This assigned to each tweet in the dataset a
confidence score, defined as its distance to separating hyper-
planes determined by the support vectors.
After separating
positive- and negative-labeled (job-related vs.
not) tweets,
each group was sorted in descending order of their confidence
scores.
We used about 4,000 of
these sorted tweets in the second
round of
AMT HITs.
Part of
these tweets were randomly
chosen from the subset of the positive class,
with the 80th
percentile of confidence scores as a cutoff point for inclusion
(labeled as Type-1 in Table 2).
The rest were obtained from
those tweets in either class having confidence scores close
to zero (Type-2 ).
This latter set represents those tweets
that are ambiguous and “difficult” for the classifier to label.
Hence,
we consider both the clearer core and at the gray
zone periphery of this meaning phenomenon, which adds an
interesting challenge to the classification process.
Table 2
records how these two types of tweets were annotated.
6
http://www.turkernation.com
7
http://svmlight.joachims.org
Round 2
Number of agreements
among five annotators
job-related
not job-related
3
4
5
3
4
5
Type-1
129
280
713
50
149
1079
Type-2
11
7
8
16
67
1489
Table 2:
Summary of
the two types of
tweets in
the second crowdsourced annotation and the corre-
sponding annotations
Table
3
summarizes
the
results
from both annotation
rounds.
Round 1+2
Number of agreements
among five annotators
job-related
not job-related
3
4
5
3
4
5
Round 1
104
389
1027
78
116
270
Round 2
140
287
721
66
216
2568
Table 3:
Summary of both annotation rounds
Table 4 displays all the inter-annotator agreement combina-
tions among five annotators and sample tweet in each case
(selected from both annotation rounds).
Crowdsourced
Annotations
Y/N
Sample Tweet
Y, Y, Y, Y, Y
Really bored....., no entertainment
at work today
Y, Y, Y, Y, N
two more days of work then
I finally get a day off.
Y, Y, Y, N, N
Leaving work at 430 and
driving in this snow is going
to be the death of me
Y, Y, N, N, N
Being a mommy is the hardest
but most rewarding job
a women can have
#babyBliss #babybliss
Y, N, N, N, N
These refs need to
DO THEIR FUCKING JOBS
N, N, N, N, N
One of the best Friday nights
I’ve had in a while
Table 4:
Inter-annotator agreement
combinations
and sample tweets
4.5
Community-based annotation
The job-related salience of those tweets in which the major-
ity — but not all — of the annotators agreed (i.e., 3 or 4 out
of 5) is less clear than of those with unanimous agreement,
but such less-clear tweets are still
potentially useful.
Inte-
grating four subsets of
tweets from both rounds of
crowd-
sourced annotations — (a) tweets with only 3 crowdworkers
answered “Y” (referred in Table 5 as job-3 ); (b) tweets with
only 3 crowdworkers answered “N” (not-job-3 );
(c) tweets
with only 4 crowdworkers answered “Y” (job-4 );
and (d)
tweets with only 4 crowdworkers answered “N” (not-job-4 )
— we asked two co-authors from the local
community to
also review them and provide a gold-standard label.
Table
5 summarizes results from this phase.
Round 1+2
Annotations collected
from the local community
job-related
not job-related
job-3
197
21
not-job-3
62
63
job-4
651
11
not-job-4
12
317
Table 5:
Summary of
community-based reviewed-
and-corrected annotations
4.6
Second classification model
Combining the tweets labeled unanimously by the crowd-
workers with those labeled by the community annotators
yielded a training set with 2,665 gold-standard-labeled “job-
related” tweets and 3,250 “not job-related” tweets.
We then
trained a new classifier using a support vector machine
8
.
Since the training data are not
class-balanced,
we grid-
searched on a range of
class weights and chose the model
that optimized F1 score, using 10-fold cross validation.
The
parameter settings that gave the best results on the held-out
data were a linear kernel with the penalty parameter of the
error term C = 0.1 and class weight ratior of
1:1 between
the classes.
Table 6 shows the top features.
Positive
work, job, manager, jobs, managers
working, bosses, lovemyjob, shift, worked
paid, worries, boss, seriously, money
Negative
did, amazing, nut, hard, constr
phone, doing, since, brdg, play
its, think, thru, hand, awesome
Table 6:
Top 15 features in positive and negative
classes
To evaluate this second model,
we used another held-out
data set consisting of 5,200 tweets – 200 with “job-related”
and 5,000 with “not job-related” gold-standard labels.
This
second model obtained 98% precision and 93% recall perfor-
mance for the positive class (“job-related”) after testing the
optimal model on this held-out data set.
We then used this model to classify the tweets in our dataset
not used for training or evaluation.
Almost 200,000 of these
tweets were labeled as “job-related”.
We ranked these job-
related tweets by their confidence scores in descending orders
as τ
1
.
We then ranked them by their LIWC “work” scores
similarly as τ
2
.
We used the Kendall
rank correlation co-
efficient [1] to measure the rank correlation statistically be-
tween these two ranking lists.
Our result K(τ
1
, τ
2
) = -0.055
indicates that these measures are mutually independent.
We
conjecture that there are two reasons for this independence:
“work” in LIWC lexicon is a broader category than our focus
in this study here – it comprises a set of
words related to
school activities, like “scholar, research, highschool, student,
quiz”,
which were intended to be excluded in our data fil-
tering stage.
The computational
process of LIWC score is
8
http://scikit-learn.org/stable/modules/generated/
sklearn.svm.SVC.html
another potential cause – it is derived from the numbers of
single words matched in “work” category divided by the total
words in each message – consequentially it lost all the con-
textual
information compared to the n-gram features used
in our classification model.
4.7
Separating individual users from others
To get a sense of the variety of topics discussed, we manually
examined the tweets labeled by this process as job-related.
A number of
tweets are for job openings or personnel
re-
cruitment ads,
posted by companies or commercial
agents,
for
example “Panera Bread:
Baker -
Night
(#Rochester,
NY)
http://URL #Hospitality #VeteranJob #Job #Jobs
#TweetMyJobs.” We searched for tweets with similar pat-
terns and then divided the job-related tweets into two sub-
classes:
those from individual
users and from commercial
users.
Basic lexical differences between these two classes are
summarized in Table 7.
The TweetNLP POS tagger (with the Penn Treebank-style
tagset) was used to explore different structural
attributes
between individual users group and commercial users group.
The POS tagger assigns parts of
speech at a fine-grained
level to words used in different contexts accordingly.
4.8
Measuring
individuals’
affective
at-
tributes
We measured seasonal
and diurnal
variations in mood in
job-related discourse.
We considered two affective LIWC
dimensions:
positive affect (PA) and negative affect (NA).
These two dimensions are defined as the ratios of the num-
bers of words in each tweet that are in the PA/NA LIWC
lexica to the total number of words in the tweet.
4.9
Topic model analysis
Another part of content analysis is modeling the topics hid-
den in the job-related messages at individual
users level.
Topic models are a suite of algorithms which enable us sum-
marize and discover thematic information about job-related
focuses, interests and trends.
We used latent Dirichlet allo-
cation (LDA) [2].
The intuitions behind LDA include that
a number of “topics” are distributed over the words in the
whole collection of
documents.
We aggregated the tweets
posted by the same user as a single document.
We used the
Gensim implementation of
LDA [26]
with default settings
and 20 topics, with the number of topics chosen empirically,
based on experimental results.
4.10
Time series analysis
The data we collected from DataSift use the Coordinated
Universal
Time standard (UTC) to record when each mes-
sage was created.
Timestamps were converted to local time
zone with daylight saving time taken into consideration.
The
time series analysis relies on the local
time at which each
message was posted.
5.
RESULTS AND DISCUSSION
5.1
POS tagging comparisons
Figure 2 shows the POS tagging comparisons between indi-
vidual
and commercial
users groups.
It describe a total
of
36 different part-of-speech tags
9
with average frequencies of
each tag for different users groups after being normalized.
The individual
users group use CC,
CD,
DT,
IN,
JJ,
NN,
NNS, PRP, PRP$, RB, RP, TO, UH, VB, VBD, VBG, VBP,
VBZ and WRB more frequently than the commercial users
group does.
The only attribute that the commercial
users
group surpasses the individual
counterpart is NNP.
Both
groups have barely the rest items detected in each language
usage.
The commercial
users group use many NNP,
for example
“New York,
Accountant,
Apple” in their posts,
which sup-
ports our assumptions that this group of
accounts posted
quantities of job openings or advertisements with names or
job titles mentioned to give general descriptions.
Compared
to that,
the individual
users used the NNP less frequently
and in a casual way, like “Jojo, galactica, Valli”.
The most frequent tag used by the individual users was NN:
e.g.
“application,
check,
efficiency”.
The second broadly-
used tag in the individual
users group was IN:
for instance
“causee, @, backto, cuz”.
Other tags used heavily by individ-
ual
users were illustrated as the following tag and samples
pairs.
CC — “aaaaaaand, Buttttt, yeeeeet”; CD — “$12.50,
9am-10pm,
twoooo”;
DT — “Whose,
thissss,
Yahoo’s”;
JJ
— “PROUD, greeeeeat, Hhhhhhaaaaaaappppppyyyyyy”; NNS
— “Weekss, bbqs, complainers”; PRP — “imma, yourselves,
watcha”;
RB — “Sadly,
tirelessly,
FINALLY”;
TO — “To,
2keep, t0”;
UH — “Yayyyyy, Ahahaha, lololol”;
VB — “git,
guarantee, re-do”;
VBD — “Upset, planned, debated”;
VBG
— “tryin,
working,
starvin”;
VBP — “hate,
harass,
do”;
WRB — “YYYYYYY, Wot, where”
5.2
Topic analysis
We performed LDA topic analysis to determine what indi-
vidual
users particularly talked about in job-related mes-
sages.
We observed that several topics show notable signals
about job-related theme.
See Table 8.
topic number
representative words
Topic 4
tomorrow, working, today, week,
monday, time, weekend, day,
minute, hour, morning, night
Topic 6
accept, canceled, trust, quit,
working, support, #struggling,
unemployed, helping, corporate,
planning, professional
Topic 14
ugh, exhausted, feeling,
competition, effort, celebrate
Topic 20
technician, #jobs, manager,
productivity, contractor, associate,
assistant, intern, industry
Table 8:
Example of topics and corresponding rep-
resentative words
Among the more salient topics,
topic 4 contains notions of
time related to routine work.
Topic 6 mixes the rise and fall
statuses about career life.
Topic 14 manifests the conceivable
tensions and challenges at work.
Topic 20 illustrates diverse
occupations and roles in work force.
9
The relevant abbreviations were borrowd from [30].
Individual users group
Commercial users group
Total number of tweets
119,376
17,641
Total number of unique accounts
80,537
227
Total number of tokens
1,837,304
1,400,647
Average number of tokens per tweet
15.391
17.391
Total number of unique tokens
103,089
22,547
Average number of unique tokens per tweet
0.864
0.280
Unique tokens :
tokens ratio
0.056
0.016
Number of hapax legomena
69,542
7,884
Average number of hapax legomena per tweet
0.583
0.098
Table 7:
Basic lexical statistics comparisons between the two groups.
hapax legomena are those tokens that
appear only once in the dataset.
Figure 2:
POS tagging comparisons between individual and commercial accounts
5.3
Analysis of Twitter usage
Figure 3 shows that the total
number of tweets per month
and the number of job-related tweets per month follow sim-
ilar seasonal trends, though the overall count peaks in Jan-
uary and the job-related count peaks in December.
The
overall
count and the job-related count both drop to the
bottom in September.
Figure 4 shows weekly trends of
both overall
tweets and
the job-related tweets.
The average number of
job-related
tweets starts steadily on Monday, peaks on Wednesday and
decreases gradually until
bottoming out on Saturday,
and
stays stable until
Sunday,
which follows the standard work
week periodicity.
Sunday had the largest volume of tweets
– greatly exceeding the job-related tweets – many of which
were related to active social activities.
Friday and Saturday
were the least active days from online interactions perspec-
tive.
Figure 5 shows daily trends in volume.
Job and overall
Figure 3:
Numbers of tweets in each month
trends ran parallel before 5 o’clock and then diverged.
The
average number of job-related tweets increased faster than
the volume of overall tweets, until 3pm.
This suggests that
people posted more job-related tweets in morning and early
Figure 4:
Average numbers of tweets on each day of
week
Figure 5:
Average numbers of tweets in each hour
afternoon.
The average number of job-related tweets sharply
decreased until
6pm.
After that,
it increases modestly and
reaches another high point around 9pm.
The average overall
volume of tweets peaked at 9 and 10 in the evening.
5.4
Affective changes observations
To observe the affective changes and temporal
correlations
hidden behind job-related tweets, we accumulated the posi-
tive affect (PA) and negative affect (NA) for the job-related
tweets in each hour on different days in a week.
Figure 6 and 7 show hourly and daily changes of average PA
and NA for individual
users group in local
time,
with 95%
confidence intervals.
Both PA and NA affect changes have
fluctuations during each day and share the similar shape
respectively across days of the week.
PA levels are generally higher on weekends (Saturday and
Sunday) than other days during the week.
Tuesdays and
Thursdays witness the PA peak at 2am in the morning.
And
PA bottoms out at 4am on Mondays and 5am on Wednes-
days.
NA has its highest point at 5am on Wednesdays, then at 2am
on Fridays.
Relatively to PA, NA does not change inversely
which indicates that PA and NA vary independently and are
not mutually exclusive.
6.
CONCLUSION
We used crowdsourcing techniques and local
expertise to
power
a supervised learning pipeline that
iteratively im-
proves the classification accuracy of job-related tweets.
Us-
Figure 6:
Hourly PA changes broken down by dif-
ferent days of the week
Figure 7:
Hourly NA changes broken down by dif-
ferent days of the week
ing this fine-grained text-based classification model,
we ex-
tracted high quality job-related tweets from our local region.
We separated commercial accounts from individual accounts
and measured psychological states for individual users using
LIWC.
Our findings show that even though jobs take up an enor-
mous amount of
most adults’
time,
job-related tweets are
rather infrequent — about 1% to 2% of overall
tweets (see
Figure 3).
Inspecting the usage patterns of Twitter on each
day of
the week,
we find that Sunday is busiest and Fri-
day the quietest.
People post most job-related messages on
Wednesday, and tweet much less about jobs on the weekends.
The volume of job-related tweets starts increasing from 5am
each day and reaches the peak at 3pm.
It is interesting to
see another increase of
job-related tweets after 6pm until
9pm.
We examined affective changes in job-related tweets — pri-
marily PA and NA — in daily and hourly settings and con-
cluded that PA and NA change independently,
thus,
e.g.,
low NA indicates the absence of
negative feelings,
not the
presence of
positive feelings.
Usually tweets on weekends
convey higher PA and NA than those on weekdays.
Our work has several
limitations.
The data are not mas-
sive enough to conduct year-to-year comparison studies on
seasonal
job-related trends.
This work is a preliminary ex-
ploration that relies heavily on linguistic models built upon
the manual
annotations.
We have not examined whether
providing contextual information in annotation tasks would
influence the model
performance.
Also due to the demo-
graphic characteristics of Twitter,
we are less likely to ob-
serve working senior citizens.
Future research would benefit
from more tightly integrated quantitative and qualitative
analyses,
such as geographical
analysis of
the job-related
data in local communities.
7.
ACKNOWLEDGMENTS
This work was supported in part by a GCCIS Kodak En-
dowed Chair Fund Health Information Technology Strategic
Initiative Grant and NSF Award #SES-1111016.
8.
REFERENCES
[1]
H. Abdi. The Kendall Rank Correlation Coefficient.
Encyclopedia of Measurement and Statistics. Sage,
Thousand Oaks, CA, pages 508–510, 2007.
[2]
D. M. Blei. Probabilistic Topic Models. Commun.
ACM, 55(4):77–84, Apr. 2012.
[3]
J. Bollen, H. Mao, and X. Zeng. Twitter Mood
Predicts The Stock Market. Journal
of Computational
Science, 2(1):1–8, 2011.
[4]
Bureau of Labor Statistics. Time use on an average
work day for employed persons ages 25 to 54 with
children, 2013.
[5]
C. Callison-Burch. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon’s
Mechanical Turk. In Proceedings of the 2009
Conference on Empirical
Methods in Natural
Language
Processing:
Volume 1-Volume 1, pages 286–295.
Association for Computational Linguistics, 2009.
[6]
Centers for Disease Control and Prevention. Cost
Estimates of Violent Deaths:
Figures and Tables,
2013.
[7]
Z. Cheng, J. Caverlee, and K. Lee. You Are Where
You Tweet:
A Content-Based Approach to
Geo-Locating Twitter Users. In Proceedings of the 19th
ACM international
conference on Information and
knowledge management, pages 759–768. ACM, 2010.
[8]
comScore. It’s a Social World:
Top 10 Need-to-Knows
About Social Networking and Where It’s Headed,
2011.
[9]
M. De Choudhury and S. Counts. Understanding
Affect in the Workplace via Social Media. In
Proceedings of the 2013 conference on Computer
supported cooperative work, pages 303–316. ACM,
2013.
[10]
M. De Choudhury, M. Gamon, S. Counts, and
E. Horvitz. Predicting Depression via Social Media. In
AAAI Conference on Weblogs and Social
Media, 2013.
[11]
K. Evanini, D. Higgins, and K. Zechner. Using
Amazon Mechanical Turk for Transcription of
Non-Native Speech. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical
Turk, pages 53–56.
Association for Computational Linguistics, 2010.
[12]
J. L. Fleiss. Measuring Nominal Scale Agreement
Among Many Raters. Psychological
Bulletin,
76(5):378, 1971.
[13]
Gallup. State of the American Workplace, 2013.
[14]
S. A. Golder and M. W. Macy. Diurnal and Seasonal
Mood Vary with Work, Sleep, and Daylength Across
Diverse Cultures. Science, 333(6051):1878–1881, 2011.
[15]
Hazards Magazine. Work Suicide, 2014.
[16]
iKeepSafe. Suicide:
Using Technology for Detection
and Intervention, 2014. [Online; accessed
9-December-2014].
[17]
K. Klaus. Content Analysis:
An Introduction to Its
Methodology, 1980.
[18]
T. Liu, C. M. Homan, C. Ovesdotter Alm,
A. Marie White, M. C. Lytle-Flint, and H. A. Hkutz.
Detecting Job-related Messages in Twitter. Submitted.
[19]
T. Nasukawa and J. Yi. Sentiment Analysis:
Capturing Favorability Using Natural Language
Processing. In Proceedings of the 2nd international
conference on Knowledge capture, pages 70–77.
Association for Computing Machinery, 2003.
[20]
T. E. Oxman, S. D. Rosenberg, and G. J. Tucker. The
Language of Paranoia. The American Journal
of
Psychiatry, 1982.
[21]
A. Pak and P. Paroubek. Twitter as a Corpus for
Sentiment Analysis and Opinion Mining. In Language
Resources and Evaluation, 2010.
[22]
M. J. Paul and M. Dredze. You Are What You Tweet:
Analyzing Twitter for Public Health. In International
Conference on Weblogs and Social
Media, 2011.
[23]
A. Pirzadeh and M. S. Pfaff. Emotion Expression
under Stress in Instant Messaging. In Proceedings of
the Human Factors and Ergonomics Society Annual
Meeting, volume 56, pages 493–497. Sage Publications,
2012.
[24]
C. Poulin, B. Shiner, P. Thompson, L. Vepstas,
Y. Young-Xu, B. Goertzel, B. Watts, L. Flashman,
and T. McAllister. Predicting the Risk of Suicide by
Analyzing the Text of Clinical Notes. PLOS ONE,
9(1):e85733, 2014.
[25]
D. Rao, D. Yarowsky, A. Shreevats, and M. Gupta.
Classifying Latent User Attributes in Twitter. In
Proceedings of the 2nd international
workshop on
Search and mining user-generated contents, pages
37–44. ACM, 2010.
[26]
R.
ˇ
Reh˚uˇrek and P. Sojka. Software Framework for
Topic Modelling with Large Corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for
NLP Frameworks, pages 45–50, Valletta, Malta, May
2010. ELRA.
[27]
S. Rude, E.-M. Gortner, and J. Pennebaker. Language
Use of Depressed and Depression-Vulnerable College
Students. Cognition & Emotion, 18(8):1121–1133,
2004.
[28]
A. Sadilek, C. Homan, W. S. Lasecki, V. Silenzio, and
H. Kautz. Modeling Fine-Grained Dynamics of Mood
at Scale. In Workshop on Diffusion Networks and
Cascade Analytics in Web Search and Data Mining,
2014.
[29]
A. Sadilek, H. A. Kautz, and V. Silenzio. Modeling
Spread of Disease from Social Interactions. In
International
Conference on Weblogs and Social
Media, 2012.
[30]
B. Santorini. Part-of-Speech Tagging Guidelines for
the Penn Treebank Project (3rd Revision). 1990.
[31]
N. Schrading, C. Ovesdotter Alm, R. Ptucha, and
C. Homan. #Whyistayed, #Whyileft:
Microblogging
to Make Sense of Domestic Abuse. In Human
Language Technologies:
The 2015 Annual
Conference
of the North American Chapter of the ACL, pages
1281–1286, 2015.
[32]
R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng.
Cheap and Fast – But is it Good? Evaluating
Non-Expert Annotations for Natural Language Tasks.
In Proceedings of the conference on empirical
methods
in natural
language processing, pages 254–263.
Association for Computational Linguistics, 2008.
[33]
A. Tamersoy, M. De Choudhury, and D. H. Chau.
Characterizing Smoking and Drinking Abstinence
from Social Media. In Proceedings of the 26th ACM
Conference on Hypertext & Social
Media, pages
139–148. ACM, 2015.
[34]
Y. R. Tausczik and J. W. Pennebaker. The
Psychological Meaning of Words:
LIWC and
Computerized Text Analysis Methods. Journal
of
Language and Social
Psychology, 29(1):24–54, 2010.
[35]
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and
I. M. Welpe. Predicting Elections with Twitter:
What
140 Characters Reveal about Political Sentiment.
ICWSM, 10:178–185, 2010.
