Proceedings of the Workshop on Cognitive Aspects of the Lexicon,
pages 110–113, Osaka, Japan, December 11-17 2016.
CogALex-V Shared Task: LOPE
Kanan Luce
University of California,
Berkeley
kanan.luce@berkeley
.edu
Jiaxing Yu
Nanjing University
jiaxinguy@gmail.com
Shu-Kai Hsieh
National Taiwan University
shukaihsieh@ntu.edu.
tw
Abstract
This paper attempts the answer two questions posed by the CogALex shared task: How to
determine if two words are semantically related and, if they are related, which semantic
relation holds between them. We present a simple, effective approach to the first problem,
using word vectors to calculate similarity, and a naive approach to the second problem, by
assigning word pairs semantic relations based on their parts of speech. The results of the
second task are significantly improved in our post-hoc experiment, where we attempt to apply
linguistic regularities in word representations (Mikolov 2013b) to these particular semantic
relations. 
1
Introduction
Automatic discovery of semantically-related words is one of the most important NLP tasks, and has
great impact on the theoretical psycholinguistic modeling of the mental lexicon. In this shared task, we
employ the word embeddings model (Mikolov 2013a) to reflect paradigmatic relationships between
words. Previous work has shown that word representations extracted from simple recurrent neural
networks could hierarchically categorize words based on their collocational distribution (Elman 1990).
Word representations also hold other regularities. More recently, Mikolov et al. (2013b) showed that
word vectors could be added or subtracted to isolate certain semantic and syntactic features. The well-
known example is to take the representations for king, subtract man, and add woman. This produces a
vector very near by queen. This method was tweaked by Levy and Goldberg (2014) by representing
the same idea as three pairwise similarities, and is the basis for the post hoc revisions to our system.
The particular semantic relations we are concerned with in this paper are synonymy,
antonymy, hypernymy, and meronymy. The shared task consists of two subtasks. The first is to, given
a pair of words, identify if they are semantically related. The second task is to determine, if the pair is
related, what relation there is between them. We will present first our official system for each subtask,
followed by our post-hoc changes.
2
Subtask 1
Subtask 1 was to see if two words were semantically related. For our system, we returned true if word
2 was in the top n similar word vectors for word 1, or vice versa. We used the pretrained Google News
vectors (Mikolov 2013a), which are 300 dimensions, and contain a vocabulary of 3 million words, and
used the Gensim Word2Vec library (Rehurek and Sojka 2010) to manipulate the data.
We found that the best results were achieved when we considered the top 600 similar words.
This number had the best coverage without suffering from too many false positives. We also found it
helpful to limit the vocabulary from 3 million to only the top 50,000 most frequently occurring tokens,
which eliminated unlikely candidate word-forms. 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: 
http://creativecommons.org/licenses/by/4.0/
110
Initially, we found the 600 most similar words by building a dictionary of each word in the
training data and its corresponding related words. Because we used the same dictionary made from the
training data when running the finalized version on test data, our official submission only looked up
words that occurred in the training data. In our post-hoc experiment for subtask 1, we use all the words
from the test data as well. The results of both systems are below.
LOPE
P = 0.596
R = 0.886
F1 = 0.713
LOPE-PH
P = 0.623
R = 0.884
F1 = 0.731
Table 1: Compares F1 scores of the original (LOPE) and post-hoc (LOPE-PH) systems on Subtask 1
While the performance of the post-hoc system was slightly better, it did not make substantial
gains on the original system. The recall for the two systems was almost the same despite the original
system not containing the test data, because as long as one of the two words in a pair was in the
dictionary, the system could still find related words.
3
Subtask 2
Subtask 2 asks us to take the related pairs from subtask 1 and determine what their relation is: either
synonyms, antonyms, hypernyms, or meronyms. Our original system used a crude method to
categorize the pairs based on their parts of speech. In the training data, nouns, verbs, and adjectives
occurred at different frequencies for each relation, and we used that information to sort them into their
mostly likely categories. Noun-noun pairs were sorted as hypernyms, adjective-adjective pairs were
sorted as antonyms, and verb-verb pairs were split between antonyms and synonyms based on where
the word pair occurred in the list of 600 similar words. If the word occurred in the first 100 most
similar it was sorted as a synonym, otherwise it was sorted as an antonym. The part of speech
information of each pair was determined by finding the most frequent shared part of speech between
the two words as they appeared in the Brown corpus. While this approach was better than a random
baseline, it is not helpful in that it does not provide us with any useful information and the results were
lackluster. 
We significantly improved our results in the post hoc system by completely changing
approach and using a method inspired by Mikolov et al.'s method (2013b) of finding linguistic
regularities in word representations. We were curious if this method could be applied to this particular
problem of finding differences between synonyms, antonyms, hypernyms, and meronyms.
We initially implemented a method inspired by Levy and Goldberg (2014). We used three
word representations, one related pair from the training data and one word from the input pair, in order
to predict the other word from the input pair as one of the most similar vectors. The idea being that the
cosine similarity of the target fourth word will be different in the case where the semantic relation of
the input pair matches that of the training pair. 
This assumption was incorrect, and we had to revise our approach. We instead started to find
the cosine similarity between the two sets of words (the input set, and a related set from the training
data). The cosine similarity was often higher if the two sets shared a relation. While this was
inconsistent when comparing only two sets, we found that we could compare (find the cosine
similarity of) an input set to each antonym, synonym, hypernym and meronym set in the training data,
average the results for each semantic relation, and then assign the input pair to the class that had the
highest average. 
LOPE
P
R
F1
LOPE-PH
P
R
F1
SYN
0.304
0.191
0.235
SYN
0.089
0.438
0.148
ANT
0.417
0.217
0.285
ANT
0.447
0.405
0.425
HYPER
0.328
0.406
0.363
HYPER
0.199
0.514
0.287
PART_OF
0.000
0.000
0.000
PART_OF
0.411
0.365
0.387
Subtask 2
0.289
0.231
0.247
Subtask 2
0.373
0.414
0.374
Table 2: Compares F1 scores of the original (LOPE) and the post-hoc (LOPE-PH) system on Subtask 2
111
So if the average of the input pair compared to all of the antonym pairs was higher than it was
for the same comparison to all of the synonym, hypernym, and meronym sets, then the input pair was
assigned the relation antonym. Using this method, we were able to significantly increase our F1 score
for the task and our coverage of which relations we were able to get right (we completely ignored
meronyms in the first system). However, we still struggled with some relations more than others.
Synonyms in particular had a very poor precision, and the accuracy of the system on synonyms was
much lower than for the other three relations.
4
Conclusion
There were two mistakes in the initial version of our system. First, the there was no reason not to use
the test data rather than the training data when looking at the top 600 similar representations for each
word in a pair. The difference, however, in the results was relatively small. A much more significant
error was our original system for solving subtask 2, which was both relatively ineffective and didn't
show anything interesting.. 
Despite these errors, we were able to propose a system that while very simplistic and easy to
implement, was able to achieve good results compared to the rest of the field. Table 3 shows the
results of the various systems in the shared task on both subtask 1 and subtask 2.
Subtask 1
Subtask 2
Team
F1
Team
F1
GHHH
0.790
LexNET
0.445
Mach5
0.778
GHHH
0.423
LexNET
0.765
LOPE-PH
0.374
ROOT18
0.731
Mach5
0.295
LOPE-PH
0.731
ROOT18
0.262
LOPE
0.713
CGSRC
0.252
HsH-Supervised
0.585
LOPE
0.247
CGSRC
0.431
Table 3: Results of the different systems in the CogALex shared task, with the addition of our post-hoc system
In particular, our method of comparing the input pair to each related pair in the training data,
averaging the results in each relational category, and assigning the input pair the relation with the
highest average, appears to be effective at categorizing pairs according to their semantic relationship.
Although our exact method was different than that of Mikolov et al. and Levy and Goldberg, it shows
that the linguistics regularities they found in word embeddings are useable to find this kind of
paradigmatic information about the semantic relationships synonymy, antonymy, hypernymy, and
meronymy.
While not at the top of the table for either subtask, we believe we were able to put up
respectable results for a simple system. It is possible that with a more complex expansion of the
system, we could improve the results even more, particularly by finding ways to increase the accuracy
of synonym detection in subtask 2. 
4.1
Further Study
As a further study, we would like to attempt the task in Chinese. We argue that relation extraction is a
task that could be language/writing system dependent. For example, in Chinese, it would be possible
to exploit morpho-semantic relations and the character radical ontology (paradigmatic information
embedded in the characters) to re-conduct subtask 2. We are currently underway creating original
Chinese language data from Chinese Word Net to mirror the English data, so as to avoid translating
polysemous words in English that aren't polysemous in Chinese, such as cell (a cell could be a small
room or a part of an organism). 
112
Reference
Jeffrey Elman. 1990 . Finding structure in time. Cognitive Science, 14, 179-211
Omar Levy and Yoav Goldberg. 2014. Linguistic regularities in sparse and explicit word representations. In
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171-180, Baltimore,
Maryland USA, June 26-27 2014. Association for Computational Linguistics
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word
representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June.
Association for Computational Linguistics.
Radim Rehurek and Petr Sojka. 2010. Software framework for topic modeling with large corpora. In
Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45-50, Valletta,
Malta, May 22. ELRA
Enrico Santus, Frances Yung, Alessandro Lenci, and Chu-Ren Huang. 2015. EVALution 1.0: An evolving
semantic dataset for training and evaluation of distributional semantic models. In Proceedings of the 4
th
Workshop on Linked Data in Linguistics, Beijing
113
