Tuning word embeddings for neural
dependency parsing of Norwegian
Henrik Hillestad Løvold
Thesis submitted for the degree of
Master in Informatics: Technical and Scientific
Applications
(Language Technology group)
60 credits
Department of Informatics
Faculty of mathematics and natural sciences
UNIVERSITY OF OSLO
Spring 2017
Tuning word embeddings for
neural dependency parsing of
Norwegian
Henrik Hillestad Løvold
© 2017 Henrik Hillestad Løvold
Tuning word embeddings for neural dependency parsing of Norwegian
http://www.duo.uio.no/
Printed: Reprosentralen, University of Oslo
Abstract
In the recent years, data driven dependency parsers using neural networks
have been developed and show promising results in parsing accuracy. An-
other area that has seen an increase in interest among researchers is using
distributional semantics in the form of word embeddings as input features
in dependency parsers, replacing traditional feature engineering.
No pre-
vious research has been done on systematically tuning of word embed-
dings for neural network parsing of Norwegian.
We present the results of tuning word embeddings for the task of neural
network dependency parsing of Norwegian. We also present the results of
comparing the performance of word embeddings in dependency parsers,
to their performance in intrinsic evaluation metrics.
i
ii
Acknowledgements
First of all, I want to express my gratitude for my supervisors Lilja Øvrelid
and Erik Velldal, for their guidance, and the motivation and help offered
during my work on this project.
I want to thank the researchers and developers of the tools used in this
project, and for making them publicly available.
Without these tools, this
project would have been impossible.
I also want to thank my fellow students at the University of Oslo, in par-
ticular Kjetil B. Kristoffersen, Camilla E. Stenberg, Hans P. T. Kragset and
A. Helene Holter, for giving me the motivation to keep on going, and for
pulling me through in my darkest moments.
Finally I would like to thank my parents and my family for believing in
me, and for the unconditional support offered to me.
To my dear daughter Ella.
iii
iv
Contents
1
Introduction
1
1.1
The project
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
1.2
Outline .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
2
Background
5
2.1
Distributional semantics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
2.1.1
Machine learning and Artificial Neural Networks
.
.
6
2.1.2
Language models .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
2.1.3
Word embeddings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
2.1.4
Count-based vs. prediction-based models .
.
.
.
.
.
.
10
2.1.5
Evaluation of word embeddings
.
.
.
.
.
.
.
.
.
.
.
.
10
2.2
Dependency parsing
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
2.2.1
Graph-based dependency parsing
.
.
.
.
.
.
.
.
.
.
.
11
2.2.2
Transition-based dependency parsing .
.
.
.
.
.
.
.
.
12
2.2.3
Learning .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
2.3
Parsing with distributional semantics
.
.
.
.
.
.
.
.
.
.
.
.
.
13
2.3.1
Clustering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
2.3.2
Word embeddings as features in dependency parsing
15
2.3.3
Word embeddings and word order .
.
.
.
.
.
.
.
.
.
.
16
2.3.4
A neural network approach to dependency parsing .
17
2.3.5
Recurrent neural networks
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
2.3.6
Dependency parsing using stack long short-term
memory
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
2.3.7
A trainable NLP pipeline
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19
2.3.8
Word embeddings in UDPipe and Dyer’s parser .
.
.
19
2.4
Dependency parsing of Norwegian .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
2.4.1
The Norwegian Dependency Treebank
.
.
.
.
.
.
.
.
20
2.5
Intrinsic versus extrinsic evaluation of word embeddings .
.
23
2.5.1
Intrinsic evaluation datasets .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
2.5.2
Evaluation of Norwegian analogies
.
.
.
.
.
.
.
.
.
.
23
3
Data and tools
27
3.1
Unlabelled text
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
3.1.1
The Norwegian Newspaper Corpus
.
.
.
.
.
.
.
.
.
.
27
3.1.2
NoWAC - Norwegian Web as Corpus
.
.
.
.
.
.
.
.
.
28
v
3.1.3
Merging the corpora .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
3.2
Labelled text
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
3.2.1
Norwegian Dependency Treebank .
.
.
.
.
.
.
.
.
.
.
29
3.2.2
Normalisation of the corpora and treebank .
.
.
.
.
.
31
3.3
Tools
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
3.3.1
wang2vec and word2vec
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
3.3.2
Long Short-Term Memory Parser (Dyer’s parser)
.
.
32
3.3.3
UDPipe
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
34
3.3.4
Dependency parser evaluation software .
.
.
.
.
.
.
.
35
3.3.5
Intrinsic evaluation software
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
3.3.6
HPC Environment
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
4
Baseline and dataset experiments
39
4.1
Tuning workflow .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
39
4.1.1
Establishing a baseline .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
40
4.1.2
Parsing without pre-trained word embeddings .
.
.
.
40
4.2
Dataset experiments
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
4.2.1
Discussion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
4.2.2
The Norwegian Newspaper Corpus experiments
.
.
41
4.2.3
Concatenation experiments
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
4.2.4
Swapping the word embeddings .
.
.
.
.
.
.
.
.
.
.
.
42
4.2.5
Results and conclusion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
5
Tuning the word embeddings
45
5.1
Introduction .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45
5.1.1
Quantifying the degree of non-determinism .
.
.
.
.
46
5.1.2
Training word embeddings using the Skip-Gram
and Structured Skip-Gram models .
.
.
.
.
.
.
.
.
.
.
48
5.2
Dimensionality
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
49
5.3
Window size .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50
5.4
Frequency cutoff
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50
5.5
Algorithms for optimising similarity .
.
.
.
.
.
.
.
.
.
.
.
.
.
52
5.5.1
Negative examples .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
52
5.5.2
Hierarchical softmax .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
53
5.5.3
Noise Contrastive Estimation (NCE)
.
.
.
.
.
.
.
.
.
.
54
5.5.4
Isolating Negative Examples
.
.
.
.
.
.
.
.
.
.
.
.
.
.
55
5.5.5
Similarity algorithm conclusion .
.
.
.
.
.
.
.
.
.
.
.
.
56
5.6
Conclusion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
56
5.7
Held-out evaluation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
57
6
Intrinsic evaluation and error analysis
61
6.1
Intrinsic evaluation of word embeddings
.
.
.
.
.
.
.
.
.
.
.
61
6.1.1
Evaluating our word embeddings intrinsically .
.
.
.
62
6.1.2
A closer look at neighbouring words .
.
.
.
.
.
.
.
.
.
63
6.2
Error analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
65
vi
7
Conclusion and future work
69
7.1
Conclusion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
69
7.2
Future work .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71
vii
viii
List of Figures
2.1
Example feed-forward neural
network with two hidden
layers. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
2.2
Example dependency graph .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
2.3
Example of a hierarchical clustering dendrogram .
.
.
.
.
.
.
14
2.4
Example RNN with one layer.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
2.5
Example dependency graph .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
3.1
Example dependency graph .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
31
4.1
Tuning workflow .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
39
5.1
Mean formula .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
46
5.2
Sample standard deviation formula
.
.
.
.
.
.
.
.
.
.
.
.
.
.
46
5.3
Quantification results .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
47
5.4
LAS relative to window size .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
51
5.5
LAS relative to negative samples
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
53
ix
x
List of Tables
2.1
Nivre’s algorithm operations
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
2.2
Source ratio for NDT .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
2.3
Categories of the Google Analogies dataset
.
.
.
.
.
.
.
.
.
.
24
2.4
Categories of the Norwegian Analogies dataset .
.
.
.
.
.
.
.
25
3.1
Number of tokens for the Norwegian Newspaper Corpus
and NoWaC .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
3.2
Source ratio for NDT .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
3.3
Explanation of the CoNLL-X data fields column-wise
.
.
.
.
30
3.4
Example sentence in the CoNLL-X format
.
.
.
.
.
.
.
.
.
.
.
31
3.5
Tuning options for word2vec and wang2vec.
.
.
.
.
.
.
.
.
.
33
3.6
Tuning options for Dyer’s parser, from the help screen .
.
.
.
34
3.7
Tuning options for UDPipe.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
36
4.1
Standard embedding tunings for Dyer’s parser and UDPipe
40
4.2
Results without word embeddings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
4.3
Results of parsing with NAK based word embeddings
.
.
.
42
4.4
Results of parsing with NAK+NoWAC based word embed-
dings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
4.5
Results of parsing with swapped word embeddings
.
.
.
.
.
43
4.6
Dataset experiments results recap .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
44
5.1
Comparing skip-gram to structured skip-gram .
.
.
.
.
.
.
.
48
5.2
Parsing results with 50, 100 and 200 dimensions
.
.
.
.
.
.
.
49
5.3
Parsing results with varying window size .
.
.
.
.
.
.
.
.
.
.
50
5.4
Results of frequency cutoff experiments
.
.
.
.
.
.
.
.
.
.
.
.
52
5.5
Results of experiments with negative samples
.
.
.
.
.
.
.
.
54
5.6
LAS and UAS with hierarchical softmax .
.
.
.
.
.
.
.
.
.
.
.
54
5.7
LAS with varying values for NCE .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
55
5.8
LAS with 5 and 25 negative examples with and without NCE 56
5.9
Results of all embedding tuning experiments
.
.
.
.
.
.
.
.
.
58
5.10 Final embedding settings.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
59
5.11 LAS and UAS in Dyer’s parser and UDPipe on the parsing
of the test dataset
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
59
xi
6.1
Recognised analogies and LAS for skip-gram and struc-
tured skip-gram .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
62
6.2
Intrinsic evaluation details .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
64
6.3
Five closest neighbours of "katt" with SSG and SG .
.
.
.
.
.
65
6.4
Five closest neighbours of "beregne" with SSG and SG .
.
.
.
65
6.5
Five closest neighbours of ambiguous words using skip-gram 66
6.6
Five closest neighbours of ambiguous words using skip-gram 66
6.7
Deprel+attachment f-score of SSG and SG .
.
.
.
.
.
.
.
.
.
.
67
xii
Chapter 1
Introduction
1.1
The project
The aim of this project is to explore how different word embedding models
and tuning of embedding hyperparameters used as input representations
affect the performance of neural network-based dependency parsers, with
the goal of improving dependency parsing of Norwegian.
Dependency parsing is a field within the realm of language technology.
A dependency parser analyses text with the goal of being able to recog-
nise the syntactic relations between words within a sentence in a given
language.
Modern dependency parsers use statistics and probability to
achieve this goal.
In the recent years, extensive research has been carried
out to apply machine learning algorithms known as artificial
neural
net-
works to statistic dependency parsers.
Another area that has seen a rise in interest recently is distributional se-
mantics in the form of word embeddings.
Word embeddings are multidi-
mensional vector representations of words in a vocabulary. Whereas tradi-
tional distributed word representations use occurrence counting of words
to generate vector representations, word embedding software utilises ma-
chine learning techniques to allow for much more densely populated word
vectors of
lower dimensionality.
This makes the representations well
suited for specifying the input layer to artificial neural networks.
A huge advantage of using distributional semantics in the form of word
embeddings as features in dependency parsing is its ability to replace
manual feature engineering.
Traditionally,
features used in dependency
parsing had to be carefully selected by linguists.
The modern word em-
bedding models show promise to be a competitive alternative to these fea-
tures.
1
Many tools for generating word embeddings exist,
and exploring them
all is out of reach of this project. In this project we will train models using
the neural network-based word2vec (Mikolov,
Sutskever,
Chen,
Corrado,
& Dean, 2013) and wang2vec (Ling, Dyer, Black, & Trancoso, 2015).
In order to train word embeddings,
a corpus of text is required.
In this
project we will
carry out experiments with the Norwegian Newspaper
Corpus and a concatenation of this and the Norwegian Web as Corpus
(NoWAC) in order to find an optimal
setup for dependency parsing of
Norwegian.
Recently,
researchers at the University of Oslo and the Norwegian Na-
tional Library have developed a treebank, a resource which is instrumental
in training and evaluating dependency parsing,
for Norwegian (Solberg,
Skjærholt,
Øvrelid,
Hagen,
& Johannessen,
2014).
This is the first of its
kind for Norwegian, and will form the basis of the parsing performed in
this project.
The word embedding software we use offers a wide range of hyperpa-
rameters to be tuned by the user.
To reach our goal of optimising word
embeddings to be used as features in dependency parsing,
we will
ex-
periment with various language models,
datasets and parameters in the
training of word embeddings.
We show that the choice of configuration
can have a large impact on downstream parser performance.
Recently, a dataset for evaluation of word embeddings in the task of recog-
nising analogies has been developed for Norwegian (Stadsnes, forthcom-
ing).
Using this dataset,
we show that the choice of embedding model
largely affects the performance of the embeddings in terms of recognising
semantic and syntactic analogies. We also show that there is no one-to-one
correlation between models performing well in downstream dependency
parsing, and performance in the task of recognising analogies.
1.2
Outline
Chapter 2 provides an introduction to the theoretical background and an
overview of previous research within the field of dependency parsing and
word embeddings.
Chapter 3 describes the data and tools used in this project,
and the
preparatory work that had to be carried out to make them usable in our
experiments.
The experiments we carried out to find parameters for optimal word em-
2
bedding models to be used in dependency parsing of Norwegian are de-
scribed in chapter 4 and 5.
As mentioned,
we also performed experiments to evaluate our embed-
dings in traditional, intrinsic evaluation tools.
These experiments are de-
scribed in chapter 6.
Finally,
chapter 7 contains our conclusion,
our contributions,
as well as
a description of possible future work based on the outcome of this project.
3
4
Chapter 2
Background
This project aims to explore how different word embedding models and
embedding hyperparameters affect the performance of statistical depen-
dency parsers for Norwegian.
The experiments will be performed on the
Norwegian treebank (Solberg et al.,
2014).
Dependency parsing has re-
cently seen an increase in interest among computational linguists. Another
area that has seen an increase in interest by scientists recently is word em-
beddings.
Word embeddings are vector representations for words.
Com-
pared to traditional
vector space models,
word embeddings offer more
dense, dimensionality reduced vectors.
As labelled data is a scarce resource,
various means of using unlabelled
data have been proposed.
This project aims to perform experiments to
fine-tune embeddings for Norwegian.
Embeddings will
be trained us-
ing the Norwegian Newspaper Corpus
1
and the Norwegian Web Corpus
(NoWaC) corpora (Guevara,
2010).
Two modifications to the traditional
word embedding models proposed by Mikolov, Chen, Corrado, and Dean
(2013) have been made by Ling et al. (2015).
In this project we will look at
how these embeddings perform when trained on Norwegian language.
This chapter describes the theoretical
background of
this project.
The
practical details regarding the tools and data are described in Chapter 3.
The experiments carried out to optimise word embeddings and syntactical
parsing of norwegian language are described in Chapter 4.
2.1
Distributional semantics
Before looking at dependency parsing,
we need to understand the con-
cept of distributional
semantics.
The basic component in distributional
semantics is vector space models.
Vector space models are numeric meth-
ods used to represent features of words as vectors.
Each dimension of
1
https://www.nb.no/sprakbanken/show?serial=sbr-4&lang=nb
5
a word’s feature vector represents the weight of a given feature for this
word.
In semantic vector space models, the vector features are generally
based on co-occurrence (Jurafsky & Martin,
2000).
The numeric vectors
can be used to calculate the geometric distance between words in our vec-
tor space, typically by simple metrics such as Euclidean distance or cosine
similarity. This information about word distribution can be used to repre-
sent the syntactic similarity between words.
The core hypothesis in distributional semantics, the distributional hypothe-
sis, states that words with similar meaning often occur in close proximity
to the same words;
the context is similar.
Utilising the distributional hy-
pothesis, we can use the distributional properties of natural languages to
estimate the semantic meaning of words (Sahlgren, 2008).
Consider the following sentences:
Too much Spoosh makes you gain weight.
Spoosh tastes like cocoa and nuts.
A bar of Spoosh will melt in the sun.
Although Spoosh is a made up word, it is obvious to the human mind that
Spoosh is some kind of chocolate bar with nuts. But how about a computer
that has no prior knowledge of the concept of "chocolate"? By looking at
the context words,
the computer might find that the word Spoosh occurs
within the vicinity of many words that also occur in the vicinity of the
word chocolate.
By applying the distributional hypothesis,
the computer
can assume that these two words have a semantic similarity,
only based
on the surrounding words.
2.1.1
Machine learning and Artificial Neural Networks
Machine learning is a set of methods used to make machines capable of
making decisions based on experience.
Machine learning has long tradi-
tions,
and many algorithms,
such as support vector machines,
decision
trees and Bayesian classifiers,
exist.
One of them is called artificial neural
networks.
Artificial neural networks consist of a set of nodes, dubbed neurons, con-
taining activation functions, which are distributed over a varying amount
of hidden layers.
Between the layers are weighted edges.
At the end of
the network is an output layer of one or more neurons, which output rep-
resents the decision made by the network.
6
During training, the artificial neural network is given examples.
Each ex-
ample consists of an input, and the desired output given that input. When
the network makes a prediction, an error function calculates how far away
from the desired output this prediction was.
The error is then fed back-
wards through the network,
adjusting weights accordingly.
This step is
called backpropagation.
Training is finished when the error converges, or
after a given number of iterations.
The Artificial neural network is a popular machine learning algorithm in
many fields.
As computer hardware has improved drastically over the
last couple of decades, artificial neural networks, which both requires and
scales well with huge amounts of data, have become more viable in many
applications. An important part of the success of artificial neural networks
within the field of natural
language processing,
is its ability to replace
manual feature engineering.
Representation learning is a computer’s ability to both learn how to use
the features, and to actually learn how to learn. Artificial neural networks
with multiple hidden layers have a high level of abstraction, making them
suited for this task. Another important feature of artificial neural networks
in the field of natural language processing,
is the low dimensionality of
each word’s vector representation.
The low dimensionality of the vectors
enables us to concatenate the word vectors into a high dimensional model,
which is used as input directly to the parser.
These low dimensional vec-
tors called word embeddings, will be described further in chapter 2.1.3.
Many artificial neural network models, such as shallow feed-forward net-
works, recurrent neural netowrks (including long short-term memory net-
works),
convolutional
neural
networks exist,
and are used for different
tasks.
In this project we will use tools that utilise neural networks both in
the training of word embeddings, and in the dependency parsers Dyer’s
parser,
which will be presented in 2.3.6,
and UDPipe,
which will be pre-
sented in 2.3.7.
2.1.2
Language models
Language models are statistical probability distributions over the likeli-
hood of sequences of words.
The probability of a sequence w is given by
P
(
w
1
, w
2
, ..., w
n
)
. Language models are useful in many applications of nat-
ural language processing, such as machine translation, word suggestions
in word processing, etc.
In search technology, language models are used
to return the most relevant documents given a query string.
In statistical
parsing, we are often interested in the likelihood of a word w occurring in
a given context c, P
(
w
|
c
)
.
7
Input #1
Input #2
Input #3
Input #4
Output
Hidden
layer 1
Hidden
layer 2
Input
layer
Output
layer
Figure 2.1: Example feed-forward neural network with two hidden layers.
The N-gram model
The most commonly used language model is known as the n-gram model.
N-grams are sequences of n words in a text or document. In n-gram mod-
els, the probability of a word w in a given by the preceding n
−
1 words:
P
(
w
i
|
w
i
−(
n
−
1
)
, w
i
−(
n
−
2
)
, ..., w
(
i
−
1
)
In this model, we assume that the probability of an occurrence of a word
only depends on the n
−
1 preceding words.
Although this is a naive
assumption,
N-gram models are often used when modelling languages.
The main reason is that training N-gram models requires little complexity,
and the trade-off in terms of precision compared to higher complexity lan-
guage models is low.
A problem often encountered when working with n-gram models is called
the data sparsity issue. Regardless of how big the training data is, it is likely
that certain sequences only occur once, or that a sequence we attempt to
look up never occurs at all in the training data.
This leads to low, or even
zero probabilities (unless a technique known as smoothing is applied).
In
order to avoid the data sparsity issue,
other language models have been
proposed.
The neural network language model (NNLM) was proposed by Bengio,
Ducharme, Vincent, and Jauvin (2003).
The aim of this model is to avoid
the data sparsity issue by training an artificial neural network on the a text
corpus,
and create a probability function based on the distributed repre-
sentation for words.
"Generalization is obtained because a sequence of
words that has never been seen before gets high probability if it is made
8
of words that are similar (in the sense of having a nearby representation)
to words forming an already seen sentence" (Bengio et al., 2003).
The Neural Network Language Model
A problem with traditional language models like the n-gram model, is that
sequences of words which are not seen in the training corpus either yield a
zero probability, or a low probability, in cases where smoothing is applied.
For instance, in a case where the sentence "The dog eats food" occurs in the
training corpus,
but "The cat eats food" does not occur,
the probability of
the latter sentence will be low. The words "dog" and "cat" are semantically
close to each other,
and it would make sense that the probability of the
sentence "The cat eats food" should be close to that of "The dog eats food".
Several attempts at solving this problem,
including variants of the tradi-
tional
models and entirely new language models,
have been proposed.
One of the newer language models,
and the one we will focus on in this
project, is called the Neural network language model.
The idea behind the NNLM approach is by Bengio et
al.
(2003)
sum-
marised as follows:
1.
Associate with each word in the vocabulary a distributed word
feature vector (a real valued vector in
R
m
),
2.
Express the joint probability function of word sequences in terms of
the feature vectors of these words in the sequence, and
3.
Learn simultaneously the word feature vectors and the parameters
of that probability function.
A byproduct
of
the neural
network language model
is
the vectors
representing each word in the vector space.
These vectors have proven
to be useful
for many tasks in natural
language processing,
including
dependency parsing, and are often referred to as word embeddings.
2.1.3
Word embeddings
Traditionally,
vector
estimation has
largely been based on frequency
counting.
This is a simple,
low-cost process,
but it has some drawbacks.
First of all,
it requires some sort of smoothing to be applied to the vec-
tors, to avoid zero probabilities when used in statistical language models.
Secondly, the problem of data sparsity (as described in chapter 2.1.2) may
also cause problems with low or zero probabilities.
As a byproduct of
the research on neural network language models, continuous vector space
models known as word embedding, have been proposed.
9
Word embedding is a technique that is used to estimate the proximity be-
tween words.
One of the most popular family of methods is known as
word2vec (Mikolov, Chen, et al., 2013). Word2vec implements the methods
skip-gram and CBOW (continuous bag of words).
The aim of word embed-
ding is to reduce the dimensionality of the feature vectors, to make it easier
to calculate the semantic proximity between words.
Bansal, Gimpel, and
Livescu (2014) compared different types of embeddings to Brown clusters
as features for dependency parsing, and found that all embeddings yield
significant parsing gains.
In this project, we will use word2vec (Mikolov,
Chen, et al., 2013) and wang2vec (Ling et al., 2015) embeddings as features
in dependency parsing.
The skip-gram model takes a word w
t
as input,
and outputs the context
window.
In opposite, the CBOW method takes the context as input, and
predicts w
t
.
Although the methods are similar and produce similar em-
beddings, they behave quite differently. There is usually one choice which
is better than the other for a given task (Jurafsky & Martin, 2000).
In addition to the different models,
word2vec also offers a wide range
of hyperparameters,
which values can be specified by the user.
As the
aim of this project is to explore how different models and parameters af-
fect parsing by utilising word embeddings, we will focus on tuning these
parameters to find a setup that works well for Norwegian.
2.1.4
Count-based vs. prediction-based models
Research on comparing traditional count-based language models to the
newer prediction-based language models has been done by Baroni, Dinu,
and Kruszewski (2014).
Count-based models were trained using the DIS-
SECT Toolkit
2
, and prediction-based models were trained using word2vec
(Mikolov, Chen, et al., 2013). Baroni et al. (2014) found that their word2vec
models beat the count-based models in every single task, and encouraged
researchers to use prediction-based models rather than count-based mod-
els.
When we decided to use word embeddings, this encouragement was
taken into account.
2.1.5
Evaluation of word embeddings
Traditionally, word embeddings have been evaluated using computation-
ally low-cost datasets which measure how well the embeddings recognise
predefined analogies and word similarities.
This is known as intrinsic
2
DISSECT Toolkit: http://clic.cimec.unitn.it/composes/toolkit/
10
evaluation, and will be further described in chapter 2.5.
Although intrinsic evaluation is computationally cheap, recent discussion
about whether the results of intrinsic evaluation actually measures the vi-
ability of the embeddings in the task they are trained for, has emerged in
the NLP community. In this project we will use our embeddings only as a
tool for improving dependency parsing of Norwegian language, and eval-
uation will be performed by passing the embeddings as parameters to our
dependency parsers, and then measuring the accuracy of the results. This
is known as extrinsic evaluation of word embeddings.
After our extrinsic evaluation of word embeddings in the downstream
task of dependency parsing, we will take a closer look at how a sample of
our models perform in intrinsic tests for comparison.
These experiments
are presented in chapter 6.1.1.
2.2
Dependency parsing
The goal of dependency parsing is to determine the syntactic relationship
between words
in a sentence.
In contrast
to constituency parsers,
where the aim is to decide the lexical
phrase structure of
a sentence,
dependency parsers find relationships between words, and has no concept
of constituents.
The reason that dependency structure grammars lately
have gained popularity compared to phrase structures,
is
that
they
are more efficient
to learn and parse while still
encoding much of
the predicate-argument information needed in applications (McDonald,
Pereira, Ribarov, & Hajiˇ
c, 2005).
Modern dependency parsers are trained
using large corpora of
annotated text,
so-called treebanks.
There are
primarily two approaches to data-driven dependency parsing – graph-
based and transition-based parsing (McDonald et al.,
2005;
Nivre,
Hall,
& Nilsson, 2006).
2.2.1
Graph-based dependency parsing
Graph-based parsers
use
graph algorithms
to find the
most
likely
dependency structure of a sentence.
One such parser is the MSTParser.
The aim of
the MSTParser is to "(...)
formalize weighted dependency
parsing as searching for maximum spanning trees (MSTs)
in directed
graphs" (McDonald et al.,
2005).
The algorithm takes an input sentence
x
=
x
1
, x
2
, ..., x
n
, and assumes a proper feature representation, as well as
a weight vector w.
A graph is constructed,
where each word acts as a
vertex,
and an additional root vertex is created.
Directed edges connect
all vertices to each other.
This process yields a list of all the possible trees
11
for the input sequence. After creating the trees, a maximum spanning tree
algorithm is used to find the most likely parse tree for the input sequence.
2.2.2
Transition-based dependency parsing
The second approach to dependency parsing is called transition-based
dependency parsing.
In opposite to graph-based parsing,
which finds
the globally most likely dependency graph, transition-based dependency
parsers are state machines,
and find the most likely transition between
given states.
Nivre et
al.
(2006)
introduce MaltParser,
a multilingual
transition-based parser.
MaltParser makes greedy decisions based on
probabilities in each step of the parsing.
It can thus be looked at as an
opposite to MST’s approach,
which looks at the graph as a whole and
maximises probability for the entire graph.
"While a traditional parser-
generator constructs a parser given a grammar,
a data-driven parser-
generator constructs a parser given a treebank" (Nivre et al.,
2006).
The
reason that decision based parsers such as MaltParser have become popu-
lar, is its speed.
In linear time complexity, MaltParser can produce parses
very close to the state of the art dependency parsers in terms of precision
and accuracy.
The MaltParser framework supports any deterministic parsing algorithm
compatible with its architecture.
One of them is Nivre’s algorithm,
which
is a linear time algorithm limited to projective graphs.
It has two modes:
arch-standard and arch-eager (Nivre et al., 2006).
Nivre’s arc-eager algo-
rithm uses three data structures;
the Stack,
which starts with the ROOT
symbol,
the Buffer,
which starts with the input sequence,
and a set of de-
pendency arcs, which starts off empty.
In each transition, it has four possi-
ble operations (table 2.1), and its finishing condition is reached when the
buffer is exhausted.
Many modern dependency parsers are based on the datastructure pre-
sented by Nivre et al. (2006).
Two examples are the LSTM-Parser (Dyer,
Ballesteros, Ling, Matthews, & Smith, 2015) and UDPipe (Straka, Hajic, &
Straková, 2016), which we will use in this project. These parsers are further
described in chapter 2.3.6 and 2.3.7.
2.2.3
Learning
By analysing an annotated corpus with various properties associated
with each word (POS-tag,
dependency relation,
etc.),
the parser is able
to learn relationships between words,
which is required to later parse
untagged text.
Various machine learning algorithms for classification
12
Shift
Shifts the next object from the buffer onto the stack.
Left arc
Creates a left
arc between the next
element
in the
buffer and the top element on the stack, and adds the
arc to the set of dependencies. Pops the dependent off
the stack.
Right arc
Shifts the first element in the buffer onto the stack,
and creates a right arc dependency between the two
elements on the top of the stack. The new arc is added
to the set of dependencies.
Reduce
Pops the top element off the stack.
Table 2.1: Nivre’s algorithm operations
Den
viser
at
han
har
trengt
å
komme
i
modus
.
It
shows
that
he
has
needed
to
get
into
competition-mode
.
ROOT
SUBJ
SBU
SUBJ
DOBJ
INFV
DOBJ
INFV
ADV
PUTFYLL
IP
Figure 2.2:
Dependency graph of the example sentence from the develop-
ment set.
can be used to learn relationships from a dependency treebank along
with a corresponding set
of
features.
MaltParser is compatible with
support
vector
machines (with various kernel
functions),
or
a linear
classifier to learn relationships
3
. Features are extracted from the annotated
treebank,
and usually include information about
each word’s parent,
siblings (words of similar depth depending on the same parent), and for
labelled parsing, the dependency relation.
2.3
Parsing with distributional semantics
Recent
studies have shown significant
parsing gains using continuous
word representations and word clusters derived from embeddings as fea-
tures in dependency parsing. Parsing improvements by using Brown clus-
ters based on discrete representations was shown by Koo, Carreras Pérez,
and Collins (2008).
3
Different
algorithms
described at:
http://www.maltparser.org/userguide.html#
learner
13
a
b
c
d
e
distance
Figure 2.3: Example of a hierarchical clustering dendrogram
2.3.1
Clustering
Clustering is an unsupervised machine learning technique used to find
groups of related objects, clusters, in a set. Unlike classification, supervised
machine learning techniques that require a gold standard training set with
predefined classes, clustering is used when we have no prior knowledge
of the objects to be classified. Clustering is not a specific algorithm, but the
task to be solved.
Various clustering algorithms and proximity functions
exist, and the choice of algorithm is largely based on the data to be classi-
fied.
Clustering algorithms can be divided into two major categories; flat clus-
tering and hierarchical
clustering.
Flat clustering is the basic approach
where the number of clusters,
K,
is specified prior to the clustering pro-
cess.
The results are K clusters of related objects.
Hierarchical clustering
generates a hierarchical structure of clusters,
where the clusters become
more fine grained for each level down in the hierarchy. At a given layer of
hierarchical clustering,
the clusters are similar to the clusters of flat clus-
tering at the K in that point of the hierarchy. Figure 2.3 shows an example
of the layers in a hierarchical clustering dendrogram. Notice how the clus-
ters become less fine-grained (i.e. the clusters tolerate a higher distance be-
tween their members) for each layer from the bottom and upwards.
Each
layer makes up K clusters.
14
Both flat clustering and hierarchical clustering have proven to be useful
in the creation of clusters used as features in dependency parsing.
In this
project, we will take a different approach to using distributional semantics
as features to statistical dependency parsing.
In recent studies by Dyer
et al.
(2015) and D.
Chen and Manning (2014),
using pre-trained word
embeddings directly as features has proven to be successful in the training
of parsers.
We will train our embeddings using the models proposed by
Mikolov, Chen, et al. (2013) and Ling et al. (2015), and use these as features
in the parser.
2.3.2
Word embeddings as features in dependency parsing
As mentioned in chapter 2.1.3, the CBOW model learns vectors to predict
a word from a context window w, whereas the skip-gram model does the
inverse;
it takes a word and predicts its context window w.
Bansal et al.
(2014) found that the size of w affects the embeddings substantially.
For
high w values, words seemed to group with other words that shared the
same topic (e.g.
king and queen).
For low w,
words grouped with words
that shared the same POS tag (e.g.
king and car).
The study found that
training the CBOW and skip-gram models on dependency context (parent
words in dependency graphs from the training data) instead of linear con-
text in text, made the embeddings better fit to be features in dependency
parsing.
Bansal et al.
(2014) conducted parsing experiments with the MSTParser
using continuous representations with two different approaches, and com-
pared them to Brown clusters (as described by Koo et al. (2008)).
The first
approach,
called bucket features,
consisted of creating "one indicator fea-
ture per dimension of each embedding vector, where the feature consists
of the dimension index d and a bucketed version of the embedding value
in that dimension". The second approach is called cluster bit string features.
Agglomerative hierarchical clustering was performed to take into account
all dimensions of the embeddings simultaneously. Ward’s minimum vari-
ance algorithm was used as cluster distance metric, and euclidean distance
was used to measure distance between vectors.
The experiments conducted by Bansal et al. (2014) showed that bucket fea-
tures generally do not improve accuracy in parsing, but cluster bit string
features showed statistically significant improvements.
Furthermore, the
Brown clusters did better at attaching proper nouns,
prepositions,
and
conjunctions,
while CBOW did better on plural common nouns and ad-
verbs.
As focus recently has shifted from using clusters based on word embed-
15
dings, to using word embeddings directly as features in dependency pars-
ing.
Two parsers, Dyer’s parser(LSTM-parser) (Dyer et al., 2015), further
described in 2.3.6,
and UDPipe (Straka et al.,
2016),
further described in
2.3.7, will be used in this project.
2.3.3
Word embeddings and word order
As the embeddings created by CBOW and skip-gram do not take word
order into consideration,
they have a weakness when used for order-
sensitive tasks such as POS-tagging and syntactic parsing.
Two modifi-
cations to the traditional CBOW and skip-gram models have been made
by Ling et al. (2015), with the goal of making the embeddings more suited
for these tasks,
while maintaining the simplicity and low computational
cost of the original models.
In their study, they present two new models,
the Structured Skip-gram Model and the Continuous Window Model.
Whereas the traditional skip-gram model uses a single output matrix to
predict every contextual word given the center word, the structured skip-
gram model proposed by Ling et al. (2015) defines a set of output predic-
tion matrices. Each output prediction matrix is dedicated to predicting the
word in its position relative to the center word.
When making a predic-
tion, the appropriate matrix is selected to predict a word given its position
relative to the centre word.
Similarly,
the traditional
CBOW model
uses a single prediction matrix,
which is fed with the concatenation of
the embeddings of
the context
words.
The Continuous Window Model proposed by Ling et al.
(2015)
defines a different output prediction matrix which allows the words to be
treated differently depending on where they occur in relation to the target
word.
Compared to the traditional skip-gram and CBOW models, the modifica-
tions proposed by Ling et al. (2015) do not increase the computational cost
of calculating probabilities.
Due to the increasing number of parameters
of the prediction matrix,
the models are more prone to issues with data
sparsity than the traditional models when trained on small datasets,
but
generally this is not a big problem,
as these models are typically trained
on datasets in the order of 100 million words.
Ling et al.
(2015) conducted experiments with the structured skip-gram
and continuous window models for both POS-tagging and dependency
parsing. In the dependency parsing experiments, the models were trained
on the English Penn Treebank, using the neural network parser introduced
by D. Chen and Manning (2014).
The embeddings were appended to the
16
feature vectors directly, and no bucket features or clusters were used. The
results of the experiments showed improvement over the traditional skip-
gram and CBOW models proposed by Mikolov, Chen, et al. (2013).
2.3.4
A neural network approach to dependency parsing
D. Chen and Manning (2014) addressed that current dependency parsers
classify based on millions of sparse indicator features which generalise
poorly,
and restricts parsing speed.
To solve this problem,
they con-
structed a parser based on a neural network classifier. In contrast to tradi-
tional parsers, this parser does not need to compute conjunction features
and look them up in a huge feature table.
In each step,
the neural net-
work parser extracts the corresponding word,
POS-tag and embedding
from its configuration.
It then uses matrix operations to pick the transi-
tion with the highest probability.
The parser was tested and compared to
MSTParser (McDonald et al.,
2005)) and MaltParser (Nivre et al.,
2006),
and the results showed that it outperformed both in terms of labelled and
unlabelled accuracy score and speed, when trained and tested on the Penn
treebank.
2.3.5
Recurrent neural networks
A problem that arises with the use of feed-forward neural
networks in
dependency parsers, such as the one proposed by D. Chen and Manning
(2014),
described in 2.3.4 is the constant number of inputs and outputs.
Whereas traditional neural networks, as described in chapter 2.1.1, use a
fixed-length input,
a set of hidden nodes,
and a fixed-length output,
the
recurrent neural network is fed a sequence of inputs, and the output is a
sequence corresponding to the length of the input.
Unlike traditional feed-forward neural networks, the recurrent neural net-
work typically has only a single hidden layer.
The advantage of recur-
rent neural networks, and the reason why they are able to handle varying
length input, is that the hidden layer of each input is fed forward through
a set of weights,
and used as an additional input to the hidden layer of
the next input. The hidden layer of recurrent neural networks can then be
viewed as a sequence, where the output depends on the previous layer.
This complicates the calculation of error,
and thus making backpropaga-
tion a more complex task than in standard feed-forward networks.
An
algorithm known as backpropagation through time calculates the error in
each time step, and adjusts both the weights for the hidden layer and the
between-time steps layer.
17
h
t
h
0
y
1
v
1
h
2
y
2
v
2
h
3
y
3
v
3
h
4
y
4
v
4
h
5
y
5
v
5
Figure 2.4:
Example RNN with one layer.
Each v
i
∈
V denotes the input
vectors,
each y
i
∈
Y denotes the output in each time step.
Horizontal
arrows denote multiplication of the weights h
i
→
h
i
+
1
.
Traditional
recurrent neural
networks suffer from a problem known as
the vanishing gradient problem.
Many solutions to the vanishing gradient
problem have been proposed,
such as Elman and Jordan networks,
echo
state networks and neural history compressors.
The current state-of-the-
art recurrent neural network is known as the long short-term memory net-
work.
Recently,
a dependency parser based on long short-term memory
networks has been proposed by Dyer et al. (2015).
This parser will be fur-
ther described in chapter 2.3.6.
2.3.6
Dependency parsing using stack long short-term
memory
Dyer et al.
(2015) presented a new way of utilising neural
networks in
transition-based dependency parsing.
Where traditional neural network
models often suffer from the vanishing gradient
problem,
Dyer’s parser
trains a variant of recurrent neural nets (RNNs) called a long short-term
memory neural
network,
first
presented by Hochreiter and Schmidhuber
(1997).
Backpropagation through traditional neural networks is done by comput-
ing gradients by the chain rule, and when the backpropagation reaches the
"front" nodes, the gradient becomes small making training of these nodes
very slow.
In recurrent neural networks,
where each time step creates a
new layer in the network, this might lead to a significant loss of informa-
tion. This problem is known as the vanishing gradient problem.
In order to solve this problem, Hochreiter and Schmidhuber (1997) intro-
duced the long short-term memory neural network,
placing read,
write and
keep gates, which are opened and closed depending on the input, on each
18
node in the network to control their behaviour. These gates allow nodes to
remember information for a longer time than the traditional nodes, which
are always read and written back to during backpropagation.
Dyer’s parser,
like Nivre et
al.
(2006),
uses a stack based architecture.
The difference between them is how probabilities are calculated. Whereas
Nivre et al.
(2006) uses an SVM classifier to predict transitions,
Dyer’s
parser constructs the representation used to predict transitions incremen-
tally for each time step,
using three stacks of long short-term memory
units.
Two stacks are similar to the stacks in MaltParser;
the stack and
the buffer. The third stack stores the previous actions taken by the parser.
2.3.7
A trainable NLP pipeline
Researchers at the University of Prague, Czech Republic, have developed
a tool dubbed UDPipe,
intended to streamline the processing of natural
languages, from tokenisation to dependency parsing (Straka et al., 2016).
UDPipe was built to process Universal Dependency
4
treebanks without the
need for any external
data,
but
it
works well
for all
treebanks in the
CoNLL-U format.
This project focuses on statistical dependency parsing,
and thus we are
most interested in this part of the UDPipe pipeline.
The parser used in
UDPipe is the Parsito Neural Network parser,
presented by Straka,
Ha-
jic,
Straková,
and Hajic jr (2015).
Parsito is a transition based depen-
dency parser, and utilises the right-arc, left-arc and shift operations, as well
as a stack datastructure,
similar to MaltParser (Nivre et
al.,
2006)
and
other projective arc-standard systems.
Attardi (2006) presented a parser
with additional stack operations to this system making arcs between non-
adjacent
subtrees possible,
allowing the parser to build non-projective
graphs.
Gómez-Rodriguez,
Sartorio,
and Satta (2014) presented another
parser where a restriction the operations presented in Attardi (2006) were
allowed,
making non-projectivity to a certain degree possible,
while re-
taining the O
(
n
2
)
computational time of arc-standard parsers. Parsito uses
the same set of operations as the Gómez-Rodriguez et al. (2014) parser.
2.3.8
Word embeddings in UDPipe and Dyer’s parser
Both UDPipe (Straka et al., 2016) and Dyer’s parser (Dyer et al., 2015) use
internally trained word embeddings as features in the dependency pars-
ing. Additionally, both parsers accept pre-trained word embeddings, such
as word2vec (Mikolov, Chen, et al., 2013) and wang2vec (Ling et al., 2015)
4
Universal
Dependencies
introduction:
http://universaldependencies.org/
introduction.html
19
embeddings.
In Dyer’s parser, words are represented by the concatenation of three vec-
tors;
a learnt representation for each word type,
a fixed representation
from a neural network language model (see chapter 2.1.2), and learnt rep-
resentation of the POS tag of the token. By default, Dyer’s parser uses the
provided word embeddings directly as features for each word occuring
both in the embeddings and the training data (Dyer et al., 2015).
UDPipe’s internal structure is a neural network based on the neural net-
work parser presented by D.
Chen and Manning (2014).
The input to
the network consists of nodes representing words in the tree being built
(Straka et al., 2015).
The nodes in the network are represented by embed-
dings of its form,
POS tag and arc-label (if applicable).
The embeddings
are initialised with random values and trained on the treebank data to-
gether with the network.
It is possible to provide the parser with pre-
trained form embeddings.
When pre-trained word embeddings are pro-
vided, these could either be used directly as form embeddings, or be used
as a basis for the training of the internal form embeddings.
Along with
this option, UDPipe offers a wide range of hyperparameters which can be
tuned for better result
5
.
Exploring these parameters is out of the reach of
this project.
In this project we will focus on the task of improving dependency pars-
ing of Norwegian by training embedding models,
which in turn will be
used as input to the dependency parsers.
2.4
Dependency parsing of Norwegian
An annotated text corpus, called a treebank, is required in order to train and
test a dependency parser.
During training, the parser learns relationships
based on the annotation,
and in order to evaluate the results of a parse,
we need an annotated gold set to compare our results to.
In this project,
we will
use the recently published Norwegian Dependency Treebank
6
(Solberg et al.,
2014),
which is incorporated in the Språkbanken resource
catalogue.
2.4.1
The Norwegian Dependency Treebank
A treebank,
is needed to train a data driven dependency parser.
Until
recently,
no treebank for Norwegian language was available.
As depen-
5
UDPipe user’s manual: https://ufal.mff.cuni.cz/udpipe/users-manual
6
Språkbanken
dependency
treebank:
http://www.nb.no/sprakbanken/show?
serial=oai%3Anb.no%3Asbr-10&lang=nb
20
Source
Percentage of NDT sources
Newspaper articles
82%
Government reports
7%
Parliament transcriptions
6%
Blog posts
5%
Table 2.2: Source ratio for NDT
8
dency parsing has gotten a lot of attention, and many dependency parsers
have become more available in the recent years, work was carried out by
Solberg et al. (2014) to construct a treebank for Norwegian.
When building a treebank, reference guidelines for the annotation process
have to be made.
The Norwegian Dependency Treebank has four funda-
mental principles
7
;
• Linguistic adequacy:
The annotation should be as linguistically
adequate as possible.
• Consistency:
It had to be possible for annotators to implement the
analyses consistently.
• Quick annotation: The annotators should be able to annotate quickly,
in order to cover a sizable amount of text.
• Easy retrieval:
It should be easy to retrieve specific constructions
after annotation.
As we can see from table 2.2, the Norwegian Dependency Treebank con-
sists mainly of newspaper text,
but includes government reports,
parlia-
ment transcriptions and blog posts. This has to be taken into consideration
when the treebank is used for dependency parsers trained using word em-
beddings – word embeddings trained using similar texts are more likely
to recognise words and contexts in this kind of material, and will be more
useful in the training of dependency parsers.
Looking at our example sentence in figure 2.5, we see that the finite verb
(INFV) "viser" and the auxillary verb "har" were selected as heads. Further
we see that the subjunctive "han" is dependent on the verb,
and that the
infinitive marker "å" is the head of the infinite verb.
We also see that the
7
List cites Solberg et al. (2014).
8
Information presented in Table 2 by Solberg et al. (2014).
21
Den
viser
at
han
har
trengt
å
komme
i
modus
.
It
shows
that
he
has
needed
to
get
into
competition-mode
.
INFV
SUBJ
SBU
SUBJ
DOBJ
INFV
DOBJ
INFV
ADV
PUTFYLL
IP
Figure 2.5:
Dependency graph of the example sentence from the develop-
ment set.
preposition "i" is the head of the object of the preposition "modus".
Further work on optimising a POS tag set targeted at the task of depen-
dency parsing for the Norwegian Dependency Treebank was proposed by
Hohle, Øvrelid, and Velldal (2017). This task was linguistically motivated,
and more fine-grained POS-tags containing more specific linguistic infor-
mation about each word were introduced.
On one hand,
this is good as
it adds further information about,
and distinction between,
every single
word in the corpus.
On the other hand,
this leads to more scarce infor-
mation regarding each POS, as fewer examples for each word class were
available in the dataset.
Hohle et al. (2017) showed an overall increase in
labelled accuracy in dependency parsing using a wide range of parsers to
evaluate the tagset.
Hohle et al.
(2017) also propose a data split consisting of a training set,
a development set and a test set,
with an 80-10-10 distribution.
In this
project we will use the data split proposed by Hohle et al. (2017), but not
the optimised tag set. As little research has been done using the optimised
tag set, we found it more feasible to use the standard tag set, in order to be
able to compare our results of existing and future studies using the Nor-
wegian Dependency Treebank.
The practical details regarding the usage of the Norwegian Dependency
Treebank in this project are described in chapter 3.2.1.
22
2.5
Intrinsic versus extrinsic evaluation of word
embeddings
This project focuses on the use of word embeddings as a tool to improve
dependency parsing of Norwegian language.
This is a downstream,
ex-
trinsic task.
Intrinsic evaluation,
on the other hand,
is simply a way of
evaluating language models in general.
Intrinsic evaluation of word em-
beddings is a vaguely defined task, as word embeddings are typically used
as tools to increase the precision in certain tasks, such as POS tagging and
dependency parsing.
Typically, when it comes to evaluation of word em-
beddings, a task would be considered intrinsic if it evaluates word embed-
dings in an isolated context (i.e. not as part of a larger task).
Intrinsic evaluation is often performed in order to evaluate the quality of
word embeddings, because it is quicker and less time consuming than ex-
haustively testing multiple embedding models in the actual task they are
trained for. Furthermore, intrinsic evaluation is instrumental in tasks such
as semantic analysis, where the relation between words in a word embed-
ding model is the primary data.
2.5.1
Intrinsic evaluation datasets
A number of tools and datasets for intrinsic testing of word embeddings,
such as SimLex-999 (Hill, Reichart, & Korhonen, 2016) and Google Analo-
gies (Mikolov,
Chen,
et al.,
2013),
exist for English.
The Google Analo-
gies set contains word quadruples,
and the intention is to evaluate how
well the embedding system performs at predicting the last element in each
quadruple.
The quadruples can be read as "v
1
is to v
2
as v
3
is to v
4
";
for
instance, one quadruple could be v
= [
Oslo, Norway, Stockholm, Sweden
]
.
This is typically done by v
2
−
v
1
+
v
3
=
v
4
,
as is presented by Mikolov,
Yih, and Zweig (2013).
The Google Analogies dataset is split into two major categories;
semantic
and syntactic analogies.
Each major category is further divided into sub-
categories.
Table 2.3 shows all subcategories, as well as a simple example
from each category
9
2.5.2
Evaluation of Norwegian analogies
As Norwegian is a much smaller language in terms of number of native
speakers and users (approximately 5 million), similar tools for Norwegian
are scarce resources.
Creation of tools aimed at intrinsic testing of word
9
Table 2.3 is a citation of Mikolov, Chen, et al. (2013).
23
Category
v
1
v
2
v
3
v
4
Semantic
Common capital city
Athens
Greece
Oslo
Norway
All capital cities
Astana
Kazakhstan
Harare
Zimbabwe
Currency
Angola
kwanza
Iran
rial
City-in-state
Chicago
Illinois
Stockton
California
Man-Woman
brother
sister
grandson
granddaughter
Syntactic
Adjective-to-adverb
apparent
apparently
rapid
rapidly
Opposite
possibly
impossibly
ethical
unethical
Comparative
great
greater
tough
tougher
Superlative
easy
easiest
lucky
luckiest
Present Participle
think
thinking
read
reading
Nationality adjective
Switzerland
Swiss
Cambodia
Cambodian
Past tense
walking
walked
swimming
swam
Plural nouns
mouse
mice
dollar
dollars
Plural verbs
work
works
speak
speaks
Table 2.3:
Categories of
the Google Analogies dataset.
The first
five
categories contain semantic questions, and the last nine contain syntactic
questions.
embeddings is currently being carried out in the MSc thesis of Stadsnes
(forthcoming) at the Language Technology Group, University of Oslo.
At
this point, an analogy dataset has been proposed.
The dataset proposed by Stadsnes (forthcoming) is similar to the Google
Analogies set proposed by Mikolov, Chen, et al. (2013), but as the Google
Analogies dataset contains a certain amount of content specific to English,
such as the city-in-state category, the task of creating a similar tool for Nor-
wegian is more complex than simply translating every single word.
The
categories in this dataset are presented in table 2.4.
This dataset will be used for intrinsic testing of word embeddings in this
project, and is further described in chapter 3.3.5.
Although our project aims to optimise embeddings used as parameters
for dependency parsers,
we found it interesting to run the embeddings
through the intrinsic analogy test proposed by Stadsnes (forthcoming).
The goal of the intrinsic testing presented in chapter 6 is to see how dif-
ferent embedding models perform, and to identify whether or not there is
a correlation between embeddings performing well as features for depen-
dency parsers and in the task of identifying analogies.
24
Category
v
1
v
2
v
3
v
4
Semantic
Common capital city
Athen
Hellas
Bagdad
Irak
All capital cities
Abuja
Nigeria
Accra
Ghana
Currency
Algerie
dinar
Angola
kwanza
City-in-county
Hønefoss
Buskerud
Stord
Hordaland
Man-Woman
gutt
jente
bror
søster
Syntactic
Adjective-to-adverb
munter
muntert
hel
helt
Opposite
akseptabelt
uakseptabelt
vitende
uvitende
Comparative
dårlig
dårligere
stor
større
Superlative
dårlig
dårligst
stor
størst
Nationality adjective
Albania
albansk
Argentina
argentinsk
Past tense
danser
danset
avtar
avtok
Plural nouns
banan
bananer
fugl
fugler
Plural verbs
avta
avtar
beskrive
beskriver
Table 2.4:
Categories of the Norwegian Analogies dataset proposed by
Stadsnes (forthcoming)
25
26
Chapter 3
Data and tools
In order to carry out the experiments presented in chapter 4,
5 and 6,
extensive work had to be done to prepare and preprocess the tools and
data used in this project.
In this chapter, we will discuss the preparation
of the tools and data, as well as the practical parts (parameters, technical
details, etc.) of the tools in use.
The theoretical background regarding the
methods underlying these tools is described in Chapter 2.
3.1
Unlabelled text
Unlabelled text is raw text without any annotation or additional informa-
tion.
It is typically gathered from various sources into corpora, which can
be used for a wide variety of purposes.
In this project, we use unlabelled
text to train word embeddings. By looking at a large amount of text, word
embedding tools such as word2vec (Mikolov, Chen, et al.,
2013) are able
to learn which words are similar to each other.
This is done by looking at
each individual word and its context, minimising the distance in the vec-
tor space to words occurring in similar contexts,
whilst maximising the
distance to all the other words in the vocabulary.
In this project we used
the Norwegian Newspaper Corpus and NoWAC corpora.
These corpora
will be described in this chapter.
3.1.1
The Norwegian Newspaper Corpus
The Norwegian Newspaper Corpus (NAK) is a compilation of text gath-
ered continuously from Norwegian newspaper articles.
The corpus is
available as a free download at the Norwegian National Library (Nasjon-
albiblioteket) website.
With over 1 billion words for Norwegian Bokmål
and 60 million words for Norwegian Nynorsk, it is the largest corpus of its
kind.
Tokenised texts are available for newspaper articles dated between
1998 and 2011.
Newspaper articles from 2012 through 2014 are available
as XML documents.
In this project we have extracted the raw text from
27
these XML files using our own Python script, and prepared the text for the
training of embeddings using wang2vec (Ling et al., 2015), a reimplemen-
tation of word2vec (Mikolov, Chen, et al., 2013) with extensions.
The concatenation of the pre-tokenised 1998-2011 files and the 2012-2014
files contains 1,291,982,180 tokens,
and has a vocabulary of
8,671,622
unique words.
3.1.2
NoWAC - Norwegian Web as Corpus
The Norwegian Web Corpus (NoWAC) (Guevara,
2010) is a web corpus
for Norwegian Bokmål,
developed at the Department of Linguistic and
Scandinavian studies at the University of Oslo. The corpus was created by
crawling top level
.no
internet domains in the period between November
2009 and January 2010.
NoWAC is the Norwegian contribution to the WaC Corpora Collection,
a multilingual collection of web corpora developed using the Corpus Fac-
tory method (Kilgarriff, Reddy, Pomikálek, & Avinesh, 2010). The method
used to gather the data used in NoWAC is based on the work of Ferraresi,
Zanchetta,
Baroni,
and Bernardini (2008);
first a group of seed URLs con-
taining information in the target language is collected by querying popular
search engines.
The acquired documents are then used to start a crawling
of sub-domains and hyperlinks, limited to the .no domain (Guevara, 2010).
Although Norway has two official national written standards, Bokmål and
Nynorsk, the web contains a lot of unrecognised words, such as words in-
fluenced by dialect and slang words.
Due to the complex linguistic sit-
uation in Norway,
steps had to be taken to identify the language of the
websites crawled. The target of NoWAC was to build a corpus in the Bok-
mål language, mixed with some "spoken language", making up a corpus
of combined written and spoken language.
NoWAC is distributed as a single file,
consisting of sentences one word
per line and with start of sentence and end of sentence tags.
Tokenisation
of NoWAC was carried out by Nico Kuijlen, MSc student at the language
technology group, to prepare this data for wang2vec.
3.1.3
Merging the corpora
After preprocessing the corpora,
they were merged into a single corpus.
As the Norwegian Dependency Treebank mostly consists of
text
from
newspaper sources,
as we can see from table 3.2,
we did not
conduct
experiments using only NoWAC for the training of word embeddings.
28
Dataset
Tokens
Unique tokens
NAK
1,527,414,380
8,671,622
NoWAC concat.
2,217,487,419
12,156,743
Table 3.1:
Number of tokens for the Norwegian Newspaper Corpus and
NoWaC
Source
Percentage of NDT sources
Newspaper articles
82%
Government reports
7%
Parliament transcriptions
6%
Blog posts
5%
Table 3.2: Source ratio for NDT
1
NoWAC was concatenated to the Norwegian Newspaper Corpus increase
the size of the corpus, possibly providing some extra detail.
The resulting
token counts of the Norwegian Newspaper Corpus and the concatenation
are presented in Table 3.1.
3.2
Labelled text
In order to train a data driven parser for a language, we need a treebank.
A treebank is an annotated dataset.
It typically contains information such
as form,
lemma,
POS-tag,
morphological and/or syntactical features for
each word, as well as its relation to other words within the sentence.
The
parser uses this information to learn how a language is constructed, which
is essential in the task of dependency parsing.
In this project we use the
Norwegian Dependency Treebank (Solberg et al., 2014). This treebank will
be described in chapter 3.2.1.
3.2.1
Norwegian Dependency Treebank
The Norwegian Dependency Treebank (NDT), described in chapter 2.4.1,
was developed at the Norwegian National Library in collaboration with
linguists and computational linguists from the University of Oslo.
It is a
syntactic treebank for Bokmål
and Nynorsk with manual
syntactic and
morphological
annotation (Solberg et
al.,
2014).
The treebank consists
mainly of newspaper text,
but includes government reports,
parliament
transcriptions and blog posts (see table 3.2).
The Norwegian dependency treebank uses the CoNLL-X format to en-
29
Tag
Explanation
ID
Word index.
FORM
Word form or punctuation.
LEMMA
Word lemma or stem
CPOS
Coarse grained part-of-speech tag
POS
Fine grained part-of-speech tag
FEATS
Morphological and/or syntactical features
from a language specific extension.
HEAD
Index of the word’s parent.
0 if the parent
is root.
DEPREL
Dependency relation to the HEAD.
PHEAD
Projective head of current token.
PDEPREL
Dependency relation to the PHEAD.
Table 3.3: Explanation of the CoNLL-X data fields column-wise
2
code dependency graphs, and can therefore be used without any modifica-
tions in modern dependency parsers, such as Dyer’s parser and UDPipe.
In the CoNLL-X format,
each line represents a word,
and sentences are
separated by empty lines. Word features are fixed order and tab-separated.
In order to use the treebank to train a parser,
it has to be split it into a
training set, a development set, and a testing set.
A split for the Norwe-
gian Dependency Treebank was proposed out by Hohle et al.
(2017),
in
the context of their work on optimising a POS-tag set for Norwegian. The
split divides the treebank into a training set containing 15696 sentences, a
development set containing 2410 sentences, and a test set containing 1939
sentences. In this project we aim to use this split for training, development
and testing,
since it makes it possible to compare our results to previous
work.
Table 3.3 shows an example sentence taken from the training set.
In our
treebank,
the DEPS and MISC fields are always empty,
and have been
omitted from this example for readability.
The dependency graph rep-
resenting this sentence is shown in Figure 3.1.
The Norwegian Dependency Treebank is distributed as a set of folders,
each containing CoNLL-X formatted files sorted by source.
The datasets
proposed by Hohle et al. (2017) is an 80-10-10 split of the Norwegian De-
1
Information presented in Table 2 by Solberg et al. (2014).
2
CoNLL-U format: http://universaldependencies.org/format.html
30
ID
Form
Lemma
CPOS
POS
Feats
Head
Dep
1
Den
den
pron
pron
mask(...)
2
SUBJ
2
viser
vise
verb
verb
pres
0
FINV
3
at
at
sbu
sbu
_
5
SBU
4
han
han
pron
pron
mask(...)
5
SUBJ
5
har
ha
verb
verb
pres
2
DOBJ
6
trengt
trenge
verb
verb
perf-part
5
INFV
7
å
å
inf-merke
inf-merke
_
6
DOBJ
8
komme
komme
verb
verb
inf
7
INFV
9
i
i
prep
prep
_
8
ADV
10
modus
modus
subst
subst
appell(...)
9
PUTFYLL
11
.
$.
clb
clb
<punkt>
2
IP
Table 3.4:
Example sentence from the development
set
in CoNLL-X
format, last two fields omitted.
Den
viser
at
han
har
trengt
å
komme
i
modus
.
It
shows
that
he
has
needed
to
get
into
competition-mode
.
ROOT
SUBJ
SBU
SUBJ
DOBJ
INFV
DOBJ
INFV
ADV
PUTFYLL
IP
Figure 3.1:
Dependency graph of the example sentence from the develop-
ment set.
pendency Treebank, balancing the data in terms of genre.
In order to make the treebank usable with our word embeddings,
simi-
lar normalisation had to be done to the treebank, as had been done to the
corpora used to train embedding models.
This normalisation is described
in chapter 3.2.2.
3.2.2
Normalisation of the corpora and treebank
It is common practice when working with word embeddings to replace
every numeric digit by a placeholder.
To retain information about the
number of digits in a number,
every single digit,
and not every number,
was replaced
NUM
. For instance,
the number 5 would be replaced by
NUM
,
whereas 22 would be replaced by
NUMNUM
.
In order to make the parsers
recognise the embeddings associated these placeholders,
the same nor-
malisation was done to the dependency treebank.
31
3.3
Tools
Quite a bit
of
work and code re-engineering had to be carried out
in
order to make the tools used in this project to work in our computing
environment. In this section we will discuss the preparation of the tools, as
well as a brief discussion regarding each tool and its usage in this project.
3.3.1
wang2vec and word2vec
wang2vec is a tool created by Ling et al. (2015),
built on top of Google’s
word2vec embedding toolkit (Mikolov,
Chen,
et al.,
2013).
As described
in chapter
2.3.3,
wang2vec adds two embedding models called Struc-
tured Skip-gram and Continuous Window.
The tuning options available in
wang2vec are presented in Table 3.5.
wang2vec is distributed as open-source software on GitHub
3
.
The only
requirement for installing the software is a valid C compiler.
Once the
software is installed,
it is possible start training word embeddings right
away with the
word2vec
program, an expansion of the original word2vec
adding two additional
embedding models,
CWindow and Structured
Skip-gram.
Along with
word2vec
,
the repository features various tools
for generating dashed phrases,
calculating cosine distance and more,
all
of which we do not make any use of in our project.
3.3.2
Long Short-Term Memory Parser (Dyer’s parser)
As described in 2.3.6,
Dyer’s parser is a syntactical
dependency parser
using stacks of long short-term memory neural networks to predict transi-
tions. It offers support for using pre-trained word embeddings as features,
and various tuning options are available. The available parameters are de-
scribed in Table 3.6.
Dyer’s parser is distributed as open source software on GitHub
4
.
Along
with a C++11 compiler, this software depends on a submodule called CNN
(which is actually short for C++ Neural Networks), the Boost library, as well
as the linear algebra C++ template library called Eigen.
3
https://github.com/wlin12/wang2vec
4
https://github.com/clab/lstm-parser
32
Parameter name
Description
Dimensions
Size of the embedding vectors (default is
100).
Window size
Default is 5 for CBOW, 10 for Skip-gram.
Sample
Threshold for occurence of words,
where
those appearing with higher frequency in
the training data will be randomly down-
sampled.
The useful range is (0,
1e-5) and
the default value is 1e-3,
according to the
manual.
HS
Whether or not to use hierarchical softmax
(off by default).
Negative
Number of
negative examples;
default
is
5,
common values are 3 - 10 according to
the manual, but Mikolov, Sutskever, Chen,
Corrado, and Dean (2013) states that values
between 2 and 20 might be feasible.
NCE
Number of negative examples for nce;
de-
fault is 0, common values are 3 - 10 (0 = not
used).
Iterations
How many iterations to run during train-
ing (default value is 5).
Min-count
Frequency
cutoff/minimum
occurence
threshold for words (default value is 5).
Alpha
Alpha/learning rate.
Default is 0.025 for
skip-gram and 0.05 for CBOW.
Type
Type
of
the
embeddings
(CBOW/Skip-
gram/CWindow/SSkip-gram).
Cap
Limit the parameter values to the range [-
50, 50]; default is 0 (off)
Table 3.5: Tuning options for word2vec and wang2vec.
Initially we had some trouble with the memory handling in Dyer’s parser.
It quickly became obvious that this code was written for research pur-
poses, and in order to make it work on our HPC cluster we had to make
an overhaul of the source.
After many hours of trial and error, and a cou-
ple of dialogues with the code authors on issues filed on GitHub, we found
that this was caused by an integer overflow in the memory allocation mod-
ule of the CNN submodule.
After searching through the code for errors,
we found that changing the datatype in the memory allocation module to
size_t
solved the problem.
Dyer’s parser depends on the user to send the interrupt signal to the pro-
33
Parameter name
Description
–use_pos_tags
Make POS tags visible to parser
–layers
Number of LSTM layers (2 by default)
–action_dim
Action embedding size (16 by default)
–input_dim
Input embedding size (32 by default)
–hidden_dim
Hidden dimension (64 by default)
–pretrained_dim
Pretrained input dimension (50 by default)
–pos_dim
POS dimension (12 by default)
–rel_dim
Relation dimension (10 by default)
–lstm_input_dim LSTM input dimension (60 by default)
Table 3.6: Tuning options for Dyer’s parser, from the help screen
cess in order to stop training.
As we conduct our experiments on the Abel
supercomputer at the University of Oslo,
which uses a workload man-
ager (Slurm) in order to start and end jobs,
interacting with the process
this closely proved impossible.
We also found it reasonable to end all our
parsing experiments after the same number of iterations for better com-
parability.
In order to solve these issues,
we rewrote the source code of
Dyer’s parser to allow for stopping after a given number of iterations.
3.3.3
UDPipe
As described in 2.3.7,
UDPipe offers a trainable pipeline for tokenisa-
tion,
tagging and parsing of multilingual
text (Straka et al.,
2016).
As
the aim of this project is to explore how different embedding models and
parameters affect dependency parsing, we will focus on the dependency
parser Parsito,
embedded in UDPipe’s pipeline.
The UDPipe software is
open sourced,
and can be downloaded from GitHub
5
,
and an extensive
user’s manual is offered by the Faculty of Formal and Applied Linguistics,
Charles University, Czech Republic
6
.
In this project, UDPipe will be used as one out of two dependency parsers,
for comparison with the results yielded by Dyer’s parser (Dyer et
al.,
2015).
As our goal is to investigate the effect of tuning the parameters of
embeddings which will be used as input representations in dependency
parsers,
using two different parsers helps us distinguish parser-specific
properties from general improvement.
UDPipe offers a wide variety of hyperparameters available for tuning. As
UDPipe serves both as a tokeniser, tagger and dependency parsers, most
5
UDPipe download: https://github.com/ufal/udpipe
6
UDPipe user’s manual: https://ufal.mff.cuni.cz/udpipe
34
parameters are out of the reach of this project. Table 3.7 describes the avail-
able parameters related to dependency parsing in UDPipe.
Interestingly, the embeddings used by Dyer et al. (2015) and Straka et al.
(2016) are different.
Dyer et al. (2015) used Structured Skip-Gram embed-
dings with a dimensionality of 100, whereas the example script for UDPipe
uses standard (non-structured) Skip-Gram embeddings with a dimension-
ality of 50.
Furthermore,
no research has,
to the best of our knowledge,
been carried out using the embeddings of
wang2vec
as features in the UD-
Pipe dependency parser.
We will experiment with various word embed-
dings in order to find the optimal settings for Norwegian.
UDPipe is available as a library for C++,
Python,
Perl,
Java,
C#,
and as
a web service, on GitHub
8
.
Compared to Dyer’s parser, the code of UD-
Pipeis much more polished and user-friendly.
We chose to download the
C++ library, and installation went smoothly, only requiring an up-to-date
C++ compiler.
UDPipe can parse input as raw text, files in the Stanford Dependency for-
mat,
as well as CoNLL-U files.
Our treebank is in the CoNLL-X format,
and we were initially concerned that this would lead to compatibility is-
sues.
We were relieved to find that UDPipe accepted our CoNLL-X for-
matted files,
due to the fact that we do not use the fields specific to the
CoNLL-U format.
3.3.4
Dependency parser evaluation software
In order to evaluate the parser results, they have to be compared to a gold
standard dataset. Several scripts and applications are available to perform
this task.
During the 2006 and 2007 CoNLL shared tasks,
a script called
eval.pl
was used to evaluate the dependency parsers of the participants.
In order to evaluate and compare different parsers, it is crucial that the ex-
act same script and metrics are used. In this project, we are using
MaltEval
(Nilsson & Nivre, 2008), as this is the default evaluation software for UD-
Pipe.
MaltEval
is an evaluation tool written in Java.
It is able to combine both
quantitative and qualitative evaluation,
and on top of the features pro-
vided in
eval.pl
it includes many different metrics and features, such as
a tool used to display parse trees in a graphical user interface (Nilsson &
Nivre, 2008).
In this project we limit the evaluation to labelled and unla-
beled attachment scores. In opposite to
eval.pl
,
MaltEval
includes punc-
8
https://github.com/ufal/udpipe
35
Parameter name
Description
transition_system
Projective,
swap or link2 (described at the
UDPipe website
7
)
transition_oracle
Which transition oracle to use for the cho-
sen transition_system.
structured_interval (default 8)
use search-based oracle in addition to the
translation_oracle specified.
embedding_upostag (default 20)
the
dimension of
the
UPos
embedding
used in the parser
embedding_feats (default 20)
the dimension of the Feats embedding used
in the parser
embedding_xpostag (default 0)
the dimension of the XPos embedding used
in the parser
embedding_form (default 50)
the
dimension of
the
Form embedding
used in the parser
embedding_lemma (default 0)
the dimension of
the Lemma embedding
used in the parser
embedding_form_file
pre-trained word embeddings created by
word2vec can be optionally used
embedding_lemma_file
pre-trained lemma embeddings created by
word2vec can be optionally used
iterations (default 10)
number of training iterations to use
hidden_layer (default 200)
the size of the hidden layer
batch_size (default 10)
batch size
used during neural
network
training
learning_rate (default 0.02)
the learning rate used during neural
net-
work training
learning_rate_final (0.001)
the final learning rate used during neural
network training
l2 (0.5)
the L2 regularization used during neural
network training
Table 3.7: Tuning options for UDPipe.
tuation in the evaluation.
Unlabelled attachment score is given by the number of correct dependen-
cies in a parse tree.
If a dependency graph for a given parsed sentence
matches the dependencies in the parse tree given in the gold standard
dataset,
the score is 1.0,
or 100%,
regardless of the labels on each depen-
dency relation. Labelled attachment score, on the other hand, includes the
dependency labels in the score.
For instance,
if a sentence with 10 rela-
tions is parsed with all dependencies correctly attached according to the
36
gold standard, but one attachment labelled incorrectly, the labelled attach-
ment score would be 0.9, or 90%.
3.3.5
Intrinsic evaluation software
Although the focus of this project has been on developing word embed-
dings used as input to dependency parsers, we found it interesting to see
if there was a correlation between embeddings performing well as input
to dependency parsers, and in intrinsic tests. Intrinsic evaluation is a mea-
sure of how well embeddings perform in the task of recognising similarity
between words and recognising analogies, in an isolated setting.
The the-
oretical background on intrinsic evaluation is featured in chapter 2.5.
In this project,
we will use a Norwegian analogy dataset recently devel-
oped by Cathrine Stadsnes at the Language Technology Group, University
of Oslo, as part of her forthcoming MSc project. The dataset is a translation
of the Google Analogies dataset proposed by Mikolov, Chen, et al. (2013),
with some slight modification to fit with language-specific analogies.
The
dataset contains 5 categories of semantic analogy tests and 9 categories of
syntactic analogy tests.
In order to use the dataset, we use Gensim (Rehurek & Sojka, 2011) to load
our word embeddings.
Gensim features functions specifically written to
evaluate embeddings using analogy datasets, and computes the accuracy
of the embeddings in recognising the correct analogies.
3.3.6
HPC Environment
The experiments performed in the context of this thesis are computation-
ally intensive by nature.
Initially we used a local server at the Language
Technology Group for our experiments.
This machine has 40 cores and
400G of working memory. After performing a few experiments, we found
that the computational resources required by wang2vec were too heavy
for this server to handle.
A single wang2vec process using our configu-
ration and corpus consumes approximately 120G of ram and occupies 11
cores.
Considering that training a single wang2vec model takes approxi-
mately 2 days, we needed to be able to run multiple wang2vec processes
in parallel.
As we are not the only ones using this server, exhausting the
entire RAM and all cores was not an option.
In order to solve the problem of computational cost, we had to move our
projects to the Abel high-performance computer. This computer is the sec-
ond largest supercomputer in Norway, only surpassed by the Vilje cluster
at NTNU. It consists of more than 10K cores, distributed on more than 650
37
nodes.
All nodes have a minimum of 64G of memory,
with a total of 40
TebiBytes across all nodes in the cluster. In addition to the standard nodes
with 64G of memory,
there are also a few hugemem nodes with 1TB of
memory and 32 cores.
Moving all our data and reinstalling the tools required a lot of attention
and consumed a fair bit of time.
Additionally,
we had to learn how to
use the cluster’s queue system, Slurm, which uses batch files for each pro-
cess.
We quickly found that all training of wang2vec models and Dyer’s
parser parsing required more memory than the standard 64G nodes had
to offer, requiring us to use the hugemem nodes. This further delayed our
experiments throughout the entire project, as hugemem nodes are a scarce
resource with long queues.
38
Chapter 4
Baseline and dataset experiments
In this chapter we will describe the experiments we have carried out in
order to choose the best dataset for the training of word embeddings used
as features in dependency parsing of Norwegian.
In the first part of this
chapter, we describe our experimental methodology.
We then proceed to
describe our experiments regarding choice of dataset.
After settling with
a dataset, we proceed to tune our word embeddings in the next chapter.
4.1
Tuning workflow
Our experiments will follow a three-step workflow.
First we will exper-
iment with the default embedding and parser settings and focus on the
dataset
in use.
As we have both the the Norwegian Newspaper Cor-
pus and the Norwegian Newspaper Corpus+Norwegian Web as Corpus
(NoWAC)
concatenation available,
we wish to find out
which corpus
yields the optimal embeddings for dependency parsing,
and why.
Due
to the nature of the corpus,
we decided not to train embeddings using
NoWAC only; both our treebank and the Norwegian Newspaper Corpus
consists mainly of text from newspaper sources, whereas NoWAC consists
purely of text crawled off the web.
Dataset
Embedding tuning
Intrinsic evaluation
Figure 4.1: Tuning workflow
Once we have settled with one of our corpora, we will continue to perform
experiments with various embedding tunings.
We will see how different
embedding models and various hyperparemeters such as dimensionality,
window size, number of negative examples, and frequency cutoff affects
the parsing results, before evaluating our setup using the held-out test set
from the Norwegian Dependency Treebank.
39
Dyer’s parser
UDPipe
Type
Structured skip-gram Skip-gram
Dimensionality
100
50
Window size
5
10
Negative examples
5
5
Negative examples for nce
5
0
Hierarchical softmax
Off
Off
Sample
1e-3
1e-1
Frequency cutoff
5
2
Parameter value limit (cap)
Off
Off
Iterations
5
15
Table 4.1: Standard embedding tunings for Dyer’s parser and UDPipe
In chapter 6 we will take a closer look at the embeddings performing well
in dependency parsing, and evaluate them intrinsically to see if there is a
correlation between embeddings well suited for this task, and their intrin-
sic scores.
4.1.1
Establishing a baseline
As research has been done to optimise both Dyer’s parser and UDPipe in
a multilingual setting, we will start out with settings replicating those in
use by (Dyer et al., 2015) and (Straka et al., 2016) in the training of word
embeddings.
Calling the results produced a "baseline" is a bit unfair,
as
much work has been put into optimising the embeddings by researchers
already.
Since experiments with Norwegian have not been conducted in
the case of Dyer’s parser, we make the qualified assumption that the set-
tings used for English will be sufficient for Norwegian as well.
The standard settings used for word embeddings are presented in 4.1. We
see that Dyer’s embedding settings use the structured skip-gram model
and 100 dimensions, whereas UDPipe embedding settings use skip-gram
and 50 dimensions. Furthermore, Dyer uses 5 negative examples for NCE,
a frequency cutoff of 5, and 5 iterations, where UDPipe uses a frequency
cutoff of 5, 15 iterations, and does not use NCE.
4.1.2
Parsing without pre-trained word embeddings
For comparison with future results,
we have produced parses without
pre-trained word embeddings as features using both parsers.
This will
give us a hint at how much improvement the use of pre-trained word
embeddings yields in dependency parsers using neural
networks.
The
40
Parser
LAS
UAS
Dyer’s parser
87.05% 89.79%
UDPipe
88.41% 90.49%
Table 4.2: Results without word embeddings
results are presented in table 4.2. We see that Dyer’s parser gives a labelled
accuracy score of 87.05%,
and UDPipe gives a labelled accuracy score of
88.41%.
These scores are fairly low when compared to those of Dyer et al.
(2015) and Straka et al.
(2016),
where pre-trained word embeddings are
used.
4.2
Dataset experiments
4.2.1
Discussion
We will now experiment with two different datasets for the tuning of word
embeddings in this project; the Norwegian Newspaper Corpus and a con-
catenation of the Norwegian Newspaper Corpus and NoWAC. Both cor-
pora have been tokenised,
and normalised to some degree (replacing all
numerals with
NUM
, as is described in chapter 3.2.2).
It is intuitive to think
that the results will only get better when you train the embeddings using
more data.
However,
as NoWAC was created by crawling the web,
we
are uncertain of how well it represents "good" Norwegian language.
Fur-
thermore,
our treebank consists mainly of newspaper text,
possibly ren-
dering parts of NoWAC irrelevant for our purpose.
Another concern is
noise generated by online slang, usernames from forum posts, and other
features of online text which are not present in the dictionary of the tree-
bank.
A way of avoiding this kind of noise is to increase the frequency
cutoff when training embeddings.
A concern of increasing the cutoff too
much, however, is that we might lose information as well, which will lead
to less useful embeddings.
The first step in our pipeline of experiments is therefore to decide which
dataset we will proceed with in our further experiments. This will be done
by training embeddings using the Dyer/UDPipe baseline settings on both
corpora, and then parsing with the baseline settings, to see which dataset
yields the best results.
4.2.2
The Norwegian Newspaper Corpus experiments
Parsing with Dyer’s parser and UDPipe using standard-settings word
embeddings
(Dyer
et
al.,
2015;
Straka et
al.,
2016)
(see
Table
4.1)
41
Parser
LAS
UAS
Dyer’s parser
89.66% 91.60%
UDPipe
89.55% 91.32%
Table 4.3:
Baseline results with embeddings trained using the Norwegian
Newspaper
Corpus.
The settings for
Dyer’s parseris Dyer’s default
embedding settings. Settings for UDPipe are the settings suggested in the
embedding training script for UDPipe.
Parser
LAS
UAS
Dyer’s parser
89.50% 91.53%
UDPipe
89.63% 91.39%
Table 4.4: Baseline results for word embeddings trained on a concatenation
of
the Norwegian Newspaper
Corpus and NoWAC.
The settings for
Dyer’s parseris Dyer’s default embedding settings.
Settings for UDPipe
are the settings suggested in the embedding training script for UDPipe.
trained using only the Norwegian Newspaper Corpus lead to the results
presented in Table 4.3.
As we can see,
the results for Dyer’s parser and
UDPipe are quite similar, at about 89.6%.
4.2.3
Concatenation experiments
Parsing with Dyer’s parser and UDPipe using the suggested embedding
parameters for both parsers lead to the results presented in table 4.4.
As we can see by comparing the numbers in tables 4.3 and 4.4,
Dyer’s
parser yielded a slightly higher score when trained using embeddings
trained using NAK only.
For UDPipe,
the score is slightly higher when
trained using embeddings trained with a concatenation of
NAK and
NoWAC. With differences in score this low we find it feasible to proceed
using NAK only, as the computational cost of adding NoWAC cannot be
justified.
In order to validate the outcome of these experiments,
we per-
formed an experiment where we passed UDPipe embeddings to Dyer’s
parser and vice versa, which we will describe in chapter 4.2.4.
4.2.4
Swapping the word embeddings
The numbers presented in 4.2.2 and 4.2.3 are results of parsing with the
suggested word embedding parameters for both parsers.
In order to
validate the outcome of these experiments and decide on which dataset to
use in future experiments, we passed the word embeddings created using
42
Parser
NAK
Concatenation
LAS
UAS
LAS
UAS
Dyer’s parser
88.26% 90.42% 85.67% 87.90%
UDPipe
90.06% 91.83% 90.37% 92.03%
Table 4.5:
Default setting results where Dyer’s parser embeddings have
been passed to UDPipe and vice versa.
Dyer’s parser default settings to UDPipe and vice versa.
The results are
presented in Table 4.5.
As we can see from the results, both Dyer’s parser
and UDPipe seem to perform better using Dyer’s standard embedding
settings.
As Straka et al.
(2016) suggest on GitHub to use the standard
skip-gram model,
this is an interesting observation as Dyer’s default
embedding settings use the structured skip-gram model of wang2vec.
4.2.5
Results and conclusion
The results presented in Tables 4.5,
4.3 and 4.4 indicate two things.
First
of all,
the fact that there is no definitively best dataset;
UDPipe seems to
perform slightly better using the concatenation regardless of the hyperpa-
rameters used in the training of the word embeddings, whereas the Dyer’s
parser seems to perform better using the Norwegian Newspaper Corpus
only.
Secondly, the standard embedding parameters used by Dyer et al. (2015)
seems to be superior to those used by Straka et al. (2016), also when em-
ployed in UDPipe. We can draw this conclusion from the fact that the both
labelled and unlabelled attachment scores drop in Dyer’s parser when
UDPipe’s standard embeddings are used,
and that the scores increase in
UDPipe when the standard embeddings of Dyer’s parser are used.
This
leads us to conclude that further experiments are needed.
The Norwegian Newspaper Corpus dataset is smaller than the concatena-
tion,
and training word embeddings using this corpus requires less time
than traning using the concatenation.
Spending as little time as possible
training,
and as much time as possible experimenting with different hy-
perparameters is crucial in this project. Furthermore, it is reasonable to ex-
pect that the results of tuning different parameters is close to independent
of which dataset we use during training.
As both datasets provided suffi-
cient results, and the increase in score is minimal when training with the
concatenation, we have decided to continue experimenting with the NAK-
only corpus as we proceed to the next step in our experiment pipeline.
43
Parser
Dataset
Embeddings
LAS
UAS
Dyer
N/A
None
87.05% 89.79%
UDPipe
N/A
None
88.41% 90.49%
Dyer
NAK
Dyer std.
89.66% 91.60%
UDPipe
NAK
UDPipe std.
89.55% 91.32%
Dyer
Concat
Dyer std.
89.50% 91.53%
UDPipe
Concat
UDPipe std.
89.63% 91.39%
Dyer
NAK
UDPipe std.
88.26% 90.42%
Dyer
Concat
UDPipe std.
85.67% 87.90%
UDPipe
NAK
Dyer std.
90.06% 91.83%
UDPipe
Concat
Dyer std.
90.37% 92.03%
Table 4.6: Results of all dataset experiments. We observe that both parsers
seem to perform better using Dyer’s default
settings for embeddings.
There is also little difference between embeddings trained using only
the Norwegian Newspaper
Corpus and embeddings trained using a
concatenation of NAK and NoWAC.
44
Chapter 5
Tuning the word embeddings
In this chapter we will investigate how different hyperparameters used in
the training of word embeddings affect the results of dependency parsing.
While we tune the word embeddings, the default parameters will be used
in the dependency parsers (Dyer et al., 2015; Straka et al., 2016).
5.1
Introduction
As
the
training
of
word embeddings
using
wang2vec
is
a
non-
deterministic process, we wish to quantify the degree of non-determinism
before proceeding with further experiments.
In order to do this,
we will
train five models using the exact same parameters,
and look at the vari-
ance in the results.
The models will be trained with the standard Dyer’s
parser settings.
In the first step of our experiments we will look at how different embed-
ding models affect the attachment scores in both parsers. Common agree-
ment in the NLP community says that Skip-Gram is superior to CBOW in
the task of dependency parsing, and it is natural that we focus on the dif-
ferences between the skip-gram model of word2vec (Mikolov, Chen, et al.,
2013),
and the modification to this model dubbed structured skip-gram,
available in wang2vec (Ling et al., 2015).
In chapter 4.2 we observed that
Dyer’s parser standard embeddings, which use the Structured Skip-Gram
model,
perform better in both parsers,
but as there are other differences
between Dyer’s parser standard settings and UDPipe standard settings
than just the model in use,
such as dimensionality,
frequency cutoff and
negative examples for NCE, we cannot draw a conclusion from these ex-
periments.
Training word embeddings and parsing with the word embeddings is a
task that is quite time consuming.
Training a single model with our set-
tings takes between 12 and 40 hours,
and parsing takes 10-20 hours in
45
M
=
∑
x
i
N
Figure 5.1: Mean formula
SD
=
q
(
∑
x
i
−
M
)
2
N
−
1
Figure 5.2: Sample standard deviation formula
Dyer’s parser, and approximately 3-4 hours in UDPipe.
This means a sin-
gle run through our pipeline takes more than a day of active computation
to complete. Adding the time it takes to get access on the hugemem nodes
of Abel, which often takes a day or two, and the fact that we have to queue
wang2vec, Dyer’s parser and UDPipe as separate jobs, the latter two de-
pending on the completion of the first, a single experiment takes about 4-5
days.
The consequence of this is that we need to limit the number of ex-
periments. Performing an exhaustive search through all hyperparameters
would be very time consuming, and is out of reach of this project.
5.1.1
Quantifying the degree of non-determinism
As described in section 2.1.3, the training of models in wang2vec is a non-
deterministic process. In order to quantify the degree of non-determinism,
we trained 5 models using the standard Dyer’s parser settings.
By look-
ing at the variance between the best and worst results, and calculating the
standard deviation in the distribution of scores yielded by the dependency
parsers, we will get an indication of whether a given result is statistically
significant or not. Five is a low number of samples for this kind of test, but
hopefully it is enough to give an indication of how much the scores vary
between equal
models.
Considering the computational
cost of creating
models and performing dependency parsing with two different parsers,
we found it unfeasible to increase this number.
Figure 5.3 shows the distribution of
LAS yielded by our dependency
parsers when trained using five different wang2vec models with the same
hyperparameter settings.
We observe that the difference between the best
and worst results in Dyer’s parser is 1.25 percentage points. The difference
between the best and worst results in UDPipe is 0.31 percentage points.
In order to get an indication of the degree of non-determinism in our data,
we calculate the sample standard deviation in the distribution of scores
yielded by each parser.
The mean in the distribution of scores yielded by Dyer’s parser is 88.926%,
46
Figure 5.3:
Distribution of the LAS yielded by dependency parsers with
five wang2vec models using equal hyperparameters.
giving a sample standard deviation of 0.476%.
The mean in the distribu-
tion of scores yielded by UDPipe is 89.756%, giving a sample standard de-
viation of 0.117%.
This might indicate that Dyer’s parser is affected more
by the variance in the pre-trained word embeddings. On a side note, these
results are interesting as they might indicate that the pre-trained word em-
beddings play a larger role in the training of Dyer’s parser.
From the results of
this experiment,
we see that
the degree of
non-
determinism in wang2vec models certainly affects the accuracy of
our
parsers. Our set of samples in this experiment is too low to draw a conclu-
sion regarding distribution and confidence intervals.
47
Setup
Parser
LAS
UAS
Dyer SG
Dyer
85.78% 88.26%
Dyer SSG
Dyer
89.66% 91.60%
UDPipe SG
UDPipe
89.55% 91.32%
UDPipe SSG
UDPipe
90.15% 91.94%
Table 5.1: Results of comparing the Skip-Gram model with the Structured
Skip-Gram model
using the standard settings for
Dyer’s parser
and
UDPipe. SG denotes skip-gram, SSG denotes structured skip-gram.
5.1.2
Training word embeddings using the Skip-Gram and
Structured Skip-Gram models
In the experiments described in this section,
we look at how exchanging
Skip-Gram for Structured Skip-Gram (for UDPipe standard settings), and
vice versa (for Dyer’s parser standard settings),
affects the results of de-
pendency parsing.
The difference between these models is described in
2.3.3.
The parameters used in this section are specified in Table 4.1, with
the only difference being that the Skip-Gram model used in UDPipe has
been changed to Structured Skip-Gram,
and the Structured Skip-Gram
model used in Dyer’s parser has been changed to standard Skip-Gram.
The goal of these experiments is to settle with the best model for the task
of dependency parsing.
The results of
these experiments are presented in Table 5.1,
and indi-
cate that using the Structured Skip-Gram model yields significant gains
in terms of LAS and UAS for both parsers. The increase in LAS is almost 4
percentage points for Dyer’s parser, and a little above .5 percentage points
for UDPipe. Looking at both parsers’ standard settings, this indicates that
the choice of Structured Skip-Gram over standard Skip-Gram was crucial
for Dyer et al. (2015) in the development of Dyer’s parser, whereas train-
ing time, which is significantly higher for Structured Skip-Gram than the
nonstructured version, might have been prioritised by Straka et al. (2016)
during the development of UDPipe.
Although the CBOW (Mikolov,
Chen,
et al.,
2013) and CWindow (Ling
et al., 2015) architectures cost less in regards of computational time, previ-
ous research has indicated that these models generally do not perform as
well as Skip-Gram and Structured Skip-Gram in the task of dependency
parsing.
We have therefore decided not to train models using these algo-
rithms.
48
Dimensions
Dyer LAS
UDPipe LAS
50
89.03%
90.10%
100
89.66%
90.06%
200
87.54%
90.37%
Table 5.2:
LAS in UDPipe and Dyer’s parser
with 50,
100 and 200
dimensional embeddings.
5.2
Dimensionality
The next step in our experiments on looking at the effects of varying hy-
perparameters of the word embeddings is dimensionality.
In these ex-
periments we use the Dyer’s parser standard settings,
only varying the
dimensionality.
We would like to see how decreasing and increasing the dimensionality
of the word embeddings affect the attachment scores of our parsers.
The
standard setting for Dyer’s parser is 100, and the standard setting for UD-
Pipe is 50.
Hypothetically, higher dimensionality should lead to less gen-
eralisation and allow the vectors to capture more fine grained distinctions
between similar words.
As we can see from Table 5.2,
changing the dimensionality affects the
performance of UDPipe only slightly.
This variation in accuracy is much
smaller than the standard deviation in the experiments carried out in chap-
ter 5.1.1.
Dyer’s parser seems to be more sensitive to changes in dimen-
sionality, and 100 dimensions (the default setting) seems to be optimal for
this parser.
Perhaps somewhat surprisingly, the LAS and UAS of Dyer’s
parser drops by more than 2 percentage points when the dimensionality
of the embeddings are increased from 100 to 200 dimensions.
Ideally we would continue our experiments with embeddings of 100 di-
mension for Dyer’s parser and 200 dimensions for UDPipe, but this would
lead to twice the computational
cost
for all
future experiments,
as we
would have to train separate embeddings for both Dyer’s parser and UD-
Pipe.
Due to the fact that our experiments indicate that Dyer’s parser
performs better using word embeddings of 100 dimensions and UDPipe
responds very little to changes in dimensionality, we will continue our ex-
periments with word embeddings of 100 dimensions.
49
Window Dyer LAS
UDPipe LAS
2
89.20%
90.15%
5
89.66%
90.06%
10
89.33%
90.32%
15
89.03%
90.11%
20
89.15%
89.96%
Table 5.3:
LAS in UDPipe and Dyer’s parser with embeddings using a
window size of 2, 5, 10, 15 and 20.
5.3
Window size
The window size used when training embeddings affects their concept of
similarity.
A smaller window size has proven to emphasise syntactic sim-
ilarity (i.e.
words with the same part-of-speech tag,
tense,
etc.
get a low
distance),
whereas a bigger window emphasises semantic similarity (i.e.
words with similar meaning, such as "boat" and "ship" get a low distance).
In order to look at the effects of changing the window size, we have per-
formed dependency parsing with a window size of 2, 5, 10 and 20, using
both parsers.
As we can see from the graph presented in figure 5.4 and the results pre-
sented in table 5.3, the results of these experiments are inconclusive.
The
results seem to indicate that Dyer’s parser yiels slightly better results us-
ing a window size of 5,
whereas UDPipe performs slightly better with a
window size of 10.
Due to the fact that training word embeddings with
a smaller window size is less time consuming,
as is described in chapter
5.1.1, and the fact that Dyer’s parser performed better with a size of 5, we
continue our experiments using a window size of 5.
5.4
Frequency cutoff
Large corpora typically contain some lower frequency words leading to
less evidence and noisier representations.
These can be typos,
certain
slang words,
foreign words,
and other words occurring infrequently.
In
order to distinguish these words,
which may be considered noise,
from
relevant words, we use frequency cutoff.
As all corpora are different, the
amount of noise words varies from corpus to corpus. No research on find-
ing an optimal value for frequency cutoff in the Norwegian Newspaper
Corpus has been done, so we will experiment using five values within the
spectrum suggested in the help text of wang2vec, to see which performs
better.
50
Figure 5.4: LAS relative to window size
Frequency cutoff
works by simply neglecting words with a frequency
lower than the specified value.
All models in word2vec (Mikolov, Chen,
et al., 2013) and wang2vec (Ling et al., 2015) support user-specified values
for frequency cutoff. The default value for frequency cutoff in wang2vec is
5. Dyer et al. (2015) do not specify what number they used in their parsing
experiments, other than stating that they used the same hyperparameters
as Mikolov, Sutskever, et al. (2013), who in turn do not specify the value of
frequency cutoff. In our previous experiments we have therefore used the
value 5, which is suggested on GitHub
1
by the authors of wang2vec (Ling
et al., 2015). UDPipe (Straka et al., 2016) suggests using a frequency cutoff
of 2 in their source code
2
,
but as the embeddings trained using UDPipe
standard embedding settings were discarded in chapter 5.1, we have used
the value 5 in our experiments up to this point.
1
wang2vec on GitHub: https://github.com/wlin12/wang2vec
2
UDPipe
embedding
generating
script
on GitHub:
https://github.com/ufal/
udpipe/blob/master/training/ud-1.2-embeddings/gen.sh
51
Frequency cutoff
Dyer LAS
UDPipe LAS
0
89.26%
90.03%
2
89.16%
90.30%
5
89.58%
90.03%
10
89.11%
90.40%
20
89.29%
90.44%
Table 5.4:
LAS in Dyer’s parser and UDPipe for a range of values for
frequency cutoff within the suggested spectrum.
The embeddings are
trained using the standard settings for embeddings in Dyer’s parser, only
changing negative sampling to 20 (according to the results in 5.5.1),
and
frequency cutoff.
We experimented using a frequency cutoff of 0,
2,
5,
10 and 20.
The re-
sults, presented in table 5.4, indicate that Dyer’s parser performs slightly
better with a frequency cutoff of 5, whereas UDPipe performs slightly bet-
ter with a frequency cutoff of 20. The difference between the LAS in Dyer’s
parser for values 5 and 20 is .29 percentage points,
which is much lower
than the sample standard deviation calculated from the quantification ex-
periments presented in chapter 5.1.1.
For UDPipe the difference in LAS
for values 5 and 20 is .41 percentage points, which is almost four standard
deviations off the mean in our quantifying experiments.
Taking this into
account, we continue with a frequency cutoff of 20.
5.5
Algorithms for optimising similarity
5.5.1
Negative examples
The goal of any word embedding system is to minimise the distance be-
tween similar words, and maximise the distance to all other words in the
dictionary.
Ideally this would be calculated by maximising the context of
each word multiplied by the word vector, divided by the sum of all other
words multiplied by their vectors. This task would be very computational
expensive as the computational
time would be proportional
to the size
of the vocabulary.
Various solutions to lowering the computational cost
while retaining the precision of the model have been proposed, including
the hierarchical softmax, noise contrastive estimation (nce), and Negative
Sampling (Mikolov, Sutskever, et al., 2013).
Negative Sampling uses logistical regression in order to distinguish a tar-
get word from the noise distribution.
(Mikolov,
Sutskever,
et al.,
2013)
found that a k of 5-20 was feasible for small datasets,
whereas values as
small as 2-5 were sufficient for larger datasets (k being the number of neg-
52
Figure 5.5: LAS relative to negative samples
ative examples).
The size of the datasets used in the experiments carried
out by Mikolov, Sutskever, et al. (2013) are not stated, and no experiments
with varying negative sampling have been published.
We carried out experiments using the Norwegian Newspaper Corpus with
k
=
2, 5, 10, 15 and 20 to determine the optimal number of negative sam-
ples for Norwegian language. As we can see from the results presented in
figure 5.5 and table 5.5, both parsers seem to perform slightly better using
20 negative examples.
This might indicate that our dataset could be con-
sidered small, compared to the datasets used in Mikolov, Sutskever, et al.
(2013).
Although the increase in LAS is only slight for both parsers,
we
continue using k
=
20 for negative examples in our future experiments.
5.5.2
Hierarchical softmax
An alternative to the negative examples approach (as is described in chap-
ter 5.5.1 to lowering the computational cost of training word embeddings,
53
Neg.ex.
Dyer LAS
UDPipe LAS
2
89.35%
90.03%
5
89.02%
90.24%
10
89.22%
90.22%
15
89.63%
90.20%
20
89.67%
90.39%
Table 5.5:
LAS in Dyer’s parser and UDPipe for a range of
negative
examples.
We observe that both parsers seem to benefit from a higher
number of negative examples.
Embedding
Dyer LAS
UDPipe LAS
Dyer’s parser standard
88.66%
90.27%
Table 5.6:
LAS and UAS with negative examples and NCE replaced by
hierarchical softmax. Dyer’s parser standard settings are used otherwise.
is the hierarchical softmax.
The hierarchical softmax, proposed by Morin
and Bengio (2005)
and further described by Mikolov,
Sutskever,
et
al.
(2013) uses a binary tree representation of the output layer. This leads to a
lower computational cost than evaluating all the output nodes of the neu-
ral network.
Each word in the output layer represents a leaf in the binary
tree, and the relations to their children nodes represent their probabilities.
Together these represent a random walk, assigning probabilities to words
(Mikolov, Sutskever, et al., 2013).
We have experimented with replacing the negative examples with the hi-
erarchical softmax.
The results, presented in table 5.6, might indicate that
parsing with word embeddings trained using hierarchical softmax yields
results close to,
but slightly lower than,
those trained using negative ex-
amples together with NCE. We will therefore discard hierarchical softmax
and continue using negative examples and NCE in future experiments.
5.5.3
Noise Contrastive Estimation (NCE)
Noise Contrastive Estimation, often referred to simply as NCE, is an alter-
native to negative examples (chapter 5.5.1) and hierarchical softmax (chap-
ter 5.5.2).
NCE aims to differentiate data from noise by the use of logistic
regression (Mikolov, Sutskever, et al., 2013).
Mikolov,
Sutskever,
et al.
(2013) states that NCE can maximise the log
probability of softmax.
The negative examples approach,
described in
chapter 5.5.2,
is an attempt at simplifying NCE,
due to the fact that the
54
NCE Value
Dyer LAS
UDPipe LAS
3
88.14%
89.80%
5
88.65%
90.00%
7
87.49%
89.90%
10
88.09%
89.96%
Table 5.7: LAS in Dyer’s parser and UDPipe for a range of values for NCE.
Apart from the varying NCE and negative examples set to 0, the wang2vec
parameters used are those of Dyer’s parser standard settings. We observe
that both parsers perform better with an NCE valua of 5.
skip-gram model is only concerned with high-quality vector representa-
tion.
We were interested to see how our parsers would perform with
structured skip-gram models trained using NCE,
without the use of the
negative examples presented by Mikolov, Chen, et al. (2013).
The manual
page of wang2vec states that reasonable values range between 3 and 10.
We performed experiments with 3, 5, 7 and 10.
The results, presented in table 5.7, show that both parsers seem to perform
slightly better with an NCE value of 5. Due to the fact that the variation in
results is much smaller than the standard deviation in our quantification
experiments (chapter 5.1.1),
and that an NCE value of 5 is the standard
setting used by Dyer et al. (2015), we will proceed with an NCE value of 5.
5.5.4
Isolating Negative Examples
As briefly mentioned in 5.5.1,
Negative Examples,
NCE and Hierarchi-
cal Softmax are three different approaches to minimising the distance be-
tween similar words,
and maximising the distance to all other words in
the dictionary.
In our experiments up until 5.5.1, we used both the Nega-
tive Examples and NCE together, due to the fact that it was unclear which
approach was taken by Dyer et al. (2015).
According to the authors of wang2vec (Ling et al.,
2015),
using both al-
gorithms simultaneously might lead to suboptimal results
3
.
In order to
validate the results presented in 5.5,
we repeated the experiments with
values 5 and 20, only this time without using NCE.
Table 5.8 shows a side-by-side comparison of the results of parsing with
embeddings trained using both negative examples and NCE,
and only
3
https://github.com/wlin12/wang2vec/issues/7
55
Setup
Dyer LAS
UDPipe LAS
5 neg. ex. with NCE
89.02%
90.24%
25 neg. ex. with NCE
89.67%
90.39%
5 neg. ex. without NCE
88.95%
90.11%
25 neg. ex. without NCE
89.17%
90.24%
Table 5.8:
LAS in Dyer’s parser and UDPipe for a range of values for
NCE. Apart from the varying NCE and negative examples, the wang2vec
parameters in use are those of Dyer’s parser standard settings.
negative examples,
for values 5 and 20.
Our results,
contrary to the in-
formation given by the authors of wang2vec, indicate that the parsers per-
form slightly better when trained using embeddings with both Negative
Examples and NCE enabled. We will continue using this setup for our fu-
ture experiments.
5.5.5
Similarity algorithm conclusion
In chapter 5.5.1, 5.5.2 and 5.5.3 we have experimented with three different
algorithms
for
minimising the distance between similar
words,
and
maximising the distance to noise.
We found that the embeddings trained
using negative sampling yielded slightly better results in experiments
with both our dependency parsers.
This is in line with the findings of
Mikolov, Sutskever, et al. (2013).
Whereas Dyer et al. (2015), to the best of
our knowledge, used 5 negative samples in their embeddings, we found
that
using 20 negative samples worked slightly better for embeddings
trained using our datasets.
Without specifying what is to be considered
a "large" or a "small" dataset, Mikolov, Sutskever, et al. (2013) stated that a
higher value of negative samples would perform better on small datasets.
Our findings might indicate that our dataset, the Norwegian Newspaper
Corpus, could be considered a small dataset in this context.
5.6
Conclusion
The results of all experiments conducted in this chapter are presented in
table 5.9.
As we can see,
the embedding parameters used by Dyer et al.
(2015) in their experiments on parsing of English seem to be close to op-
timal for Norwegian language.
In terms of structured vs.
non-structured
skip-gram (5.1.2), window size (5.3) and dimensionality (5.2), our conclu-
sion is that using the same settings as Dyer et al. (2015) provided optimal
results.
56
We observe that different settings often lead to optimal performance in
each parser,
and that there is no common set of best values.
We also ob-
serve that structured skip-gram seems to outperform the standard skip-
gram model for the purpose of training embeddings to be used as input
representations for dependency parsers.
The only hyperparameter we
changed that yielded better results than the standard Dyer’s parser set-
tings in both parsers,
was negative examples (5.5.1).
Where Dyer et al.
(2015) used k
=
5 for negative examples,
we found that k
=
20 yielded
slightly better results with our data.
Mikolov,
Sutskever,
et
al.
(2013)
stated that a higher number of negative examples might be better for small
datasets, whereas a lower number might be sufficient for bigger datasets.
The article does not actually state what, in terms of number of tokens and
size in gigabytes, is considered a small dataset, but from our results with
a higher number of negative examples, we found it reasonable to assume
that our NAK dataset could be considered small.
Furthermore it is interesting to note that the Dyer’s parser standard em-
bedding settings were significantly better than the standard UDPipe set-
tings in both parsers.
This is especially interesting as Dyer et al.
(2015)
seems to have put little effort into optimising embeddings, whereas UD-
Pipe even comes with its own embedding-generating script.
5.7
Held-out evaluation
In the evaluation of dependency parsers, it is common practice to use the
development set for evaluation during parameter tuning.
Once a suffi-
cient model has been proposed, the researcher moves on to the test set for
final evaluation.
Up to this point, all evaluation has been done using the development set
in the 80-10-10 split of the Norwegian Newspaper Corpus (Solberg et al.,
2014; Hohle et al., 2017).
In this section we will perform a final evaluation
of the embeddings using the embedding settings presented in table 5.10.
The results of our final evaluation of dependency parsing of the test set
are presented in table 5.11.
We see that Dyer’s parser yields a LAS of
89.52%, which is close to the result with the initial setup used by Dyer et
al. (2015) presented in table 5.1. UDPipe yields a LAS of 90.54%, almost .4
percentage points better than the results with the initial setup.
57
Model
Dim
Win
Cutoff
Neg
Softmax
NCE
Dyer
UDPipe
SG
100
5
5
5
0
5
85.78%
89.55%
SSG
100
5
5
5
0
5
89.66%
89.15%
SSG
50
5
5
5
0
5
89.03%
90.10%
SSG
100
5
5
5
0
5
89.66%
90.06%
SSG
200
5
5
5
0
5
87.54%
90.37%
SSG
100
2
5
5
0
5
89.20%
90.15%
SSG
100
5
5
5
0
5
89.66%
90.06%
SSG
100
10
5
5
0
5
89.33%
90.32%
SSG
100
15
5
5
0
5
89.03%
90.11%
SSG
100
20
5
5
0
5
89.15%
89.96%
SSG
100
20
0
20
0
5
89.26%
90.03%
SSG
100
20
2
20
0
5
89.16%
90.30%
SSG
100
20
5
20
0
5
89.58%
90.03%
SSG
100
20
10
20
0
5
89.11%
90.40%
SSG
100
20
20
20
0
5
89.29%
90.44%
SSG
100
20
5
2
0
5
89.35%
90.03%
SSG
100
20
5
5
0
5
89.02%
90.24%
SSG
100
20
5
10
0
5
89.22%
90.22%
SSG
100
20
5
15
0
5
89.63%
90.20%
SSG
100
20
5
20
0
5
89.67%
90.39%
SSG
100
20
5
0
1
0
88.66%
90.27%
SSG
100
20
5
0
0
3
88.14%
90.80%
SSG
100
20
5
0
0
5
88.65%
90.00%
SSG
100
20
5
0
0
7
87.49%
89.90%
SSG
100
20
5
0
0
10
88.09%
89.96%
Table 5.9:
Results of
all
embedding tuning experiments,
evaluated on
the development set.
win denotes window size,
Neg denotes number of
negative examples.
The Dyer and UDPipe columns represent LAS for
both parsers respectively.
Note that
the frequency cutoff
experiments
were carried out after the experiments with negative samples,
hence the
negative examples value set to 20 for these experiments.
58
Parameter
value
Type
Structured skip-gram
Dimensionality
100
Window size
5
Negative examples
20
Negative examples for nce
5
Hierarchical softmax
Off
Sample
1e-3
Frequency cutoff
20
Parameter value limit (cap)
Off
Iterations
5
Table 5.10: Final embedding settings.
Parser
LAS
UAS
Dyer’s parser
89.52% 91.66%
UDPipe
90.54% 92.36%
Table 5.11:
LAS and UAS in Dyer’s parser and UDPipe on the parsing of
the test dataset
59
60
Chapter 6
Intrinsic evaluation and error
analysis
6.1
Intrinsic evaluation of word embeddings
Traditionally, evaluation of word embeddings has been done by running
them through a set of standardised tests.
The goal of this kind of evalu-
ation is to optimise the word embeddings simply for the purpose of cre-
ating the "best" word embeddings in terms of distance to neighbouring
words according to a predefined gold standard.
This kind of evaluation
is referred to as intrinsic evaluation.
Many evaluation datasets,
such as
SimLex999 (Hill et al., 2016) and Google Analogies (Mikolov, Chen, et al.,
2013), have been proposed. Additionally, intrinsic evaluation has been the
focus of the SemEval shared task several times.
The SemEval shared task
of 2012 resulted in a dataset of graded similarity ratings (Jurgens, Turney,
Mohammad, & Holyoak, 2012).
Recently,
an intrinsic evaluation dataset for Norwegian using analogies
has been developed by Stadsnes (forthcoming) at the University of Oslo.
This dataset is a translation of the Google Analogies dataset,
with some
language specific modifications, such as the city-in-state category being re-
placed by city-in-county.
Our word embeddings are created solely for the purpose of maximising
the accuracy score of dependency parsers.
As the word embeddings are
tuned to optimise another task, the motivation of the tuning can be said to
be extrinsic.
As briefly mentioned in chapter 2.5, we decided to run some
of our models through the dataset proposed by Stadsnes (forthcoming), to
see whether or not there is a correlation between embeddings performing
well in the extrinsic task of dependency parsing, and in intrinsic test.
61
Embeddings
Analogies
LAS
Dyer SG
48.7% 85.78%
Dyer SSG
42.8% 89.66%
UDPipe SG
35.8% 89.55%
UDPipe SSG
32.1% 90.15%
Table 6.1:
Table presents recognised analogies for the the baseline Dyer’s
parser and UDPipe settings,
exchanging structured skip-gram for skip-
gram i row 1 and 3.
LAS in the first two rows is for parsing with Dyer’s
parser
and LAS in the last two rows is for parsing with UDPipe.
We
observe that the ranking in terms of recognised analogies is different than
the ranking in parser LAS.
6.1.1
Evaluating our word embeddings intrinsically
In chapter 5.1.2 we describe experiments with the standard Dyer’s parser
and UDPipe settings, using both word2vec’s skip-gram model (Mikolov,
Chen, et al., 2013) and wang2vec’s structured skip-gram model (Ling et al.,
2015).
Our results indicate that the structured skip-gram model outper-
forms the non-structured version using both Dyer’s parser and UDPipe
standard settings.
Interestingly this is not the case in the task of recog-
nising analogies.
In table 6.1,
we present the results of the tests using
skip-gram and structured skip-gram.
The results indicate that the skip-
gram model performs better in recognising analogies, whereas the struc-
tured skip-gram model performs better when used as input in dependency
parsers.
Row 1 in table 6.1 shows the performance of our embedding with the de-
fault settings of Dyer et al.
(2015).
Row 2 shows the performance of the
embedding trained with the same set of parameters,
only replacing the
structured skip-gram model of wang2vec (Ling et al.,
2015) by the skip-
gram model of word2vec (Mikolov, Chen, et al., 2013). Rows 3 and 4 show
similar values for embeddings trained using the default UDPipe settings,
with LAS in UDPipe. In table 6.2 we take a closer look at the performance
of these two embeddings for each category in the analogy dataset.
It is
interesting to see how the skip-gram model seems to excel in recognising
semantic analogies, whereas structured skip-gram scores higher in out of
5 out of 8 syntactic categories.
The main difference between the skip-gram model
and the structured
skip-gram model,
as is explained in chapter 2.3.3,
is that the structured
skip-gram version weigh words by distance within the context window,
whereas the skip-gram model treats the entire window as a bag of words.
62
The results in table 6.2 might indicate that the bag of words approach is
better at recognising semantic similarities.
This might be due to the fact
that semantically similar words do not necessarily appear in order in our
corpus.
For instance, in the sentence "Yen is the currency of Japan" the non-
structured skip-gram model would give the word "Yen" the same magni-
tude as the word "of" in the embedding of the word "Japan".
The struc-
tured skip-gram model, on the other hand, would put more emphasis on
the words close in proximity of "Japan",
and nearly neglect the semanti-
cally related word "Yen", as it appears at the border of the context window.
This seems to be backed up by the fact that the structured skip-gram model
seems to perform as good as,
or even slightly better than,
the skip-gram
model when it comes to recognising syntactic analogies.
Given the sen-
tences "Adam performs a slow dance" and "Jess and Adam dance slowly", we
have an adjective-to-adverb analogy between "slow" and "slowly".
Notice
how the words occur in close proximity to the word "dance".
When look-
ing at the word "dance" in our sentences,
the non-structured skip-gram
model would put an equal emphasis on its relation to the word "slowly"
as it would to "Jess", whereas the structured skip-gram model would give
the word "slowly" a much higher weight than "Jess" due to the distance
between the two words.
From our experiment,
we observe that there is no correlation between
embeddings performing well in the downstream, extrinsic task of depen-
dency parsing,
and embeddings performing well in analogy recognition
intrinsic evaluation.
Furthermore,
there is great variation in the perfor-
mance of the embeddings in different parts of the analogy test set;
skip-
gram outperforms structured skip-gram in every category under the se-
mantic part of the dataset, whereas the structured skip-gram embeddings
are marginally better in the syntactic categories.
6.1.2
A closer look at neighbouring words
In this section we will take a closer look at the word embeddings evalu-
ated in 6.1.
By taking a closer look at the neighbours of certain words, we
wish to see if there is a difference in the relationships emphasised by each
model;
for instance whether the neighbours are semantically or syntacti-
cally similar.
We will now take a look at the words closest to our example words "katt"
(cat)
and "beregne" (calculate)
in terms of
cosine similarity.
Table 6.3
shows the closest
words and their distance in the embeddings trained
63
Category
Structured
Skip-gram
Semantic
hovedstad-vanlige-land
53.0%
61.3%
hovedstad-verden
48.5%
57.4%
valuta
5.1%
5.7%
by-i-fylke
23.2%
34.9%
familie
63.6%
69.4%
Syntactic
adjektiv-til-adverb
28.3%
26.6%
motsetning
15.3%
14.5%
komparativ
57.2%
55.8%
superlativ
26.9%
30.1%
nasjonalitet-adjektiv
64.6%
78.8%
preteritum
61.1%
60.7%
substantiv-flertall
34.7%
37.8%
presens-verb
62.5%
64.9%
Table 6.2:
Detailed results of intrinsic evaluation of embeddings trained
using structured and non-structured skip-gram.
We observe that there
is no correlation between embeddings performing well
in dependency
parsing, and embeddings performing well in intrinsic evaluation.
using Dyer’s default embedding parameters with structured skip-gram.
Table 6.4 shows the closest words and their distance in the embeddings
trained using Dyer’s default embedding settings, only exchanging struc-
tured skip-gram for skip-gram.
Both models seem to place the word "katt" (cat) close to other domestic
animals,
and "beregne" (calculate) close to words related to calculation.
An interesting thing to note is how the structured skip-gram model places
similar words closer in the vector space than the skip-gram model; all top
five neighbouring words for "katt" are within a .91 cosine similarity range,
whereas the fifth word in the skip-gram model is further away with a sim-
ilarity of .88. This seems to be the case for the word "beregne" as well.
These findings might indicate that the precision of the structured skip-
gram model in terms of simple distance between neighbouring words is
higher than that of the standard skip-gram model.
Furthermore,
it is interesting to take a closer look at the word classes of
the most similar words.
For the verb "beregne", the structured skip-gram
model only has verbs in the same tense among its neighbours.
The skip-
gram model on the other hand, includes the word "beregner", which is the
64
Word
Distance
Word
Distance
katt
1.000
katt
1.000
hund
0.949
hund
0.930
papegøye
0.930
kanin
0.907
kanin
0.924
rotte
0.891
skilpadde
0.916
hamster
0.890
elefant
0.910
papegøe
0.882
Table 6.3:
Five closest neighbours of word "katt".
Left side is neighbours
and distances using structured skip-gram,
right side is neighbours and
distances using skip-gram.
Word
Distance
Word
Distance
beregne
1.000
beregne
1.000
estimere
0.869
estimere
0.821
omregne
0.843
kalkulere
0.790
kvantifisere
0.839
beregner
0.784
kalkulere
0.825
omregne
0.781
stipulere
0.821
stipulere
0.778
Table 6.4:
Five closest
neighbours
of
word "beregne".
Left
side is
neighbours
and distances
using structured skip-gram,
right
side
is
neighbours and distances using skip-gram.
same verb in another tense.
In tables 6.6 and 6.5,
we look at similar statistics for the words "trender"
("trends") and "beregner" ("calculates"). These words are more ambiguous,
as they can be used as both verbs in the present,
and as nominative
nouns.
Interestingly,
both models placed "trender" close to other nouns
related to trends.
The word "beregner" was placed close to words related
to calculation in both models,
meaning that
semantic similarity was
emphasised over syntactically similar words.
6.2
Error analysis
As described in chapter 6.1.1, skip-gram seems to perform better at recog-
nising analogies in the semantic category,
whereas structured skip-gram
seems to perform better at recognising syntactic analogies.
In this section
we will take a closer look at the error metrics output by the CoNLL Shared
Task evaluation tool eval.pl for the results of parsing using these embed-
dings.
65
Word
Distance
Word
Distance
trender
1.000
beregner
1.000
trendene
0.894
utregning
0.949
moteretninger
0.857
beregningen
0.824
moter
0.852
omregning
0.821
fenomener
0.843
verdisetting
0.821
utviklingstrender
0.839
utregningen
0.818
Table 6.5: Five closest neighbours to words "trender" and "beregner" using
Dyer’s parser
standard settings for
embeddings,
replacing structured
skip-gram by skip-gram.
Word
Distance
Word
Distance
trender
1.000
beregner
1.000
trendene
0.878
utregning
0.935
motetrender
0.835
beregningsmodell
0.820
moteretninger
0.832
utregningen
0.818
moter
0.829
beregningsmøte
0.816
markedstrender
0.818
fastsettelse
0.807
Table 6.6: Five closest neighbours to words "trender" and "beregner" using
Dyer’s parser
standard settings for
embeddings,
replacing structured
skip-gram by skip-gram.
In table 6.7 we present the f-scores for each dependency relation in the
parses output by Dyer’s parser using skip-gram and structured skip-gram.
We find that SSG has a higher f-score for a majority of the dependency re-
lations.
In particular, highly frequent relations like adverbials (ADV), at-
tributive modifiers (ATR), determiners (DET), subjects (SUBJ) and preposi-
tional objects (PUTFYLL) contributes to the overall higher scores observed
for this model.
The greatest variation in score is found in FOBJ, IK, IOBJ and POBJ, where
SSG performs much better than SG. As these relations have very low oc-
curences, they do not affect the total score much, and the differences might
be caused by the non-determinism in the embeddings.
Furthermore,
we
observe that both parsers perform very poorly at parsing of the complex
relations of free object predicates (FOPRED) and elliptical constructions
(KOORD-ELL).
66
Deprel
Occurrences
SSG
SG
ADV
5101
82.82% 78.78%
APP
285
56.47% 51.45%
ATR
4010
85.79% 82.90%
DET
2435
96.48% 95.40%
DOBJ
1982
91.03% 84.22%
FINV
2097
97.51% 96.52%
FLAT
691
86.74% 81.11%
FOBJ
10
77.78% 66.67%
FOPRED
7
0.00%
0.00%
FRAG
301
89.98% 83.83%
FSPRED
49
35.19% 32.73%
FSUBJ
360
86.16% 84.20%
IK
5
66.66% 36.36%
INFV
1761
98.09% 77.61%
INTERJ
34
73.02% 68.75%
IOBJ
74
73.00% 55.85%
IP
98
98.98% 96.00%
KONJ
1297
95.48% 93.66%
KOORD
1344
78.49% 76.32%
KOORD-ELL
33
3.28%
8.69%
OPRED
87
47.06% 36.19%
PAR
188
75.21% 76.74%
POBJ
7
57.14% 18.19%
PSUBJ
205
75.44% 48.95%
PUTFYLL
4433
96.21% 94.75%
SBU
1071
98.60% 98.60%
SPRED
1025
86.75% 82.56%
SUBJ
3102
94.74% 90.37%
UKJENT
15
0.00%
0.0%
Table 6.7:
F-score of Dyer’s parser parse for each dependency relation us-
ing structured skip-gram and skip-gram with Dyer’s embedding settings.
Best result is marked with bold (where the scores are equal,
no result is
marked). "Occurrences" denotes the number of occurrences of each deprel
in the gold parse.
67
68
Chapter 7
Conclusion and future work
7.1
Conclusion
The aim of this project has been to explore the effects of tuning word em-
beddings to be used as input representations in neural network-based de-
pendency parsers.
The fact that little research has been carried out on
parametrisation of word embeddings for Norwegian specifically, has been
the motivation for this project.
A wide range of word embedding tools
have become publicly available in the recent years.
We used the neu-
ral network-based skip-gram and structured skip-gram models from the
word2vec and wang2vec (Mikolov, Chen, et al., 2013; Ling et al., 2015) to
train embeddings.
The parsing was carried out in Dyer’s parser and UD-
Pipe (Dyer et al., 2015; Straka et al., 2016).
Initially, we met many challenges with our tools and data. The source code
of Dyer’s parser had to be partially rewritten in order to work properly in
our environment.
One of the corpora we used in our experiments,
the
Norwegian Newspaper Corpus,
was distributed in XML format in fold-
ers according to their origin.
Using it for the purpose of training embed-
dings required preprocessing.
Both the Norwegian Newspaper Corpus
and NoWAC also required some normalisation, which was also performed
on our treebank, the Norwegian Dependency Treebank.
When all the preparation had been done, we proceeded to perform experi-
ments. We quickly found that the HPC server at the Language Technology
Group offered too little capacity to handle our needs, and we had to mi-
grate our tools and data to the Abel supercomputer.
Initially, we performed experiments to decide which corpus to use for the
training of word embeddings.
These experiments are described in chap-
ter 4.
We found no conclusive results on whether the embeddings trained
using only the Norwegian Newspaper Corpus or a concatenation of the
69
Norwegian Newspaper Corpus and NoWAC were best suited for our task,
and decided that using the concatenation did not justify the much higher
computational cost of training embeddings using this combination corpus.
We therefore moved on to train our embeddings on the Norwegian News-
paper Corpus alone.
After settling with a corpus,
we proceeded to experiment with various
embedding models and parameters, to find an optimal setup for Norwe-
gian.
Chapter 5.1.1 presents results of training five models with identi-
cal parameters to quantify the degree of non-determinism in our models.
We found that the sample standard deviation was approximately 0.48% in
Dyer’s parser and approximately 0.12% for UDPipe.
After the quantifi-
cation, we trained models using the standard setups of Dyer et al. (2015)
and Straka et al.
(2016).
Although the recommended embedding model
for UDPipe is skip-gram,
we found that the structured skip-gram model
proposed by Ling et al.
(2015) outperformed this model in both parsers.
In chapter 5.2 through 5.5 we present experiments with different values
for dimensionality, window size and frequency cutoff, as well as the neg-
ative examples,
hierarchical softmax and NCE algorithms for optimising
similarity.
We found that the settings used by Dyer et al.
(2015) to train
embeddings used for parsing English,
were close to optimal for Norwe-
gian.
The only parameter we experienced an increase in performance by
changing was negative examples, which we changed from 5 to the value
of 20.
In the end we had a look at how our embeddings performed in the tradi-
tional, intrinsic embedding evaluation task of recognising analogies.
Re-
cently,
an analogy dataset,
and the first of its kind,
for Norwegian has
been developed by Stadsnes (forthcoming).
We decided to look at how
structured skip-gram embeddings,
which were superior in dependency
parsing, performed compared to standard skip-gram embeddings.
Inter-
estingly, we found that the skip-gram model performed much better than
the structured version in the semantic analogies tests. In syntactic analogy
recognition, the structured skip-gram model performed slightly better.
In
total, the skip-gram model performed up to 6% better than the structured
skip-gram model, which was superior in the task of dependency parsers.
We present the first parsing results for the Norwegian Dependency Tree-
bank using neural network parsers.
We also propose a set of parameters
to wang2vec which has been carefully tuned for Norwegian, with the task
of statistical
dependency parsing in mind.
As our models are too big
to be distributed on traditional platforms such as GitHub,
ranging from
586 megabytes to 7.8 gigabytes of size,
we intend to distribute an open
source,
license-free embedding file based on the final set of parameters
and trained on the Norwegian Newspaper Corpus through the commu-
70
nity repository of large-text resources presented by Fares, Kutuzov, Oepen,
and Velldal (2017).
The work presented in this project can be used in any research on Nor-
wegian where word embeddings are utilised.
As we publish our final
embeddings,
these may also be used directly,
saving researchers time by
making possible the use of pre-trained word embeddings.
7.2
Future work
Word embeddings are usable in a wide variety of applications within the
field of natural language processing.
If we were to continue working on
this project, the natural course to take would be to optimise parameters for
the dependency parsers. Although sets of parser parameters are proposed
for Dyer’s parser and UDPipe (Dyer et al., 2015; Straka et al., 2016), no ex-
periments have been carried out to see what effects changing these param-
eters would have on dependency parsing of Norwegian. The results of our
experiments indicate that structured skip-gram embeddings perform bet-
ter in UDPipe when trained using the Norwegian Dependency Treebank.
As Straka et al. (2016) train their parser using the Universal Dependencies
treebank for Norwegian, it would be interesting to see if structured skip-
gram is as suitable for parsing with this treebank.
Furthermore, training other word embedding models using tools such as
GloVe (Pennington, Socher, & Manning, 2014) and FastText (Joulin, Grave,
Bojanowski, & Mikolov, 2016) for Norwegian with the aim of optimising
embeddings for dependency parsing could be interesting.
Word embed-
dings are also usable for other downstream tasks such as POS-tagging and
sentiment analysis, and it would be interesting to see if embeddings per-
forming well in dependency parsing would be suitable in these tasks.
71
72
Bibliography
Attardi,
G.
(2006).
Experiments with a multilanguage non-projective de-
pendency parser.
In Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning (Pages 166–170).
Bansal,
M.,
Gimpel,
K.,
& Livescu,
K.
(2014).
Tailoring continuous word
representations for dependency parsing.
In Proceedings of
the 52nd
Annual Meeting of the Association for Computational Linguistics (ACL)
(Pages 809–815).
Baroni,
M.,
Dinu,
G.,
& Kruszewski,
G.
(2014).
Don’t
count,
predict!
a
systematic comparison of
context-counting vs.
context-predicting
semantic vectors.
In Proceedings of
the 52nd Annual
Meeting of
the
Association for Computational Linguistics (ACL) (Pages 238–247).
Bengio,
Y.,
Ducharme,
R.,
Vincent,
P.,
& Jauvin,
C.
(2003).
A neural
probabilistic language model. Journal of Machine Learning Research 3,
1137–1155.
Chen, D. & Manning, C. D. (2014). A fast and accurate dependency parser
using neural networks. In Proceedings of Empirical Methods in Natural
Language Processing (EMNLP) (Pages 740–750).
Dyer,
C.,
Ballesteros,
M.,
Ling,
W.,
Matthews,
A.,
& Smith,
N.
A.
(2015).
Transition-based dependency parsing with stack long short-term
memory.
In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics (ACL) (Pages 334–343).
Fares, M., Kutuzov, A., Oepen, S., & Velldal, E. (2017). Word vectors, reuse,
and replicability:
towards
a community repository of
large-text
resources. In Proceedings of the 21st Nordic Conference of Computational
Linguistics (Nodalida) (Pages 271–276).
Ferraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S. (2008). Introducing
and evaluating ukWaC, a very large web-derived corpus of english.
In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat
Google? (Pages 47–54).
Gómez-Rodriguez,
C.,
Sartorio,
F.,
& Satta,
G.
(2014).
A polynomial-
time dynamic oracle for
non-projective dependency parsing.
In
Proceedings of
the 2014 Conference on Empirical
Methods in Natural
Language Processing (EMNLP) (Pages 917–927).
Guevara,
E.
(2010).
NoWaC:
a large web-based corpus for Norwegian.
In Proceedings of
the North American Chapter of
the Association for
73
Computational
Linguistics:
Human Language Technologies 2010,
Sixth
Web as Corpus Workshop (Pages 1–7).
Hill,
F.,
Reichart,
R.,
& Korhonen,
A.
(2016).
Simlex-999:
evaluating
semantic models with (genuine) similarity estimation. Computational
Linguistics, 41(4), 697–699.
Hochreiter, S. & Schmidhuber, J. (1997). Long short-term memory. Neural
Computation, 9(8), 1735–1780.
Hohle,
P.,
Øvrelid,
L.,
& Velldal,
E.
(2017).
Optimizing a PoS tag set for
Norwegian Dependency Parsing.
In Proceedings of
the 21st
Nordic
Conference of Computational Linguistics (Nodalida) (Pages 142–151).
Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016). Bag of tricks for
efficient text classification. arXiv preprint arXiv:1607.01759.
Jurafsky,
D.
& Martin,
J.
H.
(2000).
Speech and language
processing:
an
introduction to natural
language processing,
computational
linguistics,
and speech recognition (1st). Prentice Hall PTR.
Jurgens, D. A., Turney, P. D., Mohammad, S. M., & Holyoak, K. J. (2012).
Semeval-2012 task 2:
measuring degrees of relational similarity.
In
Proceedings of
the First Joint Conference on Lexical
and Computational
Semantics (*SEM) (Pages 356–364).
Kilgarriff, A., Reddy, S., Pomikálek, J., & Avinesh, P. V. S. (2010). A corpus
factory for many languages. In Proceedings of the Language Resources
Evaluation Conference Workshop on Web Services and Processing Pipelines
(LREC) (Pages 904–910).
Koo, T., Carreras Pérez, X., & Collins, M. (2008). Simple semi-supervised
dependency parsing. In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (Pages 595–603).
Ling,
W.,
Dyer,
C.,
Black,
A.,
& Trancoso,
I.
(2015).
Two/too simple
adaptations of word2vec for syntax problems.
In Proceedings of
the
2015 Conference of
the North American Chapter of
the Association for
Computational Linguistics: Human Language Technologies (Pages 1299–
1304).
McDonald,
R.,
Pereira,
F.,
Ribarov,
K.,
& Hajiˇ
c,
J.
(2005).
Non-projective
dependency parsing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technology and Empirical Methods
in Natural Language Processing (Pages 523–530).
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation
of word representations in vector space. (Pages 1–12).
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Dis-
tributed representations of words and phrases and their composi-
tionality. In Proceedings of Neural Information Processing Systems (NIPS
2013) (Pages 3111–3119).
Mikolov, T., Yih, W.-t., & Zweig, G. (2013). Linguistic regularities in contin-
uous space word representations. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (Pages 746–751).
74
Morin,
F.
& Bengio,
Y.
(2005).
Hierarchical
probabilistic neural
network
language model.
In Proceedings
of
the
International
Workshop on
Artificial Intelligence and Statistics (Pages 246–252).
Nilsson,
J.
& Nivre,
J.
(2008).
MaltEval:
an evaluation and visualization
tool for dependency parsing. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation LREC’08.
Nivre,
J.,
Hall,
J.,
& Nilsson,
J.
(2006).
Maltparser:
a data-driven parser-
generator for dependency parsing.
In Proceedings of
the 5th edition
of
the International
Conference on Language Resources and Evaluation
(LREC) (Pages 2216–2219).
Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: global vectors
for word representation.
In Proceedings of
the Empiricial
Methods in
Natural Language Processing (EMNLP 2014) 12 (Pages 1532–1543).
Rehurek,
R.
& Sojka,
P.
(2011).
Software framework for topic modelling
with large corpora.
In Proceedings of
the international
conference on
language resources and evaluation 2010 workshop on new challenges for
nlp frameworks (Pages 45–50).
Sahlgren,
M.
(2008).
The
distributional
hypothesis.
Italian Journal
of
Linguistics, 20(1), 33–54.
Solberg,
P.
E.,
Skjærholt,
A.,
Øvrelid,
L.,
Hagen,
K.,
& Johannessen,
J. B. (2014). The Norwegian Dependency Treebank. In Proceedings of
the ninth international
conference on language resources and evaluation
(LREC 2014) (Pages 789–795).
Stadsnes,
C.
(forthcoming).
Evaluation of
norwegian embeddings
(MSc
Thesis).
Straka, M., Hajic, J., & Straková, J. (2016). UD-Pipe: trainable pipeline for
processing CoNLL-u files performing tokenization,
morphological
analysis,
POS tagging and parsing.
In Proceedings
of
the
tenth
international
conference
on language
resources
and evaluation (LREC
2016) (Pages 4290–429).
Straka,
M.,
Hajic,
J.,
Straková,
J.,
& Hajic jr,
J.
(2015).
Parsing universal
dependency treebanks
using neural
networks
and search-based
oracle.
In Proceedings of
the International
Workshop on Treebanks and
Linguistic Theories (TLT14) (Page 208).
75
