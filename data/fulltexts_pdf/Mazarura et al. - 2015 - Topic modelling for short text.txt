Topic Modelling for Short Text
Jocelyn Mazarura,
Alta de Waal,
Frans Kanfer and Sollie Millard
Department of Statistics,
University of Pretoria
Centre for Artificial Intelligence Research (CSIR Meraka)
South Africa
Email: jocelynmazarura@yahoo.com,
alta.dewaal@up.ac.za
Abstract—The
purpose
of
this
work is
to understand the
performance of
probabilistic topic models
on short
text
such
as
microblogs
and tweets.
We
compared two topic
models
-
the Multinomial
Mixture (MM) and Latent Dirichlet Allocation
(LDA) - using perplexity as the performance measure.
The MM
model
assumes
that
a document
is
associated with one topic,
whereas LDA assumes that
a document
is a mixture of
topics.
Our initial
results indicate that MM performs better on a short
text corpus (tweets) than LDA.
I.
I
NTRODUCTION
Topic modelling is a text
mining technique that
is used
to uncover
the underlying hidden topics
or
themes
from a
large archive of
documents.
In addition,
it
also enables one
to cluster documents based on thematic similarity.
This topic
identification is achieved by assuming that each document was
formed through some generative process.
There are different
types
of
topic models,
such as
the
Mixture of Multinomials (MM) [1], Gamma-Poisson [2], Prob-
abilistic Latent
Semantic Analysis
[3]
and Latent
Dirichlet
Allocation (LDA) [4].
LDA is one of the most
popular topic
models and over the years it has proven to be very successful
when applied to long text, such as news articles and academic
abstracts
[5].
In recent
times,
there has
arisen a surge of
interest
in the analysis
of
short
text,
such as
microblogs,
which typically arise from social
media services,
such as
Twitter
and Facebook.
This
is
due
to the
realisation that
posts on microblogging websites could hold useful information
that
could be potentially useful
for
marketing (see [6])
and
prediction purposes
(see [7];
[8])
as
well
as
for
sentiment
analysis (see [9]),
for instance.
However,
unlike long text,
the
fact that short text is made up of few words poses considerable
problems when applying traditional topic models. For example,
posts made on Twitter are restricted to a mere 140 characters.
[10] Unfortunately,
LDA does not perform well on short text,
however
much research is being conducted to find methods
and techniques that can be implemented to effectively handle
short text. Pooling [5] the short text to form a longer document
is one such method (see also [10],
[11]).
In this paper we posit that the simpler MM topic model will
perform better on short text than its more popular counterpart,
LDA. The latter topic model makes the generative assumption
that
each document
in the corpus is derived from multiple
different topics with varying probabilities.
On the other hand,
the simpler MM topic model
assumes that
each document
is
associated with only a single topic, which we believe to be an
intuitively sensible assumption for short text,
such as a tweet.
To this end,
our aim is to compare the performance of LDA
and MM on a corpus with normal sized texts and a corpus of
short texts and we use perplexity as the performance measure.
The paper is organised as follows. We first give an overview
of the MM and LDA topic models.
After presenting the data
we give a brief
discussion of
the experimental
design,
then
present and evaluate the results.
II.
G
ENERATIVE
T
OPIC
M
ODELS
As formerly mentioned, in this section we present the basic
framework under which the MM and LDA are applied.
Both
models make the following assumptions:
1) Documents follow a bag-of-words representation:
Each
document,
denoted
C
d
,
is regarded as a collection of words
which occur
with varying frequencies.
In essence the gram-
matical
structure is
entirely ignored and each document
is
regarded as
a vector
of
word counts
and thus,
the corpus,
denoted
C = (C
wd
)
w=1,...,n
w
,d=1,...,n
d
,
is viewed as a high
dimensional
vector
containing the various word frequencies
where
n
w
and
n
d
denote the number
of
documents in the
corpus and the number of words in a document
respectively
([12],[1]).
2) Each document
belongs to one (MM)
or more (LDA)
topics:
A topic is regarded as a distribution over the words in
the vocabulary set of the corpus and the vocabulary is the set
of unique words that make up the corpus. In practice, a corpus
containing many documents is likely to contain many topics
and each topic is assumed to follow a multinomial distribution.
3) Documents are formed by a generative process:
The
generative process for each model is presented in detail in the
following sections.
A.
Multinomial Mixture
Under
the mixture of
multinomials model,
it
is assumed
that each word belongs to each topic with a specific probability,
thus a topic is regarded as following a multinomial distribution
over words.
Since a document
is associated with exactly one
topic the corpus is modelled as a mixture of multinomials and
each document follows a multinomial mixture.
Letting
n
T
denote the number of topics and
l
d
denote the
length of document
C
d
then the probability density function
of a document can be written as:
p(C
d
|α, β) =
n
t
X
t=1
α
t
l
d
Q
n
w
w=1
C
wd
!
n
w
Y
w=1
β
C
wd
wt
.
The notation
α = (α
1
, α
2
, ..., α
n
t
)
is the vector of mixing
weights and
β
t
= (β
1t
, β
2t
, ..., β
n
w
t
)
(for
t = 1, 2, ..., n
T
) are
the parameters of the multinomial distribution associated with
topic
t
such that
P
n
t
t=1
α
t
= 1
and
P
n
w
w=1
β
wt
= 1
.
Due to
the conjugacy of the Dirichlet
distribution to the multinomial
distribution, Dirichlet priors are set on
α
(with hyperparameter
λ
α
> 0
) and on the columns of
β
(with hyperparameter
λ
β
>
0
).
[1]
Thus,
according to [1]
it
follows that
under
the mixture
of
multinomials
model
the following underlying generative
process
for
the
whole
corpus,
C = (C
1
, C
2
, ..., C
n
d
)
,
is
assumed:
1)
sample
α ∼ Dir(λ
α
, ..., λ
α
)
2)
for
every
topic
t
= 1, 2, ..., n
T
,
sample
β
t
∼
Dir(λ
β
, ..., λ
β
)
3)
for every document
d = 1, ..., n
d
:
a)
sample a topic
T
d
in
{1, 2, ..., n
T
}
with prob-
abilities
α = (α
1
, α
2
, ..., α
n
t
)
b)
sample
l
d
words from a multinomial
distri-
bution with probability vector
β
T
d
Graphically,
this process is represented in Figure 1.
Fig.
1.
Graphical Representation of Mixture of Multinomials
w
i
denotes the
i
-th word in the vocabulary.
Unshaded and
shaded nodes (circles) represent observed and unobserved ran-
dom variables, respectively. Plates (rectangles) denote repeated
structures whilst arrows between nodes, called edges, eg. x
→
y,
denote a possible dependency of y on x.
In order
to estimate
the
parameters
of
the
model
we
introduce the latent variable
z
d
= (z
d1
, z
d2
, ..., z
dn
T
)
where
z
dt
=

1
if document
C
d
is from topic
t
0
otherwise
for
t ∈ {1, ..., n
T
}
.
The latent
indicator
vector
and the observed document
vector form the complete dataset whose joint density function
is
p(C
d
, z
d
|α, β) =
n
T
Y
t=1
α
t
l
d
Q
n
w
w=1
C
wd
!
n
w
Y
w=1
β
C
wd
wt
!
z
dt
.
Since each document
is
independent
it
follows
that
the
corpus likelihood is
L = log
n
d
Y
d
p(C
d
, z
d
|α, β)
∝
n
d
X
d=1
n
T
X
t=1
z
dt
log α
t
+
n
w
X
w=1
C
wd
log β
wt
!
.
(1)
In the supervised scenario where the documents are la-
belled,
that
is
z
d
is known,
the unknown parameters can be
estimated using the Naive Bayes classifier.
Unfortunately,
due
to the presence of the latent
variable,
maximizing expression
(1)
is
generally intractable,
therefore
we
turn to the
EM
algorithm.
Taking
α
0
and
β
0
to be the current
estimates of
the the
parameters,
in the expectation step (E-step),
z
dt
is estimated
by
θ
dt
= E(z
dt
|C, α, β) =
α
0
t
Q
n
w
w=1
β
0C
wd
wt
P
n
T
t
0
=1
α
0
t
0
Q
n
w
w=1
β
0
C
wd
wt
0
.
In the maximization step (M-step),
the parameters
{α
t
}
and
{β
wt
}
are updated:
α
t
∝ λ
α
− 1 +
n
d
X
d=1
θ
dt
β
wt
∝ λ
β
− 1 +
n
d
X
d=1
C
wd
θ
dt
such that

P
n
t
t=1
α
t
= 1
P
n
w
w=1
β
wt
= 1
for
t ∈ {1, ..., n
T
}
.
Since the vocabulary set
is very large,
many words will
have
very low probabilities
of
being in a
specific
topic.
According to Rigouste et
al.,
word probabilities should not
get
too small,
thus the smoothing parameter
λ
β
− 1
is set
to
be around the optimal value of 0.2.
In addition,
since
λ
α
− 1
is always negligible [1], we set
λ
α
− 1
and
λ
β
− 1
to be equal
to 0 and 0.2,
respectively,
in our experiments.
B.
Latent Dirichlet Allocation
LDA extends the generative process of
MM to a word-
level mixture: For each document a topic proportion is drawn,
denoted
θ
, and then, from each topic proportion, topic-specific
words are drawn.
Because of this flexibility,
LDA is referred
to as an admixture model
[13].
An interesting fact
is that
in
genetics an admixture model is referred to as a mixture whose
components
are
themselves
mixtures
of
different
features.
Modelling of admixtures for discrete data was done to model
population genetics before LDA was proposed for text
[14].
Graphically,
this process is represented in Figure 2.
The focus of our study is mainly on the MM model so we
do not discuss the LDA any further in this paper. However, the
reader is directed to [15] and [4] for further reading on this
topic.
Fig.
2.
Graphical Representation of Latent Dirichlet Allocation
III.
D
ATA
The input
data of a topic model
is the document
x word
matrix,
C
wd
,
where each cell
represents
the occurrence of
word
i
in
document
j
.
In order to perform the comparison of
the MM and LDA model, both methods were applied to a test
and training set of data from a 1987 Reuters-21578 newswire
(long text) and a corpus of Tweets (short
text).
The data are
described further in the following subsections.
A.
Reuters-21578
The
documents
in
the
Reuters-21578
Distribution
1.0
collection appeared on the Reuters
newswire in 1987.
The
Reuters-21578,
Distribution 1.0 test
collection is
available
from David D.
Lewis’
professional
home
page,
currently
http://www.research.att.com/lewis.
It
is a collection of 21578
documents.
In processing the data we removed a standard list
of stop words,
special
characters and numbers.
In Table I we
compare the number of unique words and also the sparseness
of
the document
x word matrix after
omitting words
that
occurred only once,
twice,
etc.
or
less
in the corpus.
As
mentioned before,
since the vocabulary is very large,
many
words
will
have probabilities
of
occurring in a topic that
are close to zero.
This is undesirable as it
makes the model
unstable,
thus removing words with low occurrences helps to
improve the performance of
the model.
We removed words
that occurred a maximum of ten times or less,
as beyond this
too much information is lost. Varying the number of words to
be omitted from the corpus is for investigative purposes only.
Typically, in literature words occurring less than two times are
omitted from the corpus.
We investigate the effect of varying
this number on the sparseness of the matrix and although it
reduces the size of the vocabulary dramatically,
the effect
on
the matrix sparseness is not that much.
TABLE I.
R
EUTERS
-21578 D
ATA
S
ET
Word
Frequency
Unique Words
Zero
element
ratio
0
69267
0.99
1
29332
0.998
2
20480
0.997
3
16644
0.997
5
12521
0.996
10
8476
0.994
B.
Twitter
The Twitter dataset
is a collection of tweets collected in
the period April 2014 - September 2014. We decided to focus
on the current
hot
debate around the ‘low carb high fat’ diet
and collected tweets from one user known to oppose the diet
(@GeorgeClaasens) and two users that strongly promotes the
diet (@ProfTimNoakes, @ditchthe carbs). It is a collection of
4,738 documents.
We removed a standard list
of stop words,
special
characters,
numbers and urls.
We also omitted tweets
with less than three words after data processing.
This results
in tweets
being omitted from the training corpus.
Table II
compares
the
Twitter
data
set
statistics
given a
minimum
number of word occurrences in the corpus.
TABLE II.
T
WITTER
D
ATA
S
ET
Word
Frequency
Unique Words
Zero
element
ratio
Number
of
tweets
in
training set
0
14455
0.999
4638
1
4965
0.998
4638
2
3127
0.997
4638
3
2297
0.997
4638
5
1427
0.995
4638
10
672
0.993
4638
Tables I and II show that
at
least
in terms of sparseness,
normal
sized documents
and short
text
documents
exhibits
similar
behaviour
-
and both pose statistical
challenges for
generative models.
IV.
E
XPERIMENTAL
S
ETUP
In our experiments,
each dataset
is split
into two parts:
a
training set
consisting of 90% of the sample data and a test
set
consisting of
the remaining data.
We then implemented
the online variational
Bayes (VB)
algorithm for
LDA [16].
Online LDA is based on online stochastic optimization with
a gradient
step.
Online LDA is implemented in the Python
Topic modelling module Gensim [17].
For
the MM model
implemention,
we programmed the algorithm in Python as
described in the article by Rigouste et al.
[1].
In order to perform the comparison of the MM and LDA
model,
both methods
are applied to each set
of
data from
the Reuters-21578 and Twitter corpus,
and the perplexity for
each test
set
is calculated in order
to assess the predictive
performance
of
each model.
In general,
the
perplexity is
calculated as follows
P erplexity(C
test
) = exp{−
1
l
d,test
L}
where
C
test
,
l
d,test
and
L
denote the test
set,
length of
the test
set
and the log-likelihood,
respectively.
Therefore,
to
calculate the perplexity for the MM model
L
is substituted by
the expression in (1). The complete log-likehood is used since
the incomplete log-likehood cannot
be computed.
In the case
of LDA we use the lower bound on perplexity as a proxy [16].
It is desirable to achieve a high likelihood on the test set,
thus a low perplexity is indicative of good performance [4].
V.
R
ESULTS
Before implementing either
algorithm it
is important
to
note that
the user must
first
select
the number of topics that
are assumed present
in the corpus.
Typically the number
of
topics is set to any value between 10 and 100.
A.
Results on Reuters Corpus
We report
on perplexity results for the Reuters corpus in
Figures 3 (omitting words ocurring only once in corpus) and
4 (omitting words occurring three times or less in corpus). We
varied the number of topics between 10 and 50 topics for the
Reuters corpus.
The experiments were repeated ten times,
but
in this paper we report on the mean value only.
The perplexity results for the MM and LDA on the Reuters
corpus correspond with the literature.
The results (especially
in Figure 4) indicate smaller perplexity values for LDA than
Multinomial Mixture which is expected.
Fig.
3.
Perplexities on Reuters Corpus - omitting words occurring once in
the corpus
B.
Results on Twitter Corpus
Figure 5 shows the perplexity results for the Twitter corpu.
Because of the small size of the corpus, we only modelled 10
topics on the Twitter corpus. Instead of varying the number of
topics that
the corpus was assumed to contain,
we observed
how the perplexities varied as words that
occurred no more
than 2, 3, 5 and 10 times were omitted. According to our results
the Multinomial Mixture performs better in general than LDA
(with lower perplexity values).
Fig. 4.
Perplexities on Reuters Corpus - omitting words occurring three times
or less in the corpus
Fig.
5.
Perplexities on Twitter Corpus - omitting words occurring 2,
3,
5
and 10 times or less in the corpus
In Tables III and IV we illustrate the top-6 words in each
topic for both models. Some topics presumably emphasise Tim
Noakes’ book The Real Meal Revolution while other focus on
the science behind the data.
TABLE III.
T
WITTER
T
OPICS FROM
M
ULTINOMIAL
M
IXTURE
M
ODEL
fat
cholesterol
noakes
lchf
welcome
lchf
heart
diet
real
share
jonathan
food
science
banting
glad
witt
risk
health
tim
please
bigfatsurprise
obesity
read
helpdietsa
low
big
diet
evidence
lost
carb
fat
share
tim
bigfatsurprise
draseemmalhotra
diet
carb
noakes
diet
science
carb
low
book
science
noakes
insulin
realfood
read
time
media
diabetes
wheatfree
years
hard
change
high
easy
thanks
doctomboyles
nobel
Although no formal
tests have been done to test
this as
yet,
it
appears that
anecdotally the topics uncovered by the
TABLE IV.
T
WITTER
T
OPICS FROM
LDA M
ODEL
fat
real
carbs
draseemmalhotra
lchf
science
meals
new
book
noakes
check
change
bigfatsurprise
draseemmalhotra
low
realfood
nation
study
annchildersmd
carb
evidence
media
reverse
sugar
share
diabetes
proven
drbriffa
scientist
best
diet
food
banting
eat
cape
high
health
old
talk
free
need
thanks
cancer
healthy
loss
insulin
cerealkillers
swim
prof
human
read
industry
lewispugh
reason
person
weight
interview
jonathanwitt
keto
protein
MM appear
to be more interpretable than those uncovered
by the
LDA.
This
serves
as
a
positive
initial
result,
but
also presents another area of research,
namely assessing topic
model performance based on the quality of the inferred topics.
VI.
C
ONCLUSION
The purpose of this paper is to understand the challenges
associated with applying topic modelling to short
text
and
focus in particular on two generative topic models.
We report
on initial
results with perplexity calculations indicating that
the Multinomial Mixture topic model performs better than the
LDA on short
text.
We also calculated baseline perplexities
on ‘normal size’ text - the Reuters corpus.
We want to achieve two goals with topic modelling:
1)
Analyse the corpus.
For
example,
if
a user
has no
knowledge about the lfhc diet and the debate around
it, then analysing the topics should give a categorised
summary about the text in the corpus.
2)
Analyse the topics.
For example,
if one of the topics
exhibits special
interest,
then the user want
to view
the documents associated with that topic.
We acknowledge techniques to either supplement
or con-
catenate short texts with various techniques in order to improve
topic model
performance,
but
the purpose of this article is to
specifically address the short text generative assumptions.
Although our initial results indicate that we are on the right
track to advocate the use of the Multinomial Mixture on short
text
rather than the LDA,
the following limitations from this
work need to be taken into account:
1)
Our Twitter corpus was very small.
The test set size
on the Twitter
corpus was only 483 tweets.
Future
work should include experiments on a significantly
larger Twitter corpus.
2)
We only used perplexity as a performance measure.
Future work should include other performance mea-
sures, such as word intrusion and topic intrusion [?],
using labelled data sets and alignment techniques.
A
CKNOWLEDGMENTS
We thank the National
Research Fund of
South Africa,
CAIR and the Department
of
Statistics of
the University of
Pretoria for
their
support.
In addition,we would also like to
thank the anonymous reviewers for their comments.
R
EFERENCES
[1]
L.
Rigouste,
O.
Capp,
F.
Yvon,
”Inference
and evaluation of
the
multinomial mixture model for text clustering.”,
Information processing
management,
43(5),
pp.
1260-1280,
2007.
[2]
J.
Canny.
”GaP:
a factor
model
for
discrete data.” In Proceedings of
the 27th annual
international
ACM SIGIR conference on Research and
development in information retrieval,
pp.
122-129.
ACM,
2004.
[3]
T.
Hofmann,
”Probabilistic latent semantic indexing.” In Proceedings of
the 22nd annual international ACM SIGIR conference on Research and
development in information retrieval,
pp.
50-57.
ACM,
1999.
[4]
D.M. Blei, A.Y. Ng M.I. Jordan, ”Latent dirichlet allocation.” the Journal
of machine Learning research,
3,
993-1022,
2003.
[5]
R. Mehrotra, S. Scott, B. Wray X Lexing, ”Improving lda topic models
for microblogs via tweet pooling and automatic labeling.” In Proceedings
of
the
36th international
ACM SIGIR conference
on Research and
development in information retrieval,
pp.
889-892.
ACM,
2013.
[6]
T. Iwata
H. Sawada,Topic model for analyzing purchase data with price
information. Data Mining and Knowledge Discovery, 26(3), pp.559-573,
2013.
[7]
J.
Jollen,H.
Mao
X.
Zeng,
”Twitter mood predicts the stock market.”
Journal of Computational Science,
2(1),
1-8,
2011.
[8]
A.
Bermingham A.F.
Smeaton,
”On using Twitter to monitor political
sentiment and predict election results.” 2011.
[9]
C. Lin H. Yulan, ”Joint sentiment/topic model for sentiment analysis.” In
Proceedings of the 18th ACM conference on Information and knowledge
management,
pp.
375-384.
ACM,
2009.
[10]
L.
Hong
D.D Brian,
”Empirical
study of topic modeling in twitter.”
In Proceedings of
the First
Workshop on Social
Media Analytics,
pp.
80-88.
ACM,
2010.
[11]
Y.
Song,
H.
Wang,
Z.
Wang,
H.
Li
W.
Chen,
”Short
text
concep-
tualization using a probabilistic knowledgebase.” In Proceedings of the
Twenty-Second international
joint
conference on Artificial
Intelligence-
Volume Volume Three,
pp.
2330-2336.
AAAI Press,
2011.
[12]
A. De Waal, ”Topic Models with Structured Features.” PhD diss., North-
West University,
Potchefstroom Campus,
2010.
[13]
H.
Heinrich,
”Parameter estimation for text analysis”,
2004.
[14]
D.
Falush,
M.
Stephens
J.K.
Pritchard,
”Inference
of
population
structure using multilocus genotype data: linked loci and correlated allele
frequencies” Genetics 164(4),
pp.
1567-1587,
2003.
[15]
D.M Blei,
”Probabilistic topic models.” Communications of the ACM,
55(4),
77-84,
2012.
[16]
M.D.
Hoffman,
D.M Blei
F.
Bach,
”Online
Learning for
Latent
Dirichlet Allocation.” ,
in John D.
Lafferty; Christopher K.
I.
Williams;
John Shawe-Taylor; Richard S. Zemel
Aron Culotta, ed., ’NIPS’ , Curran
Associates,
Inc.,
pp.
856-864,
2010.
[17]
ˇ
R.
Radim S.
Petr,
“Software Framework for
Topic Modelling with
Large Corpora”,
Proceedings
of
the LREC 2010 Workshop on New
Challenges for NLP Frameworks,
pp.
45-50,
2010.
[18]
J.
Chang et
al.,
”Reading tea leaves:
How humans
interpret
topic
models.” Advances in neural information processing systems.
2009.
