Proceedings of the Workshop on Figurative Language Processing, pages 121–123
New Orleans, Louisiana, June 6, 2018.
c
2018 Association for Computational Linguistics
Conditional Random Fields for Metaphor Detection 
Anna Mosolova
1
, Ivan Bondarenko
2
and Vadim Fomin
3 
1,2,3
Novosibirsk State University 
1
a.mosolova@g.nsu.ru 
2
i.yu.bondarenko@gmail.com 
3
wadimiusz@gmail.com
Abstract 
We 
present 
an 
algorithm 
for 
detecting 
metaphor in sentences which was used in 
Shared Task on Metaphor Detection by 
First Workshop on Figurative Language 
Processing. The algorithm is based on dif-
ferent features and Conditional Random 
Fields. 
1
Introduction 
In this paper, we present a system which predicts 
metaphoricity of the word depending on its neigh-
bors. We used VU Amsterdam corpus (Steen et 
al., 2010) given by competition’s organizers, 10 
features which were also given by competition’s 
organizers and algorithm of Conditional Random 
Fields for predictions that are depending on previ-
ous ones. 
2
Related Work 
A lot of papers describe methods for metaphor de-
tection, but the closest in performance is the arti-
cle by Rai et al. (2016). It proposes to use Condi-
tional Random Fields for metaphor detection. The 
authors also use features based on syntax, con-
cepts, affects, and word embeddings from MRC 
Psycholinguistic 
Database 
and 
coherence 
and 
analogy between words which are taken from 
word embeddings given by Huang et al. (2012). 
Moreover, they use synonymy from WordNet. 
This work is very similar to our due to some 
similar features and the main algorithm which is 
CRF. 
3
Data 
3.1
Dataset 
As a dataset was used VU Amsterdam corpus 
(Steen et al., 2010). It consists of 117 texts divid-
ed into 4 parts (academic, news, fiction, conversa-
tion). 
It was divided into two parts: train and test. The 
model was trained on the train set and evaluated 
on the test set. 
3.2
Features 
Features were given by competition’s organiz-
ers. Set of features consists of: 
•
Unigrams: All words from the training data 
without any changes; 
•
Unigram lemmas: All words from the train-
ing data in their normal form; 
•
Part-of-Speech tags: They were generated 
by Stanford POS tagger 3.3.0 (Toutanova et 
al. 2003); 
•
Topical LDA: Latent Dirichlet Allocation 
(Blei et al., 2003) for deriving a 100-topic 
model from the NYT corpus years 2003-
2007 
(Sandhaus, 
2008) 
for 
representing 
common topics of public discussions. The 
NYT 
data 
was 
lemmatized 
using 
NLTK 
(Bird, 2006) and the model was built using 
the 
gensim 
toolkit 
(R. 
Řehůřek 
and 
P. 
Sojka, 2010); 
•
Concreteness: 
For 
this 
feature 
was 
used 
Brysbaert 
et 
al. (2013) database of con-
creteness ratings for about 40,000 English 
words. The mean ratings, ranging 1-5, are 
binned in 0.25 increments; each bin is used 
as a binary feature; 
121
•
WordNet: 15 lexical classes of verbs based 
on their general meanings; 
•
VerbNet: Classification based on syntactic 
frames of verbs ; 
•
Corpus: 150 clusters of verbs using their 
subcategorization 
frames 
and 
the 
verb’s 
nominal arguments as features for cluster-
ing. 
All of these features were described in Beig-
man Klebanov et al. (2014), Beigman Kleba-
nov et al. (2015) and Beigman Klebanov et al. 
(2016). 
3.3
Algorithm 
As an algorithm for classification was used Con-
ditional Random Fields which was described in 
Lafferty et al. (2001). This algorithm depends on 
previous predictions making the future ones and it 
was crucial because metaphoricity of a word in a 
sentence relies on its neighbors. Also, this classi-
fier can work with a big amount of features, so we 
used a lot of them in this work and it was helpful 
for the further results. 
4
Experiments 
We tried different parameters that were provid-
ed in the crfsuite (Okazaki, 2007). There were five 
training algorithms such as lbfgs (gradient de-
scending using the L-BFGS method), l2sgd (sto-
chastic gradient descend with L2 regularization 
term), Averaged Perceptron, Passive Aggressive, 
Adaptive Regularization Of Weight Vector. The 
best training algorithm was lbfgs. 
Moreover, we used a different amount of itera-
tions, and its amount affects the loss because there 
is no limit to the number of iterations in the lbfgs-
algorithm. 
Furthermore, some experiments with regulari-
zation were conducted. Regularization was used 
for reducing the generalization error and it is im-
portant in CRF. For the selection of the most ap-
propriate parameters for regularization, we used 
RandomizedSearchCV 
from 
scikit-learn 
(http://scikit-learn.org). 
We 
used 
sklearn-crfsuite 
that 
is 
the 
special 
wrapper 
of 
crfsuite 
written 
in 
C 
for 
Python 
(https://github.com/TeamHG-Memex/sklearn-
crfsuite) for computing the algorithm. 
As a metric for evaluating the score was taken 
F-score. 
The best F-score had the algorithm with 200 it-
erations, lbfgs-algorithm, c1 regularization and c2 
regularization that equal to 0.1. 
The result obtained with these parameters was 
evaluated using a held-out set from the train set. 
F-score of this model and other experiments are 
presented in table 1 for All-POS track and for 
Verb track. 
Parameters 
F-score 
for 
all-POS 
F-score 
for 
Verbs track 
lbfgs, 
200 
iterations, 
c1=c2=0.1 
0.8621 
0.7417 
lbfgs, 
100 
iterations, 
c1=c2=0.1 
0.8593 
0.739 
lbfgs, 
50 
iterations, 
c1=c2=0.1 
0.8601 
0.7333 
lbfgs, 
100 
iterations, 
c1=0.2353, 
c2=0.0329, 
0.8586 
0.7528 
l2sgd, 
100 
iterations, 
c2=0.1 
0.8455 
0.6343 
Averaged Per-
ceptron, 
100 iterations 
0.8303 
0.7165 
Passive 
Ag-
gressive, 
100 iterations 
0.8483 
0.7327 
Adaptive Reg-
ularization 
Of 
Weight Vector, 
100 iterations 
0.8459 
0.6973 
5
Results 
As a result, our best-trained model was based on 
10 features described below and CRF classifier 
with lbfgs and 200 iterations and it has F-score 
equal to 0.8621 for All-POS track. As for the Verb 
track, the best model was also based on lbfgs, had 
100 iterations and c1 equal to 0.2353, c2 equal to 
0.0329 with F-score 0.7528. 
Table 1The results of the experiment for All-POS and 
Verb tracks. 
122
These results are obtained using validation with a 
part of the train set, and as for the test set, for All-
POS 
track, 
the 
result 
measured 
by 
F-score 
is 
0.138 and for Verb track is 0.246. 
The results differ as it is possible that validation 
on a small part of the train set (33%) is not as ac-
curate as validation on the test set which usually 
consists of the larger number of sentences. 
6
Conclusion 
We used Conditional Random Fields for the 
task of metaphor detection. Due to the large 
number of features, this classifier worked very 
well, and it is assumed that increasing the num-
ber of features will improve the performance of 
the algorithm. 
References 
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. 
Latent Dirichlet Allocation
. Journal of Ma-
chine Learning Research, 3:993–1022. 
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. 
Concreteness ratings for 40 thou-
sand generally known english word lemmas
. Be-
havior Research Methods, pages 1–8. 
Beata Beigman Klebanov, Chee Wee Leong, Michael 
Heilman, and Michael Flor. 2014. 
Different texts, 
same metaphors: Unigrams and beyond
. In Pro-
ceedings of the Second Workshop on Metaphor in 
NLP, pages 11–17, Baltimore, MD, June. Associa-
tion for Computational Linguistics. 
Beata Beigman Klebanov, Chee Wee Leong, and Mi-
chael Flor. 2015. 
Supervised word-level metaphor 
detection: Experiments with concreteness and re-
weighting of examples
. In Proceedings of the Third 
Workshop 
on 
Metaphor 
in 
NLP, 
pages 
11–20, 
Denver, Colorado, June. Association for Computa-
tional Linguistics. 
Huang, Eric H., Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012
. Improving word 
representations 
via 
global 
context 
and 
multiple 
word prototypes.
Proceedings of the 50th Annual 
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1. Association for 
Computational Linguistics. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. 
Conditional 
random 
fields: 
Probabilistic 
models for segmenting and labeling sequence data.
Proc. 18th International Conf. on Machine Learn-
ing. Morgan Kaufmann. pp. 282–289. 
Naoaki Okazaki. 2007. 
CRFsuite: a fast implementa-
tion 
of 
Conditional 
Random 
Fields 
(CRFs). 
http://www.chokkan.org/software/crfsuite/ 
Sunny Rai, Shampa Chakraverty, and Devendra K. 
Tayal. 2016. Supervised metaphor detection using 
conditional random ﬁelds. In Proceedings of the 
Fourth Workshop on Metaphor in NLP, pages 18– 
27, San Diego, California. Association for Compu-
tational Linguistics. 
Radim 
Řehůřek 
and 
Petr 
Sojka. 
2010. 
Software 
Framework for Topic Modelling with Large Cor-
pora
. In Proceedings of the LREC 2010 Workshop 
on New Challenges for NLP Frameworks, pages 
45– 50, Valletta, Malta, May. ELRA. 
Evan Sandhaus. 2008. 
The New York Times Annotated 
Corpus
. LDC Catalog No: LDC2008T19. 
Gerard Steen, Aletta Dorst, Berenike Herrmann, Anna 
Kaal, Tina Krennmayr, and Trijntje Pasma. 2010. 
A 
Method 
for 
Linguistic 
Metaphor 
Identification
. 
Amsterdam: John Benjamins. 
Kristina Toutanova, Dan Klein, Christopher Manning, 
and Yoram 
Singer. 
2003. 
Feature-Rich 
Part-of-
Speech Tagging with a Cyclic Dependency Net-
work
. In Proceedings of NAACL, pages 252–259. 
123
