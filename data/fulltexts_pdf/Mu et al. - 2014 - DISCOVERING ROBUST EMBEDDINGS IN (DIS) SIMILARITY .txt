Computational Intelligence, Volume 0, Number 0, 2012
DISCOVERING ROBUST EMBEDDINGS IN (DIS)SIMILARITY SPACE
FOR HIGH-DIMENSIONAL LINGUISTIC FEATURES
T
INGTING
M
U
,
1
M
AKOTO
M
IWA
,
1
J
UNICHI
T
SUJII
,
2
AND
S
OPHIA
A
NANIADOU
1
1
National Centre for Text Mining, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
Recent
research has shown the effectiveness of rich feature representation for tasks in natural
language
processing (NLP). However, exceedingly large number of features do not always improve classification performance.
They may contain redundant information, lead to noisy feature presentations, and also render the learning algorithms
intractable. In this paper, we propose a supervised embedding framework that modifies the relative positions between
instances to increase the compatibility between the input features and the output labels and meanwhile preserves the
local distribution of the original data in the embedded space. The proposed framework attempts to support flexible
balance between the preservation of intrinsic geometry and the enhancement of class separability for both interclass
and intraclass instances. It takes into account characteristics of linguistic features by using an inner product-based
optimization template.
(Dis)similarity features,
also known as empirical kernel mapping,
is employed to enable
computationally tractable processing of extremely high-dimensional
input,
and also to handle nonlinearities in
embedding generation when necessary.
Evaluated on two NLP tasks with six data sets,
the proposed framework
provides better
classification performance than the support
vector
machine without
using any dimensionality
reduction technique. It also generates embeddings with better class discriminability as compared to many existing
embedding algorithms.
Received 18 March 2011; Revised 24 June 2012; Accepted 24 June 2012
Key words:
dimensionality reduction, embedding, classification, (dis)similarity, rich feature representation,
natural language processing.
1.
INTRODUCTION
Many tasks in natural language processing (NLP) are treated as classification problems
and tackled by employing a rich representation with large number of linguistic features, for
instance,
dependency parsing (Zhao et
al.
2009),
semantic role labeling (Surdeanu et
al.
2007), named entity recognition (Nadeau and Sekine 2007), relation extraction (Miwa et al.
2009), coreference resolution (Ng 2010), sentiment analysis (Zaidan and Eisner 2007) and
textual entailment recognition (Zanzotto, Pennacchiotti, and Moschitti 2009) . The extracted
features represent the target problem using various combination of syntactic and semantic
information obtained from multiple source (such as NLP tools and dictionaries) with the aim
of obtaining a classifier with state-of-the-art performance. Possible linguistic features include
words, semantic and (or) syntactic features, and their generalization and (or) combination.
Words are treated as fundamental features and can be normalized by linguistic resources like
dictionaries and tools,
such as lemmatizers,
stemmers,
and part-of-speech (POS) taggers.
Because the word-based features on their own can be ambiguous and highly correlated,
and cannot fully cover the context of the target instances,
other features are employed to
handle these issues, e.g., semantic features like named entities and semantic roles, syntactic
features produced by parsers, combined features like n-grams, as well as conjunctive features
(Yoshinaga and Kitsuregawa 2010). In general, the extracted linguistic features can be very
sparse, with large amount of zero values in the corresponding feature matrices. Sometimes,
when kernel-based classifiers are used, it is possible to construct the kernel function directly
from the semantic and syntactic information of the text,
such as tree-kernel,
subsequence
kernel, etc., together with their compositions (Zhou, Qian, and Fan 2010).
Address correspondence to Tingting Mu, National Centre for Text Mining, University of Manchester, 131 Princess Street,
M1 7DN, Manchester, UK; e-mail: tingtingmu@me.com
C

2012 Wiley Periodicals, Inc.
C
OMPUTATIONAL
I
NTELLIGENCE
Given the many possible ways to obtain different types of linguistic features, the final fea-
ture presentation is manually decided from these huge amount of possible features according
to the task itself, linguistic knowledge, and the performance of the system. Since the feature
size is large, individual features are not informative, and many features are highly correlated
with each other, the final feature presentation is usually decided by feature types, which is
a simple combination of different types of features,
e.g.,
bag-of-words,
dictionary-based.
However, it is very difficult to find an optimal combination and many researchers attempt to
include as many features as possible to prevent information loss. Such strategy of rich feature
representation often produces large-scale, sparse, and noisy data sets containing redundant
information, where perceptually meaningful data structure contained in the large amount of
linguistic features has much fewer independent degrees of freedom than the input dimen-
sionality. To classify and process this type of data sets, apart from support vector machines
(SVMs) that show quite robust and are widely used in the field (Sch
¨
olkopf 1997; Cristianini
and Shawe-Taylor 2000; Leopold and Kindermann 2002; Bergsma,
Lin,
and Schuurmans
2010; Wang and Chiang 2011),
feature dimensionality reduction is also a very important
technique. It can remove redundant feature information, eliminate noise, and finally provide
an accurate low-dimensional representation of the original features while (when applicable)
supporting improved discrimination between classes .
In NLP, commonly used dimensionality reduction techniques include feature selection
(Lewis 1992; Bekkerman et al. 2003; Li et al. 2009), feature clustering (Bekkerman et al.
2003; Dhillon, Mallela, and Kumar 2003), and latent semantic indexing (LSI) (Deerwester
et
al.
1990;
Gong and Liu 2001;
Blei
et
al.
2003;
Howland,
Jeon,
and Park 2003;
Kim,
Howland, and Parl 2005; Lee 2007). Feature selection and feature clustering can process large
number of features and reduce data redundancy but possess various drawbacks. For example,
feature selection removes uninformative features evaluated according to the performance
of a learning algorithm or certain feature scores (e.g., information gain). This may not be
appropriate for highly correlated features because features that work inefficiently on their
own may boost the classification performance when combined with other features. Feature
clustering employs feature clusters as a compact representation of an input instance, however,
this may cause information loss by merging features from the same cluster and is heavily
affected by the used clustering algorithm. As for LSI, it produces k-rank approximation of the
original feature matrix and performs quite well when being combined with SVMs. Various
singular value decomposition (SVD) algorithms for processing large-scale sparse data matrix
(Berry 1992; Golub and Loan 1996; Brand 2006) enable the application of LSI to process
large-scale sparse feature matrix. However, LSI is only a compression method relying on the
input features. It is unable to generate embeddings with improved class separability when
the data sets possess incompatible arrangements between the input features and the output
labels, such as when patterns from different/same classes are located closely/distantly in the
feature space.
In machine learning, many sophisticated methodology for dimensionality reduction has
been developed through manifold learning and spectral analysis, such as principal component
analysis (PCA) (Jolliffe 1986), multidimensional scaling (MDS) (Torgerson 1952), Laplacian
eigenmaps (LE) (Belkin and Niyogi
2003),
locally linear embedding (LLE) (Roweis and
Saul
2000),
locality preserving projection (LPP) (He and Niyogi
2003),
and orthogonal
LPP (OLPP) (Kokiopoulou and Saad 2007).
These methods attempt
to preserve certain
properties and characteristics of the original high-dimensional data, such as data variance
by PCA,
Euclidean distances between the original
data points by MDS and the intrinsic
geometry of the data by LE,
LLE,
LPP,
and OLPP.
For classification tasks that
present
difficulties in representing differences between classes using original
input
features,
the
class information of training instances can be considered during embedding generation in
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
order to produce a better match between the feature structure and the class structure. This
corresponds to supervised embedding methods, such as Fisher discriminant analysis (FDA)
(Sugiyama 2007), maximum margin criterion (MMC) (Li, Jiang, and Jhang 2006), canonical
correlation analysis (CCA) (Hardoon, Szedmak, and Shawe-taylor 2004) and repulsion OLPP
(OLPP-R) (Kokiopouloua and Saadb 2009),
etc.
Different
ways to incorporate the class
structure into the embedding generation lead to different
algorithms (Mu et
al.
2012a,
b),
and may generate embeddings with varying discriminative power.
The above methods
require to compute eigendecomposition or inverse of a full matrix with its size equal to the
dimensionality of input features. They become computationally expensive as the number of
input features dramatically increases. Given comparatively low number of training instances,
their kernelized versions (Sch
¨
olkopf,
Smola,
and M
¨
uller 1998; Mika et al.
1999; Shawe-
Taylor and Christianini 2004; Kokiopoulou and Saad 2007) can be more efficient, of which
the main computational cost cubically depends on the number of training instances.
In this work, our research is focused on learning a small number of robust embeddings
from extremely high-dimensional
(millions) linguistic features to facilitate classification
problems with fairly low number (thousands) of training instances. This type of classification
problem appears in many recent high-level NLP tasks, where it is quite expensive to obtain
annotated training instances,
but
easier to generate rich features using sophisticated text
analysis tools.
We propose a computationally tractable supervised embedding framework
that works on an inner product-based optimization template and generates embeddings from
(dis)similarity features.
The proposed framework supports local
distribution preservation
and class separability enhancement for both interclass and intraclass instances via proximity
weight
design.
Compared to many existing embedding methods,
our
framework shows
improved discriminating power in the embedded space, evaluated by the text categorization
task with two text corpora and the protein-protein interaction (PPI) extraction task with four
PPI corpora.
The remainder of this paper is organized as follows: Section 2 provides a brief descrip-
tion on the existing techniques used for dimensionality reduction, as well as discussions on
their advantages and disadvantages in terms of computational efficiency and class discrim-
inability. Section 3 explains the proposed embedding framework in detail. Section 4 reports
experimental results and comparative analysis. Section 5 presents conclusions and directions
for future work.
2.
EXISTING DIMENSIONALITY REDUCTION TECHNIQUES
First of all, we clarify the mathematical notations used in this paper: Let {x
i
}
n
i=1
denote
a set
of n data points (instances) of dimension d,
where x
i
= [x
i 1
, x
i 2
, . . . , x
i d
]
T
.
The
n-dimensional
label
vector
y = [y
1
, y
2
, . . . , y
n
]
T
is used to identify which classes these
instances belong to,
where y
i
∈ {1, 2 . . . , c}.
Alternative to y,
the class information can
also be modeled as an n × c binary matrix Y = [y
ij
],
where y
ij
= 1 if the i th instance
belongs to the j th class and y
ij
= 0 otherwise. The vector notation of y is only applicable
to single-label classification,
while the binary matrix Y can be used for both single-label
and multilabel classification. We use n
l
to denote the total number of instances belonging to
the lth class where l ∈ {1, 2 . . . , c}. The goal of dimensionality reduction is to generate a
set of optimal embeddings {z
i
}
n
i=1
of dimension k (k  d), where z
i
= [z
i 1
, z
i 2
, . . . , z
i k
]
T
.
The transformed n × k feature matrix Z = [z
ij
] should be an accurate representation of the
original n × d feature matrix X = [x
ij
] and possess improved discriminating ability where
applicable. In typical NLP tasks, the use of high-dimensional linguistic features usually yields
C
OMPUTATIONAL
I
NTELLIGENCE
very sparse feature matrix X, with large feature number d and most elements zero-valued. In
the following, we briefly review popular dimensionality reduction techniques (Section 2.1)
and discuss their advantages and disadvantages for processing high-dimensional linguistic
features (Section 2.2).
2.1.
Brief Review
LSI (Deerwester et al. 1990) is a feature compression method based on optimal matrix
approximation.
It
defines a d × k orthogonal
projection matrix P so that
the projected
features are the most representative ones of the original features, by minimizing the following
reconstruction error defined via the Frobenius norm:
min
P ∈ R
d×k
,
P
T
P = I
k×k
X − XPP
T

2
F
.
(1)
The optimal
projection matrix is obtained by computing SVD of the matrix X,
and the
projected embeddings are then computed by Z = XP.
PCA (Jolliffe 1986) is the most popular dimensionality reduction tool in exploratory data
analysis. It maps feature vectors into a smaller number of uncorrelated directions. A d × k
orthogonal projection matrix P is extracted so that the variance of the projected vectors is
maximized:
max
P ∈ R
d×k
,
P
T
P = I
k×k
1
n − 1
n

i=1






P
T
x
i
−
1
n
n

j=1
P
T
x
j






2
2
.
(2)
The optimal projection matrix is obtained by computing SVD of the centered feature ma-
trix, given as (I
n×n
−
1
n
1
n
1
T
n
)X, where 1
n
denotes an n-dimensioanl column vector with all
elements equal to one.
Another set of dimensionality reduction approaches is of manifold learning and spectral
analysis, which attempt to minimize the following penalized distances between embeddings:
min
{z
i
∈R
k
}
n
i=1
1
2
n

i, j =1
w
ij
z
i
− z
j

2
2
= tr[Z
T
(D
w
− W)Z],
(3)
where w
ij
represents the degree of similarity or closeness between the i th and j th instances,
W = [w
ij
] denotes the n × n weight matrix, and D
w
is a diagonal matrix with its i th diagonal
element d
i
equal to

n
j=1
w
ij
. Different types of constraints can be imposed on the above
optimization problem to obtain k different columns in Z, e.g., the orthogonality condition
Z
T
Z = I
k×k
(Luxburg 2007) and Z
T
D
w
Z = I
k×k
(Shi and Malik 2000; Belkin and Niyogi
2003).
In order to achieve out-of-sample extension,
which is to compute embeddings for
query instances, projection technique is employed by incorporating Z = XP into equation (3).
A set of optimal projections are computed instead, based on
min
P
∈R
d×k
tr[P
T
X
T
(D
w
− W)XP].
(4)
Different types of constraints can be applied to the projection matrix, e.g., P
T
X
T
XP = I
k×k
and P
T
X
T
D
w
XP = I
k×k
(He and Niyogi 2003), also P
T
P = I (Kokiopoulou and Saad 2007).
The setting of w
ij
plays a very important part in the algorithm design, which constitutes the
main difference between various approaches of this type. For example, OLPP (Kokiopoulou
and Saad 2007) defines its weights based on a local adjacency graph obtained based on the
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
feature matrix X. The most frequently used setting for OLPP weight is shown as follows:
w
ij
=
⎧
⎪
⎨
⎪
⎩
exp
−x
i
− x
j

2
2
2σ
2
if x
i
and x
j
are adjacent,
0
otherwise,
(5)
where σ is the parameter controlling the Gaussian-based similarity weight. The adjacency of
the graph can be defined by including nodes satisfying a certain “closeness” measure between
each other to the adjacency list, or alternatively including those nodes that are the (mutual or
undirected) h-nearest neighbors of each other. MMC (Li et al. 2006) can be reformulated in
the form of equation (3) with the weights defined solely based on class information y, given
as
w
ij
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
2
n
l
−
1
n
if y
i
= y
j
= l, where l ∈ {1, 2, . . . , c},
−
1
n
otherwise.
(6)
OLPP-R (Kokiopouloua and Saadb 2009) determines its weights based on both feature and
label information as follows:
w
ij
=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1
n
l
if y
i
= y
j
= l, where l ∈ {1, 2, . . . , c},
−β
σ +
x
i
− x
j

2
2
x
i

2
2
+ x
j

2
2
−1
if [x
j
∈ N (x
i
, h) ∨ x
i
∈ N (x
j
, h)] ∧ y
i
= y
j
,
0
otherwise,
(7)
where N (x
i
, h) represents the h-nearest neighbors of x
i
based on Euclidean distances, β > 0
and σ are user-defined parameters.
The optimal
solution of the constrained optimization
problem in equation (4) can be obtained by computing the eigendecompositon of X
T
(D
w
−
W)X,
or generalized eigendecompostion of the two matrices of X
T
(D
w
− W)X and X
T
X
(or X
T
D
w
X) depending on the used constraint.
FDA (Fisher 1936) motivates another set of supervised embedding methods. It maximizes
the ratio of between-class scatter and within-class scatter, given as
tr[
S
b
]
tr[
S
w
]
, where S
b
and S
w
are the between-class and within-class scatter matrices, given as
S
b
=
c

l=1
n
l

y
j
=l
z
j
n
l
−

n
i=1
z
i
n

y
j
=l
z
j
n
l
−

n
i=1
z
i
n
T
,
(8)
S
w
=
c

l=1

y
i
=l
z
i
−

y
j
=l
z
j
n
l
z
i
−

y
j
=l
z
j
n
l
T
.
(9)
Various embedding techniques are developed via exploiting different settings of S
b
and S
w
,
leading to different variations of Fisher criterion. For example, local FDA (LFDA) (Sugiyama
2007) incorporates local neighborhood information into equations (8) and (9) and creates
C
OMPUTATIONAL
I
NTELLIGENCE
the following scatter matrices:
S
b
=
c

l=1
⎛
⎜
⎜
⎝

y
i
= l
y
j
= l
w
ij
1
n
l
−
1
n
(z
i
− z
j
)(z
i
− z
j
)
T
+

y
i
=y
j
1
n
(z
i
− z
j
)(z
i
− z
j
)
T
⎞
⎟
⎟
⎠
,
(10)
S
w
=
c

l=1

y
i
= l
y
j
= l
w
ij
n
l
(z
i
− z
j
)(z
i
− z
j
)
T
,
(11)
where w
ij
is the feature-based similarity weight used by OLPP and defined in equation (5).
By assuming Z = XP,
both FDA and LFDA compute the optimal
projection matrix via
performing generalized eigen-decomposition.
CCA (Hardoon, Szedmak, and Shawe-Taylor 2004; Sun, Ji, and Ye 2008) is a classical
data analysis tool
that
can be used for supervised dimensionality reduction.
The projec-
tion matrix is computed by maximizing the correlation coefficient between the projected
embeddings and the class labels:
max
P ∈ R
d×k
,
P
T
X
T
c
X
c
P = I
k×k
tr

P
T
X
T
c
Y
c
(Y
T
c
Y
c
)
−1
Y
T
c
X
c
P

,
(12)
where X
c
= (I
n×n
−
1
n
1
n
1
T
n
)X and Y
c
= (I
n×n
−
1
n
1
n
1
T
n
)Y are the centered feature and label
matrices. To prevent overfitting and avoid singularity of X
T
c
X
c
, the constraint can be modified
to P
T
(X
T
c
X
c
+ λI
d×d
)P = I
k×k
,
where λ > 0 is the user-defined regularization parameter.
This leads to the regularized CCA (RCCA) (Bach and Jordan 2002; Sun et al. 2008). The
final
projection matrix is obtained by computing the generalized eigendecomposition of
X
T
c
Y
c
(Y
T
c
Y
c
)
−1
Y
T
c
X
c
and X
T
c
X
c
+ λI
d×d
. When λ = 0, RCCA becomes CCA.
So far, the above reviewed dimensionality reduction approaches support linear projection
Z = XP only. By working in a kernel-induced feature space, they can also produce nonlinear
transformation of the original features (Sch
¨
olkopf et al. 1998; Mika et al. 1999; Kokiopoulou
and Saad 2007). For example, kernel PCA (KPCA) (Sch
¨
olkopf et al. 1998) assumes that, in
the kernel-induced feature space, the eigenvectors of the covariance matrix between training
instances are linear combinations of the centered training instances. Such assumption drives
KPCA to compute eigenvectors of the following centered kernel matrix K
c
, given as
K
c
= K −
1
n
1
n
1
T
n
K −
1
n
K1
n
1
T
n
+
1
n
2
1
n
1
T
n
K1
n
1
T
n
,
(13)
where K is the original
kernel
matrix between training instances.
Letting P
k
denote the
obtained eigenvectors,
the KPCA embeddings are computed by Z = K
c
P
k
.
As for those
projection-based embedding methods unified by equation (3), their kernel extensions, e.g.,
kernel OLPP (KOLPP), assume that the projection vectors can be expressed as linear combi-
nations of the training instances in the kernel-induced space. This gives rise to the following
constrained optimization problem (Kokiopoulou and Saad 2007),
min
P
k
∈ R
n×k
,
P
T
k
P
k
= I
k×k
tr

P
T
k
K(D
w
− W)KP
k

,
(14)
where P
k
represents the combination coefficients. The optimal solution of P
k
can be obtained
by computing the eigendecomposition of the matrix K(D
w
− W)K. Other constraints, such
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
as P
T
k
KKP
k
= I
k×k
and P
T
k
KD
w
KP
k
= I
k×k
,
can also be used,
which lead to generalized
eigendecomposition (Kokiopoulou and Saad 2007). The embeddings are then computed by
Z = KP
k
. Kernel extensions of FDA and its variations can also be obtained by expanding
projection vectors to linear combinations of training instances (Mika et al. 1999; Yan et al.
2007), working in a quite similar way as above.
2.2.
Discussion
We compare below advantages and disadvantages of previously described algorithms
when being used to process NLP data sets,
according to the task nature and algorithm
characteristics.
Computational cost of different algorithms is discussed first, which is a very important
issue to practitioners in the field. LSI computes SVD of the original feature matrix that is
highly sparse for typical NLP feature representations.
Various efficient large-scale sparse
SVD algorithms (Berry 1992; Golub and Loan 1996; Brand 2006) and tools (Hern
´
andez
et
al.
2007;
ˇ
Rehu
ˇ
rek and Sojka 2010),
such as SVDLIBC
1
and Gensim,
2
enable wide
applications of LSI to NLP tasks with its computational cost highly related to the sparsity
of the data.
PCA requires to compute SVD of a centered feature matrix that is nonsparse
but of the same size as the one processed by LSI, with a computational cost around O(dn
2
)
(Trefethen and Bau 1997).
Given a fairly low number
of
training instances,
there is a
more practical way to implement LSI and PCA,
instead of basing LSI and PCA on SVD
computation.
It
is known that
the projection matrix of LSI (or PCA) is computed from
the top k ≤ rank(X) = min(n, d) = n right singular vectors and the corresponding singular
values of X (or X
c
). These k ≤ n right singular vectors can be approximated from the top
k left singular vectors, their corresponding singular values and the input matrix X (or X
c
).
Also, both the left singular vectors and singular values can be obtained by computing the
eigendecomposition of XX
T
(or X
c
X
T
c
). Thus, to compute eigendecomposition of the n × n
matrix XX
T
for LSI and X
c
X
T
c
for PCA leads to a faster way of implementing LSI and
PCA than to directly compute the SVD of X and X
c
.
In this case,
the computational cost
of LSI (or PCA) includes the cost of computing the inner product matrix XX
T
(or X
c
X
T
c
)
that is O(dn
2
), and the cost of eigendecompostion of this n × n matrix that is around O(n
3
)
(Coppersmith and Winograd 1990). As for the dimensiotnality reduction approaches based
on linear projections,
e.g.,
those unified by equation (3),
FDA and its variations,
as well
as CCA and RCCA,
they require to compute (generalized) eigendecomposition of d × d
nonsparse matrices. The corresponding computational cost is around O(d
3
) (Coppersmith
and Winograd 1990), which is the main cost of this type of algorithms given d n. This
makes these algorithms unsuitable to process hundreds thousands (or millions) of linguistic
features within realistic computing time.
When the number of training instances is fairly
low, the kernel versions of the above approaches become more efficient, as they compute the
(generalized) eigendecomposition of n × n matrices instead. The total computational cost
of a kernel-based embedding method includes the cost of eigendecomposition that is around
O(n
3
) and the cost
of kernel
matrix computation.
Various research has been conducted
studying economical computation of kernel matrices for large-scale sparse features (Rousu
and Shawe-Taylor 2005; Haffner 2006; Shi et al. 2009).
Given a classification task,
class discriminability is the most
important
criterion for
assessing the quality of embeddings. Unsupervised dimensionality reduction methods only
1
http://tedlab.mit.edu/ dr/SVDLIBC/
2
http://radimrehurek.com/gensim/
C
OMPUTATIONAL
I
NTELLIGENCE
provide a compact
representation of the input
data and do not
focus on improving the
separability between classes. For example, LSI is a compression method producing k-rank
approximation of the original feature matrix. PCA attempts to preserve global data structure
in terms of data variance.
OLPP targets the preservation of the intrinsic geometry of the
data, as captured by the aggregate pairwise proximity information based on a local neigh-
borhood graph.
However,
when large amount of linguistic features are combined together
with difficulty on performing careful feature selection by domain experts, the resulting fea-
ture presentation can be quite noisy and lead to incompatible arrangements between the
input features and the output labels. Consequently, embeddings computed by unsupervised
techniques may accentuate such incompatibilities and may not
improve the classification
performance. Differently, supervised embedding techniques are more focused on sharpening
the class structure in terms of discriminability. For example, MMC, FDA, and CCA (RCCA)
encourage all the intraclass points to be close while interclass points away in the embed-
ded space. However, by forcing the embedded feature structure to match exactly the class
structure, the resulting space could be distorted and may lose track of the intrinsic geometry
of the original instances. This may lead to an overfitted space with reduced generalization.
To improve this, OLPP-R and LFDA attempt to preserve not only global class structure but
also local feature geometry for either interclass instance group or intraclass instance group,
however, not for both groups.
3.
THE PROPOSED EMBEDDING FRAMEWORK
We have defined the input
feature matrix X,
the resulting embedding matrix Z,
the
label
vector
y,
and the binary label
matrix Y for training instances in previous section.
Other frequently used mathematical notations include the following: The test instances are
denoted as m points {
ˆ
x
i
}
m
i=1
of dimension d with the m × d feature matrix
ˆ
X = [
ˆ
x
ij
], where
ˆ
x
i
= [
ˆ
x
i 1
,
ˆ
x
i 2
, . . . ,
ˆ
x
i d
]
T
. The generated embeddings of test instances are denoted as {
ˆ
z
i
}
m
i=1
with
ˆ
z
i
= [
ˆ
z
i 1
,
ˆ
z
i 2
, . . . ,
ˆ
z
i k
]
T
, leading to an m × k embedding matrix
ˆ
Z = [
ˆ
z
ij
].
We consider the following when designing an embedding framework so that
it
can
better serve an NLP task: (1) to support flexible balance between the preservation of intrinsic
geometry and the enhancement of class separability for both interclass and intraclass instance
groups;
(2) to take into account
the general
behavior of linguistic features for indicating
similarities between instances; and (3) to make the framework computationally tractable for
processing high-dimensional features.
3.1.
Embedding Generation
In order to encourage the embedded feature structure to match the class structure but not
to lose track of the intrinsic geometry of the original data, we project the data points into a
new space by simultaneously achieving the following two goals: (1) preserving the original
local
distribution in the projected space for all
the training instances,
and (2) modifying
relative positions of the projected interclass and intraclass instances by increasing similarities
between the intraclass instances while reducing similarities between the interclass instances.
The proposed embedding algorithm is named as dual-goal projection (G2P). As shown in
Section 2,
many supervised embedding algorithms only achieve the second goal,
such as
MMC, FDA, and CCA. Some algorithms partially achieve both goals but ignore the local
distribution of either the intraclass or the interclass instances, such as OLPP-R and LFDA.
Unsupervised algorithms only achieve the first goal, e.g., OLPP.
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
Driven by the above two goals, we define the following weights in order to directly control
and differentiate the vicinities of both intraclass and interclass instances in the embedded
space:
w
ij
=
⎧
⎪
⎨
⎪
⎩
s
ij
if [x
j
∈ N (x
i
, S, h) ∨ x
i
∈ N (x
j
, S, h))] ∧ y
i
= y
j
,
−s
ij
if [x
j
∈ N (x
i
, S, h) ∨ x
i
∈ N (x
j
, S, h))] ∧ y
i
= y
j
,
0
otherwise,
(15)
where S = [s
ij
] is a similarity matrix computed from X, and N (x
i
, S, h) represents the h-
nearest neighbors of x
i
searched based on the similarity values in S. The operation ∨ defines
nonzero weights only for instances that are undirected neighbors of each other. Alternatively,
the operation ∧ can be used instead of ∨, defining nonzero weights for mutual neighbors.
When the local
feature structure is not
considered at
all,
a hard version of w
ij
can be
used only depending on class information, which is applicable to single-label classification,
given as
w
ij
=

1
if y
i
= y
j
,
−1
if y
i
= y
j
.
(16)
To compute the similarity matrix S, cosine function s
ij
=
x
T
i
x
j
x
i

2
x
j

2
is one of the most fre-
quently used similarity measures in NLP. It is a scaled version of dot-product determined by
word co-occurrences or co-occurred linguistic patterns. Many other sophisticated similarity
measures in NLP, e.g., string and tree kernels (Collins and Duffy 2001; Lodhi et al. 2002;
Culotta and Sorensen 2004; Shi et al. 2009) are created by first defining a language-oriented
feature representation (mapping) for each instance and then computing the inner products
based on the defined mapping.
These show that inner product is an important measure to
capture the proximity information for natural language, where the high sparsity of the lin-
guistic representations also highlights the importance of the co-occurrence feature. Inspired
by this, we attempt to preserve the same proximity structure also in an inner product space in
order to maintain such nature of the original input. This can be achieved by maximizing the
weighted sum of inner products (penalized similarities) between all the embedded instances
based on the following optimization template,
max
{z
i
∈R
k
}
n
i=1
n

i, j =1
w
ij
z
T
i
z
j
= tr[Z
T
WZ],
(17)
where w
ij
is computed by equation (15).
This is a different
formulation from the one
used by many existing methods in equation (3). With the above template, we create a new
configuration in the embedded space, where the interclass neighbors possess lower values
of pairwise inner products and intraclass neighbors possess higher values of pairwise inner
products. The class structures and separabilities are reinforced and the local closeness are
maintained in the inner product matrix between embeddings.
Many NLP softwares favor
cosine function to evaluate the similarity between the projected embeddings {z
i
}
n
i=1
(Osi
´
nski,
Stefanowski, and Weiss 2004; Osi
´
nski and Weiss 2005; Wu 2009). Our design is also suitable
for such setting, as it is aimed at producing embeddings with highly structured inner product
matrix. The corresponding proximity structure will also be maintained in its scaled version
of cosine similarity matrix.
To activate out-of-sample extension,
we employ Z = XP to
C
OMPUTATIONAL
I
NTELLIGENCE
compute the optimal projections other than directly optimizing Z, leading to
max
P
∈R
d×k
tr[P
T
X
T
WXP].
(18)
The proposed embedding algorithm is controlled by two main components:
One is the
similarity weights provided in equation (15) attempt to satisfy both goals as explained in
the beginning of this section. The other is the inner product-based optimization template in
equation (17) attempting to accommodate the characteristics of linguistic representations.
In the following,
we discuss connections and differences between the proposed opti-
mization template in equations (17) and (18) and the commonly used one in equations (3)
and (4). We re-express the objective function of equation (3) as follows:
1
2
n

i, j =1
w
ij
z
i
− z
j

2
2
= tr[Z
T
D
w
Z] − tr[Z
T
WZ] =
n

i=1
d
i
z
i

2
2
−
n

i, j =1
w
ij
z
T
i
z
j
.
(19)
Thus, with appropriate constraints on Z, minimizing the sum of penalized Euclidean distances
in equation (3) can be equivalent
to maximizing the sum of penalized inner products in
equation (17).
However,
our template is more general
than the existing one for unifying
various embedding methods via assigning different types of weights w
ij
. For example, LSI
and PCA can be obtained by optimizing equation (18) with the corresponding weights set as
w
ij
=

1
if i
= j,
0
otherwise,
and
w
ij
=

1 − 1/n
if i
= j,
−1/n
otherwise,
respectively. CCA can be unified by setting the weight matrix as W = Y
c
(Y
T
c
Y
c
)
−1
Y
T
c
. Given
only the existing template in equation (4), similar unification treatment can not be applied
to LSI and CCA, because their corresponding weight matrices do not possess zero-valued
row sums,
which is required by equations (3) and (4) due to the use of D
w
− W.
In fact,
equations (3) and (4) can be viewed as a special case of our template of equations (17) and
(18), only allowing weight matrices with zero-valued row sums.
Considering the simplest case of binary classification where y
i
∈ {−1, +1},
our inner
product-based optimization together with proposed similarity weights can be written as
max
{z
i
∈R
k
}
n
i=1
n

i, j =1
y
i
y
j
δ
ij
s
ij
z
T
i
z
j
=

y
i
=y
j
δ
ij
s
ij
z
T
i
z
j
−

y
i
=y
j
δ
ij
s
ij
z
T
i
z
j
(20)
=

y
i
=y
j
δ
ij
s
ij
x
T
i
PP
T
x
j
−

y
i
=y
j
δ
ij
s
ij
x
T
i
PP
T
x
j
,
(21)
where δ
ij
is a neighbor indicator for the i th and j th instances, given as
δ
ij
=

1
if [x
j
∈ N (x
i
, S, h) ∨ x
i
∈ N (x
j
, S, h))].
0
otherwise.
(22)
In equation (22), the operation ∧ can also be used instead of ∨ so that mutual neighbors are
searched instead of undirected neighbors. These indicators {δ
ij
}
n
i, j =1
constitute the indicating
matrix N(S, h) = [δ
ij
] derived from the similarity matrix S and controlled by the neighbor
number h. The larger value s
ij
possesses, the corresponding indicator δ
ij
has more chances to
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
be nonzero. The first item in equation (20) moves more similar instances from the same class
with larger weights to come even closer in the embedded space; this improves within-class
compactness. At the same time, the second term forces more similar instances from different
class with larger weights to move further apart from each other in the embedded space, as they
are very likely to be boundary points and this improves the overall between-class separation.
To obtain k different columns in Z,
we apply the orthogonality condition Z
T
Z = I
k×k
to
the above optimization problem, leading to the constraint of P
T
X
T
XP = I
k×k
regarding to
the projection matrix.
By representing equation (21) in matrix presentation,
the objective
function becomes tr[P
T
X
T
( y y
T
◦ N(S, h) ◦ S)XP], where ◦ denotes the Hadamard product.
When the class-based weight in equation (16) is used, the objective function can be simplified
as tr[P
T
X
T
( y y
T
)XP].
To extend the above embedding computation to accommodate multiclass classification,
we define the following class-based similarity matrix,
S
Y
=
⎧
⎪
⎨
⎪
⎩
y y
T
for binary classification,
2YY
T
− 1
for multiclass single-label classification,
YY
T
for multiclass multilabel classification.
(23)
For binary and multiclass single-label
classification,
each element
of S
Y
is equal
to one
for intraclass instances while minus one for interclass instances. For multiclass multilabel
classification,
as there is no clear definition on interclass and intraclass instances,
each
element of S
Y
is defined as the number of classes two instances belong to at the same time.
We also introduce a regularization term to prevent overfitting and to avoid singularity of
XX
T
. These subsequently lead to the following constrained optimization problem in matrix
presentation,
max
P ∈ R
d×k
,
P
T
(X
T
X + λI
d×d
)P = I
k×k
tr[P
T
X
T
(
S
Y
◦ N(S, h) ◦ S
)
XP].
(24)
When only class information is considered for weight computation, we have
max
P ∈ R
d×k
,
P
T
(X
T
X + λI
d×d
)P = I
k×k
tr[P
T
X
T
S
Y
XP].
(25)
In both equations, λ > 0 is the regularization parameter. Adding a small value of λ to the
diagonal elements of X
T
X will not break the matrix structure but make sure X
T
X + λI
d×d
is positive definite.
It is possible to compute the parameter λ by multiplying the smallest
diagonal element of the matrix X
T
X by a small positive value. Given m new query instances,
their embeddings can be computed by
ˆ
Z =
ˆ
XP.
The two constrained optimization problems in equations (24) and (25) lead to the fol-
lowing standard trace optimization problem,
max
P
T
BP
=
I
k×k
tr[P
T
AP],
(26)
where the constraint
matrix is B = X
T
X + λI
d×d
for
both problems,
and the objective
matrix is A = X
T
A
z
X,
where A
z
= S
Y
◦ N(S, h) ◦ S for equation (24) and A
z
= S
Y
for
equation (25). Since the similarity matrix S and the neighbor indicating matrix N(S, h) used
in our algorithm are both symmetric,
A and B are symmetric,
also B is positive definite
because of the incorporation of the positive regularization parameter λ > 0.
C
OMPUTATIONAL
I
NTELLIGENCE
Lemma.
Given a symmetric d × d matrix A and a symmetric, positive definite d × d
matrix B, the optimal solution of the following constrained optimization problem
max
P ∈ R
d×k
,
diag[P
T
BP] = 1
k
tr[P
T
AP],
(27)
is the top k eigenvectors of the generalized eigendecomposition of A and B corresponding
to the largest k real-valued eigenvalues {
i
}
k
i=1
,
and the maximum value of the objective
function is

k
i=1

i
.
The function diag[·] returns the corresponding diagonal vector for a
matrix input.
Proof .
Letting p
i
denote the i th column of the matrix P, the constraint of diag[P
T
BP] =
1
k
is equivalent to k equality constraints of
p
T
i
B p
i
= 1, for i
= 1, 2, . . . , k. Consider the
Lagrangian of the problem in equation (27), we have
L
(P, {λ
i
}
k
i=1
) = tr[P
T
AP] −
k

i=1
λ
i

p
T
i
B p
i
− 1

(28)
= tr[P
T
AP] − tr

P
T
BP

+
k

i=1
λ
i
,
(29)
where  is a k × k diagonal matrix with the i th diagonal element equal to λ
i
. The Karush-
Kuhn-Tucker (KKT) conditions for optimality are
∂
L
∂λ
i
= 0 ⇒ p
T
i
B p
i
= 1, for i
= 1, 2, . . . , k.
(30)
∂
L
∂P
= 0 ⇒
∂
∂P
tr[P
T
AP] −
∂
∂P
tr[P
T
BP] = 0,
⇒ (A + A
T
)P − (BP + B
T
P
T
) = 0,
⇒ AP = BP,
⇔ A p
i
= λ
i
B p
i
,
for i
= 1, 2, . . . , k.
(31)
Equation (31)
indicates each column of
the optimal
solution P is
the eigenvector
of
the generalized eigendecompsition of the matrices A and B,
the Lagrange multiplier λ
i
is the corresponding eigenvalue.
Because A and B are symmetric and B is positive defi-
nite, all the eigenvalues are real-valued. By incorporating equations (30) and (31) into the
Lagrangian, we have
L
(P, {λ
i
}
k
i=1
) = tr[P
T
AP] =
k

i=1
λ
i
.
(32)
Thus, to maximize the objective function, {λ
i
}
k
i=1
has to be the largest k eigenvalues, and the
k columns of the optimal solution of P are their corresponding eigenvectors.

Comparing equation (27) with our targeted problem in equation (26),
the formal one
possesses more relaxed constraint of diag[P
T
BP] = 1
k
than the later one of P
T
BP = I
k×k
.
However,
since the optimal
solution of equation (27) are eigenvectors,
the orthogonality
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
condition of P
T
BP = I
k×k
can also be satisfied. So the two optimization problems in equa-
tions (26) and (27) share the same optimal solution.
3.2.
Kernel Extension and (Dis)similarity Features
So far all the above setup for computing embeddings is linear,
it could be restrictive
to improve classification performance for some data sets possessing nonlinear structures.
To incorporate the handling of nonlinearities into the embedding generation,
one way is
to kernelize the proposed algorithm by applying the standard kernel trick (Sch
¨
olkopf et al.
1998; Shawe-Taylor and Cristianini 2004). The kernel function
K
(·, ·) defines the dot prod-
uct
in a high-dimensional
(possibly infinite dimensional) feature space known as kernel-
induced space (Mercer 1909).
Letting φ
κ
:
R
d
→
H
denote the mapping that transforms
the original data to the kernel-induced space
H
,
then 
κ
= [φ
κ
(x
1
), . . . , φ
κ
(x
n
)]
T
is the
feature matrix in
H
.
Letting K = [k
ij
] denote the n × n kernel matrix between n training
instances,
we have k
ij
=
K
(x
i
, x
j
) = φ
T
κ
(x
i
)φ
κ
(x
j
).
The goal
is to look for a projection
matrix
¯
P = [
¯
p
1
, . . . ,
¯
p
k
] to project the kernel-induced features to a k-dimensional subspace,
where the embeddings are computed by Z = 
κ
¯
P.
After expanding each of the transfor-
mation vectors {
¯
p
j
}
k
j=1
to a linear summation of all the training instances in
H
,
given as
¯
p
j
=

n
i=1
γ
ij
φ
κ
(x
i
),
the embedding matrix can be computed by Z = 
κ
¯
P = 
κ

T
κ
 =
K,
where  = [γ
ij
] denotes the n × k coefficient matrix.
By incorporating this into the
optimization problem in equation (26), we have
max
 ∈ R
n×k
,

T
B
K
 = I
k×k
tr[
T
A
K
],
(33)
where A
K
= K
T
A
z
K and B
K
= K
T
K + λI
n×n
. The optimal coefficients can then be obtained
by computing the eigendecomposition of the n × n matrices A
K
and B
K
. The embeddings
for m query instances can be computed by
ˆ
Z =
ˆ
K,
where
ˆ
K denotes the m × n kernel
matrix between the queries and the training instances. The kernel function has to be positive
semidefinite, as it defines the inner product between data points in the kernel-induced space.
It can be observed from equations (26) and (33) that the kernelized algorithm works in a
similar way to the one performed in the original space, but with K used as the input feature
matrix instead of X, and  used as the projection matrix instead of P. Since the kernel matrix
can be viewed as a similarity structure between instances, kernel-based projection is equiva-
lent to using a set of similarity values to training instances as the input features for computing
embeddings. Therefore, by dropping the positive semidefinite property of the kernel function,
it is possible to use an arbitrary set of similarity values or even dissimilarity values to the
training instances as the input features. The training instances become n data points {r
i
}
n
i=1
of dimension n with an n × n feature matrix R = [r
ij
] where r
i
= [r
i 1
, r
i 2
, . . . , r
i n
]
T
. The
test instances become m points {
ˆ
r
i
}
m
i=1
where
ˆ
r
i
= [
ˆ
r
i 1
,
ˆ
r
i 2
, . . . ,
ˆ
r
i n
]
T
, possessing an m × n
feature matrix
ˆ
R = [
ˆ
r
ij
]. Each (dis)similarity feature is computed by
r
ij
= φ(x
i
, x
j
), i,
j
= 1, 2, . . . , n,
(34)
ˆ
r
ij
= φ(
ˆ
x
i
, x
j
), i
= 1, 2, . . . , m,
j
= 1, 2, . . . , n,
(35)
where φ(·, ·) is the employed (dis)similarity measure.
This is known as empirical
kernel
mapping (Sch
¨
olkopf 1997;
Sch
¨
olkopf et
al.
2002;
Tsuda 1998),
generalized dissimilarity
kernels (Duin 2000;
Pekalska,
Paclik,
Duin 2002),
(dis)similarity features (Pekalska and
Duin 2002; Pekalska, Duin, and Paclik 2006), and also relation features (Mu et al. 2012a, b).
C
OMPUTATIONAL
I
NTELLIGENCE
Standard kernel extension requires positive semidefinite property on the kernel matrix K
(Cristianini and Shawe-Taylor 2000), which sometimes makes the design of a kernel function
mathematically difficult. Additionally, it only allows similarity due to the definition of a kernel
function. In contrast, the use of (dis)similarity features does not require any extra property
on the (dis)similarity matrix R,
which does not
even need to be symmetric,
and allows
both similarity and dissimilarity measures.
Thus,
it provides more freedom to users when
designing or selecting (dis)similarity measures favored by the tasks.
By replacing the original feature matrix X with the (dis)similarity feature matrix R, the
embedding computation in equation (26) becomes
max
P ∈ R
n×k
,
P
T
B
R
P = I
k×k
tr[P
T
A
R
P],
(36)
where A
R
= R
T
A
z
R and B
R
= R
T
R + λI
n×n
. The use of R not only compresses the dis-
criminative information contained in the original features without causing considerable loss,
but also reduces the high sparsity usually possessed by high-dimensional linguistic features.
Given extremely high-dimensional features (d n),
it becomes more efficient to process
R rather than X. For our method, its standard kernel extension and the extension based on
(dis)similarity features (empirical kernel mapping) lead to identical optimization problems in
equations (33) and (36), and so do OLPP, OLPP-R, MMC, FDA, and its variations, etc. This is
because they all expand projection vectors to a linear summation of all the training instances,
which leads to the consistency between two extensions. However, this does not apply to all the
embedding methods. For example, the kernel PCA (KPCA) based on standard kernel trick
are slightly different from the one based on empirical kernel mapping (Sch
¨
olkopf et al. 1998;
Kim, Kim, and Kim 2004). This is because standard KPCA expands the projection vectors
to a linear summation of all the centered training instances in the kernel-induced space. As a
result, standard KPCA eigen-decomposes the centered kernel matrix in equation (13), while
PCA with (dis)similarity features eigen-decomposes K(I
n
−
1
n
1
n
1
T
n
)K instead.
In addition to the cosine function commonly used in NLP for computing (dis)similarity
features, an arbitrary set of measures can be used as φ(·, ·), including Euclidean distances,
various similarity coefficients,
e.g.,
Tanimoto similarities,
various correlation coefficients,
e.g.,
Pearson’s and Spearman’s correlations,
various kernel
functions,
e.g.,
Gaussian and
polynomial kernels, and of course dot product as the simplest case which can be considered
as an and-based similarity.
When working with domain specific NLP tasks,
it is possible
to employ domain specific kernels to compute similarity features, e.g., string, tree, graph,
subsequence kernels computed based on the syntactic and (or) lexical information derived
from the text (Collins and Duffy 2001; Lodhi et al. 2002; Culotta and Sorensen 2004; Kim
et al. 2008; Airola et al. 2008; Moschitti 2006, 2008; Nguyen, Moschitti, and Riccardi 2009;
Tikk, Palaga, and Leser 2010; Zhou et al. 2010), and functions combining tree and lexical
similarity kernels (Bloehdorn et al. 2006; Bloehdorn and Moschitti 2007a,b) . Algorithms on
metric learning (Lebanon 2006) can also be used for computing the (dis)similarity features.
Sometimes,
different
types of features can be extracted for the same task,
e.g.,
features
extracted by different
tools,
resources,
or approaches.
In this case,
p different
groups of
(dis)similarity features can be independently calculated, denoted as {R
i
}
p
i=1
and {
ˆ
R
i
}
p
i=1
for
training and test instances, respectively. The final (dis)similarity features can then be obtained
by weighted summation, given as
R =

p
i=1
a
i
R
i

p
i=1
a
i
,
(37)
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
ˆ
R =

p
i=1
a
i
ˆ
R
i

p
i=1
a
i
,
(38)
where 0 ≤ a
i
≤ 1 is the combination weight indicating the degree of contribution for i th
feature type.
In order to apply the proposed algorithm in a (dis)similarity feature space, we need to
compute one similarity matrix S for preserving the local geometry of the data and conducting
neighbor search, and one (dis)similarity matrix R as features. Theoretically, the two matrices
S and R can be computed in different ways. However, this is not necessary, it is sufficient to
use the same measure to compute both matrices, leading to the following algorithm,
max
P ∈ R
n×k
,
P
T
(S
T
S + λI
n×n
)P = I
k×k
tr[P
T
S
T
(
S
Y
◦ N(S, h) ◦ S
)
SP],
(39)
where S is a similarity matrix. When a dissimilarity measure is used, it is easy to convert the
resulting dissimilarity matrix to a similarity one by using simple reverse max
i

, j

(s
i

j

) − s
ij
or Gaussian function exp(−
s
2
ij
2σ
2
).
Equation (39) is the final
formulations of the proposed
embedding framework, which is controlled by three parameters, the regularization parame-
ter λ > 0, the neighbor number 0 < h ≤ n, and the embedding dimensionality 1 ≤ k ≤ n.
Instead of directly setting an integer value for k,
it is possible to determine k by keeping
those eigenvectors associated with a certain percentage,
e.g.,
0 < α ≤ 1,
of accumulated
eigenvalues during the computation of eigendecomposition.
The following formulation is
used to compute k from eigenvalues {
i
}
n
i=1
based on the percentage parameter α,
min
1≤k≤n
k
s.t.

k
i=1

2
i

n
i=1

2
i
≥ α.
(40)
In order to offer full flexibility between the preservations of the global class structure and
local feature geometry, we allow the use of class-based weight in equation (23) by replacing
S
Y
◦ N(S, h) ◦ S with S
Y
for equation (39). In this case, only class structure is preserved in
the embedded space.
3.3.
Complexity Analysis
The main implementation required by the proposed embedding algorithm in equation (39)
includes the following components: (a) computation of an n × n proximity matrix S,
(b)
neighbor search performed on S, and (c) generalized eigendecomposition of an n × n matrix.
When the class-based weight is used, (b) is not needed, but (a) and (c) are always compulsory.
For (a), to compute the proximity matrix using standard measures, e.g., cosine or Euclidean,
the computational cost is O(dn
2
) for a nonsparse feature matrix, which can be reduced when
the input feature matrix is highly sparse.
To compute the proximity matrix using domain
specific kernels for natural language, various research has been conducted to investigate the
efficient computation of the kernel matrix for large-scale sparse input (Rousu and Shawe-
Taylor
2005;
Haffner
2006;
Shi
et
al.
2009).
For
(b),
the nearest
neighbor
search is a
fundamental problem,
of which the computation complexity is around O(log
2
n) (Lifshits
and Zhang 2009). For (c), computational cost of eigendecompostion of a full matrix cubically
depends on the matrix size,
given as O(n
3
).
Similar to existing kernel-based embedding
methods,
the memory cost of the proposed method is related to the storage of the n × n
C
OMPUTATIONAL
I
NTELLIGENCE
similarity matrix S and the original feature matrix,
and the computational cost
is mainly
related to the number of training instances and the sparsity of features. When the number of
training instances grows larger, the computational and memory cost becomes high. In this
case, it is possible to consider incremental training using the incremental eigendecomposition
technique (Kwok and Zhao 2003). Additionally, we can also compute embeddings from only
p ( p < n) (dis)similarity features with respect to a selected collection of p seed prototypes
(Pekalska et al. 2006).
We also analyze the computational cost for classifying a new query instance
ˆ
x.
This
is usually considered more important than the training cost for online users.
Letting w =
[w
1
, w
2
, . . . , w
k
]
T
and b denote the weight vector and bias of the classifier trained on the
k-dimensional embeddings, the prediction function for a test instance
ˆ
x can be written as
f (
ˆ
x) =
k

j=1
n

i=1
w
j
p
ij
φ(
ˆ
x, x
i
) + b.
(41)
The prediction complexity depends on the used (dis)similarity measure φ(
ˆ
x, x
i
).
In the
following,
we analyze the complexity for several specific (dis)similarity measures.
When
the used similarity measure can be explicitly expressed as the inner product of two mapped
vectors with known mapping function, given as φ(
ˆ
x, x
i
) = ϕ
T
(
ˆ
x)ϕ(x
i
), which applies for
dot product,
cosine,
some correlation coefficients,
and many kernel functions specifically
designed for natural language, we have
f (
ˆ
x) =
k

j=1
w
j
ϕ
T
(
ˆ
x)
n

i=1
p
ij
ϕ(x
i
) + b = 
T
ϕ(
ˆ
x) + b,
(42)
where  =

k
j=1

n
i=1
w
j
p
ij
ϕ(x
i
) and b can be calculated during the training procedure.
In this case, the prediction time is nearly the same as a linear classifier with extra cost for
computing ϕ(
ˆ
x). The extra cost is zero for dot product given ϕ(
ˆ
x) =
ˆ
x, and O(d) for cosine
given ϕ(
ˆ
x) =
ˆ
x

ˆ
x
2
. When squared Euclidean distance is used, although the above condition
is not
satisfied,
we incorporate φ(
ˆ
x, x
i
) = 
ˆ
x
2
2
+ x
i

2
2
− 2
ˆ
x
T
x
i
into equation (41) and
have
f (
ˆ
x) = 
ˆ
x
2
2
k

j=1
n

i=1
w
j
p
ij
+
k

j=1
n

i=1
w
j
p
ij
x
i

2
2
− 2
ˆ
x
T
k

j=1
n

i=1
w
j
p
ij
x
i
+ b
= 
T
0
ˆ
x + 
1

ˆ
x
2
2
+ b
0
,
(43)
where

0
= −2

k
j=1

n
i=1
w
j
p
ij
x
i
,
1
=

k
j=1

n
i=1
w
j
p
ij
,
and
b
0
= b +

k
j=1

n
i=1
w
j
p
ij
x
i

2
2
can be computed during the training procedure.
The extra cost
com-
pared with a linear classifier only relates to the computation of 
ˆ
x
2
2
, and is thus the same
as that of cosine. With the above measures, the prediction cost is only related to the input
dimensionality d, because the remaining computation can be conducted during the training
procedure. However, in more general cases, with measures that do not support similar induc-
tions as above and cannot assign most computation to the training procedure, the prediction
cost is also related to the number of training instances n.
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
4.
EXPERIMENTAL RESULTS AND ANALYSIS
In order to examine performance of the proposed method and compare it with existing
methods, two types of NLP tasks are studied in our experiments, including text categorization
and sentence-based pair-wise PPI extraction.
•
Text categorization automatically assigns each document in a set to one or more cat-
egories,
based on their contents.
It is usually solved by a combination of information
retrieval
and machine learning technology.
In this study,
we consider one multiclass
single-label and one multiclass multilabel text categorization tasks, both of which can
be tackled by training a binary classifier for each category,
known as one-against-all
classification scheme (Hsu and Lin 2002). We used a publicly available corpus “Reuters-
21578 Text Categorization Test Collection” (Reuters) containing articles taken from the
Reuters newswire (Lewis 1997) and the “education evidence portal” (EEP) document
collection containing lengthy full papers or reports supplied by the education evidence
portal (Ananiadou et al. 2010). For the Reuters set, we used articles from 10 different
categories where each article only belongs to one single category. For the EEP document
collection, domain experts have developed a taxonomy of 108 concept categories in the
area and manually assigned categories to documents stored in the database. Since many
of these categories only contain one or two documents, we used the largest 42 categories
in our experiments, where each article is allowed to belong to multiple categories. Af-
ter applying the Porter stemming algorithm (Porter 1980) to the documents, extracting
meaningful multiword terms based on c-value (Frantzi,
Ananiadou,
and Mima 2000),
and filtering out the low-frequency words and terms, the tf-idf values of the remaining
word uni-grams and terms are used as features for Reuters data set, while the frequencies
of the remaining word uni-grams are used as features for EEP data set.
•
PPI extraction is a significant
task of information extraction from biomedical
text,
which has attracted increasing attention in NLP,
TM,
and bioinformatics (Krallinger
et al. 2008). It is an instance of a relation extraction task, which can be formulated as a
binary classification problem that predicts whether a given pair of proteins in a sentence
is interacting or not. Four publicly available corpora of AIMED, HPRD, IEPA, and LLL
are used in experiments. These four corpora are commonly used in most evaluations of
PPI extraction, for which more detailed information can be found in Airola et al. (2008),
Miwa et al. (2009), and Fayruzov et al. (2009). Using the Enju 2.3.0 and KSDEP beta
1 parsers (Miyao et al. 2008), various linguistic features are extracted to represent each
protein – protein pair, including bag-of-words features, shortest path features, as well as
dependency and linear graph features (Miwa et al. 2009a, b). Feature extraction has been
conducted for all the four corpora together so that the four resulting data sets possess the
same features allowing us to perform the cross-corpus evaluation in Section 4.2.2.
For each used data set, we display in Table 1 the number of studied instances, the number
of original features, the total number of classes, as well as the ratio between the number of
in-class samples and the number of out-of-class samples, known as class imbalanced ratio
(CIR). The following equation is used to compute the average CIR for all the classes, based
on the n × c binary label matrix Y = [y
ij
] that is defined in the beginning of Section 2:
CIR =
1
c
c

j=1
min


n
i=1
y
ij
, n −

n
i=1
y
ij

max


n
i=1
y
ij
, n −

n
i=1
y
ij

,
(44)
C
OMPUTATIONAL
I
NTELLIGENCE
T
ABLE
1.
Data Set Information.
Data sets
Task
No. instances
No. features (d)
No. classes (c)
CIR
Reuters
Text categorization
8,202
69,593
10
15.6%
EEP
Text categorization
2,149
1,721,881
42
13.6%
AIMED
PPI extraction
5,834
3,762,146
2
20.7%
HPRD
PPI extraction
433
3,762,146
2
60.4%
IEPA
PPI extraction
817
3,762,146
2
69.5%
LLL
PPI extraction
330
3,762,146
2
98.8%
where the two functions min(·, ·) and max(·, ·) return the smaller and larger number between
the two inputs,
respectively.
The CIR value reveals imbalanced class distributions of the
data, of which a very low value indicates a highly imbalanced data set and a ratio equal to
one implies a perfectly balanced data set. It can be seen from Table 1 that the chosen data
sets for evaluation not
only represent
different
NLP applications but
also cover different
types of classification problems. These include imbalanced (e.g., Reuters, EEP, and AIMED)
and balanced classification (e.g.,
LLL),
binary (e.g.,
AIMED,
HPRD,
IEPA,
and LLL),
multiclass single-label (e.g.,
Reuters),
and multiclass multilabel (e.g.,
EEP) classification,
also classification with sufficient
training instances (e.g.,
Reuters,
AIMED) and without
many training instances (e.g., LLL and HPRD).
4.1.
Evaluation and Comparative Analysis
We compare the proposed embedding framework with various existing dimensionality
reduction techniques that are computationally tractable for processing large-scale features.
These existing techniques include the commonly used LSI in NLP, the kernel-based embed-
ding methods, such as KPCA, KOLPP, KFDA, and the kernelized RCCA (KRCCA), as well
as their corresponding linear versions of PCA, OLPP, FDA, and RCCA but using similarity
features as input, which are also known as empirical kernel mapping and denoted as EPCA,
EOLPP, EFDA, and ERCCA. Different types of embeddings are compared using the same
classifier linear SVM (LSVM).
We also compare classification performance with LSVM
and kernel SVM (KSVM) on their own without performing any dimensionality reduction, as
well as LSVM with similarity features, known as SVM based on empirical kernel mapping
and denoted as ESVM. Gaussian kernel was employed for all the kernel-based methods. Co-
sine similarity was used for all the methods with similarity features as input. The proposed
embedding framework selects measure for computing similarity features from the Gaussian
kernel and cosine function.
The embedding dimensionality k of LSI was set as the number of singular values that are
larger than the threshold of max(n, d) × 2
−52
s, where s is the largest singular value of the
input feature matrix. We adopted this threshold as it is used by the default MATLAB function
for computing the matrix rank. For the rest embedding methods, the reduced dimensionality k
was determined by keeping eigenvectors with 99.99% of accumulated eigenvalues. This can
be achieved by setting the percentage parameter for dimensionality control in equation (40)
as α = 0.9999. The above settings for determining the embedding dimensionality is to keep
the maximum number of independent degrees of freedom contained in the given data. The
formulation exp(−10
γ
x
i
− x
j

2
) was used to implement the Gaussian kernel, where the
value of γ was selected from the integers {−12, −11, −10, . . . , −4, −3, −2} for all the
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
methods based on standard kernel extension.
Our methods,
KFDA,
EFDA,
KRCCA,
and
ERCCA require to set a regularization parameter λ, for which the value of log
10
λ was chosen
from the integers {−6, −5, −4, . . . , 0, 1, 2}. For the SVMs, the regularization parameter
c was selected by varying log
10
c among the integers {−3, −2, −1, . . . , 8, 9, 10}.
These
regions for investigating parameter selection of γ , λ, and c were all preliminarily determined
by a coarse tuning on wider grids. Our methods, KOLPP and EOLPP also require to set the
number of nearest neighbors h for modeling the local distribution of the data, for which h
was chosen from the integer set of {15 + i 
n−15
4
}
4
i=1
. The notation · denotes round down,
keeping the largest integer that does not exceed the input value. We also allowed the use of
class-based weights for our method. Given a total number of 8,202 instances for Reuters data
set, we divided them into three partitions, with 2,730 instances for training, 2,737 validation
instances for parameter selection, and 2,735 test instances for final model assessment. Given
2,149 instances in total for EEP data set, we used 1,928 instances for training and 221 for
test, where the parameter selection was conducted within the 1,928 training instances using
fivefold cross validation (CV). For the four data sets of PPI extraction, a 10-fold CV based on
the same splitting scheme as used by Airola et al. (2008) and Miwa et al. (2009) was employed
for model assessment, where parameter selection was conducted within the training instances
using a ninefold CV for each of the 10 training-test partitions. Classification performance
was evaluated using macro F
1
score, which is the mean of the F
1
scores from all categories.
For each category, F
1
=
2
Precision
×
Recall
Precision
+
Recall
, where Precision =
TP
TP
+
FP
, Recall =
TP
TP
+
FN
, TP denotes
true positive, TN true negative, FP false positive, and FN false negative samples.
LSI was implemented in two ways: One uses the existing LSI tool Gensim0.8.0 with
Python 2.7.2 (
ˇ
Reh
ˇ
rek and Sojka 2010) and is referred as LSI
Gensim
, which is based on so-
phisticated implementation of large-scale sparse SVD. The other is our own implementation
based on eigendecomposition of the matrix XX
T
(see Section 2.2) and referred as LSI
fast
,
which is faster to process data sets with fairly low number of training instances.
The rest
embedding methods and LSI
fast
were implemented using MATLAB R2011. The SVMs were
implemented using LIBSVM (Chang and Lin 2011).
Since it is time consuming to apply
SVD to feature matrices with millions of features even when the matrices are sparse, we run
LSI
Gensim
on a 2.27GHz×32 CPU with hyper-threading and 314GB memory, while the rest
algorithms on a 3.06GHz CPU with 2GB memory.
Table 2 compares classification performance of different
embeddings methods based
on the same classifier LSVM, as well as different SVMs on their own without performing
dimensionality reduction.
The final
number of features (k) used as input
to a classifier
is displayed together with the classification performance.
For LSVM and KSVM,
since
dimensionality reduction is not
applied,
k is equal
to the original
feature number d.
For
ESVM,
since similarity features are used as input,
k is equal
to the number of training
instances n. It can be seen from Table 2 that some embedding methods only compute one
single embedding. For KFDA, EFDA, KRCCA, and ERCCA, the use of k = 1 results from
the algorithm design.
These methods preserve only the class structure in the embedded
space,
which leads to eigendecomposition of a matrix with its rank no more than c − 1.
Thus, the resulting number of independent embeddings is no more than c − 1. For binary
classification with c = 2, we have k = 2 − 1 = 1. As shown in existing research, such drastic
dimensionality reduction may lose important information (Goulermas et al. 2005). However,
this is inevitable due to the algorithm nature. Differently, for our method and KOLPP, the use
of k = 1 results from the model selection procedure. The model selection criterion indicates
that the use of one embedding provides better performance than multiple ones. To examine
whether the parameter selection is done properly,
we check whether the selected values
are inside the searching region.
It is observed that for all the methods in Table 2 none of
C
OMPUTATIONAL
I
NTELLIGENCE
T
ABLE
2.
Comparison of Classification Performance in Macro F
1
Score for Different Embedding Methods Followed by a Linear SVM, as Well as Three Types
of SVMs without Performing Dimensionality Reduction. The (Average) Input Feature Dimensionality k to the Classifier Is Shown in Parenthesis. For Each Data Set,
the Best Performance Is Shown in Bold and Underlined, and the Second-Best Performance in Bold Only.
Methods
Reuters
EEP
AIMED
HPRD
IEPA
LLL
LSVM
0.950 (69593)
0.537 (1721881)
0.593 (3762146)
0.677 (3762146)
0.717 (3762146)
0.851 (3762146)
KSVM
0.902 (69593)
0.529 (1721881)
0.470 (3762146)
0.528 (3762146)
0.572 (3762146)
0.796 (3762146)
ESVM
0.961 (2735)
0.586 (1928)
0.525 (5251)
0.630 (390)
0.680 (735)
0.785 (297)
LSI
0.948 (2729)
0.512 (1928)
0.593 (5011)
0.695 (302)
0.705 (622)
0.843 (297)
KPCA
0.927 (654)
0.473 (185)
0.531 (1438)
0.703 (287)
0.704 (636)
0.703 (169)
KOLPP
0.500 (1)
0.242 (5)
0.349 (1)
0.556 (11)
0.605 (1)
0.761 (43)
KFDA
0.931 (8)
0.516 (24)
0.589 (1)
0.704 (1)
0.728 (1)
0.825 (1)
KRCCA
0.937 (8)
0.523 (39)
0.589 (1)
0.672 (1)
0.723 (1)
0.832 (1)
EPCA
0.717 (537)
0.253 (5)
0.288 (68)
0.653 (135)
0.678 (259)
0.784 (59)
EOLPP
0.739 (4)
0.306 (5)
0.314 (79)
0.658 (184)
0.682 (255)
0.783 (57)
EFDA
0.947 (8)
0.609 (5)
0.594 (1)
0.649 (1)
0.712 (1)
0.794 (1)
ERCCA
0.906 (8)
0.411 (39)
0.531 (1)
0.642 (1)
0.689 (1)
0.731 (1)
Proposed
0.962 (116)
0.622 (247)
0.616 (1)
0.725 (12)
0.729 (30)
0.853 (62)
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
the selected values of the gaussian and regularization parameters (γ , λ, and c) lies on the
boundary of the parameter space, which indicates the used regions for parameter selection are
sufficient. Table 2 shows that, in general, LSVM provides comparatively good performance,
however, SVM with Gaussian kernels (KSVM) performs worse than LSVM for most data
sets. As for ESVM based on cosine similarity features, it performs better than LSVM for
the two text categorization data sets but worse than LSVM for the four PPI data sets. For
LSI, we record performance of the better performed implementation between LSI
Gensim
and
LSI
fast
for each data set, where LSI
fast
performs better for the Reuters, EEP, and LLL data
sets while LSI
Gensim
performs better for the remaining data sets. Among LSI, KPCA, KOLPP,
KFDA, KRCCA, EPCA, EOLPP, EFDA, and ERCCA, many of them are able to improve the
LSVM performance to some extent for certain data sets, but very few of them can perform
constantly well for all the data sets, where only EFDA and KRCCA provide comparatively
good performance for all the data sets. The proposed method performs the best among all the
compared embedding methods for all the data sets. This shows that performance of existing
embedding methods is more data dependent than ours, and our method is more robust. The
proposed method also performs better than the three types of SVMs on their own without
using any embedding technique for all the data sets. This shows that the proposed method can
satisfactorily improve the discriminating power of the original features and (dis)similarity
features.
Figure 1 compares computing time versus number of training instances (n), as well as
computing time versus input feature dimensionality (d) for different embedding methods
using the AIMED data set.
The proposed method is compared to two versions of LSI in
Figures 1(a) and (b) with the same sets of training instances and input features. It can be seen
that, although sophisticated SVD software is employed, it is still time consuming to compute
the SVD of a sparse matrix with large-scale features. To generate k = 600 embeddings (and
projections) for n = 5,000 training instances, LSI
Gensim
took around 10 hours with powerful
computing environment of 2.27 GHz ×32 CPU with hyper-threading and 314 GB memory.
The other implementation LSI
fast
took only around an hour to generate more than 5,000
embeddings for n = 5,834 training instances under a much cheaper computing environment
of 3.06 GHz CPU with 2 GB memory (see LSI
fast
in Figure 1).
The proposed method
requires comparable computing time to LSI
fast
.
Thus,
our method and LSI
fast
are more
suitable than LSI
Gensim
for processing fairly low number of training instances with extremely
high feature dimensionality. Figures 1(c) and (d) compare the embedding methods with their
computational cost mainly related to the instance number.
These include EPCA,
EOLPP,
EFDA,
and ERCCA that all work with cosine-based similarity features,
as well as LSI
fast
and the proposed method.
It can be seen from Figure 1(c) that,
with increased number of
training instances, all these methods require computing time in similar scales, which is around
seconds for processing less than 1,000 instances, around minutes for 2,000 – 3,000 instances,
and about 1 to 1.5 hours for 3,000 – 6,000 instances.
It can be observed from Figure 1(d)
that computing times of these method are less sensitive to the input dimensionality, which
vary between 1 to 13 minutes when the feature number increases from 50 to 3,762,146,
for processing n = 3,000 instances. Overall, the supervised embedding methods of EFDA,
ERCCA, and the proposed one require slightly more computing time than the unsupervised
ones of LSI
fast
, EPCA, and EOLPP.
4.2.
Further Analysis on PPI Extraction
In this section, we conduct more analysis for PPI extraction task to further demonstrate the
effectiveness of the proposed embedding framework. In the above experiments, we applied
SVM to assess the embeddings,
which is a very sophisticated classifier and is currently
C
OMPUTATIONAL
I
NTELLIGENCE
(a)
Computing time vs. instance number
(b)
Computing time vs. feature dimensionality
(c)
Computing time vs. instance number
(d)
Computing time vs. feature dimensionality
F
IGURE
1.
Comparison of computing time using the AIMED data set. (a) Comparison of computing time
vs.
instance number
for
two versions of
LSI
and the proposed method with input
dimensionality fixed as
d = 3,762,146. (b) Comparison of computing time vs. feature number for two versions of LSI and the proposed
method with number of training instances fixed as n = 3,000. (c) Comparison of computing time vs. instance
number for various embedding methods with input dimensionality fixed as d = 3,762,146. (d) Comparison of
computing time vs. feature number for various embedding methods with number of training instances fixed as
n = 3,000. All computing time is recorded in seconds.
among the best
performers for a number of classification tasks.
In order to demonstrate
the robustness of the proposed embeddings,
we employed a naive nonparametric linear
classifier in the following experiments instead of SVM for classification, which is the Fisher
linear discriminant analysis (FLDA) based on minimum-squared-error (MSE) (Duda, Hart,
and Stork 2001),
referred as MSE-FLDA.
Such setup allows to compare the combination
of robust
embeddings and naive classifier with the combination of original
features and
sophisticated classifier that is used by most state-of-the-art PPI extraction systems (Airola
et al. 2008; Miwa et al. 2009).
4.2.1 Combined (Dis)similarity Features.
In order to better take into account
prior
known linguistic structure of the features, we compute four groups of (dis)similarity features
independently from the bag-of-words,
shortest
path,
dependency graph,
and linear graph
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
T
ABLE
3.
Settings for Computing Combined (Dis)similarity Features.
(Dis)similarity meaure
Combination weights [a
1
, a
2
, a
3
, a
4
]
AIMED
Cosine similarity
[0.241, 0.713, 0.190, 0.579]
HPRD
Cosine similarity
[0.250, 0.250, 0.250, 0.250]
IEPA
Gaussian kernel
[0.031, 0.807, 0.673, 0.161]
LLL
Euclidean distance
[0.375, 0.691, 0.356, 0.581]
T
ABLE
4.
Performance Comparison of Different PPI Extraction Systems in Terms of AUC Value and F
1
Score. Performance Ranking of the Competing Method Is also Shown in Parentheses for Each Measure and Each
Data Set.
AUC (ranking)
F
1
score (ranking)
Miwa
Airola
Airola
Data
et al.
et al.
et al.
sets
(2009)
(2008)
Proposed-S
Proposed-C
(2008)
Proposed-S
Proposed-C
AIMED 0.891 (1)
0.848 (4)
0.881 (2)
0.888 (3)
0.564 (3)
0.616 (2)
0.633 (1)
HPRD
0.828 (3)
0.797 (4)
0.830 (2)
0.844 (1)
0.634 (3)
0.725 (1)
0.698 (2)
IEPA
0.856 (1)
0.851 (2)
0.848 (3)
0.847 (4)
0.751 (1)
0.729 (3)
0.748 (2)
LLL
0.860 (1)
0.834 (4)
0.849 (3)
0.856 (2)
0.768 (3)
0.853 (1)
0.835 (2)
features, respectively, using the same (dis)similarity measure chosen from cosine function,
Gaussian kernel, and Euclidean distance, and obtain the combined (dis)similarity features
based on equations (37) and (38).
Since there are multiples model
parameters to be de-
termined,
including the four combination weights,
the option for (dis)similarity measure,
and the parameters for the proposed embedding algorithm, we use genetic algorithm (GA)
(Goldberg 1989) for model
selection within the training set
of each of the 10 training-
test
partitions.
The “Genetic Algorithm Toolbox for
use with MATLAB (version 1.2)”
(Chipperfield et al. 1994) was employed to implement GA. The fitness function was calcu-
lated using linear ranking. The population size was set as 20,
and the generation gap was
set as 0.6. Two-point crossover was employed, with the corresponding crossover rate set as
0.6.
Bit flip was used for mutation,
with the corresponding mutation rate set as 0.1.
The
maximum number of generations was set as 200. We observed that, for each corpus, the 10
tuned models for 10-fold CV are similar apart from slight difference between the values of
{a
i
}
4
i=1
.
The final setting for computing the combined (dis)similarity features is shown in
Table 3, where the combination weights were computed by averaging the parameter values
on the 10 tuned models. It can be observed from Table 3 that the shortest path features always
possess the highest combination weight as compared with the other three types of features,
which implies it is perhaps the most important feature type for recognizing the interacting
protein pairs.
Table 4 compares two PPI extraction systems based on the proposed embedding frame-
work with two state-of-the-art systems (Airola et al. 2008; Miwa et al. 2009).
One of our
systems computes embeddings from the combined (dis)similarity features and uses the naive
MSE-FLDA classifier for classification, denoted as Proposed-C. The other system combines
the embeddings computed in Section 4.1 with the LSVM classifier, which is the same sys-
tem as the one producing the performance in the last
row of Table 2 and is denoted as
C
OMPUTATIONAL
I
NTELLIGENCE
T
ABLE
5.
Performance Comparison with Airola et al.
(2008) in Terms of F
1
Score and AUC Value for Cross-Corpora Evaluation to Test the Generalization
Ability.
The Sign Pair (+/−, +/−, +/−) Shows Whether Our System Possesses Improved Test Performance for the Three Test Data Sets,
Where “+” Indicates
Positive Improvement and “−” the Opposite.
F
1
score
AUC value
AIMED
HPRD
IEPA
LLL
Improvement
AIMED
HPRD
IEPA
LLL
Improvement
AIMED
AIMED
Existing
–
0.646
0.229
0.177
Existing
–
0.824
0.761
0.778
Proposed
–
0.672
0.208
0.196
(+, −, +)
Proposed
–
0.811
0.769
0.747
(−, +, −)
HPRD
HPRD
Existing
0.409
–
0.563
0.455
Existing
0.725
–
0.749
0.640
Proposed
0.433
–
0.564
0.644
(+, +, +)
Proposed
0.766
–
0.780
0.799
(+, +, +)
IEPA
IEPA
Existing
0.384
0.656
–
0.770
Existing
0.702
0.800
–
0.825
Proposed
0.388
0.637
–
0.687
(+, −, −)
Proposed
0.735
0.785
–
0.886
(+, −, +)
LLL
LLL
Existing
0.326
0.583
0.639
–
Existing
0.618
0.694
0.748
–
Proposed
0.360
0.650
0.648
–
(+, +, +)
Proposed
0.666
0.746
0.757
–
(+, +, +)
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
T
ABLE
6.
Performance Comparison with Miwa et al.
(2009) in Terms of AUC Value for Cross-Corpora
Evaluation to Test the Generalization Ability.
The Sign Pair (+/−, +/−, +/−) Shows Whether Our System
Possesses Improved Test Performance for the Three Test Data Sets, Where “+” Indicates Positive Improvement
and “−” the Opposite.
AIMED
HPRD
IEPA
LLL
Improvement
AIMED
Existing
–
0.791
0.757
0.744
Proposed
–
0.811
0.769
0.747
(+, +, +)
HPRD
Existing
0.768
–
0.773
0.765
Proposed
0.766
–
0.780
0.799
(−, +, +)
IEPA
Existing
0.725
0.761
–
0.871
Proposed
0.735
0.785
–
0.886
(+, +, +)
LLL
Existing
0.656
0.729
0.753
–
Proposed
0.666
0.746
0.757
–
(+, +, +)
Proposed-S. To solve the same PPI extraction tasks, Miwa et al. (2009) employed two-norm
LSVM (L2-LSVM) for classification with exactly the same linguistic features as ours as
the input. Airola et al. (2008) used a sparse regularized least squares kernel-based machine
learning method (RLS) with all-paths graph kernel for classification. Both L2-LSVM and
RLS are famous classifiers for producing good classification performance and possessing
good generalization ability in NLP. In order to assess the system performance, Airola et al.
(2008) used two common performance measures in the area, which are the AUC value and
the F
1
score (see the cited paper for more details).
To compare with Airola et al.
(2008)
under the same evaluation setting, we computed the same measures of AUC value and F
1
score.
Miwa et al.
(2009) also included the AUC value but reported the highest
F
1
score
by selecting an appropriate value of the decision threshold for the classifier.
Because this
introduces optimistic bias in the model assessment, we only compare with Miwa et al. (2009)
based on AUC values.
For
binary classification,
a reliable and valid AUC value can be interpreted as the
probability that
the classifier
will
assign a higher
score to a randomly chosen positive
example than to a randomly chosen negative example. As compared to Airola et al. (2008),
our two systems provide better AUC values for three out of the four competing data sets (see
Table 4). However, the AUC value does not directly indicate the real classification accuracy
of a decision system and sometimes can be quite noisy as a classification measure (Lobo
et al. 2008; Hanczar et al. 2010). A classification system with higher AUC values does not
necessarily possess higher classification accuracy if the decision threshold determined by
the training set cannot be well generalized to the testing set. Thus, for a better comparison,
it is also necessary to observe the F
1
score that directly measures the classification accuracy
of a decision system. It can be seen from Table 4 that, apart from higher AUC values, our
two systems also provide much higher F1 score than the one in Airola et al. (2008) for most
of the data sets. As compared to the system in Miwa et al. (2009), our two systems provide
slightly worse but comparable AUC values.
4.2.2 Cross-corpus Evaluation.
We also compare the generalization property of dif-
ferent PPI extraction systems by training on one corpus and testing on different corpora.
C
OMPUTATIONAL
I
NTELLIGENCE
For our system,
cosine-based similarity features were used as the input
of the proposed
embedding framework and the resulting embeddings were classified by MSE-FLDA. Table 5
compares the cross-corpus performance of our system with that of the existing one developed
by Airola et al. (2008), in terms of the two measures of F
1
score and AUC value as used in
their paper. It can be seen from Table 5 that, amongst the 12 sets of training-test trials, our
system produces better F
1
score and AUC value in nine trials. We also compare our AUC
values with those reported by Miwa et al. (2009) in Table 6, where our system provides better
AUC values in 11 out of the 12 trials. Comparison in Tables 5 and 6 indicates that our system
possesses improved generalization ability as compared to the existing ones.
5.
CONCLUSION
We have proposed a computationally tractable dimensionality reduction method to gen-
erate low-dimensional and robust embeddings for classification tasks based on extremely
high number of features (millions) but fairly low number of training instances (thousands).
This type of classification problems appears in many recent
high-level
NLP tasks.
The
proposed method works with an inner product based optimization template that takes into
account general behavior of how linguistic features indicate similarities between instances.
In the embedded space,
the intrinsic geometry of the original data is preserved while the
class separability for both interclass and intraclass instances is enhanced via defining su-
pervised proximity weights between instances.
We also employ (dis)similarity features to
enable computationally tractable processing for extremely high-dimensional
input
and to
handle nonlinearities during embedding generation. Evaluation has been conducted on two
NLP tasks of text categorization and PPI extraction. We have compared the proposed method
with several existing embedding methods, as well as different types of SVMs on their own
without dimensionality reduction, in terms of classification performance (see Table 2) and
computing time (see Figure 1). Analysis on combined (dis)similarity features (see Table 4)
and cross-corpus evaluation for testing the generalization ability of a system (see Tables 5
and 6) has been conducted for the PPI extraction task. Experimental results show that the
proposed method generates embeddings with powerful class discriminability and possesses
tractable computing time.
In this research,
we have attempted to improve the performance of
a classification
system during the feature preprocessing stage,
instead of the later training procedure of
a classifier.
The robust embeddings computed by the proposed method possess improved
discriminative ability and lower dimensionality than the original
linguistic features.
The
proposed technique is applicable to many problems in NLP, TM, and bioinformatics. Future
research will
contribute on how to generate robust
features for multitask learning,
e.g.,
discovering a common embedding space by learning from multiple corpora.
Owing to
the improved generalization property of the generated low-dimensional embeddings,
it is
possible to extend the method to benefit domain adaptation, aiming to learn robust features
from source domains in order to facilitate the classification task in the target domain, with
(or without) the use of the seed knowledge from target domain.
ACKNOWLEDGMENT
The work was supported by the UK Biotechnology and Biological Sciences Research
Council (BBSRC project BB/G013160/1 Automated Biological Event Extraction from the
Literature for Drug Discovery).
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
REFERENCES
A
IROLA
, A., S. P
YYSALO
, J. B
JRNE
, T. P
AHIKKALA
, F., G
INTER
F, and T. S
ALAKOSKI
2008, All-paths graph kernel
for protein-protein interaction extraction with evaluation of cross-corpus learning,
BMC Bioinformatics,
9:(Suppl 2):1 – 12.
A
NANIADOU
, S., P. T
HOMPSON
, J. T
HOMAS
, T. M
U
, S. O
LIVER
, M., R
ICKINSON
, Y. S
ASAKI
, D. W
EISSENBACHER
,
and J. M
C
N
AUGHT
2010, Supporting the education evidence portal via text mining, Philosophical Transca-
tions of the Royal Society A, 1925:3829 – 3844.
B
ACH
, F., and M. J
ORDAN
2002, Kernel independent component analysis, Journal of Machine Learning Research,
3:1 – 48.
B
EKKERMAN
,
R.,
N.
T
ISHBY
,
Y.
W
INTER
,
I.
G
UYON
,
and A.
E
LISSEEFF
2003,
Distributional word clusters vs,
words for text categorization, Journal of Machine Learning Research, 3:1183 – 1208.
B
ELKIN
,
M.,
and P.
N
IYOGI
2003,
Laplacian eigenmaps for dimensionality reduction and data representation,
Neural Computation, 15(6):1373 – 1396.
B
ERGSMA
, S., D. L
IN
, and D. S
CHUURMANS
2010, Improved natural language learning via variance-regularization
support
vector machines.
In Proceedings of the 14th Conference on Computational
Natural
Language
Learning (CoNLL), Uppsala, Sweden.
B
ERRY
,
M.
W.
1992,
Large scale sparse singular value computations,
International Journal of Supercomputer
Applications, 6:13 – 49.
B
LEI
,
D.
M.,
A.
Y.
N
G
,
M.
J
ORDAN
,
and J.
L
AFFERTY
2003,
Latent
Dirichlet
allocation,
Journal
of Machine
Learning Research, 3:993 – 1022.
B
LOEHDORN
, S., R. B
ASILI
, M. C
AMMISA
, and A. M
OSCHITTI
2006, Semantic kernels for text classification based
on topological measures of feature similarity. In Proceedings of the 6th IEEE International Conference on
Data Mining (ICDM), Hong Kong.
B
LOEHDORN
, S., and A. M
OSCHITTI
2007a. Combined syntactic and semantic kernels for text classification. In
Proceedings of the 29th European Conference on Information Retrieval (ECIR), Rome, Italy.
B
LOEHDORN
,
S.,
and A.
M
OSCHITTI
2007b.
Exploiting structure and semantics for expressive text kernels.
In
Proceedings of the Conference on Information Knowledge and Management, Lisbon, Portugal.
B
RAND
, M. 2006. Fast low-rank modifications of the thin singular value decomposition, Linear Algebra and Its
Applications, 415:20 – 30.
C
HANG
, C., and C. L
IN
2011, LIBSVM: A library for support vector machines, ACM Transactions on Intelligent
Systems and Technology, 2:1 – 27.
C
HIPPERFIELD
, A. J., P. J. F
LEMING
, H. P
OHLHEIM
, and C. M. F
ONSECA
1994, Genetic Algorithm Toolbox for use
with MATLAB (version 1.2). Sheffield, UK: University of Sheffield.
C
OLLINS
, M., and N. D
UFFY
2001, Convolution kernels for natural language. In NIPS, pp. 625 – 632.
C
OPPERSMITH
, D., and S. W
INOGRAD
1990, Matrix multiplication via arithmetic progressions, Journal of Sym-
bolic Computation, 9:251 – 280.
C
RISTIANINI
, N., and J. S
HAWE
-T
AYLOR
2000, An introduction to support vector machines and other kernel-based
learning methods. Cambridge University Press: Cambridge, UK.
C
ULOTTA
, A., and J. S
ORENSEN
2004, Dependency tree kernels for relation extraction. In Proceedings of the 42nd
Annual Meeting on Association for Computational Linguistics, Barcelona, Spain.
D
EERWESTER
, S., S. T. D
UMAIS
, G. W. F
URNAS
, T. K. L
ANDAUER
, and R. H
ARSHMAN
1990, Indexing by latent
semantic analysis, Journal of the American Society for Information Science, 41:391 – 407.
D
HILLON
, I. S., S. M
ALLELA
, and R. K
UMAR
2003, A division information-theoretic feature clustering algorithm
for text classification, Journal of Machine Learning Research, 3:1265 – 1287.
D
UDA
, R. O., P. E. H
ART
, and D. G. S
TORK
2001, Pattern Classification (2nd ed.). John Wiley and Sons: New
York.
D
UIN
, R. P. W. 2000, Classifiers in almost empty spaces. In Proceedings of the 15th International Conference on
Pattern Recognition (ICPR), Valencia, Spain.
C
OMPUTATIONAL
I
NTELLIGENCE
F
AYRUZOV
, T., M. D
E
C
OCK
, C. C
ORNELIS
, and V. H
OSTE
2009, Linguistic feature analysis for protein interaction
extraction, BMC Bioinformatics, 10(1):374.
F
ISHER
, R. A. 1936. The use of multiple measurements in taxonomic problems, Annals of Eugenics, 7(2):179 –
188.
F
RANTZI
, K. T., S. A
NANIADOU
, and H. M
IMA
2000, Automatic recognition of multi-word terms: The c-value/
nc-value methodn, International Journal on Digital Libraries, 3(2):115 – 130.
G
OLDBERG
, D. E 1989, Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley:
Reading, MA.
G
OLUB
,
G.
H.,
and C.
F.
V.
L
OAN
1996,
Matrix Computation (3rd ed.).
Johns Hopkins University Press:
Baltimore, MD.
G
ONG
, Y., and X. L
IU
2001, Generic text summarization using relevance measure and latent semantic analysis.
In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, Association for Computational Linguistics, New Orleans, LA.
G
OULERMAS
, J., A. F
INDLOW
, C. N
ESTER
, D. H
OWARD
, and P. B
OWKER
2005, Automated design of robust dis-
criminant analysis classifier for foot pressure lesions using kinematic data, IEEE Transaction on Biomedical
Engineering, 52(9):1549 – 1569.
H
AFFNER
,
P.
2006.
Fast
transpose methods for
kernel
learning on sparse data.
In Proceedings of
the 23rd
international conference on Machine learning (ICML), Pittsburgh, PA.
H
ANCZAR
, B., J. H
UA
, C. S
IMA
, J. W
EINSTEIN
, M. B
ITTNER
, and E. R. D
OUGHERTY
2010, Small-sample precision
of roc-related estimates. Bioinformatics, 26(6):822 – 830.
H
ARDOON
, D. R., S. R. S
ZEDMAK
, and J. R. S
HAWE
-
TAYLOR
. 2004. Canonical correlation analysis: An overview
with application to learning methods. Neural Computation, 16(12):2639 – 2664.
H
E
, X., and P. N
IYOGI
2003, Locality preserving projections. In Proceedings of Neural Information Processing
Systems 16 (NIPS), Vancouver, Canada.
H
ERN
´
ANDEZ
,
V.,
J.
E.
R
OM
´
AN
,
A.
T
OM
´
AS
,
and V.
V
IDAL
2007,
A survey of software for sparse eigenvalue
problems. Technical Report, Universidad Polit
´
ecnica de Valencia. SLEPc Technical Report STR-6.
H
OWLAND
,
P.,
M.
J
EON
,
and H.
P
ARK
2003,
Structure preserving dimension reduction for clustered text data
based on the generalized singular value decomposition, SIAM Journal on Matrix Analysis and Applications,
25(1):165 – 179.
H
SU
, C., and C. L
IN
2002, A comparison of methods for multiclass support vector machines, IEEE Transaction
on Neural Networks, 13(2):415 – 425.
J
IANG
, J.-Y., and S.-J. L
EE
. 2007. A weight-based feature extraction approach for text classification. In Proceedings
of the 2nd International Conference on Innovative Computing, Information and Control, Kumamoto City,
Japan.
J
OLLIFFE
, I. T 1986, Principal Component Analysis. Springer-Verlag: New York.
K
IM
, B. J., K. K
IM
, and K. B. K
IM
2004, Feature extraction and classification system for nonlinear and online
data. In Advances in Knowledge Discovery and Data Mining, Lecture Notes in Computer Science, volume
3056, pp. 171 – 180.
K
IM
, H., P. H
OWLAND
, and H. P
ARL
2005, Dimension reduction in text classification with support vector machines.
Journal of Machine Learning Research, 6:37 – 53.
K
IM
, S., J. Y
OON
, and J. Y
ANG
2008, Kernel approaches for genic interaction extraction, Bioinformatics, 24:118 –
126.
K
OKIOPOULOU
,
E.,
and Y.
S
AAD
2007,
Orthogonal
neighborhood preserving projections:
A projection-based
dimensionality reduction technique,
IEEE Transaction on Pattern Analysis and Machine Intelligence,
29(12):2143 – 2156.
K
OKIOPOULOUA
,
E.,
and Y.
S
AADB
2009,
Enhanced graph-based dimensionality reduction with repulsion
laplaceans, Pattern Recognition, 42:2392 – 2402.
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
K
RALLINGER
, M., F. L
EITNER
, C. R. P
ENAGOS
, and A. V
ALENCIA
2008, Overview of the protein-protein interaction
annotation extraction task of biocreative II, Genome Biology, 9(Suppl 2):S4.
K
WOK
, J. T., and H. Z
HAO
2003, Incremental eigen decomposition. In Proceedings of the International Conference
on Artificial Neural Networks (ICANN), Istanbul, Turkey.
L
EBANON
,
G.
2006,
Metric learning for text documents,
IEEE Transaction on Pattern Analysis and Machine
Intelligence, 28:497 – 508.
L
EOPOLD
,
E.,
and J.
K
INDERMANN
2002,
Text categorization with support vector machines.
How to represent
texts in input space? Machine Learning, 46(1):423 – 444.
L
EWIS
, D. D. 1992, Feature selection and feature extraction for text categorization. In Proceedings of the Workshop
on Speech and Natural Language, Harriman, New York.
L
EWIS
,
D.
D.
1997,
Reuters-21578
Text
Categorization
Test
Collection.
http://www.daviddlewis.com/
resources/testcollections/reuters21578/.
L
I
, H., T. J
IANG
, and K. Z
HANG
2006, Efficient and robust feature extraction by maximum margin criterion, IEEE
Transaction on Neural Network, 17(1):157 – 165.
L
I
, S., R. X
IA
, C. Z
ONG
, and C.-R. H
UANG
2009, A framework of feature selection methods for text categorization.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, Association for Computational Linguistics,
Suntec, Singapore.
L
IFSHITS
, Y., and S. Z
HANG
2009, Combinatorial algorithms for nearest neighbors, near-duplicates and small-
world design. In Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),
New York.
L
OBO
,
J.
M.,
A.
J
IM
´
ENEZ
-V
ALVERDE
,
and R.
R
EAL
2008,
AUC: A misleading measure of the performance of
predictive distribution models, Global Ecology and Biogeography, 17:145 – 151.
L
ODHI
, H., C. S
AUNDERS
, J. S
HAWE
-T
AYLOR
, N. C
RISTIANINI
, and C. W
ATKINS
2002, Text classification using
string kernels, Journal of Machine Learning Research, 2:419 – 444.
L
UXBURG
, U. 2007, A tutorial on spectral clustering, Statistics and Computing, 17(4):395 – 416.
M
ERCER
,
J.
1909,
Functions of positive and negative type and their connection with the theory of integral
equations,
Philosophical
Transaction of the Royal
Society of London.
Series A,
Containing Papers of a
Mathematical or Physical Character, 209:415 – 446.
M
IKA
,
S.,
G.
R
¨
ATSCH
,
J.
W
ESTON
,
B.
S
CH
¨
OLKOPF
,
and K.
M
ULLER
1999,
Fisher discriminant
analysis with
kernels. In Proc. of IEEE Neural Networks for Signal Processing Workshop, Madison, WI.
M
IWA
, M., S. R
UNE
, Y. M
IYAO
, and J. T
SUJII
2009, Protein-protein interaction extraction by leveraging multiple
kernels and parsers, International Journal of Medical Informatics, 78(12):e39 – e46.
M
IWA
, M., R. S
ÆTRE
, Y. M
IYAO
, and J. T
SUJII
2009, A rich feature vector for protein-protein interaction extraction
from multiple corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language
Processing, Association for Computational Linguistics, Singapore.
M
IYAO
, Y., R. S
TRE
, K. S
AGAE
, T. M
ATSUZAKI
, and J. T
SUJII
2008, Task-oriented evaluation of syntactic parsers
and their representations. In Proceedings of the 46th Annual Meeting of the Association for Computational
Linguistics on Human Language Technologies (ACL-HLT), Columbus, OH.
M
OSCHITTI
, A. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings
of the 17th European Conference on Machine Learning (ECML), Berlin, Germany.
M
OSCHITTI
, A. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceedings of
ACM 17th Conference on Information and Knowledge Management (CIKM), Napa Valley, CA.
M
U
,
T.,
J.
Y.
G
OULERMAS
,
J.
T
SUJII
,
and S.
A
NANIADOU
2012a,
Proximity-based frameworks for generating
embeddings from multi-output data.
In IEEE Transaction on Pattern Analysis and Machine Intelligence.
doi:10.1109/TPAMI.2012.20.
M
U
, T., J. J
IANG
, Y. W
ANG
, and J. Y. G
OULERMAS
2012b, Adaptive data embedding framework for multi-class
classification. In IEEE Transaction on Neural Networks and Learning Systems, 23(8):1291 – 1303.
C
OMPUTATIONAL
I
NTELLIGENCE
N
ADEAU
, D., and S. S
EKINE
2007, A survey of named entity recognition and classification, Linguisticae Investi-
gationes, 30:3 – 26.
N
G
, Vincent. 2010. Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the
48th Annual
Meeting of the Association for Computational
Linguistics,
Association for Computational
Linguistics, Uppsala, Sweden.
N
GUYEN
,
T.
T.,
A.
M
OSCHITTI
,
and G.
R
ICCARDI
2009,
Convolution kernels on constituent,
dependency and
sequential structures for relation extraction. In Proceedings of the 20th European Conference on Machine
Learning (ECML) Barcelona, Spain.
O
SI
´
NSKI
, S., J. S
TEFANOWSKI
, and D. W
EISS
2004, Lingo: Search results clustering algorithm based on singular
value decomposition.
In Proceedings of the International
IIS Advances in Soft
Computing,
Intelligent
Information Processing and Web Mining (IIPWM’04) Conference Zakopane, Poland.
O
SI
´
NSKI
,
S.,
and D.
W
EISS
2005,
A concept-driven algorithm for clustering search results,
IEEE Intelligent
Systems, 20:48 – 54.
P
EKALSKA
, E., and R.P.W. D
UIN
2002, Dissimilarity representations allow for building good classifiers, Pattern
Recognition Letters, 42(8):943 – 956.
P
EKALSKA
, E., R.P.W. D
UIN
, and P. P
ACLIK
2006, Prototype selection for dissimilarity-based classifiers, Pattern
Recognition, 39(2):189 – 201.
P
EKALSKA
, E., P. P
ACLIK
, and R. P. W. D
UIN
2002, A generalized kernel approach to dissimilarity-based classi-
fication, Journal of Machine Learning Research, 2:175 – 211.
P
ORTER
, M.F. 1980, An algorithm for suffix stripping, Program, 14(3):130 – 137.
ˇ
R
EH
ˇ
REK
,
R.,
and P.
S
OJKA
2010,
Software framework for
topic modelling with large corpora.
In Pro-
ceedings
of
the LREC 2010 Workshop on New Challenges
for
NLP Frameworks,
Valletta,
Malta.
http://pypi.python.org/pypi/gensim.
R
OUSU
, J., and J. S
HAWE
-T
AYLOR
2005, Efficient computation of gapped substring kernels on large alphabets,
Journal of Machine Learning Research, 6:1323 – 1344.
R
OWEIS
, S. T., and L. K. S
AUL
2000, Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323 – 2326.
S
CH
¨
OLKOPF
, B. 1997, Advances in Kernel Methods: Support Vector Learning. MIT Press, R. Oldenbourg Verlag:
Munich.
S
CH
¨
OLKOPF
, B., A. S
MOLA
, and K. M
¨
ULLER
1998, Nonlinear component analysis as a kernel eigenvalue problem,
Neural Computation, 10(5):1299 – 1319.
S
CH
¨
OLKOPF
, B., J. W
ESTON
, E. E
SKIN
, C. L
ESLIE
, and W. S. N
OBLE
2002, A kernel approach for learning from
almost orthogonal patterns. In Proceedings of the 13th European Conference on Machine Learning (ECML),
Helsinki, Finland.
S
HAWE
-T
AYLOR
, J., and N. C
RISTIANINI
2004, Kernel Methods for Pattern Analysis. Cambridge University Press:
Cambridge, UK.
S
HI
, J., and J. M
ALIK
2000, Normalized cuts and image segmentation, IEEE Transaction on Pattern Analysis and
Machine Intelligence, 22(8):888 – 905.
S
HI
, Q., J. P
ETTERSON
, G. D
ROR
, J. L
ANGFORD
, A. S
MOLA
, and S.V.N. V
ISHWANATHAN
2009, Hash kernels for
structured data, Journal of Machine Learning Research, 10:2615 – 2637.
S
UGIYAMA
, M. 2007, Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis,
Journal of Machine Learning Research, 8:1027 – 1061.
S
UN
,
L.,
S.
J
I
,
and J.
Y
E
2008,
A least squares formulation for a class of generalized eigenvalue problems in
machine learning.
In Proceedings of International
Conference on Machine Learning (ICML),
Helsinki,
Finland.
S
URDEANU
, M., L M
`
ARQUEZ
, X. C
ARRERAS
, and P. R. C
OMAS
2007, Combination strategies for semantic role
labeling, Journal of Artificial Intelligence Research, 29:105 – 151.
D
ISCOVER
E
MBEDDINGS IN
(D
IS
)
SIMILARITY
S
PACE
T
IKK
, D., P. P
ALAGA
, and U. L
ESER
2010, A fast and effective dependency graph kernel for PPI relation extraction,
BMC Bioinformatics, 11(Suppl 5):8.
T
ORGERSON
, W. S. 1952, Multidimensional scaling: I. Theory and method, Journal Psychometrika, 17(4):401 –
419.
T
REFETHEN
, L. N., and D. B
AU
1997, Numerical linear algebra. Society for Industrial and Applied Mathematics:
Philadelphia.
T
SUDA
,
K.
1998.
Support vector classifier with asymmetric kernel functions.
In Proceedings of the European
Symposium on Artificial Neural Networks (ESANN) Bruges, Belgium.
W
ANG
, T., and H. C
HIANG
2011, Solving multi-label text categorization problem using support vector machine
approach with membership function, Neurocomputing, 74(17):3682 – 3689.
W
U
,
D.
2009.
A comparison of approaches to determine topic similarity of weblogs for privacy protection.
Master’s thesis, Computer Science, Stony Brook University, Stony Brook, NY.
Y
AN
, S., D. X
U
, B. Z
HANG
, H. Z
HANG
, Q. Y
ANG
, and S. L
IN
2007, Graph embedding and extensions: A general
framework for dimensionality reduction, IEEE Transaction on Pattern Analysis and Machine Intelligence,
29(1):40 – 51.
Y
OSHINAGA
, N., and M. K
ITSUREGAWA
2010, Kernel slicing: Scalable online training with conjunctive features.
In Proceedings of the 23rd International Conference on Computational Linguistics,
Coling 2010,
Coling
2010 Organizing Committee, Beijing, China.
Z
AIDAN
, O. F., and J. E
ISNER
2007, Using “annotator rationales” to improve machine learning for text catego-
rization. In NAACL-HLT, Rochester, NY.
Z
ANZOTTO
, F. M., M. P
ENNACCHIOTTI
, and A. M
OSCHITTI
2009, A machine learning approach to textual entail-
ment recognition, Natural Language Engineering, 15(4):551 – 582.
Z
HAO
, H., W. C
HEN
, C. K
IT
, and G. Z
HOU
2009, Multilingual dependency learning: A huge feature engineering
method to semantic dependency parsing. In Proceedings of the Thirteenth Conference on Computational
Natural Language Learning: Shared Task (CoNLL’09), Boulder, CO.
Z
HOU
,
G.,
L.
Q
IAN
,
and J.
F
AN
2010,
Tree kernel-based semantic relation extraction with rich syntactic and
semantic information, Information Sciences: An International Journal, 180(8):1313 – 1325.
