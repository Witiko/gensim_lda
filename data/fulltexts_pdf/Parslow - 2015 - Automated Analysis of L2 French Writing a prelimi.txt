UNIVERSIT
´
E PARIS DIDEROT, PARIS 7
Automated Analysis of L2 French
Writing:
a preliminary study
by
Nicholas Lynton Parslow
A thesis submitted in partial fulfillment for the
degree of Master of Computational Linguistics
in the
UFR de Linguistique
July 2015
Declaration of Authorship
I,
Nicholas Lynton Parslow,
declare that this thesis titled,
‘Automated Analysis of L2
French Writing:
a preliminary study’ and the work presented in it are my own.
I confirm
that:

This work was done wholly or mainly while in candidature for a research degree
at this University.

Where any part of this thesis has previously been submitted for a degree or any
other qualification at this University or any other institution, this has been clearly
stated.

Where I have consulted the published work of
others,
this is always clearly at-
tributed.

Where I have quoted from the work of others,
the source is always given.
With
the exception of such quotations, this thesis is entirely my own work.

I have acknowledged all main sources of help.

Where the thesis is based on work done by myself jointly with others, I have made
clear exactly what was done by others and what I have contributed myself.
Signed:
Date:
i
“If anything can go wrong, it will.”
Capt.
Edward A. Murphy
UNIVERSIT
´
E PARIS DIDEROT, PARIS 7
Abstract
Lettres et Sciences Humaines
UFR de Linguistique
Master of Computational Linguistics
by Nicholas Lynton Parslow
This report presents an overview of automatic assessment of L2 texts and the potential
for applications to be developed for French leaning and assessment as a foreign or second
language.
Currently automatic assesment is rapidly gaining mainstream acceptance in
English,
and as a major international
language,
potentially great opportunities exist
for similar applications in French.
From this perspective,
existing tools and evaluation
metrics for both French and English are reviewed along with the Corpora currently
freely available for research and development.
An analysis is carried out of the L2, error-
annotated FipsOrtho corpus which shows that NLP tools designed principally for use
with formal L1 French are surprisingly robust when applied to (often highly unorthodox)
L2 productions.
Examples of over 50 possible metrics of vocabulary,
corrections,
verb
use,
clause use,
syntax and cohesion are studied on the CEFLE corpus.
From these a
preliminary level classification is established with promising results.
Finally suggestions
to further improve tools are presented and compared.
Acknowledgements
This project involved a large number of unforeseen hurdles and I am deeply indebted
to those who have helped me overcome them.
In particular Eric de la Clergerie for
his energy and constant willingness to adapt the output of
frmg to suit the needs of
the project.
Of great help too was Freiderikos Valetopoulos who saved the day with a
serious L2 corpus and a ton of information and links, and Hiyon Yoo for helping me break
the contact barrier,
likewise Pascal
Some for saving his corpus despite the great train
robbers and Elisabeth Delais-roussare for the friendly pointers.
Much appreciated were
all
the helpful
replies from remote Academics,
particularly Jeanine Treffers-Daller for
bravely facing the long lost directories, and to Xanthos Aris, S´ebastien L’Haire and Akira
Murakami for similar searches.
Big thanks to all the Alpage crew for their support and
ideas, particularly Marion Baranes for the help with sxpipe, Maximin Coavoux for the
advice on Neural Networks over coffee, Corentin Ribeyre for helping me get into Inria,
and Benoit Sagot for his patience with my constant stream of obscure bug notifications.
Thanks also to my fellow stagaires,
in particular Thimot´ee Bernard who went above-
and-beyond helping with last minute corpus corrections,
and to Olga Seminck for the
pause-caf´es,
Rachel
Bawden for keeping things fun ...
and the tennis ball,
Caroline
Brogniez for keeping perspective and Yuliya Matveyeva for giving an unexpected view
when needed (we miss you Yuliya!).
Thanks too to my brother Michael Parslow for the
supportive texts from afar despite his own troubles.
Above all though,
I couldn’t have
made it through though without the unending patience of a very special
Marta Zator
who has put up with my mood swings and vents at my own poor coding with nary a
blink of an eye ...
the lovelies avocado addict in the world :)
iv
Contents
Declaration of Authorship
i
Abstract
iii
Acknowledgements
iv
List of Figures
viii
List of Tables
x
1
Introduction
1
2
Learner Language Traits and Metrics
3
2.1
Language Standards
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
2.1.1
CEFR .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
2.1.2
CEFR: French
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
2.2
Grammar
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
2.2.1
Grammatical stages in L2 French .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
2.2.2
Grammatical stages in L1 French .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
2.3
Vocabulary
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
2.3.1
Vocabulary Metrics .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
2.4
Spelling
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
2.5
Cohesion and Coherence .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
3
Corpora
20
3.1
Learner Corpora
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
3.1.1
CEFLE .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
3.1.2
HELLAS-CHY-FLE corpora
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
3.1.2.1
CHY FLE
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
3.1.2.2
HELLAS FLE .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
3.1.2.3
CENTRE FLE .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
3.1.2.4
Pre-Treatment
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
3.1.3
FipsOrtho corpus .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
25
3.1.3.1
Pre-treatment
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
3.2
Native Corpora .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
v
Contents
vi
3.2.1
Corpus “Litt´eracie avanc´ee” .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
4
Language Evaluation Tools
29
4.1
Automatic Essay Assessment / Computer-Aided Scoring in English .
.
.
.
29
4.1.1
Automatic Essay Assessment in L2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
4.2
French-Specific Tools .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
4.2.1
Direkt Profil
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
4.2.2
FreeText .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
35
4.2.3
Auto-
´
Eval
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
4.2.4
FipsOrtho Spell Checker .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
38
4.3
Other related tools
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
40
4.3.1
Coh-Metrix 3.0 and Coh-Metrix-TEA .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
40
5
NLP tools
43
5.1
The ALPAGE treatment chain
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
5.1.1
Lefff
v.3.40 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
5.1.2
SX Pipe .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
5.1.3
MElt .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45
5.1.4
FRMG parser .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45
5.2
Gensim / Word2Vec
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
51
5.2.1
Review of Neural Networks and Word-Embeddings .
.
.
.
.
.
.
.
.
51
5.2.1.1
Word embeddings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
52
5.2.2
Word2Vec .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
52
5.3
Lexique
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
54
6
Results
55
6.1
Errors Corpus .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
55
6.1.1
Reconstructed Tree comparison .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60
6.2
MElt
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
62
6.3
Variables for Classification .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
65
6.3.1
General Structure Variables .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
65
6.3.2
Vocabulary variables .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
67
6.3.3
Error counting variables
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
72
6.3.4
Verb and Clause variables
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
77
6.3.5
Syntactic Complexity variables
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
79
6.3.6
Cohesion variables .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
85
6.4
Preliminary results:
Classification .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
86
7
Conclusions and Prospects
92
7.1
The Alpage treatment chain .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
92
7.2
Language Analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
93
A CEFLE prompts
95
B HELLAS prompts
99
Contents
vii
C detailed CEFR descriptors by genre
103
C.1
Range/Complexity descriptors
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 104
D POS tag alignement between MElt and FRMG
106
Bibliography
108
List of Figures
3.1
the ‘voyage en italie’ task
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
4.1
Direkt Profil
interface with the Noun Group information selected and
highlighted for an example of a stage 2 learner’s text
.
.
.
.
.
.
.
.
.
.
.
.
35
4.2
Example of the sentence analyser in FreeText for the sentence ‘il mange
un pomme’, translation and text-to-speech elements not shown.
.
.
.
.
.
.
36
4.3
FipsOrtho Interface (from [54] .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
39
4.4
Analysis of a text by the cohmetrix Text-Easability-Analyser
.
.
.
.
.
.
.
41
5.1
“il mange une pomme” as analysed by FRMG (detailed version)
.
.
.
.
.
47
5.2
“il mange une pomme” as analysed by FRMG (visually appealing version)
47
5.3
The architecture for CBOW and skip-grams compared, figure from [118]
.
53
6.1
Errors missed by frmg
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
57
6.2
Distribution of
the most commonly observed trees in the frmg analysis
of the FipsOrtho corpus,
the 4 four most common are ‘lexical’
which is
used for pronouns,
punctuation and some discourse markers,
‘86’
which
covers common nouns, ‘0’ which covers determiners and ‘364’ which covers
transitive verbs
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
61
6.3
Trees with a low level of overlap, Entry 332, sentence 2 in the FipsOrtho
corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
61
6.4
Distributions of parser weights per word in errored and error-free samples
62
6.5
Probability distributions for POS tags using MElt,
according to general
error type.
The bottom right plot indicates cases where no error was
tagged for the word.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63
6.6
Phrase tag sequence probability versus error density for sentences .
.
.
.
.
64
6.7
General Variable distributions by level on the CEFLE corpus, sd is stan-
dard deviation.
Some outliers may have been cut to make the main dis-
tributions more visible .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
68
6.8
Distributions by level of the number of words per sentence for the CEFLE
corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
69
6.9
Lexical
Diversity Variables distributions by level
on the CEFLE corpus,
sd is standard deviation.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70
6.10 The difficulty in extracting S at low levels.
Blue crosses denote the ob-
served cumulative text coverage while the dashed red line shows the fit .
.
72
6.11 Lexical Sophistication Variable distributions by level on the CEFLE corpus
73
6.12 Lexical Frequency Profile component distributions by level for the CEFLE
corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
74
6.13 Distributions by level of HD-D for the CEFLE corpus
.
.
.
.
.
.
.
.
.
.
.
75
viii
List of Figures
ix
6.14 Error variable distributions by level on the CEFLE corpus, sd is standard
deviation.
Some outliers may have been cut to make the main distribu-
tions more visible .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
76
6.15 Distributions by level
of
the rate of
MElt-FRMG POS disconcordances
per word for the CEFLE corpus .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
77
6.16 fractions of
VPs in various moods and tenses grouped by level
for the
CEFLE corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
80
6.17 fractions of
VPs which are with auxiliaries,
single-verbed and multiple-
verbed (note that currently multiple verbs include only goups with 2 verbs)
81
6.18 numbers of relative, nominative, accusative and locative clauses per sen-
tence, as grouped by level on the CEFLE corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
81
6.19 Distributions by level of the rate of auxilliary verb use per VP for the CE-
FLE corpus, a gaussian plot is included though should largely be ignored
as these distributions are more Poissonian. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
82
6.20 Syatactic complexity variables as grouped by level on the CEFLE corpus .
86
6.21 Distributions by level
of
the number of
words before the main verb for
the CEFLE corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
87
6.22 Cohesion variables as grouped by level on the CEFLE corpus
.
.
.
.
.
.
.
88
6.23 Distributions by level of cosθ between subsequent words, as extracted by
word2vec, for the CEFLE corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
89
A.1
the ‘l’homme sur l’ˆıle’ task .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
96
A.2
the ‘Un souvenir de voyage’ task
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
97
A.3
the ‘Moi, ma famille et mes amis’ task .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
98
B.1
HELLAS B2 activities
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 100
B.2
HELLAS C1 activity 1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 101
B.3
HELLAS C1 activity 2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 102
List of Tables
2.1
General description of levels in the CEFR (Table 1 in [9])
.
.
.
.
.
.
.
.
.
6
2.2
Writing self-assessment skill descriptions in the CEFR (from Table 2 in [9])
7
2.3
General writing skill descriptions in the CEFR (p.
61 in /citeCouncilofEu-
rope2001)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
2.4
Division du corpus Google N-Grammes en 10 groupes de fr´equence ´egale .
9
2.5
(slightly simplified) Grammatical
stages of
Bartning and Schlyter [17]
opp.
= opposition,
- = none observed,
+ = native-like,
cop.
= copula,
exp.
= expressions, pb.
= problems, pers.
= person, sing.
= singular, f.
simp.
= future simple,
prod.
= productive,
(x) = x is optional,
agr.
=
agreement, isol.
= isolated, amal.
= amalgams
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
2.6
The points system for the Morphosyntactic Complexity Scale[20]
.
.
.
.
.
13
3.1
Learner Corpus attributes [49]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
3.2
Properties of texts in the transverse corpus (from [50]) the native speakers
wrote shorter texts than group D,
this may reflect the natives writing 2
exercises straight after each other (i.e.
fatigue) or the fact that the most
advanced students will tend to be more interested and motivated by the
activity than an average French student writing a simple task in their own
language.
The absence of Year 3 reflects the way language options must
have been chosen in the Swedish system by the time a student reaches
high school.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
3.3
Number of textes for each level in Centre FLE corpus (ref personal comm)
24
3.4
Typography of language learner errors used in FipsOrtho, from [54].
Note
that many of these pass beyond spelling errors, and that this is not con-
sistent with the FreeText typography.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
3.5
Subcorpora of the “Litt´eracie avanc´ee” corpus, B. = Bachelor, M. = Master
28
3.6
Summary of Corpora used (Var.
= various, uni.
= university age)
.
.
.
.
28
4.1
Features used to estimate learner stage from text [94], f(X-Y) is the num-
ber of words used in the range of the Xth to Yth most frequently used
words from a native corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
34
4.2
Precision and Recall results for the DirektProfil SVM fit when grouping
levels in pairs [94] .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
34
4.3
Syntactic Errors searched for in FreeText from [96]
.
.
.
.
.
.
.
.
.
.
.
.
.
37
4.4
Auto-
´
Eval correction capabilities[99]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
38
5.1
Tagset used by MElt[108]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
45
x
List of Tables
xi
5.2
% parsed on diverse Corpora [113]
showing the difference with respect
to the French Tree Bank test set using the Labelled Attachment Score
(LAS) metric (the percentage of tokens with the correctly predicted head
and dependency relation)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
49
5.3
Corrections attempted by frmg in order of priority from top to bottom,
note that lightverb is in effect an in-place extention of Lefff rather than
a correction of a writer’s error.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50
6.1
Parsing success rates for sentences in their original
and correct formats
in the FipsOrtho corpus.
‘ok’ means a full parse, ‘corrected’ means some
corrections were made,
‘robust’
means a full
parse failed and a partial
parsing was performed.
If
a sentence is split into multiple parts these
are weighted so that the total
number of sentences matches the original
count.
‘m.e.p.’
indicates the mean number of annotated error occurances
per sentence .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
55
6.2
Parsing errors and correction distribution for appropriate errors .
.
.
.
.
.
56
6.3
Missed AGR errors analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
57
6.4
Missed Verbal error analysis .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
58
6.5
Spelling error retrieval rates in cases not found by the parser, + indicates
multiple errors including the one mentioned.
Example(c) is of a correctly
identified error, Example(i) of an unidentified or misidentified error.
.
.
.
59
6.6
Precision and Recall
for tree matching between original
and corrected
parses as a function of global parse result, each group of 3 shows number
of sentences, average precision, average recall
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60
6.7
Mean,
Standard Deviation and Root Mean Square deviation from unity
for the MElt probability distributions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
64
6.8
General
Structure Variables and their intercorrelations on the CEFLE
corpus, the number of paragraphs is 1 for each text so its corelations and
standard deviation are no shown, the number of sentences per paragraph
is also equivalent to the number of sentences .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
66
6.9
Correlations of vocabulary variables on the CEFLE corpus .
.
.
.
.
.
.
.
.
71
6.10 Correlations of error-related variables in the CEFLE corpus.
P melt is the
average POS probability,
meltdiff is the number of
POS disagreements
between MElt and FRMG per word,
ok,
corr.
and rob.
refer to the
fraction of sentences that were parsed ok, with corrections and in robust
mode respectively.
w/word is the parser average weight per word and
spell is the number of spelling corrections per word
.
.
.
.
.
.
.
.
.
.
.
.
73
6.11 Correlations between verb and clause variables.
vsingle represents VPs
containing only one verb,
vaux represents those containing auxiliaries,
vcomp other compound verb phrases.
indic, cond, subj, imp, fut and pres
are short for indicative,
conditional,
subjunctive,
imperfect,
future and
present respectively, vnotense is for infinitives and gerunds.
rel, nom, acc
and loc are short for clause types relative,
nominative,
accusative,
and
locative respectively.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
83
6.12 correlations of syntactic complexity variables,
TT is ‘Tree Types’,
TT/s
is ‘Tree Types per Sentence’
and WBMW is for ‘words before the main
verb’
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
85
6.13 Correlations between cohesion variables,
in column titles,
W = word,
S
= Sent, T = text and ct is cosθ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
87
List of Tables
xii
6.14 Naive Bayes classification of learner texts from the CEFLE corpus, with
text length included as a feature
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
88
6.15 Naive Bayes classification of learner texts from the CEFLE corpus without
text length correlated variables as features .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
89
6.16 Classification via automated Bayesian Inference Network construction us-
ing only presence/absence of meta-grammar trees .
.
.
.
.
.
.
.
.
.
.
.
.
.
90
C.1
Creative writing skill descriptions in the CEFR (p.62 of [9])
.
.
.
.
.
.
.
. 103
C.2
Essay/Report writing skill descriptions in the CEFR (p.62 of [9])
.
.
.
.
. 104
C.3
General Linguistic Range descriptors in the CEFR (p.110 of [9])
.
.
.
.
. 105
D.1
Alignment of MElt tags and FRMG tags .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 107
for my dearest Marta
xiii
Chapter 1
Introduction
Second Language learning has challenged humanity since the dawn of time, and despite
our eons of practice, significant improvements in methods of acquisition remain elusive.
With the rapid advances in Natural Language Processing (NLP) of recent years, the field
of Computer Assisted Language Learning (CALL) has begun to integrate more and more
advanced NLP tools, leading to the development of the new field of ‘Intelligent CALL’
(ICALL) which offers the potential to make great strides in this ancient problem.
The
main themes of research in ICALL focus on assessment such as automated essay scoring,
along with providing the learner tailored feedback and suggested resources via learner
modelling and intelligent tutoring systems.
English, for example, as the current lingua
franca, has already seen the integration of automatic scoring in high-stakes assessment
such as the Test of English as a Foreign Language (TOEFL) in which it is used in tandem
with human markers in the online version of
the test.
Although French is one of
the
most studied languages in the world and one of the most spoken as a second language,
currently there are very few resources that integrate the latest advances in parsing and
semantic networks with a goal of aiding learners.
The aim of this project then is firstly
to survey the literature for relevant techniques,
then to assess the applicability and
reliability of
existing French NLP tools on non-native produced texts,
and finally to
examine how well
they can be used in the automated analysis and grading of written
work.
This report is divided into five sections,
we begin with a review of
kind of
difficulties
language-learners exhibit in their written productions and the many metrics which exist
to quantify both differences with the native version of the target language (mistakes or
slips) and with the sophistication of
production (complexity).
The second chapter is
focused on the learner and native corpora used in this report,
and the pre-treatments
required to normalise them for convenient use.
A third section examines the tools which
1
2
provide examples of NLP applications in language assessment in general,
and particu-
larly those which have been made specifically for French.
In the forth chapter we look
at results of how well the analysis tools perform on the non-native texts and how their
accuracy is affected by the deviations from native writing.
Further we perform an exam-
ination of the variables which can be extracted from the NLP tools and their evaluate
their qualities as metrics of
language ability.
We also attempt a writing classification
implementation based on the discussed variables.
Finally we offer some perspectives,
based on the experiences of
this project on how to improve the existing tools for the
task and what new approaches might offer in developing specific applications to language
learners.
Chapter 2
Learner Language Traits and
Metrics
In terms of
language learning it is usually important to differentiate non-native (L2)
learning from the native (L1) acquisition which occurs in all of us as children [1].
Despite
the ‘2’ in ‘L2’, this in fact refers to any non-native language learning, whether it be the
second, third or tenth language learnt.
It can be useful to further differentiate between
learning as Second language (SL) - where one learns the language surrounded by it (for
example when a person moves to another country) from Foreign language (FL) learning
-
which occurs in a cultural
context of
the learner’s native language.
Of
course,
the
distinction has a range of intermediate situations (thus for example working in language
X and taking lessons in language Y in a country of language Y).
Surprisingly though,
the path of language learning across all three situations shares a great deal in common.
In particular the progression from an initial
stage of
verb-less sentences through an
intermediate stage of
infinitive verb use (even in finite contexts) to a final
stage of
finite verb use within the correct context is well documented ([2]).
Some tenses appear
universally before others,
regardless of training.
For example in French,
the imparfait
and pass´e compos´e are sure to appear in a learner’s language before the plus-que-parfait
or subjonctif.
Language learners show a surprising consistency in the order in which
L2 sytactic and morphological
elements are learnt [3].
This suggests that the rules
learnt are hierarchical in psycholinguistic complexity, attempts to teach advanced forms
early so as to ‘jump stages’
have also failed.
A second surprising result from studies
of
the productions of
language learners is that they construct a gradually changing
‘Interlanguage’[4] which is neither an extension of their first language nor a strict subset
of
the second language.
In other words a learner with use rules that exist neither in
their L1 nor the target language.
3
4
As a result, understanding a learner’s level can be very important to planning a teaching
program.
Indeed,
under the ‘Teachability Hypothesis’,
there is no point teaching a
grammatical
point until
the student is ‘ripe’
to learn it[5].
Of
course,
L1 influence
(or ‘transfer’) does play a role,
such as in spelling where errors can arise due to L1
phonology.
Here we have an example from an L1 Greek-Cypriot:
“De plus en ville il ya
plusier chances d’etucation pour les enfants et aussi
on peut trouve rapitement un
hopital, si c’est nessesaire.”[6] The spelling error comes from the devoicing of occlusives
in Greek.
However L1,
is not the only extenal
source,
in particular English,
which is
often learnt before French as an L2, can present itself not just through non-French words,
but also through mixed morphologies:
“Donc,
ce n’est pas surprisant que beaucoup
de gens rentrent `a la campagne,
o`
u la vie est plus calme est douce”[6].
Tranfer can
manifest itself
in three main ways:
avoidance,
overuse and facilitation.
The latter is
actually positive,
and easy to imagine when similar lexical
terms exist in both L1 and
L2.
Despite the existence of transfer effects,
error analysis suggests that the nature of the
errors that a learner makes vary more as a function of their progression in the language,
than of their L1 or other L2’s they have learnt.
The interlanguage is not neccessarily
self-consistent,
and syntactic rules being acquired are rarely applied systematically by
students:
“Il y a pleins de caf´es,
des discoth`eques,
de cinema pour aller et passer
des beaux moments” [6].
For all this, a pure error-based analysis proves unsatisfactory
in language analysis,
due in part to the effect of
so-called ‘Fossilised errors’,
simple
mistakes that persist even at intermediate and higher levels [7].
In French, these include
verbal inflections (particularly spelling and irregular forms), feminin form construction
and agreement of past participles,
determinants as much at a morphological
level
as a
syntactic one.
The most obvious addition is to include usage-based measures to compensate, this does
require some refinement though.
When considering language elements,
it is important
to be consistent in considering an element as acquired after first use, or once it is used
accurately/appropriately.
As in the examples above, variance exists not simply between
learners but also within a learner’s own language,
for example the learner may use a
correct structure on one occasion but then another incorrect one in a similar situation.
A problem with usage-based measures is that,
at all
levels,
learners show a preference
for simpler expressions whenever possible and in doing so often understate their true
abilities.
Additionally usage often follows a U-shaped acquisition sequence,
that is,
a
high initial use followed by avoidance, followed finally by an increase in usage [2], which
can make estimation of level by simple presence or absence of usage problematic.
As a
result, this form-based (or ‘usage’) view of a sequence of stages in language learning has
given way recently to a more functional
(‘use’) view of language stages.
The yardstick
5
is not what the learner knows of the language, but what they can achieve with it.
Thus
elements such as appropriate register may be more important than correct morphology
in terms of communicative goals.
From an automatisation point of view though, this is
far more difficult to implement as it requires a strong capacity in pragmatics.
2.1
Language Standards
The need to assess and certify language levels has lead to a large volume of research on
the subject, and most notably to detailed sets of standards, particularly in multi-lingual
areas such as in Europe.
2.1.1
CEFR
The Common European Framework of Reference for Languages /citeCouncilofEurope2001
(French version [8]) is a cross-linguistic comprehensive description of
levels in the ca-
pabilities of
students in second language acquisition and perhaps the most commonly
used standard today.
The 6 levels used are those which “have been found transparent,
useful
and relevant by groups of
non-native and native-speaker teachers from a vari-
ety of educational
sectors and very different profiles in terms of linguistic training and
teaching experience.” They can be further split into pairs, for example B1.1 and B1.2,
and the process iterated for more finer-grained descriptions as required.
The Frame-
work is designed with all aspects of second language learning in mind, from curriculum
and learning-objective development,
through teaching,
tracking and assessment.
The
levels are all
communicative in approach,
representing skills rather than knowledge of
the language,
and designed to consider as much as possible diverse paths of
language
acquisition.
Thus for example an accented phonetic system in speaking production is
not considered significant unless it impedes communication.
Likewise pragmatic and
sociolinguistic considerations play a role, not simply the syntax, morphology, phonology
and lexicon of the language.
The 6 levels are described in Table 2.1
More detailed descriptions are provided for Listening, Reading, Spoken Interaction, Spo-
ken Production and Writing, the last of which is shown in Table 2.2.
Clearly the main
focus at lower levels is on the range of texts that can be produced, while at higher levels
control
and sophistication of the writing become more important.
It should be noted
also that formal writing capabilities are only considered from level B2 onwards.
From a language,
rather than skills point of
view the CEFR offers the following for
writing (Table 2.3) which clearly shows the importance of
cohesion and coherence,
as
well as the ability to manipulate and manage information in the language.
6
Proficient User
C2
Can understand with ease virtually everything heard or
read.
Can summarise information from different
spoken
and written sources, reconstructing arguments and accounts
in a coherent presentation.
Can express him/herself
spon-
taneously,
very fluently and precisely,
differentiating finer
shades of meaning even in more complex situations.
C1
Can understand a wide range of
demanding,
longer texts,
and recognise implicit meaning.
Can express him/herself flu-
ently and spontaneously without much obvious searching for
expressions.
Can use language flexibly and effectively for so-
cial, academic and professional purposes.
Can produce clear,
well-structured, detailed text on complex subjects, showing
controlled use of organisational patterns, connectors and co-
hesive devices.
Independent User
B2
Can understand the main ideas of
complex text on both
concrete and abstract topics, including technical discussions
in his/her field of specialisation.
Can interact with a degree
of
fluency and spontaneity that makes regular interaction
with native speakers quite possible without strain for either
party.
Can produce clear,
detailed text on a wide range of
subjects and explain a viewpoint on a topical
issue giving
the advantages and disadvantages of various options.
B1
Can understand the main points of
clear standard input
on familiar matters regularly encountered in work,
school,
leisure,
etc.
Can deal
with most situations likely to arise
whilst travelling in an area where the language is spoken.
Can produce simple connected text on topics which are fa-
miliar or of personal interest.
Can describe experiences and
events, dreams, hopes and ambitions and briefly give reasons
and explanations for opinions and plans.
Basic User
A2
Can understand sentences and frequently used expressions
related to areas of most immediate relevance (e.g.
very basic
personal and family information, shopping, local geography,
employment).
Can communicate in simple and routine tasks
requiring a simple and direct exchange of
information on
familiar and routine matters.
Can describe in simple terms
aspects of his/her background, immediate environment and
matters in areas of immediate need.
A1
Can understand and use familiar everyday expressions and
very basic phrases aimed at the satisfaction of
needs of
a
concrete type.
Can introduce him/herself
and others and
can ask and answer questions about personal details such as
where he/she lives, people he/she knows and things he/she
has.
Can interact in a simple way provided the other person
talks slowly and clearly and is prepared to help.
Table 2.1:
General description of levels in the CEFR (Table 1 in [9])
There are also slightly more detailed descriptors for the contexts of Creative writing and
Report/Essay Writing (shown in Appendix C).
7
Proficient User
C2
I can write clear,
smoothly flowing text in an appropriate
style.
I can write complex letters,
reports or articles which
present a case with an effective logical structure which helps
the recipient to notice and remember significant points.
I
can write summaries and review of
professional
or literary
works.
C1
I can express myself in clear, well-structured text, expressing
points of
view at some length.
I can write about complex
subjects in a letter,
an essay or a report,
underlining what
I consider to be the salient issues.
I can select style appro-
priate to the reader in mind.
Independent User
B2
I can write clear,
detailed text on a wide range of subjects
related to my interests.
I can write an essay or report, pass-
ing on information or giving reasons in support of or against
a particular point of
view.
I can write letters highlighting
the personal significance of events and experiences
B1
I can write simple connected texts on topics which are fa-
miliar or of
personal
interest.
I can write personal
letters
describing experiences and impressions.
Basic User
A2
I
can write short,
simple notes and messages relating to
matters in areas of
immediate need.
I
can write a very
simple personal
letter,
for example thanking someone for
something.
A1
I can write a short,
simple postcard,
for example sending
holiday greetings.
I can fill
in forms with personal
details,
for example entering my name,
nationality and address on
a hotel registration form.
Table 2.2:
Writing self-assessment skill
descriptions in the CEFR (from Table 2 in
[9])
In terms of linguistic range,
descriptors focus on possessing the linguistic resources to
describe what you need to at low levels with minimal
difficulties and to formulate the
same thing in diverse ways at higher levels (see the tables in Appendix C). More specif-
ically,
for lexical and grammatical assessment,
the CEFR focuses on range and control
of
vocabulary,
morphology and structural
elements.
Orthographic competence is also
included in the descriptors,
not just via spelling,
but also through punctuation,
para-
graphing and layout conventions.
Writing ranges from these to small-scale features to
very broad scale effects such as thematic development and coherence and cohesion with
the emphasis on a shift from linear productions with minimal connectors at low levels to
narratives with subthemes/subpoints and a diversity of connective linguistic elements at
higher ones.
From a pragmatic, discourse and semantic perspective, epistemic modula-
tion of speech and writing is described under ‘Propositional Precision’ with lower levels
being able to provide simple information without degrees in certainty and upper levels
the opposite.
8
C2
Can write clear, smoothly flowing, complex texts in an appropriate
and effective style and a logical
structure which helps the reader
to find significant points.
C1
Can write clear, well-structured texts of complex subjects, under-
lining the relevant salient issues, expanding and supporting points
of view at some length with subsidiary points, reasons and relevant
examples, and rounding off with an appropriate conclusion.
B2
Can write clear, detailed texts on a variety of subjects related to
his/her field of
interest,
synthesising and evaluating information
and arguments from a number of sources.
B1
B1
Can write straightforward connected texts on a range of familiar
subjects within his field of interest,
by linking a series of shorter
discrete elements into a linear sequence.
A2
Can write a series of
simple phrases and sentences linked with
simple connectors like ‘and’, ‘but’ and ‘because’.
A1
A1
Can write simple isolated phrases and sentences.
Table 2.3:
General writing skill descriptions in the CEFR (p.
61 in /citeCouncilofEu-
rope2001)
2.1.2
CEFR: French
Although the CEFR is designed to be applicable across all languages, often more language-
specific definitions are produced,
and this is the case for some French levels,
notably:
A1 [10], A2 [11], B1[12] and B2 [13].
The official certification of French is the sequence
of four ‘Diplˆome d’´etudes en langue fran¸caise’
(DELF) exams for levels A1 to B2 and
two ‘Diplˆome approfondi en langue fran¸caise’ (DALF) exams for levels C1 and C2.
Each
year over 400,000 people sit these exams across 173 countries[14].
Matching the CEFR,
the exam has four sections;
reading,
writing,
listening and speaking.
The writing re-
quirements of
the exams are shown in Table 2.4,
each writing exam has one or two
tasks.
Clearly the DELF written exams are less academic and more expressive-skills focussed,
while the higher level DALF exams involve a mix of reading, analytical and argumenta-
tive skills.
The diversity of tasks and time constraints involved show that any automated
assessment must be highly flexible in those respects.
2.2
Grammar
While the CEFR provides a wide range of criterion,
they are designed for human use
and few are easily implementable.
The remainder of this chapter will
thus be devoted
to more specific ‘usage’ measures, starting with Grammar.
9
A1
30mins
Fill out a form and write a postcard or message
on a day-to-day topic
A2
45mins
2 brief letters or messages,
describing an expe-
rience in your life, and writing an invitation, in-
formal apology etc.
B1
45 mins
Express a personal
opinion on a general
theme
(essay, mail, article
B2
1hr
Argumented opinion piece (debate,
formal
let-
ter, critical review)
C1
2hr30min
2 parts:
Review and analysis of a group of doc-
uments (1000 words), and an essay in either the
humanities or science (candidate’s choice)
C2
3hr30min
Article,
editorial,
report in response to a group
of documents (2000 words)
Table 2.4:
Division du corpus Google N-Grammes en 10 groupes de fr´equence ´egale
Studies of learner corpora generally show a large number of general differences between
L1 and L2 productions which occur across all L2 languages.
Compared to L1 produced
writing, L2 shows [15]
• shorter sentences and clauses
• high rates of incomplete or syntactically non-standard sentences
• fewer modifying and descriptive prepositional phrases
• a higher rate of misused prepositions
• less subordination and two to three times more coordination
• fewer passive constructions
• fewer syntactic modifiers
• significantly higher rates of personal pronouns, and lower rates of impersonal/ref-
erential pronouns
• fewer adverbial modifiers and adverbial clauses
• less subject-verb agreement
• less use of verb inflection
• morphologically more incorrect word forms
• more incomplete/incorrect subordinate clauses (for example missing subjects, verbs,
clause subordinators)
10
• over- and under-use of connectors
• incorrect or omitted articles
• incorrect modal verb usage
In order to underdstand how this affects writing assessment, often studies examine the
relationship between certain features and the final mark.
Such studies of L2 essay scoring
have shown that scores are higher when (see references in [16])
• more words with more letters or syllables
• more subordination
• more instances of passive voice
• more nominalisations
• more prepositions
• more pronouns
• fewer present tense forms
2.2.1
Grammatical stages in L2 French
Bartning and Schlyter [17]
analysed the language of
14 Swedish learners of
French,
7
learning as a Foreign language and 7 as a Second language,
and arrived at a set of
6
syntactic stages of development.
These correspond loosely (though not exactly) with the
CEFR levels.
Each level is described by a large number of traits which can be used to
recognise the grammatical aptitude of a language learner.
Some of the traits are unique
to second language learners and are not seen in first language acquisition.
The stages and indices relevant to written production are shown in table 2.5,
where
percentages should be interpretted as only rough guides.
There were few differences
between the Second Language and Foreign Language learners.
As perhaps is to be
expected, those who learnt as a Second Language acquired phonological rules earlier such
as elision, while those who learnt as a Foreign Language acquired advanced grammatical
forms such as the future simple and subjunctive marginally earlier on.
They also found
evidence of the sequence NUO → IUO → FUO in sentence organisation, that is Nominal
Utterance Form (without a verb) followed by Infinitive Utterance Form (with a verb but
in a form that shows no opposition between person/number) followed by Finite Utterance
Form (with a verb that is fully inflected).
Criteria
Stage 1 (
˜
A1)
Stage 2 (
˜
A2)
Stage 3 (
˜
B1)
Stage 4 (
˜
B2)
Stage 5 (
˜
C1)
Stage 6 (
˜
C2)
learning time
1-5 months
4-9 months
8-13 months
12-24 months
3 years
> 3 years
verb opp.
1
st
, 2
nd
, 3
rd
person
-
10-20%
50%
majority
all
+
verb finite opposition
50-75%
70-80%
80-90%
90-98%
100%
+
cop.
opp.
of 1
st
, 2
nd
, 3
rd
pers.
fixed exp.
some
some errors
+
+
+
nous + -ons
-
70-80%
80-95%
>95%
+
+
3rd person plural
-
3
rd
pers.
sing.
some correct
50%
some problems
+
future (aller + inf / simple)
-
aller + inf
some f.
simp.
f.
simp.
f.
simp.
prod
+
conditional
fixed exp.
fixed exp.
fixed exp.
some
prod.
+
past tense marked
0-10%
10-40%
40-60%
60-90%
90-100%
+
imparfait
-
isolated
some
some
PC/Imp
opp.
prod.
modals
all verbs
plus-que-parfait
-
-
-
uncertain use
+
+
subjonctif
-
-
isolated
some
prod.
+
negation
Neg + all
(ne) V (pas)
V rien, jamais
(ne) V pas
personne/rien
ne V ...
+
object pronouns
-
SVO
S(v)oV
SovV
productive
y, en
Article-Noun genre agr.
55-75%
60-80%
65-85%
70-90%
75-95%
90-100%
Article-Adj.-Noun genre agr.
15%
25%
50%
70%
80%
90%
P+Det amalgams
-
au l’, de le, `a le
au l’, de le, `a le
du/des/aux
+
+
subordinating complexity
parataxis
qui
isol.,
quand,
parce
que
qui/que?
variation, si
dont,
rel.
au-
ton., g´erondif
g´erondif,
com-
plex amal.
Sentence Organisation
NUO,
IUO,
FUO
IUO/FUO
FUO
+
+
+
Table 2.5:
(slightly simplified) Grammatical stages of Bartning and Schlyter [17] opp.
= opposition, - = none observed, + = native-like, cop.
=
copula,
exp.
= expressions,
pb.
= problems,
pers.
= person,
sing.
= singular,
f.
simp.
= future simple,
prod.
= productive,
(x) = x is optional,
agr.
= agreement, isol.
= isolated, amal.
= amalgams
12
Generally,
a learner’s control
of
the Tense-Mode-Aspect system in French is a strong
indicator of
level
as is the case in other romance languages,
Pronouns also follow a
standard sequence, progressing from usages like ‘je vois lui’ with the pronoun after the
verb, through ‘j’ai le vu’ with the pronoun before but still attached to the verb through
to a more orthodox usage.
The introduction of more advanced pronouns (‘en’ and ‘y’)
occurs last.
While the study is very pertinent to automatic scoring, in the early stages
in particular care must be taken to distinguish fixed expressions from composed ones,
for example the use of
‘je voudrais’
does not necessarily indicate a knowledge of
the
conditional.
Further, it should be noted that this study was speech-based so in written
work we would expect more conscious editing and correction by learners of
their own
productions.
2.2.2
Grammatical stages in L1 French
In L1 acquisition,
at a young age,
Mean Length of
Utterance (MLU) is used to esti-
mate syntactic complexity, however after around the age of 3, it becomes a weaker and
weaker indication of complexity.
To evaluate children’s syntactic complexity there are
two main measures, one is the Index of Productive Syntax (IPSyn) [18] which (for En-
glish) includes 56 measures related to Noun groups, Adjectives, Verb groups and Phrase
Structure.
The other is the Developmental
Sentence Scoring (DSS) [19]
which has 8
categories (Indefinite pronouns, noun modifiers, personal pronouns, primary verbs, sec-
ondary verbs, negations, conjunctions, inversions interrogatives and adverbial questions)
and points for each morpheme or structure within each category.
Points are summed
and averaged per sentence over 50 sentences, with a bonus point added for a grammati-
cally correct sentence.
Studies suggest that DSS behaves consistently over a wider range
of
ages than IPSyn (for a review see [20]),
and so recently a version of
the DSS has
been developed for French[20]
called the Morphosyntactic Complexity Scale (MSCS).
The scoring system of MSCS is shown in Table 2.6.
Although some elements are more
difficult to detect automatically than others,
counts of
most can be implemented to
some degree, of particular interest to this project is the hierarchy of markedness of each
element which generally seems to correspond to L2 progression.
2.3
Vocabulary
As with other elements,
measurement of vocabulary in free writing can be difficult as
“students will
avoid words which are difficult in meaning,
in pronunciation,
or in use,
preferring words which can be generalized” [21].
Longitudinal
studies reflect this,
for
example a study of vocabulary used by Cypriot learners of French showed only a very
Score
Articles
Personal
and
Impersonal
Pro-
nouns
Prepositions
and
Adverbs
Verb Tenses
Clause Types
Relations
1
moi
Preposition
of
possession, pour
Imperative
Imperative,
Affirmative
declarative
or
Interroga-
tive based on intonation
2
un, une
toi, je, tu, il
Adverb of place
Interrogative
with
inter-
rogative
word
without
subject-verb
order
inver-
sion
Coordination
(except
of
cause or result)
3
le, la
elle,
vous,
me,
le,
la
Prepositions
of
place,
avec
(comitative)
Present
infinitve,
Present
4
des, les
nous, on
Compound
past,
periphrastic
future
5
lui,
eux,
ils,
elles,
les,
te,
soi,
se,
leur, en, y
avec
(instrumen-
tal)
Negative declarative
Relative
clause,
Noun
clause,
Adverbial
clause
or
coordination of
cause,
Adverbial
clause
or
coor-
dination of result
6
Adverb of time
Past infinitve
Adverbial clause (except of
cause, result or time)
7
Simple
future,
Imperfect
Interrogative with subject-
verb order inversion
8
Preposition
of
time
Conditional,
Other tenses
Adverbial clause of time
9
Passive
Table 2.6:
The points system for the Morphosyntactic Complexity Scale[20]
14
gradual
and non-statistically significant increase in less-frequent words with language
level
[6].
Instead,
distributions have been shown to vary much more by the domain-
specific context than over time.
In comparative studies between L1 and L2 produced writing, L2 shows [15]
• less lexical variety and sophistication
• fewer idiomatic and collocational expressions
• a smaller lexical density and lexical specificity
• more frequent non-standard vocabulary uses
• more repetition of content words and more often
• an avoidance of paraphrasing, use of simple paraphrases or use of referential pro-
nouns (this, that, it)
• use of shorter words (with fewer syllables)
• use of more emotive and private verbs (believe, feel, think)
• use of
fewer epistemic modifiers (apparently,
perhaps),
and more conversational
hedges (sort of, in a way)
• use of fewer downtoners (almost, hardly)
• use of more lexical softening devices (maybe)
2.3.1
Vocabulary Metrics
Statistical
vocabulary studies have a long history in language acquisition studies.
In
attempting to measure vocabulary, one should be clear exactly what is meant by it, in
particular is the measure based on word-forms, lemmas (stripped of flexional morphol-
ogy) or word-families (stripped of derivational
morphology).
Productive vocabulary is
usually far smaller than Receptive vocabulary, so the degree to which words are known
must also be specified.
The idea of ‘vocabulary depth’ covers this point, it is a measure
of how well a person knows the words within their vocabulary.
For language learners an
additional
question is whether one should include or remove from consideration words
which overlap from a learner’s L1 (positive transfer).
For automatic measurements,
it
can be difficult to distinguish homonyms and recognise mutiword units, so this can bias
scores.
15
A range of tools have been developed to measure the vocabulary level of children learn-
ing their L1,
these have naturally been extended to L2 vocabularly measurement.
In
particular vocabulary is often split into two variables:
lexical diversity and lexical sophis-
tication/richness.
The former considers only the words a speaker uses themselves and
is concerned with the variety therein,
while the latter compares produced words with
an index representing their difficulty (usually defined by frequency of
a large corpus
of
the language).
Measures of
lexical
diversity and lexical
sophistication usually have
a limited accuracy due to Zipf’s law.
Common words will
dominate any text sample,
while infrequent words which are more informative make up only a tiny sub-sample and
exposed to high statistical variation.
For lexical diversity, the basis of most measures is the Type:Token Ration (TTR), cal-
culated as follows:
T T R =
Number of distinguishable tokens in the text
Number of tokens in the text
This can be varied by replacing tokens with lemmas or word-family roots.
The main
difficulty with T T R is that as the length of the text increases,
for each new word the
probability that it is unique will
decrease,
so the T T R will
slowly tend towards zero
(also know as Heap’s law).
T T R also in effect reduces words to either first-appearances
or repetitions and does not include the distribution of frequencies of words beyond this
dichotomy[22].
To overcome this,
a wide variety of
solutions have been proposed(see
[23] for a review), one is the Mean Segmental TTR (MSTTR) which segments the text
into equal word sections and calculates the average T T R over them.
Segment size must
be large enough to show meaningful
variation but must be significantly smaller than
the text length to obtain an accurate measure.
Another is ‘D’
(as implemented in the
VOCD program[24]).
This is based on a fit to the TTR data of children based on the
following formula:
T T R =
D
N

(1 + 2
N
D
)
1/2
− 1

where N is the sample length and D the parameter to fit.
Optimal
results have been
found when the fit is performed on 100 random samples of
35 to 50 tokens and then
averaged.
However the theoretical basis of D has been questioned, it may not be more
discriminate than simply in effect averaging TTR for any arbitrary number of
tokens
between 35 and 50 and an alternative [25],
it can also be better measured with HD-D,
this takes a sample of fixed length and calculating the sum of the probabilities of seeing
each word in the sample:
HD =
X
wordformsi
(1 − HyperGeom(0, K, n, N )
16
where K is the sample size,
N is the whole text length,
n the number of
times the
wordform i is observed in the whole text and HyperGeom is the HyperGeometric dis-
tribution.
To match D measurements, a sample length of 42 is used in practice (midway
between the 35 and 50 of
vocd).
In the same article,
14 existing measures of
lexical
diversity (including D) are shown to correlate with text length.
However a critique of
this analysis is that better writers write longer texts and so a good measure of lexical
diversity will also correlate with text length when a mix of writer skill levels is involved.
The least correlated are K[26], M aas and M T LD.
K is calculated as follows:
K =
M
2
− M
1
M
2
1
where M
1
is the number of word forms, and
M
2
=
X
wordforms
n
2
wordform
where n
wordform
is the number of times the wordform appears in the text.
M aas uses
logarithms to normalise for length:
Maas =
log(N ) − log
(
V )
log
2
(N )
where N is the number of
tokens and V the number of
types.
Finally M T LD sets a
T T R limit (usually 0.71) and averages the number of tokens required in a segment for
the T T R to drop beloe the limit.
With respect to the question of
lemmas or word-
forms, for French and other inflected languages, most commonly lemmas are used which
provides better predictors and much more consistent results when comparing between
languages[27].
In the same study,
HD-D and D were found to be the best predictors
of
an independent French ability score,
however they are correlated with text length
whereas MTLD is to a much lesser extent, oddly though Maas seemed the most effective
when sample-size variation was controlled for.
An extention of D is to repeat the calculation with the rare words excluded, the change
in diveristy is the Rare Word Diversity (RWD). This has been criticized experimentally
though as not correlating with language level.
For Lexical sophistication, the standard
approach is to bin the frequency distribution into a Lexical Frequency Profile (LFP)[28].
For example bins of 0-1000, 10001-2000, ‘academic words’, and a fourth group of all other
words.
This profile has a correlation with writing scores, but not as strong as originally
claimed [29].
Nowadays, binning is done is 20 bins of 1000 words each [30].
To convert a LFP into a single variable P Lex [31]
takes samples of
10 words in the
text and calculates the distribution of
the number of
rare words (that is the number
17
of words with rank above some limit) per segment.
The resulting distribution is fit by
a Poisson distribution to give a λ value (typically in the range 0-4.5).
Frequency in
a corpus is not a great measure of
word difficulty though,
using Teacher Rankings of
word difficult can significantly improve measures[32], however, currently no generic lists
have been prepared.
The binning step involves a large loss in information[33][34], so an
alternative which lessens this stage is ‘S’[35].
To compute S, the cumulative percentage
C(x) of the text covered by words of a frequency ranking x is calculated, the resulting
data points of many subsamples are fit with the following function derived from Zipf’s
law:
C(x) =
log(x)
log(S)
× 100
to obtain a value of S.
Generally S is in the range of 2000 to 6000 and so is easy to
interpret as a vocabulary ‘size’.
Tests also suggest that S is reliable for smaller text
samples than other measures (as low as 200 words).
Other proposed variables are those of evenness[22] which can be measured by the stan-
dard deviation of the number of tokens per type, though more elaborate calculations are
possible[36],
of dispersion which considers how evenly words are distributed in a text,
and of disparity,
a concept taken from species diversity in biology,
which evaluates the
semantic similarity of words (for example by comparing LSA values).
2.4
Spelling
Another way in which learner texts are different from native texts is in spelling er-
rors.
Language uses (regardless of
whether L1 or L2) tend to use two processes in
spelling, phonological and visual (the dual route model).
The former involves connect-
ing phonemes to graphemes, while the latter involves bypassing this process and forming
a direct link to the word as a whole (for example in the word ‘yacht’ (/jOt/).
Both pro-
cesses are used by all
language speakers,
generally though common words will
tend to
be stored visually even if they are more or less phonetically transcribed [37].
Most likely
differences in strategy distributions across words lead to differences in L2 and L1 errors.
Specifically for L2[38] 3 effects dominate:
1.
there is a wide diversity of error types.
2.
there is a larger number of errors than in L1 users
3.
a large proportion of errors result from a lack of knowledge of the target language
Most spelling error detectors, however, are designed for native speaker errors [39].
18
For L2 users,
the most common mistakes are mis-combining morphological
structures
and confusing different words with similar spellings.
Cook[40] found differences in rates
of vowel errors compared to consonant errors in L2 and L1 writing.
Studies also suggest
that L1 affects types of
spelling mistakes made.
Japanese learners of
English [41],
Spanish learners of
English [42]
French,
Spanish and German learners of
English [38]
although [40]
found that errors of
higher level
students are generally similar to those
made by children in L1.
An analysis by Bestgen and Granger[38] grouped L2 errors into 9 categories and found
that word joining and splitting errors account for 23% of errors in English,
not tradi-
tionally treated in L1 spelling correctors, mistakes between single and double consonants
were also common in English.
With uncategorised error quantities, text scoring could be
predicted purely from spelling with an R
2
of 0.34 and 0.43 if error classes were used so
spelling is a significant correlate of writing quality.
Further, sssessing a text with a large
number of spelling errors automatically can be very difficult.
For example, attempting to
parse a text with spelling errors can provoke serious problems if a word of one category
is interpreted as a word of
another.
Likewise,
semantic similarity measurements can
suffer when the intended word from the speaker does not match that which is analysed.
2.5
Cohesion and Coherence
Argumentative writing in L2 can differ significantly from L1 productions in a large
diversity of ways, Hinkel([15] p.527) cites 22 of them, including:
• shorter texts, less support for arguments, less counterarguments
• organisation and rhetorical structure differ possibly leading to vagueness and lack
of clarity of purpose in L2 texts
• use discourse markers in non-standard ways or over-use thereof, repetition instead
of paraphrase
In automatic measurements, Latent Semantic Analysis (LSA [43]) can be used to com-
pare adjacent sections of text, or one section of the text versus the text as a whole.
Studies using LSA on learner essays found that while level correlated with measures of
cohesion,
however that surprisingly,
scores of such essays from human-graders were in
fact negatively correlated[44].
This may be explained by the “Knowledge telling” versus
“Knowledge extraction” theory of writing[45].
Knowledge telling involves starting with
the concepts in the question as simple prompts to write a sentence, and then repeating
19
but with the concepts from both the question and all previous sentences, while knowledge
extraction presents a much less linear/iterative and more tree-like,
pre-planned and
argumentative structure.
In the former case local
semantic changes will
be small
and
thus the text will appear cohesive to inter-sentence LSA comparison, while for the latter
there will
be sequences of
consistency interconnected by breaks and highlighted with
discourse connectors.
One approach is to use ‘situation models’[46].
If a dimension of
the situation model:
causation, intentionality, time, space and protagonists is disfluent,
the relevant connective should be used to aid coherence.
Whether the jump is significant
enough to warrant inclusion depends on the reader.
Instructive texts may leave them
out to challenge the reader to make connections, and a surprising result is that advanced
reader will
learn more from a text with less cohesion,
though the opposite is true for
low-level readers[47].
For French language learners, a lack of options for connectors can
lead to overuse of simple connectives such as ’parce que’
and ’mais’
([48]),
and use of
connectors has been shown to change with level [7].
Chapter 3
Corpora
3.1
Learner Corpora
Learner corpora are a relatively new phenomenon and have only been collected since the
1980s([49]).
When compared with native corpora, it should be remembered that learner
corpora are inherently much less authentic, they are almost always produced in artificial
classroom,
examination or homework style situations.
A learner corpus is defined by
two main variables, namely the learner and the nature of the task as shown in table 3.1.
Learner Information
Task Information
Learning Context
Time Limit
L1
Use of reference tools
Other L2s
Exam
Learner Level
Audience/Interlocuteur
Table 3.1:
Learner Corpus attributes [49]
The task can significantly affect the vocabulary expected in a native corpus, for example
the frequencies of Prepositional phrases,
relative clauses and participle clauses all vary
by an order of magnitude depending on the genre of writing.
Learner corpora are usually
subject to Contrastive Interlanguage Analysis, that is comparisons either with natives,
or between learners.
In comparisons with natives,
errors may be signs of
attempting
more difficult constructions rather than inherent weaknesses.
When comparing between
learners, both longitudinal and transverse studies offer insight on language development,
acquisition order and L1 transfer.
Note that a summary of the Learner corpora used in
this report is shown in Table 3.6.
20
21
3.1.1
CEFLE
The CEFLE corpus was constructed as a written complement to the existing oral Inter-
Fra corpus.
It contains about 200 texts written by Swedish high-school students (aged
15-19) of
French and a further 60 texts written by a control
group of
native French
high-school students.
The corpus can be divided into transversal and longitudinal sec-
tions.
The transversal section has students of 4 levels plus the control group.
Each level
has 30 texts except for the lowest level
which has only 16.
This shortage comes from
students of
this level
commonly being incapable of
writing a sufficiently long text to
reliably analyse.
The longitudinal
section follows 15 students through the school
year,
and contains the students’
responses to 4 questions at an interval
spacing of
about 2
months.
Figure 3.1:
the ‘voyage en italie’ task
The transversal task is a narrative description of a comic, as shown in Figure Figure 3.1.
The other prompts may be found in Appendix A. The Swedish students were in their 1st
to 5th year of studying French, and the group includes students studying under different
teachers in 5 different high schools.
The control
group includes mostly monolingual
French speakers,
however there are also a small
number of
bilinguals.
The Swedish
texts were all typed by the students directly into a computer - in a program without a
spell/grammar-checker,
however,
the control
group texts were written by hand by the
students and then transcribed by the experimenters due to facility constraints.
Students
22
were given 45-55 minutes to compose their answer.
For easier comparison with InterFra,
the tasks are relatively free and mainly communicativley focussed.
The transverse tasks
can be grouped in two pairs - personal
tasks ‘Moi,
ma famille et mes amis’
along with
‘Un souvenir de voyage’
and 3rd person narratives ’L’homme sur l’ˆıle’
along with ’Le
voyage en Italie’.
The Swedish students had been studying English since the age of 9
and many were studying a fourth language (for example German or Spanish).
The level of each student is based on their response to the ’voyage en italie’ task.
The
students’
school
years are not used as they are an unreliable measure,
rather the cri-
teria of Bartning and Schlyter are used (Section 2.2.1).
The highest two levels of the
six described in their system were not observed among the students,
the levels 1 to 4
are labelled as A to D respectively.
For convenience,
the pseudonym assigned to each
student begins with their assigned level
and the control
group are assigned level
‘E’,
with pseudonyms according to the same rule.
Level
Num textes
Year of French study
Text length
(words)
Level
Test
(out of 60)
A
16
1st, 2nd, 4th
130
12.2
B
30
1st, 2nd, 4th
202
21.4
C
30
2nd, 4th, 5th
276
37.5
D
30
4th, 5th, 6th
406
51.5
E (Native)
30
-
335
-
Table 3.2:
Properties of texts in the transverse corpus (from [50]) the native speakers
wrote shorter texts than group D, this may reflect the natives writing 2 exercises straight
after each other (i.e.
fatigue) or the fact that the most advanced students will
tend
to be more interested and motivated by the activity than an average French student
writing a simple task in their own language.
The absence of
Year 3 reflects the way
language options must have been chosen in the Swedish system by the time a student
reaches high school.
The corpus also exists in an automatically annotated xml form.
This form has a ‘words’
element as its root then
• segmentation into ‘word’s (but not sentences) where a word can be a multi-word-
expression (mwe)
• each word is assigned a class, the class can contain the POS and morphosyntactic
traits, or it may be ‘ambiguous’ or ‘unknown’
The annotation is of a lesser quality than that of MElt(see Section 5.1.3), so we do not
use this version.
23
3.1.2
HELLAS-CHY-FLE corpora
The CHY FLE corpora [51] involve 3 diverse corpora collected by Freiderikos Valetopou-
los in Greece, Cyprus and Poitiers, France.
As they is currently under construction, these
details reflect the current state rather than that of the completed version.
3.1.2.1
CHY FLE
This corpus [52] contains 27 texts composed by Greek-Cypriot students at the University
of Cyprus in 2010 and 2011.
The students were in their 1st or 4th year (aged between 18
and 20 years old), and following a course on Phonology as a part of their French studies.
There is thus some spread in French-speaking skill:
from B1 to C1,
this is,
however,
largely independent of
the university year.
The students had two tasks to complete
in their own time (thus dictionaries and other language aids were permitted),
the first
‘Strategies’ was as follows:
“Quelles sont les strat´egies que vous utilisez afin d’am´eliorer vos comp´etences orales et
´ecrites en fran¸cais.
Justifiez vos choix.”
1
The second task ‘Auto-observation’
was completed after an oral
production task.
The
students were asked to analyse their own language skills and the errors they made during
the task.
The corpus includes 7 responses to the strategies task and 20 to the Auto-
observation task, with 5 students completing both tasks.
3.1.2.2
HELLAS FLE
This corpus [52]
contains some 96 texts written for B2 level
(38 texts) and C1 level
(58 texts) state certification exams in Greece.
Candidates were high school
students,
aged 14-17 and may or may not have attained the certificate which they attempted.
Each candidate answered 2 Activities, the first of which was to write an email or forum
response to an online post.
For B2,
this was advice to a student thinking of
quiting
school,
and for C1,
an opinion of
the role of
fashion to young people.
The second
activity was,
after reading a text about a new service in Greek,
to write a facebook
or blog post passing on the information to Francophones in Greece.
For B2,
this was
a Greek language school,
for C1,
a carpooling service.
At B2 level,
65 minutes was
provided for the 2 answers,
and 90 minutes at C1 level.
The questions can be seen in
Appendix B
1
translation:
What are the strategies you use to improve your written and oral skill in French.
Justify
your choices.
24
3.1.2.3
CENTRE FLE
This corpus[7] contains 211 texts from placement tests of incoming students to the Uni-
versity of Poitiers in the years from 2004 to 2008.
The level for each test corresponds to
the class assigned after marking.
Students were given 1h30mins to complete 3 activities:
1.
Presentation and/or dialog of
2 people from an image provided to the students
(the image varies from year to year)
2.
Creative writing task to describe the people from the same activity,
designed for
using the past tense
3.
An essay on a question related to the image.
for example:
1.
Pr´esentez ‘Samia’ et ‘Damien’ (ˆage, profession, etc.) :
environ 50 mots
2
2.
Racontez comment ils se sont rencontr´es :
environ 80 mots (utilisez les temps du
pass´e)
3
3.
A votre avis,
les diff´erences culturelles dans un couple sont-elles plutˆot des avan-
tages ou des inconv´enients?
4
The number of texts per level is shown in Table 3.3
CFLE level
CEFR level
Acitivity 1
Activity 2
Activity 3
4
B1.2
20
20
20
5
B2
28
29
27
6
C1
19
22
20
7
-
2
3
1
Table 3.3:
Number of textes for each level in Centre FLE corpus (ref personal comm)
3.1.2.4
Pre-Treatment
Because the CHY FLE corpora are still
in development,
the files therein required a
significant amount of normalisation, this pretreatment is also used for any new files:
• filenames were standardised to utf-8,
spaces replaced by underscores,
brackets
removed, accented characters replaced by unaccented equivalents.
2
translation:
Present ‘Samia’ and ‘Damien’ (age, profession etc.) :
around 50 words
3
translation:
Tell the story of how they met :
around 80 words (use the past tenses)
4
In your opinion, are cultural differences more advantageous or disadvantageous in a couple?
25
• filenames not in standard form were corrected.
• double versions of files with different names were removed.
• file encodings were standardised to utf-8, for some this required establishing char-
acter translations as the true encoding was unable to be estimated.
• unusual
characters were removed or replaced with simpler equivalents,
including
unusual apostrophe types
• underscores were replaced with commas (they are residuals of a transcription code)
• sequences of a variable number of repeated ‘x’ (possibly with some letters attached)
represent words that the transcribers could not recognise,
were replaced by ‘xxx’
of a fixed length (without changing attached letters).
• unrecognised (due to encoding mixes) phonetic symbols wee replaced by the se-
quence ‘γ γ γ’, this is chosen so as to be interpreted as a foreign language sequence
by SxPipe (See Section 5.1.2).
• some long lines (having more than 990 characters) had been cut into two,
some-
times in the middle of a word.
These were reconnected.
Since sentences cut across
lines are not treated as linked by SxPipe, all lines which are long (> 120characters),
not ending in a .?!
and with a following line that begins with a lower-case letter
have the following line joined to them (with a space).
3.1.3
FipsOrtho corpus
As a part of
the FipsOrtho spell
checker (see Section 4.2.4) evaluation,
a corpus of
errors in learner-produced texts was collected and manually annotated via an online,
interactive version of the spell checker.
The corpus contains 362 entries, 1065 sentences and 14494 words (with 40.0 words per
entry on average and 13.6 words per sentence on average).
In the entries are 2468
tagged errors and corrections of a wide variety of types ranging from spelling through
grammatical.
The errors are tagged using the typography shown in Table 3.4 Each
corpus entry includes both the FipsOrtho corrected version and an Expert’s corrected
version.
Corrections are exclusively local,
with no large scale movements of
words or
replacements of more than three consecutive words.
The corpus is taken from a wide diversity of sources [53]:
• examples provides by two teachers of
French from written works of
Jamaican,
Australian and Canadian students.
26
• examples taken from published CALL articles
• sentences taken from a range of tests for spell checkers
• provided via a native speaker’s email
• sentences taken from the FRIDA corpus written by advanced level students
• sentences provided by uses of the online FipsOrtho application
3.1.3.1
Pre-treatment
Unfortunately the online version of
the corpus exhibits several
degradations meaning
that nearly every entry required some form of normalisation, these were as follows:
• parts of the web page were saved in different encodings so these were normalised
• the original form of many of the longer entries had been cut after 255 characters,
so the original
text was reconstructed from the corrected version of the text and
the list of error to correction translations.
• many original and corrected versions had become mixed, meaning corrected words
could be sometimes found in the original
version and non-corrected words in the
corrected version.
These mixings were found by a script which checked for incon-
sistencies between original, corrected and error-annotated versions.
• punctuation errors are not uniformly added to corrected versions,
some of these
were corrected, however undoubtedly some remain
• words with an error tagged but no correction had the corresponding correction
added
• spelling errors in the corrected form were repaired
• some further corrections were added in the corrected sentences that resisted pars-
ing,
in particular commas to separate subordinate clauses and a range of sugges-
tions to about 20 entries from French native speaker Timoth´ee Bernard.
3.2
Native Corpora
3.2.1
Corpus “Litt´eracie avanc´ee”
This corpus [55][56]
is composed from the works of French university students ranging
from first year Bachelor’s to final year Master’s students and mainly in linguistics, French
27
Designation
Code
Description
Example
Insertion
INS
Superfluous character
cherval→cheval
Omission
OMI
Missing character
abre→arbre
Substitution
SUB
Neighbour key on keyboard
progrqme→programme
Inversion
INV
agneda→agenda
Lexical
LEX
Existing inappropriate word
fonds→fond´e
Phonogrammatical
PHG
Non-existing word but correct
pronunciation
fon´etique→phon´etique
Phonetical
PHO
Non-existing word and incor-
rect pronun ciation
londi→lundi,
macasin→magasin
(quasi-)Homophone
HPO
Existing inappropriate word
pr´emisses→pr´emices,
est→et
Morphological
MOR
Morphological error on conju-
gation,
word formation,
plu-
rals etc.
rapident→rapides
Non-functional
char-
acters
LNF
Non-pronounced characters in
word for historical and etymo-
logical reasons
toujour→toujours
Agreement
AGR
les enfants sage
Complementation
CPL
J’attends sur Anne
Auxiliary
AUX
les invit´es sont dans´e
Tense
TPS
Verbal tense
jou´e→a jou´e
Mode
MOD
Je veux que tu viens
Missing word
MAN
colonie
tabac
Case
CAS
Incorrect upper/lower case
Fran¸
cais
(language)
→fran¸cais
Punctuation
PNC
Diacritics
DIA
accents
meme→mˆeme
Proper
noun/adjec-
tive
NPR
Existing,
correct
word,
un-
known by the lexicon
Plymouth
Unknown noun/ver-
b/adjective/adverb
INC
Existing,
correct
word,
un-
known by the lexicon
dravidien
Superfluous word
SUP
Redundancy
les pˆecheurs entre avec
les Am´eridiens
Separation by space
SPC
Missing or superfluous space
fauxsauniers→faux
sauniers
Separation by other
sign
SEP
sinstaller→s’installer
Borrowing
EMP
Borrowing
from
mother
tongue
trade→commerce
Noisy proposal
BRU
False detection
Inadequate upper case
Word order
ORD
arrives-tu→tu arrives
Table 3.4:
Typography of language learner errors used in FipsOrtho, from [54].
Note
that many of these pass beyond spelling errors, and that this is not consistent with the
FreeText typography.
as a foreign language teaching,
and education.
The writing is essentially formal
and
includes reports,
assignments,
extracts from theses and cover letters.
There are 11
sub-corpora, each containing at least 10 documents with the same theme and question
28
written by students at a similar stage in their studies.
The details of the sub-corpora
are shown in Table 3.5
Writing Style
level
degree
texts
words/text
Report
2
nd
yr.
B.
Linguistics
10
2809
Theory section of In-
ternship Report
3
rd
yr.
B.
Education (French)
15
4677
Analysis
1
st
yr.
M.
Education (French)
69
1333
Research Report
1
st
yr.
M.
Education
25
10974
Article Review
2
nd
yr.
M.
Education
10
631
Article Review
2
nd
yr.
M.
Education
10
3570
Professional Report
1
st
& 2
nd
yr.
M.
Education (Litterature)
22
220
Cover Letter
1
st
& 2
nd
yr.
M.
Education (Litterature)
22
320
Theory
section
of
thesis
1
st
yr.
M.
Education (French)
10
3530
Thesis
1
st
& 2
nd
yr.
Education (French)
41
9645
Theory review
2
nd
yr.
M.
Education (French)
10
1568
Table 3.5:
Subcorpora of the “Litt´eracie avanc´ee” corpus, B. = Bachelor, M. = Master
Corpus
Sub-
Corpus
Task/Genre
Author
No.
No.
No.
Level
L1
Age
texts
sent.
words
CEFLE
transversal
Narrative
3p
A1-B2,
Native
Swedish,
French
15-19
136
3461
39395
longitudinal
Narrative
1p, 3p
A1-B2
Swedish
15-19
59
1156
11100
HELLAS,
CHY,
FLE
CHY FLE
report
B1-C1
Greek-
Cypriot
uni.
27
2025
33180
HELLAS
FLE
email/blog
B2, C1
Greek
uni.
96
1215
17829
CENTRE
FLE
descriptive,
essay
B1-C1
Var.
uni.
211
3082
38025
FipsOrtho
-
Various
Var.
Var.
Var.
362
999
13927
Table 3.6:
Summary of Corpora used (Var.
= various, uni.
= university age)
Chapter 4
Language Evaluation Tools
In this chapter we consider the range of tools developed for language assessment both
for English (L1 and L2) and for French (L2).
4.1
Automatic Essay Assessment / Computer-Aided Scor-
ing in English
Automated scoring of
essays[57]
is attractive not only because it can save time for
educators and provide more frequent and detailed feedback to learners, but also because
it evaluates learners in more authenic situations than for example multiple choice quizzes
or other automated tests.
Williamson cites a large range of benefits [58], including:
• Objectivity, Reproducibility and Consistency:
avoiding the subjective variation of
human raters.
• Tractability, Specification and Granularity:
scores are decomposed into well-defined
and well-analysed components which can be tracked, monitored and used to pro-
vide specific feedback.
• Efficiency:
feedback time is faster, and time-wise automatic scoring is often more
cost-effective
The field was born as early as 1960 with PEG (the Project Essay Grader) [59].
However,
early systems were reliant on surface features and the transfer from written to electronic
form and so automatic graders did not become common until
the late nineties when
personal
computers had become common input devices and computational
power per-
mitted a deeper analysis.
Since then, the increasing use has created a positive feedback
29
30
cycle,
thus accelerating development.
Increasing use of
such tools has also met with
skepticism,
with critics stating concerns about their reliability,
use of surface features,
potential
inability to deal
with creativity,
and susceptibility to test-taking strategies
(‘gaming the system’).
Even to the extent that an online petition[60] against their use
has received thousands of signatures of respected academics including Noam Chomsky.
The basics of
a system relies on extracting proxy metrics for presumed latent essay
quality variables,
for example for a fixed task,
often the length of an essay is strongly
correlated with its score [61].
Multiple regression analysis then estimates appropriate
weights for these proxy variables.
Modern versions include syntactic and semantic anal-
yses.
The range of current systems available is quite diverse[62] Current Systems include:
• the modern version of PEG (now owned by Measurement Incorporated) [63][64],
• Intelligent Essay Assessor (IEA)[65], now part of Pearson[66] This includes Latent
Semantic Analysis (LSA)
1
trained on a corpus representative of the essay topic,
and thus can asses how semantically consistent the essay is, this are then compared
with a reference set of human-scored essays.
• Intellimetric[67]
from Vantage Learning[68].
This takes rated answers specific to
the question and learns over 400 features automatically from these,
rather than
having a generic set of features and learning only the weights.
In this way, it uses
a form of inductive rather than deductive reasoning.
The Graduate Management
Admissions Test (GMAT) uses Intellimetric in its scoring.
• E-Rater[69]
from Educational
Testing Services[70]
Is used as alongside a human
judge in the Graduate Management Admission Test (a second human judge ad-
judicating in the case of
a significant score discrepency) Includes a diversity of
NLP tools from Rhetorical Structure Theory (RST) analysis to bigram frequency
comparisons.
As with Intellimetric it can be trained on a specific question.
It has
a paid interface Criterion which provides both local
and global
feedback and an
annotated version of the original
text.
E-Rater is currently used for the TOEFL
English test.
• BETSY[71]
uses Measurement Decision Theory[72]
to establish a Bayesian Net-
work based on the training samples.
The network comes in two forms:
Bernoulli
1
LSA looks at all the (unordered) contexts of all words in a corpus and attempts to find a representa-
tion which maximally reduces dimensionality while minimally reducing information loss.
The contexts
are summed into word vectors and then weighted by the sum of individual word entropies.
A Singular
Value Decomposition (SVD) of the resulting matrix provides the LSA of the input corpus.
This then
can be used to compare the semantic similarities and differences between sentences and paragraphs for
example.
[43]
31
- in which the probabilities used are of features given the score, and Multinomial
- in which the probabilities used are of
the score given the features present[69].
This has the advantage of providing learnt but interpretable latent variables and
also being able to be used as easily on short answer responses as on longer ones.
Texts are analysed into words, bigrams and ‘arguments’ (skip bi-grams).
• Markit[73][74] from Blue Wren[75], it has the advantage of requiring only a single
(good quality) model
answer for training and of
being highly portable (it can
be run locally on any windows PC.
It works by mapping concepts to a standard
thesaurus, and shallow parsing.
• LightSide [76]
from TurnItIn[77]
is the only open-source platform available.
It is
a generalised NLP data extraction and classification tool.
It places token,
stem
and POS Ngram and ‘Stretchy pattern’
(the feature version of
skip ngrams)[78]
counters onto the Weka machine learning platform.
The feature selection is highly
configurable, as is the machine learning algorithm used.
• Bookette [79][62]
uses a neural
network approach on top of
a NLP base.
The
system is strongly determined by the specific essay prompt,
this gives very good
results but at the cost of flexibility.
• Lexile[80] has created measures for reading level[81] based on semantic (via word
frequency) and syntactic (sentence length) complexity and optimised using reading
comprehension testing scores.
This scoring system has been adapted to provide
a score on the same scale for writing ability[82].
It is the only system here that
scores independently from the task.
• CRASE[62] from Pacific Metrics[83] and AutoScore by the American Institutes for
Research (AIR) are two further platforms, however there is currently little public
information on their inner workings.
Automated scoring does however have two general problems, the first is that since inter-
human marking agreement can be quite variable,
it can be hard to say if
a system
is working well.
Another is that usually 100-500 marked essays are required to train
the system (depending on the task and the system).
The global problem for justifying
automatic scoring though is that to achieve computer-human agreement comparable to
human-human agreement,
one need only look at the length of
the essay[69]
to attain
about 90% of inter-human agreement levels.
So the current race is,
in some sense,
to
do better than human raters do, mainly by comparing with the average of many raters,
or by choosing high quality raters.
Alternative paths are to compare with independent
metrics such as multiple choice tests.
32
In order to improve research amongst the proprietary systems,
a recent competition
(the Automated Student Assessment Prize) with commercial and general public sections
was held between competing systems over a range of
formal
writing tasks [84].
The
results suggest a difference of between 5 and 20% between Human-Human agreement and
AES-Human agreement on exact scores and near perfect agreement if
adjacent scores
are included,
on the other hand,
quadratic weighted κ scores were very similar when
comparing inter-human agreement and human-computer agreement.
The public side of
the competition, held through the Kaggle website saw improvements on the commercial
scores, though more time was given to the competitors.
These positive results have been
called into question as being overly-reliant on word-count in their estimations[85], with
for one entrant,
it accounting for up to 85% of
the assigned score.
A response [86]
is
that in fact essay length (or rather its logarithm) is strongly correlated with essay score,
so that a successful automatic scorer will neccessarily have results correlated with it (in
particular when the task is fixed).
Despite this,
word count does play around a 10%
more significant role in automatic assessments than it does for human raters,
so there
is clearly room for systems to improve.
Systems have also been shown to fall
behind
human raters when the prompt is more open-ended[87].
Another question is whether
students actually improve their writing when they have access to such systems.
A recent
survey concluded that there is “modest evidence that AWE feedback has a positive effect
on the quality of the texts that students produce using AWE,
and that as yet there is
little evidence that the effects of AWE transfer to more general improvements in writing
proficiency”[88].
4.1.1
Automatic Essay Assessment in L2
The standard tools have all
been applied in L2 contexts.
An example is the Intelli-
gent Academic Discourse Evaluator (IADE) [89][90] is a tool designed to help language
learners improve their academic writing through automatic evaluation and feedback.
However,
objective research on the benefits is rare and suggests that well-structured
use of these tools may be the key to good learner outcomes[91].
Scoring is a function
of both second language ability and writing ability,
with the exact proportion of each
open to debate.
This adds a degree of
complexity above L1 applications.
Generally,
lower level
students will
see more of a language skill
focus,
while higher level
students
will have a more significant organisational native-like contribution to their scores.
In L2
cases in particular, constructive feedback is often more important than an actual score,
and in evaluation studies, the more detailed the feedback the more effective the learning
outcomes are [92].
A further distinguishing element of L2 writing is that tasks are often
designed to encourage the usage of
a particular range of
vocabulary and/or grammar
33
point, this means they often are not the standard 5-paragraph essay and may not easily
fit the writing assessment models used for native speaker tasks.
4.2
French-Specific Tools
Although French does not have the same range of tools as for English,
there are some
notable integrations of NLP in computer-assisted language learning.
In particular the
language profiler Direkt Profile,
the tutoring aid FreeText and the L2 specific spell-
checker FipsOrtho.
4.2.1
Direkt Profil
Direkt Profil [93] [94] (web location:
[95]) is a writing profiler which looks at a text and
estimates a grammatical level of development according to the Schlyter stages [17].
Di-
rekt Profil segments the text into words, then regroups any fixed expressions and assigns
morphosyntactic information to the words it can recognise.
A shallow parse identifies
non-recursive Noun and Verb groups.
The final stage of the analysis is to apply a deci-
sion tree of hand-written rules based on the information present in each word/group to
identify elements of writing indicative of the stage of development.
Elements without
any influence on the determination of the developmental stage are ignored.
In its anal-
ysis,
Direkt Profil
employs minimal
spelling correction - just accent changes and a list
of corrections to overgeneralised past participle constructions (‘prendu’ in place of ’pris’
for example).
When first applied to a subset of 25 texts from the CEFLE corpus,
the
analysis recognised above 80% of
sought structures for all
levels except for the lowest
where the recall dropped to 65% due to the difficulties of treating low-level texts.
To assign the developmental stage, a support vector machine module uses 25 features as
shown in Table 4.1 extracted from the text.
Based on the ‘voyage en italie’ (transverse)
subcorpus of
the CEFLE(see Section 3.1.1),
it arrived at high levels of
precision and
recall for distinguishing natives from non-natives and reasonable numbers for separating
coarse-grained levels between non-natives (results are reproduced here in Table 4.2.
Direkt Profil also comes with a web interface ([95]) as shown in Figure 4.1 which displays
its analysis via color-coded highlighting of the text and a hierarchy of observed features.
There are 5 high-level groupings:
Nominal group analysis, Verbal group analysis, Statis-
tics, Segmentation and Lexical Profile.
• The Nominal
groups are then subclassified according to their internal
structure
(Det N or Det Adj N etc.) and whether they contain an agreement error or not.
34
Sensitive to text length
∼ Independent of text length
TTR (Type:Token Ratio)
% inflected verbs
f(0-1000)
% inflected verbs with agreement
f(1001-3000)
% sentences wo a verb
f(3000+)
% lexical, present verbs with agreement
f(unknown)
% verbs in pass´e compos´e with agreement
#conjunctions
% participles with stem error
word count
% simple future ˆetre/avoir
#sentences
% lexical simple future verbs with agreement
#sentences w/o verb
% lexical conditional with agreement
#inflected + #uninflected verbs
mean sentence length
#inflected verbs
#verbs in pass´e compos´e
#modal aux.
+ infinitive
#verbs in imparfait
#ˆetre/avoir in imparfait
#lexical verbs in imparfait
#lexical verbs in imparfait with agreement
#verbs in simple future
#lexical verbs in simple future
#verbs in plus-que-parfait
#verbs in conditional
Table 4.1:
Features used to estimate learner stage from text [94], f(X-Y) is the number
of words used in the range of the Xth to Yth most frequently used words from a native
corpus
Stage
Precision
Recall
1-2
0.67
0.77
3-4
0.76
0.66
6
0.91
0.91
Table 4.2:
Precision and Recall
results for the DirektProfil
SVM fit when grouping
levels in pairs [94]
• The Verbal
group information is subgrouped into finiteness information,
accord
information, basic tenses and advanced tenses.
• Statistics contains text-level
information such as the number of
sentences,
fixed
expressions and so on.
• Segmentation contains information from the tagger on the distribution of parts-
of-speech in the text (for Direkt Profil treetagger is used).
• The Lexical
Profile category contains grouped word usage by corpus frequency
according to Lexique (see Section 5.3), either via the FRANText subcorpus or the
sub-titles subcorpus.
35
Figure 4.1:
Direkt Profil
interface with the Noun Group information selected and
highlighted for an example of a stage 2 learner’s text
4.2.2
FreeText
FreeText was an ambitious French education program for intermediate to advanced
learners.
It was developed with the goal
of
using NLP tools in combination with au-
thentic documents and free production exercises to enhance learning.
This included
grammar,
listening,
speaking,
reading and writing exercises split into 4 tutorials based
on 16 authentic documents.
From a writing perspective the FreeText project included
a multifaceted error analysis platform [96][97].
This contains three stages:
1.
a spell-checker (see Section 4.2.4)
2.
a syntactic checker
3.
a sentence comparison tool
The error analysis tool was developed with the FRIDA corpus[98] which includes 450,000
words of
which 300,000 have been analysed and manually tagged with error informa-
tion.
This was used to find the most common errors and prepare treatments for them.
The error analysis tool
(see Figure 4.2) is interactive,
and thus the student will
be
prompted with suggestions to correct errors at each stage before the following stage can
36
Figure 4.2:
Example of the sentence analyser in FreeText for the sentence ‘il mange
un pomme’, translation and text-to-speech elements not shown.
be launched.
It also has extenal
links to a dictionary,
a text-to-speech reader and a
conjugation dictionary to facilitate self-correction.
The syntactic checker uses the FIPS parser, which is based on the Generativist language
theory.
The parser performs a shallow chunk parse if
no full
analysis is found and
has two methods of
error analysis,
the first is constraint relaxation (for example of
agreement requirements) and the second a phonological
reinterpretation to deal
with
homophone errors.
As constraint relaxation can lead to an explosion of possible results,
a combination elements is used to limit the parse space, particularly:
• heuristic rules to eliminate unlikely analyses
• ranking measures based on word frequency
• ranking measures based on error type frequency
Should constraint relaxation fail,
the words at the boundaries of
the partially parsed
chunks are retrieved.
These are looked up in a phonological
dictionary and similar
words retrieved,
these possibilities are added to the parser chart and a re-analysis is
attempted.
A full list of syntactic error types treated is shown in table 4.3
Some errors will however require semantic knowledge to be noticed, for example ‘il a lu
ces livres’ may have been written instead of ‘il a lu ses livres’, or ‘je les ai vus’ instead
of ‘je les ai vues’.
In the FreeText project,
for short-answer questions,
a model answer
is provided.
From this a deep semantic structure can be obtained and compared with
37
Error Category
Acronym
Example
Homophone
HOM
Elles peuvent maˆıtre au monde des enfants.
Class
CLA
Ce montre le probl`eme.
Auxiliary
AUX
il a tomb´e.
Gender
GEN
Elle est beau.
Number
NBR
Les souris ont ´et´e mang´ee .
Person
PER
Je peut partir.
Voice
VOI
Il
r´evolte contre la soci´et´e.
Euphony-Elision
EUFE
ce aspect
Euphony-Contraction
EUFC
de le chat
Euphony-Analogic t
EUFT
`
A- il mang´e?
Adjective complementation
CPA
Il est capable `a faire cela.
Verb complementation
CPV
Il est tomb´e
la chaise.
Order
ORD
Ils ont les vus.
Adverb order
ORD AV
C’est lui plutˆot qui chasse
Adjective order
ORD AJ
un arabe pays.
Redundancy
RED
Il parle beaucoup des langues.
Missing punctuation
OUB
Peut
il partir?
Negation
NEG
C’
est pas un probl`eme.
Table 4.3:
Syntactic Errors searched for in FreeText from [96]
the student’s response.
Semantically this allows a search for problems such as with
the gender of pronouns across sentences,
discordances in lexical
meanings,
voice errors
and missing information.
FreeText does not include any semantic similarity measures
though, and this makes false positives likely.
As with Direkt Profil, output is color-coded
with information provided on the location and nature of any error found.
Early FreeText studies revealed a high degree of accuracy and a very user-friendly in-
terface were essential to successful take-up by students and teachers.
The syntax error
detector was checked with a subset of 120 spelling-corrected sentences of the FipsOrtho
corpus and compared with the detections of Word2008 by error type.
Of 74 errors,
54
were detected by FreeText and 26 by Word, the corpus revealed some errors which were
not searched for by FreeText such as a missing hyphen in ‘grand-m`ere’.
On the other
hand,
the FreeText detector also had more false positives than Word.
Freetext was
an important tool
in motivating the future directions of ICALL research,
in particular
the need for annotated learner corpora and L2-specific grammar and spelling correction
tools.
4.2.3
Auto-
´
Eval
Auto-
´
Eval[99] was a writing assistant for French with three levels of correction as shown
in Table 4.4
38
Spelling
Grammatical
Semantic
spelling errors
subject-verb agreement
improper term or expression
accents and hyphens
number agreement
homonym confusion
gender agreement
upper/lower case
past-participle agreement
word split in two
incorrect conjugation
use of numbers and abbreviations
punctuation
Table 4.4:
Auto-
´
Eval correction capabilities[99]
The first step of the analysis is tokenisation and (as far as possible) morphological dis-
ambiguation using the SATO analyser[100].
The text is then segmented into sentenced
and parsed.
The semantic analyser corrects agreement at a distance, for example ‘Une
professeure de dessin blond’ A supervising module regulates and controls the communi-
cation of information between the three sub-modules and an additional scoring module.
The output gave the number of mistakes in each category and a count of mistakes with
an upper limit threshold imposed.
The plan was to incorporate deeper analyses such as
LSA, however the project does not appear to have been continued.
4.2.4
FipsOrtho Spell Checker
The FipsOrtho Learner spell checker [54] emerged from the FreeText project (see Section
4.2.4), it has been designed to specifically deal with learner errors as opposed to generic
spell checkers.
Figure 4.3 shows the FipsOrtho interface.
The spell
checker is applied in the case when an initial
parse by the Fips parser has
failed.
Unrecognised words are assigned a lexical
category according to the parser’s
expectations.
A range of
processes are then attempted to correct the word,
each one
having various weights applied to each of its steps.
The final
output is an ordered list
from most likely to least likely correction.
The processes used search for the following
types of errors:
1.
missing apostrophe:
Learners often replace an apostrophe with a space or simply
leave it out (thus merging the two words into one).
FipsOrtho checks if replacing
a space before an unknown word with an apostrophe can make a known word.
If it can not,
it checks if the beginning of the unknown word is a character that
could be followed by an apostrophe (c, d, j, l, m, n, s and t), and if so, the spelling
checker applies all other checks on the remainder of the unknown word.
39
Figure 4.3:
FipsOrtho Interface (from [54]
2.
ad hoc:
this mostly involves rules for correcting over-regularisation in morphemic
paradigms (for example ‘voira’ for the future of ‘voir’, and also misapplications of
derivational suffixes such as ‘changeage’ instead of ‘changement’.
3.
alphacode:
this involves comparing words by their alphacode.
The code is cal-
culated by re-ordering the letters in a word with consonants first in alphabetical
order followed by vowels, diacritics are removed and upper case converted to lower,
only one copy of each letter type is retained.
The result is a large group of eas-
ily typed-as-each-other words which share the same alphacode.
In FipsOrtho a
weighted edit distance expands the range of similar alphacoded words.
To limit
combinatorial excess, the limit on the edit distance is small and linear in the length
of the word, and additionally, the initial letter of the two words must be the same.
40
4.
phonetisation:
the word is automatically translated to a phonetic form using a list
of about 700 rules.
This form is compared with those in a dictionary.
Allowing
common learner mistakes in phonomes beyond those of L1 speakers (for example
confusion of nasale vowels) helps this element detect a large number of errors.
5.
word separation:
the string is progressively cut in two at each possible location,
if
insertion of
a hyphen or an apostrophe at that point would be a word,
it is
suggested.
If insertion of a space would create two known words they are suggested
as such.
Capitalisation also is checked for the first letter of
the first word of
the sentence and
suggested if it is in lower case.
On test corpora, the alphacode and phonology error detection modules cover the highest
numbers of
errors,
there is a large amount of
overlap between the errors detected by
modules, and the types of errors detected match expectations, for example the alphacode
module is most effective for diacritics,
insertion and omission errors.
The FipsOrtho
corrector is no longer online and so could not be directly integrated into this project,
however, it serves as a useful reference point.
4.3
Other related tools
4.3.1
Coh-Metrix 3.0 and Coh-Metrix-TEA
Unlike the other tools here, Coh-Metrix[101][102] was designed to measure the cohesion
of reading texts but has since become a more general textual analyser (with the online
tool
Coh-Metrix-TEA[103]
as shown in Figure 4.4).
Coh-Matrix has been used in a
wide diversity of analyses including L2 essay writing [16].
The creators of Coh-Metrix
emphasise that ‘Cohesion’ is distinct from ‘Coherence’ in that the former represents the
connectedness of concepts within the text,
while the latter depends on the reader and
how well mental concepts are connected within them while reading the work.
In this way,
Coh-Metrix attempts to account for observed differences in text readability depending
on the level
of the reader.
Coh-metrix integrates a range of NLP tools including,
the
Medical
Research Council
Psycholinguistic database (for measures of
word difficulty),
wordnet,
the charniak parser,
a POS tagger,
and LSA.
As with the Lexiles grading
system[81], coh-metrix analyses text for reading and produced writing in the same way,
however, unlike Lexiles, it aims at providing a multidimensional analysis of text rather
than a single score.
Variables examined in Coh-metrix include:
41
Figure 4.4:
Analysis of a text by the cohmetrix Text-Easability-Analyser
• ratio of causal verbs (identified via wordnet) to causal particles (Causal cohesion)
• number of
connectives,
in five classes:
positive additive (‘moreover’),
negative
additive (‘but’), positive temporal (‘after’), negative temporal (‘until’) and causal
(‘because’)
• number of logical operators:
and/or/if ...
then
• overlap between sentences of four kinds:
stems in common, noun-stems in common,
common nouns in common and content words in common (Lexical overlap)
• semantic overlap via LSA between words sentences and paragraphs
• number of anaphoric references between sentences
• degree of
hyperonymy of
the words on a scale from 0 to 1 where 0 is highly
non-specific and 1 is extremely specific (calculated using a hierarchy built from
wordnet) .
• degree of polysemy of the words (via wordnet)
• a range of measures of Lexical Diversity
• logarithm word frequrency in the CELEX corpus
• word information according to the MRC psycholinguistic database of
English
words.
• mean number of words before the main verb
2
(a measure of Syntactic complexity)
• mean number of higher level constituents per word (also Syntactic complexity)
• similarity between syntactic stuctures used (Syntactic similarity)
2
for example,
in the sentence ‘the son of
the king went fishing’
there are 5 words before the main
verb ‘went’
42
Of these,
3 have been found to correlate with human judgements on L2 essay quality,
namely Lexcial Diversity, Mean number of words before the main verb, and the logarithm
of the word frequency in the English CELEX corpus.
Chapter 5
NLP tools
5.1
The ALPAGE treatment chain
5.1.1
Lefff
v.3.40
Lefff
[104]
is a large lexicon of
French built as an aide to syntactic parsers and mor-
phosyntactic taggers.
It was composed automatically from large corpora and then veri-
fied manually using a range of methods.
Lefff
is built around an intentional form, written
in the Alexina format[104].
The intentional form includes an entry for each lemma with
its category and a morphological class, along with pertinent syntactic function informa-
tion.
The intentional
form can be compiled into an extensional
form for direct use by
applying morphological
and syntactic function combination rules.
In this extensional
form,
there is an entry for each inflected word.
The entries contain information in-
cluding the lemma,
syntactosemantic information such as tense,
number,
gender etc.
along with syntactically detailed information pertinent for parsing.
It includes 536,375
entries covering 110,477 lemmas,
and has been shown to improve parsing and tagging
when compared to use with alternative lexicons ([104]).
Frequency information is not
included in Lefff, which can influence the parser since it is blind to lexical distributions
and thus will consider ‘aller’ to be a verb or a noun with equal probability.
5.1.2
SX Pipe
For pretreatment of the texts we use the SxPipe 2 [105] tool.
The tool is modular and
parametrised, and its range of treatments includes:
• Sentence segmentation
43
44
• Tokenisation
• Named Entity Recognition (including URLs, addresses, etc.)
• Composed Word / Amalgam detection
• Spelling Correction
• Context-Free pattern recogniser (e.g.
for the implicit ‘il’)
• Foreign word recognition
SxPipe pays careful
attention to distinguish word-forms from tokens,
and is able to
find tokens which include multiple word forms (amalgams), along with composed words
which include multiple tokens, and even combinations of the two.
The output of Sxpipe
retains any ambiguities present, and is thus a Directed Acyclic Graph (DAG) rather than
a simple sequence.
The script dag2udag can be used to show a more human-readable
version of it as a sequence of finite-state transitions.
Given the large numbers of spelling errors made by language learners, of particular inter-
est to this project is the spelling corrector.
For SxPipe, this is primarily rule-based but
augmented with Minimum Edit Distance (MED) calculations.
The rules are quite varied
and range from general elements such as the exchange of upper and lower case letters, or
the addition/replacement of diacritics, to the exchange of characters closely positioned
on the AZERTY keyboard and the correction of
accidentally inserted or not-included
spaces between words.
Each rule has both initial and secondary weights associated with
it.
The corrector uses the Lefff (see Section 5.1.1) lexicon to construct an automate of
possible transitions using the rules’ associated weights (The initial weight is used for a
first change,
the secondary for a non-initial
change).
The automate then proposes the
nearest known words possible given a total
weight limit which is a configurable option
of SxPipe.
The foreign-word recognition system is relatively weak in SxPipe (to avoid
false positives),
and often requires a sequence of
at least three tokens to posit an in-
terpretation as a foreign expression.
It relies on a mix of looking for a list of common
foreign words from the languages most likely to be seen in French and also a search for
non-French letter characters.
The sxpipe spelling corrector is only launched in the case
of an unknown word (in Lefff ) being found,
so mistakes leading to existing words will
not be detected and may lead to parsing problems later on.
45
5.1.3
MElt
The Maximum-Entropy Lexicon-enriched tagger (MElt) [106]
(available at [107]) is a
part-of-speech (POS) tagger developed for French and other languages.
It uses a Maxi-
mum Entropy Markov Model as a basis, this has the advantages of being easy to build,
fast to train and of
producing state-of-the art results (as compared with Conditional
Random Fields for example).
The Markov Model includes features of two types:
lexical
and context.
Lexical features of the the word in question include its prefixes and suffixes,
along with binary features on whether it includes numbers, hyphens, upper-case letters
and so on.
Context features include the tag of the previous word,
the tag sequence of
the two previous words and the word-forms with a 5-word context window.
This Model
is then augmented this with features derived from the morphosyntactic elements of the
lexicon Lefff.
The derived features cover the possible tagset of the word of interest and
also of the words in its context.
These additional features have proven particularly useful
in tag estimation of words unseen in the training sample.
The tagger uses a tag-set of
28 tags [108]
as shown in Table 5.1,
and achieves 97.75% accuracy on the French Tree
Bank [109]
- an annotated corpus of
the newspaper Le Monde.
This training sample
is used for the default operation of MElt.
Since the tagging is statistically based,
it is
also possible to output the probability (interpretable as a degree of confidence) in the
assignment of each tag in a sentence.
Tag
Description
Tag
Description
V
Indicative Verb
CLS
Subject Clitic
VIMP
Imperative Verb
CLO
Object Clitic
VINF
Infinitive Verb
CLR
Reflexive Clitic
VS
Subjunctive Verb
P
Preposition
VPP
Past Participle
P+D
Amalgam (Prep.
+ Determiner)
VPR
Present Participle
P+PRO
Amalgam (Prep.
+ Pronoun)
NPP
Propper Noun
I
Interjection
NC
Common Noun
PONCT
Punctuation
CS
Subordinating Conjunction
ET
Foreign Word
CC
Co-ordinating Conjunction
PROWH
Interrogative Pronoun
ADJWH
Interrogative Adjective
PROREL
Relative Pronoun
ADJ
Adjective
PRO
other Pronoun
ADVWH
Interrogative Adverb
DETWH
Interrogative Determinant
ADV
Adverb
DET
other Determinant
Table 5.1:
Tagset used by MElt[108]
5.1.4
FRMG parser
The FRench MetaGrammar (FRMG) based deep parser[110]
is a rule-driven parser
that aims primarily at a broad coverage of
French texts.
With the tool
Dyalog,
the
46
metagrammar can be compiled into a Tree-Adjoining Grammar (TAG) and from this
an efficient parser constructed.
A metagrammar([111]) is essentially an abstraction of commonality amongst TAG prim-
itive trees.
Since TAGs are normally lexicalised, and each word can often have a range
of constructions, the number of primitive trees in a TAG can rapidly become too large
to manage manually, meta-grammars help avoid this combinatorial explosion of trees to
describe, and also facilitate comparison between grammars.
A benefit of FRMG is that
it contains only 368 basic trees[112],
much less than other TAG based grammars,
this
being mainly due to efficient abstraction and compilation.
In construction of the TAG
from the metagrammar,
these basic trees are grown iteratively via a set of constraint-
limited hierarchical
rule classes to form the TAG’s primitive trees.
Classes represent
different kind of syntactic operations, for example ‘adjective POS’ or ‘modifier’.
When
fully ‘defactorised’ these expand to well over 2 million trees for the lexicon of Lefff, the
number dominated by verb-anchored elements.
The most common basic tree (no.
110
- the canonical verb construction) is used in some 700,000 of these.
It should be noted
that FRMG is not fully lexicalised,
it contains a small
number of
unanchored trees,
most often used to account for incisions.
Since TAG parsing is of
O(n
6
) in sentence
length,
the Dyalog parser compiler looks for any trees which can be parsed via a local
Tree-Insertion-Grammar (TIG) and exploits this in parsing.
Such TIG-parsable trees
make up the vast majority of
the grammar,
an example of
an exception being com-
paratives such as ‘plus fort que ...’
which require a non-Context-Free grammar to be
described, and are thus inaccessible to TIG operations.
To improve parsing speed, sta-
tistical left-corner processing is included and this significantly reduces the parse search
space.
FRMG is incorporated into the Alpage treatment chain,
and thus uses sxpipe to form
a word-form DAG from a sentence as its entry.
The DAG is parsed using a tabular
approach with transitions labelled using information from Lefff,
this is later stored as
a hypertag.
In the case of
a successful
parse,
a shared derived forest is the result.
This expresses the primitive trees, substitutions, adjunctions and anchorings used in the
forest’s derivations.
If so desired by the user, this can be disambiguated to leave a single
derivation, this is achieved by placing corpus-optimised weights on each derived step, so
as to calculate the most likely tree weight-wise.
The derived forest can be automatically
converted into a derivation (dependency) forest with appropriate labels on each of the
dependency links being automatically added.
For the convenience of the user, the final
form can be output in EASy,
PASSAGE,
CONLL or DEPXML (Dependency XML)
schemas,
the last of which is used in this report.
Also,
when possible,
lexically empty
trees in the output can be merged with their parents using the ‘transform’ option.
47
Figure 5.1:
“il mange une pomme” as analysed by FRMG (detailed version)
Figure 5.2:
“il mange une pomme” as analysed by FRMG (visually appealing version)
A sample dependency tree is shown for the phrase “il mange une pomme” in Figure5.1
(with a more visually-oriented version in Figure 5.2 For each cluster (box) there is the
information EXFY |Z where X is the sentence number and Y is the token number, and
Z is the associated token.
Below that is the range of
tokens covered by the cluster
between < and >
Inside a cluster are one or more nodes (oval) with the lemma,
assigned POS and the
relevant tree number.
Multiple tokens may occur in one node (for example in a composed
word such as ’pomme de terre’) or multiple nodes may be associated with the same token
(for example with amalgams or unanchored nodes).
Between nodes, dependencies are indicated by arrows color-coded by the TAG operation
used to combine the trees:
Substitution in light blue,
Adjunction in red rouge with a
dashed line,
co-anchorage or lexical
dependencies in purple,
skip in green
1
.
These are
1
a skip dependency is one which plays no role
48
labelled with the nature of
the dependency and the weights associated with them in
calculating the most likely disambiguated phrase.
The Dependency XML includes a list of clusters with the same information as for the
diagram form:
<? xml
version ="1.0"
encoding =" ISO -8859 -1" ? >
< dependencies
id =" E1 "
mode = " full " >
< cluster
id =" E1c_4_4 "
left =" 4"
right = "4"
token = " "
lex = ""/>
< cluster
id =" E1c_3_4 "
left =" 3"
right = "4"
token = " pomme "
lex =" E1F4 | pomme " />
< cluster
id =" E1c_2_3 "
left =" 2"
right = "3"
token = " une "
lex =" E1F3 | une "/ >
.
.
.
This is followed by the list of nodes, each node contains information on the constraints
used to form its TAG tree.
Usually verbs contain a very large number of such constraints,
while other nodes contain very few:
< node
deriv = " d1 "
xcat =" end "
id = " E1n5 "
cat =" end "
tree = " end "
lemma = " "
lemmaid =""
cluster =" E1c_4_4 "
form = ""/ >
< node
deriv = " d2 "
xcat ="S"
id =" E1n6 "
cat ="S"
tree = " 34
empty_spunct
shallow_auxiliary "
lemma =""
lemmaid = " "
cluster =" E1c_4_4 "
form ="" />
< node
xcat = " "
id =" E1n1 "
cat = " cln "
tree =" lexical "
lemma =" cln "
lemmaid = " cln "
cluster =" E1c_0_1 "
form = " il " />
< node
deriv = " d5 "
xcat ="S"
id =" E1n2 "
cat ="v"
tree = " 364
V1VMod : agreement
arg0 : caimp : agreement
clsubj : agreement
clsubj_alt : agreement
clsubj_il : agreement
clsubj_ilimp : agreement
arg0 : ilimp : agreement
arg0 : imp_subj_alt : agreement
ante : clitic_sequence
post : clitic_sequence
clitics
arg1 : collect_real_arg
arg2 : collect_real_arg
arg0 : collect_real_subject
arg1 : real_group_comp
arg2 : real_group_comp
ncpred : real_group_comp
arg0 : PP : true_subject
arg0 : cl : true_subject
arg0 : noun : true_subject
arg0 : post_PP : true_subject
arg0 : post_noun : true_subject
arg0 : post_s : true_subject
arg0 : post_v : true_subject
arg0 : s: true_subject
arg0 : v: true_subject
v_with_subcat
Infl : verb_agreement
V: verb_agreement
v: verb_agreement
V1 : verb_agreement_ancestor
arg1 : verb_argument_other
arg2 : verb_argument_other
arg0 : verb_argument_subject
verb_canonical
verb_categorization_active "
lemma =" manger "
lemmaid = " manger "
cluster = " E1c_1_2 "
form = " mange "/>
.
.
.
Dependencies are listed as edges, they contain information on their assigned weights and
weights’ decomposition:
< edge
id =" E1e1 "
source =" E1n6 "
target = " E1n5 "
type = " subst "
label =" Punct "
w = " -5 "
ws ="[- RANK1
:
-5] " >
49
< deriv
names = " d2 "
source_op = " E1o4 "
target_op = " E1o5 "
span =" 4
4 " />
</ edge >
< edge
id =" E1e2 "
source =" E1n4 "
target = " E1n3 "
type = " subst "
label =" det "
w=" 1705 "
ws ="[- RANK1
:
-5 ,+ SUBST
:
10 ,+ CLOSED_det
:
1700] " >
< deriv
names = " d4 "
source_op = " E1o2 "
target_op = " E1o3 "
span =" 2
3 " />
</ edge >
.
.
.
In addition to what has been shown here, traits such as number, gender and human/non-
human associated with intermediate trees are provided along with the hypertag infor-
mation.
Corpus
#Phrases
LAS
δ(error)
δ(%)
FTB Test
1235
87.49
89.01
90.25
Europar
561
87.97
-0.5
-3.8
Annodis
529
86.11
+1.4
+11.0
Emea-fr Dev
574
85.16
+2.3
+18.6
Emea-fr Test
544
84.67
+2.8
+22.5
FrWiki
996
83.53
+4.0
+31.7
Table 5.2:
% parsed on diverse Corpora [113] showing the difference with respect to
the French Tree Bank test set using the Labelled Attachment Score (LAS) metric (the
percentage of tokens with the correctly predicted head and dependency relation)
FRMG has been tested across a wide range of L1 corpora and shows consistent results
as shown in Table 5.2.
As a broad coverage parser,
FRMG can handle elements such
as verbs of valence up to three (including the subject), valence in nouns, adjectives and
other parts of speech.
Further, it can extract arguments of verbs in relative constructions,
interrogatives, and in cleaved/topicalised sentence constructions.
Should FRMG fail
to successfully parse a phrase,
the most common reasons are due
to unusual characters or foreign words, non-standard punctuation (for example using a
comma at the start of an incision but none at the end),
and ellipsis particularly with
co-ordination and rare syntactic phenomena.
In the case of a failed parse,
FRMG can
also attempt local
corrections (see table 5.3) by loosening constraints (for example if
the subject and main verb do not agree) or by replacing words in the case where a
past participle has been written where an infinitive would normally,
or by inserting
words such as a missing auxiliary in a pass´e compos´e form.
If,
even after corrections
have been attempted, no successful parse can be reached, FRMG also includes a robust
parsing option.
The robust option attempts a shallow parse by looping over all possible
substrings in the search for possible S,
NP,
AdjP,
PP and subordinating conjunction
groups.
A maximal
span of
words is then taken from the found groups.
Although
FRMG is a large coverage parser,
there are elements of human language which appear
to require grammars beyond the mildly context sensitive, for example ‘un livre dont vous
50
ˆetes le h´eros’ or displaced relative clauses.
In such cases FRMG may give an incorrect
dependency tree or may only give a robust analysis.
flag label
description
example
N2 agreement1
Determiner
+ Noun
agree-
ment with change to Noun
(rarely invoked)
N2 agreement2
Determiner
+ Noun
agree-
ment
with change to Deter-
miner
‘la chien aboie’
N2 agreement3
Numeric Determiner + Noun
agreement
(rarely invoked)
subject-verb agreement
will modify the verb
‘il courent’
subject-participle agreement
with ˆetre in composed tenses
‘ils sont parti’
optional args
lets a verb with an obligatory
argument
be
accepted with-
out it,
or with an alternative
prepositional phrase
‘il
regorge avec de l’or’
or ‘il affiche’
subject-auxilliary agreement
gender and number agreement
of an auxiliary and its subject
‘elle avons dormi’
auxilliary-infinitive
use of an infinitive in place of
an auxilliary
‘je l’ai chercher’
modal-participle
use of participle instead of in-
finitive after a modal
‘je vais all´e’
advneg-participle
use of
a participle instead of
infinitive after a negative ad-
verb
‘...
pour ne pas cherch´e’
adjective agreement
an adjective
after
an auxil-
liary head
‘le
tr`es
petite
chat
mange’
lightverb*
covers gaps in Lefff of expres-
sions
which have a semanti-
cally light verb followed by a
noun without a determiner
‘j’ai sommeil’
Table 5.3:
Corrections attempted by frmg in order of priority from top to bottom,
note that lightverb is in effect an in-place extention of Lefff rather than a correction of
a writer’s error.
It should be noted that frmg covers a large range of syntactic and lexical
possibilities
and thus many abnormal
sentences can be parsed without referral
to corrections.
In
frmg noun-noun agreement is not checked since it is inconsistently applied in a large
number of cases in natural French, which leads to the following sentences being parsed
without difficulty:
‘la belle chien aboie’,
‘ils sont amoureuse’,
‘je suis aller’
and ’il
a un chapeau verte’
in
which ‘belle’,
‘amoureuse’,
‘aller’
and ‘verte’
are respectively interpreted as nouns.
In
cases where an adjective is interpreted as an attached noun, this makes almost no change
to the resulting derivation tree and thus such cases do not pose a serious problem.
to the verb.
51
5.2
Gensim / Word2Vec
Gensim [114] is a highly optimised python implementation of the Word2Vec [115] neural
network models.
Before describing Word2Vec, we present a short introduction to Neural
Networks.
5.2.1
Review of Neural Networks and Word-Embeddings
A standard feed-forward neural network is composed of a sequence of layers, each layer
contains a set number of
’artificial
neurons’.
An artificial
neuron itself
can be any
function of
N input variables that maps to a single output variable,
however for the
purposes of training it is best if the activation function is continuous, differentiable and
monotonic increasing or decreasing.
Common functions used are the sigmoid, tanh and
the rectifier functions.
A layer of M neurons will then have the same N input variables
to each of its neurons and M distinct outputs.
If a discrete output is required from a
layer, often the softmax function is used.
Softmax is in effect a sigmoid function for each
neuron but with the maximum of the output values adjusted to 1 and all others set to
0.
In principle a single non-linear layer can represent any function, however in practice
multiple layers can represent complex functions much more efficiently.
As a result, ‘deep’
neural networks involving multiple non-linear layers are sometimes used.
To train such
a network,
a back propogation algorithm is used.
This involves a repeated application
of the chain rule to propagate error derivatives backwards from layer to layer.
A feedforward network involves no cycles:
the output information of a neuron can only
progress forward in the network.
In a Recurrent Neural
Network (RNN),
one layer’s
inputs at time t include values not just from the outputs of the previous layer but also
from the t − 1 outputs of the layer itself.
In other words,
a cyclical
layer.
A recurrent
network thus takes a sequence of
inputs and is able to,
via its cyclical
layer,
retain a
form of short-term memory.
A weakness of such networks, however, is that the recycled
information will
tend to either amplify or (more often) diminish itself
itself,
thus its
gradient will
usually be small,
and back propagation will
rapidly lead to a negligible
effect at a distance greater than one or two points in time.
This is called the problem
of ’vanishing gradient’ [116], [117].
To resolve this issue,
one successful
approach is ‘Long Short-Term Memory’
(LSTM)
networks.
An LSTM network has a recurrent layer,
however rather than cycled con-
tinuously, the values at a particular time are locked in place until a certain criterion is
reached, at which point they are changed to a new value and again locked in place.
The
LSTM memory cell thus maintains constant error propagation rather than expanding or
52
diminishing (as in a standard RNN). Each LSTM memory cell thus has 3 elements:
an
‘in-gate’ which determines if the input is significant enough to keep,
a memory neuron
which retains information,
and an ‘out-gate’
which determines whether to output the
value or not.
All
three elements have variable weights and are included in the train-
ing.
LSTM recurrent networks can have greater accuracy and require a smaller training
sample than feedforward networks.
5.2.1.1
Word embeddings
Word2Vec uses a neural
network to estimate ‘word embeddings’,
that is:
reduced di-
mensional
vector space representations of words,
based on their context.
Word2vec is
thus a rival to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).
It has been shown to preserve linear regularities better than the former and does not
have the computational demands of the latter when considering large datasets.
If
the size of
the vocabulary is V ,
the desired dimensionality of
the word-embedding
representation is D,
and the number of words in a context is N ,
then a basic network
to calculate word-embeddings is as follows:
• Input Layer - ‘one-hot’ (or ’1-of-V’) coding,
that is,
each word in the vocabulary
is represented by a vector of
length with all
zeros except for a 1 in the element
corresponding to the word.
With N words in the context, this layer has size V ×N ,
i.e.
V neurons for each word in the context.
• Projection Layer - a linear projection of the Input Layer onto D × N outputs
• Hidden Layer - a non-linear layer with the same number of inputs as the projection
layer (i.e.
D × N) and H outputs
• Output Layer -
in the simplest form,
takes a softmax of
the H outputs of
the
Hidden Layer to V outputs,
the result is thus a prediction of a single word from the network given a context.
However,
the large number of variables involved (N × D + N × D × H + H × V ) makes training
of this network difficult in practice.
5.2.2
Word2Vec
Word2Vec addresses this problem of computational complexity.
Firstly, the simple out-
put form is improved via a Huffman binary tree,
this reduces training time by making
53
it faster when finding the softmax of common words.
The result is a reduction in the
complexity of the output layer from V to log
2
(V ).
Figure 5.3:
The architecture for CBOW and skip-grams compared, figure from [118]
Word2Vec also reduces the complexity of
the network via two different models:
Con-
tinuous Bag-of-Words (CBOW) and Continuous Skip-gram 5.3.
In both versions the
Hidden layer is removed compared to the simple network of
the previous section.
In
word2vec,
the context of a word is defined as the C words,
before it and the C words
after it,
ignoring non-lexical words in both cases.
During training,
for each occurrence
of a word-context combination,
a random sub-context is chosen of R words before and
R words after the main word with R being an integer and following a linear distribution
0 ≤ R ≤ C.
The subcontext is used to reduce the influence of words farther from the
main word.
In the CBOW form, the order of the input words is ignored and the vectors of all words
in the sub-context are averaged into a single vector.
The Skip-gram form is in some sense
the reverse of
CBOW:
The input word is used to predict the first word from its sub-
context, this is then repeated for each word in the sub-context.
While CBOW averages
the context within the network, skip-gram averages the context through training.
The result in both cases is a network that resembles an autoencoder, except that it maps
a single-word representation of its context to a word or vice-versa rather than a word
to itself.
Optimal results on English corpora have been found for a context of 4 words
before and 4 words after the word of interest.
The simplicity of the system is its strength,
as unlike other neural
networks,
it does not involve dense matrix multiplications and
can thus be achieved quickly.
When comparing CBOW and skip-gram models, CBOW
54
works well on common words and with large amounts of data while skip-grams is slower
to train, but more accurate with rare words and small training samples.
[119] A natural
extension word2vec is to use multi-word phrases rather than words (e.g.
”Boston Globe”
is not well
composed from ”Boston” + ”Globe”).
This can be achieved by extracting
sequences of words which are more frequent than the component words themselves are
individually [120].
Famously,
Word2vec is able to detect more than syntactic regularities,
for example,
vector(“King”) - vector(“Man”) + vector(“Woman”) = vector(“Queen”),
and similar
neural networks have been successfully applied to a wide diversity of problems [121].
In
particular,
word2vec embeddings have been used in place of one-hot vectors in LSTM
recurrent networks with an improvement in perplexity over the systems individually
[118].
5.3
Lexique
Lexique 3.8 [122]
is a large lexical
database of French.
It includes 135000 word forms
corresponding to 55000 lemmas.
For each word form there are 35 pieces of information
provided, ranging from phonological and morphological to distributional.
Distributional
information is based on the more modern texts in the FranText corpus,
web page dis-
tributions and a large corpus of film subtitles.
In this project,
the features of interest
were:
• phonological form of a word This is estimated via the generalised rules of a text-
to-speech engine combined with corrections to improve specific cases and general-
isations [123],[124]
• number of syllables in a word This is calculated from the phonological form using
an algorithm based on a CVCV structure [125], [124] combined with a large number
of context-specific rules (ref)
• flexional
and derivational
morphology of
a word.
This is estimated using the
program D´erif [126],[127]
Chapter 6
Results
6.1
Errors Corpus
The Corpus contains 362 entries,
with (via SxPipe segmetation) 999 sentences in the
original
forms and 1010 sentences in the corrected forms.
For each entry there was at
most one sentence difference and this occured in 17 entries,
with the exception of one
entry whose segmentation bugged due to a feature of SxPipe:
In finding an apostrophe
before a word,
but without a closing apostrophe,
it put the whole entry into a single
sentence instead of 6.
Comparing our parsing ability, we can see that the corrections mostly improve parsability
as we might expect as shown in Table 6.1.
The 32 manually corrected sentences which
could not be parsed are a mix of bad segmentations, long phrases with multiple clausal
incisions (often in a list) and sentences for which local corrections have been made but
the sentence as a whole is agrammatical.
corrected version
ok
corrected
robust
N
m.e.s.
N
m.e.s.
N
m.e.s.
original
ok
666.0
1.5
3.0
4.7
0
-
corrected
110
3.7
2.0
1.5
0
-
robust
191
3.8
1.0
9
26.0
3.8
Table 6.1:
Parsing success rates for sentences in their original
and correct formats
in the FipsOrtho corpus.
‘ok’
means a full
parse,
‘corrected’
means some corrections
were made,
‘robust’
means a full
parse failed and a partial
parsing was performed.
If
a sentence is split into multiple parts these are weighted so that the total
number of
sentences matches the original count.
‘m.e.p.’
indicates the mean number of annotated
error occurances per sentence
If we consider the corrections made by the parser (in both the robust and corrected cases
there may be multiple corrections per sentence), then we have 297 corrections of which
55
56
145 (49%) correspond to a tagged error of some kind in the same location.
A manual
analysis of the 145 which correspond to annotated errors found 86 which were appropri-
ate (this includes cases in which there were multiple errors and frmg found at least one of
them), their distribution is shown in Table 6.2.
N2 agreement errors dominate, and these
are gender and number in determinants (le/la/les,
ce/cette,
notre/nos etc.).
Subject-
verb agreement errors were well
corrected as were auxilliary-infinitive errors where an
auxilliary is used as an infinitive or vice-versa (for example ‘je suis aller’) 59 corrections
were inappropriate to the situation, these were dominated by ‘light-verb’ (26), ‘optional
arguments’ (10) and ‘N2 agreement2’ (10).
They usually involve missing/supplementary
words - particularly in determiners (for example the use of ‘de’ instead of ‘du’ and vice-
versa), lexical mistakes and spelling errors which were not picked up by sxpipe, however
there were around 15 agreement errors which were misanalysed.
Based on this study,
removing the ‘light-verb’
and ‘optional
arguments’
corrections when applying frmg to
L2 texts should improve significantly the precision of
the correction mechanism.
The
analysis also revealed some inconsistencies in error tagging,
in particular the choice of
whether a misuse of ‘de + determinant’ corresponded to a LEX or AGR error is some-
what arbitrary
main error tag(s)
MOR
AGR
other
frmg correction
aux.
inf.
7
1
0
subject-verb agreement
2
12
2
N2 agreement2
-
47
-
subject-aux.
agreement
1
3
-
adjective agreement
-
8
-
modal participle
3
-
-
Table 6.2:
Parsing errors and correction distribution for appropriate errors
Figure 6.1 shows the distribution of errors which frmg missed,
similar erros have been
grouped, Spell is the spelling errors OMI, SUB, INV, PHG, PHO, LNF, DIA, SPC, SEP,
EMP and HPO, Verbal includes the verb-related errors of MOR, TPS, MOD, TMP and
AUX and Syntax covers CPL, MAN, ORD, SUP of missing/inserted/misordered words.
One error is taken from each set,
with the primary being (loosely) the first to cause a
problem for the analysis :
Spell Syntax LEX AGR V erbal.
PNC punctuation errors are
usually isolated.
Non-errors (from a frmg point of view) include NPR (proper nouns),
CAS (upper/lower case),
BRU (Spell-checker noise) and INC (unknown words).
The
most surprising result is the number of
‘AGR’
errors which are missed.
It should be
noted that the annotation is quite inconsistent so many ‘AGR’ errors do not seem to be
agreement,
for example there are verb tense and mixing-up ‘que’
and ‘qui’
marked as
AGR Taking a sample of 50 unseen AGR errors, gives the breakdown shown in Table6.3
57
Figure 6.1:
Errors missed by frmg
Description
Example
N
Proper Noun agreement
Ath`enes ´etait trop chaud
3
Errors with word or in its immedi-
ate group (missing words,
supple-
mentary words, spelling errors)
...
qui viennent sinstaller ...
(with-
out apostrophe)
11
co-ordination and ellipsis
...
la Chappelle de Michalangelo, in-
teressant.
4
parsed
successfully,
often
with
wrong POS
Mais n’oublies pas de mettre un jean
...
2
‘que’ /‘qui’ confusion
...
l’anglais qui je peux enseigner ...
6
Noun requres change
...
de nos jour ...
6
Other errors in sentence do not al-
low or overrule correction in search-
ing for a parse
Premier
surprise
il
y
a
femmes
que se trouvent en charge executifs,
quelques femmes ont
´et´e President
comme en Bolivie “Lidia Gueiler”.
18
Table 6.3:
Missed AGR errors analysis
Notably, many errors are found or not looked for due to the presence of other errors in
the sentence.
This also suggests that a rule in the sxpipe DAG which offers ‘qui’
as a
remplacement for ‘que’ could improve parsability, along with the introduction of a local
grammar-checker and more elaborate spell-checker.
For the verbal errors group (see Table 6.4), a survey of 50 examples shows around half
(22) are tense (TPS) related, and of these all but three involve substitutions of one tense
58
Description
Example
ignored
corrected
but
dis-
carded
uncorrected
parse failed
TPS: tense confusion
Quand
j’´etais
petite,
j’ai beaucoup aim´e ...
19
1
2
MOR: spelling errors
...
qu’ils soyent ...
4
0
0
MOR:
participle
ad-
jective/infinitive
con-
fusion
la cr`eme glac´ee empoi-
soner
0
4
2
AUX:
auxilliary
ˆetre/avoir
confu-
sion/absent
Le
temps
devenu
gla¸cant.
0
3
1
LEX, MOD, SUP: lex-
ical
difficulties
or
use
of s’
instead of passive
etc.
Le projet
...
s’exprime
par
7
0
3
other
(mistagged
er-
ror)
Asfalt´ees
0
0
1
Table 6.4:
Missed Verbal error analysis
use for another and are thus ignored by the parser (even in the case of a use of ‘si’ with
the future tense instead of the present).
MOR errors in the group make up 11 of the
group,
those which involve spelling mistakes to unknown words are recovered well
(5
cases) though not always to the correct tense (for example the use of ‘veniras’
instead
of a subjunctive viennes is replaced by ‘viendras’ in sxpipe), other MOR errors involve
the writing of finite forms when an infinitive is expected, these are usually corrected in
the parser but then discarded when a full
parse is not reached,
the notable exception
being when a complementiser (e.g.
`a) must be inserted before the infinitive as the parser
does not look for such errors.
AUX errors are seen 4 times in this group, these involve
either mistaken auxiliaries (ˆetre instead of avoir or vice-versa) and the use of se and the
passive.
Table 6.5 shows corrections recuperated at the end of the analysis and their distribution
amongst the most significant contributors.
Of
the 643 spelling type errors,
33% are
found, however due to a display bug, it is likely that this number is significantly higher
at the expence of
the ‘ignored’
column,
particularly in the cases of
simple insertions,
omissions and the like.
For example ‘chooses’ appears to be correctly repaired by sxpipe
but returns to ‘chooses’ after analysis by frmg.
Examples of the incorrectly treated spelling cases(24%) reveal ways to potentially tailor
sxpipe to L2 writers:
• enfans → ‘encans’ and famillie → ‘faillie’ :
suggest incorporating word frequency
into the edit distance.
59
Error(s)
Example(c)
Example(o)
correctly
treated
incorrectly
treated
ignored
total
DIA
enormes
a(`a)
79
16
51
146
EMP
garden
las (les)
9
19
44
72
DIA +
sc´ene
sur (sˆ
ur)
32
19
19
70
OMI +
conduir
coleur (couleur)
12
28
22
62
OMI
developement
recontrerai
(rencontrerai)
18
12
26
56
SEP
parce-que
bien-tˆot
17
10
23
50
PHO +
receuillis
tem(th`eme)
11
12
19
42
INS
finallement
chooses(choses)
17
8
10
35
PHO
ch´ere
terres-neuves
1
7
19
27
PHG +
royaune
travillie
(travaille)
9
12
4
25
HPO
-
en(un)
-
-
15
15
SPC
pour quoi
il’s (ils)
4
2
6
12
EMP +
studes
professionales
1
3
4
8
PHG
soyent
historic
1
4
3
8
SUB
cigareltes
lieus
1
1
4
6
other
acceuiller
sons(sont)
3
-
6
9
total
215
153
275
643
Table 6.5:
Spelling error retrieval rates in cases not found by the parser, + indicates
multiple errors including the one mentioned.
Example(c) is of
a correctly identified
error, Example(i) of an unidentified or misidentified error.
• respet → ‘re-speth’ and example → ‘ex-ample’:
suggest a de-prioritisation of pre-
fix/suffix analysis
• tourists
→ ‘touriste’
and stikers
→ ‘stokers’:
suggest including perhaps an L1
parameter or an English word lookup.
• plussieurs → ‘ ETR’ and ´echˆoppes → → ‘ ETR’ suggest a de-prioritisation of the
foreign word recogniser.
• apart → ‘appart’ (instead of ‘`a part’) and connetre → ‘contre’ (instead of ‘connaˆıtre’)
suggests use of the context (via for example a language model, or tagger)
A large number of mis-corrected errors come from incorrect agreement morphology being
assigned.
FipsOrtho would treat such situations after a first parse,
but perhaps some
improvement could be obtained via a shallow parse before the full parsing or ambiguity
could be included in the sxpipe DAG.
This might help the parser as it currently only
searches for variations in agreement in the case of a parsing failure, so incorrect gender
and number can lead to an incorrect parse.
The situations where no spelling correction was made are mostly when a word exists
with the (unintended) spelling.
Context and frequency information can both be used
60
to suggest an anomalous word,
this could occur with or without presenting ambiguity
to the parser, if it is present, it would have to be severly limited to avoid combinatorial
blow out.
6.1.1
Reconstructed Tree comparison
Of
the sentences having both a successful
parse in the original
and corrected version,
228 are without any error that would cause a difference to the analysis (i.e.
when
ignoring ‘NPR’,
’BRU’,
‘CAS’
and ‘INC’
types).
449 sentences have the same trees in
their structures in both original and corrected versions.
Under the assumption that the
errors made by learners are purely performance-related and thus the corrected version
is a reflection of
their competence in the language,
we can measure the precision and
recall
of attempting to extract the trees in the corrected version by using the original.
This assumption is of
course false,
for example a learner who uses the indicative in a
clause where the subjunctive is obligatory may have no idea of
the subjunctive form
and its use even if that is in the corrected version.
However, from the point of view of
testing how stable the frmg-created representation is,
the hypothesis should serve as a
valid extreme point of comparison.
Table 6.6 shows averaged precision and recall results
under this assumption according to the parse results.
corrected version
ok
corrected
robust
N
Pr
Re
N
Pr
Re
N
Pr
Re
original
ok
666.0
0.97
0.97
3.0
0.91
0.82
-
corrected
110
0.95
0.94
2.0
1.0
1.0
-
robust
191
0.89
0.92
1.0
0.85
1.0
26.0
0.94
0.95
Table 6.6:
Precision and Recall
for tree matching between original
and corrected
parses as a function of global parse result, each group of 3 shows number of sentences,
average precision, average recall
The results are surprisingly high,
so an examination of
whether particular,
common
trees were dominating the result.
A tree-type by tree-type analysis was performed to
check this.
While it is clear that some tree types do indeed dominate the observed trees
(see Figure 6.2a),
precision and recall
values between 0.90 and 1.00 for all
trees and if
anything a slight bias of higher precision and recall for rarer trees (see Figure 6.2b).
Most likely the similarity is due to the corpus containing only local corrections, that no
large-scale re-formulations or movements of elements across long distances.
This is actu-
ally a good thing though, as we seek to measure the learner’s syntactic competence and
more elaborate corrections would take the corrected version away from a true reflection
of (a parsable verson of) the learner’s interlanguage.
61
(a) Frequency v Rank
(b) F-score v Rank
Figure 6.2:
Distribution of the most commonly observed trees in the frmg analysis of
the FipsOrtho corpus, the 4 four most common are ‘lexical’ which is used for pronouns,
punctuation and some discourse markers,
‘86’
which covers common nouns,
‘0’
which
covers determiners and ‘364’ which covers transitive verbs
(a) Parse of original text
(b) Parse of corrected text
Figure 6.3:
Trees with a low level of overlap, Entry 332, sentence 2 in the FipsOrtho
corpus
An example of a sentence with relatively poor overlap of trees is shown in Figure 6.3a
(original,
robust parse),
and Figure 6.3b (parse of corrected form).
Despite the differ-
ences provoked by the dual spelling mistakes of ‘mang´e’ and ‘empoison´ee’, the original
parse provides trees 364(3), 0(2), 86(2), 170, and lexical (3) whilst the parse of the cor-
rected sentence gives 364(1), 0(2), 86(2), 170(2), 92 lexical(3) and hence a large overlap.
The FRMG parser includes a weight element for each tree used,
with common trees
62
(a) error-containing sentences
(b) error-free sentences
Figure 6.4:
Distributions of parser weights per word in errored and error-free samples
having a positive weight and rarer trees having a large negative weight.
The weights
can either be set by hand or by machine learning.
In this study we have used the
hand-weighted version of FRMG for ease of use.
In principle the weights may give some
indication of the relative likelihood of the parse and thus an indication of the presence of
an incorrect parse (for example in the case of an error).
To verify this we looked at how
the average weight per word compares between samples containing and not containing
errors.
The total weight should be proportional to the number of words as the parser is
almost fully lexicalised (where a ‘word’
here is as recognised by the parser and so can
be a compound word for example) The resulting distributions are shown in Figure 6.4.
Sentences with errors have a mean of 648.9 with a standard deviation of 614.3, will for
sentences with errors the mean is 834.7 with a standard deviation of 524.5.
So clearly if
there is an effect, it is too small to be seen by our sample size.
This is not a definitive
argument against the variable in any way,
as many sentences with no errors tagged
almost certainly have untagged errors since the annotation was focused on word-level
errors.
6.2
MElt
MElt provides a probability of POS tag as a part of its output,
so an experiment was
performed to determine whether this probability could be an indicator of
a writing
or orthographic error.
MElt was run over the original
and corrected versions of
the
FipsOrtho corpus.
In cases where groups of tokens as determined by sxpipe corresponded
to a single token in the corpus,
the geometric mean of the probability for all tokens in
the group was taken.
The results are shown in Figure 6.5 for a range of
error types
63
Figure 6.5:
Probability distributions for POS tags using MElt,
according to general
error type.
The bottom right plot indicates cases where no error was tagged for the
word.
64
Figure 6.6:
Phrase tag sequence probability versus error density for sentences
and for words with no error tagged.
Clearly there is no point to cut at which would
not include many more non-errors than errors.
In the corpus, not all errors are tagged,
so the no error distribution may not have quite as strong a tail
as visible,
however
manual inspection shows many tags have a low probability despite being correct and in
standard usage, for example ‘chez nous’ receives a surprisingly low probability for ‘nous’
even when situated in standard sentences.
The statistical properties for the distributions are shown in Table 6.7, from this we can
see that there is some correlation, however it is unclear how to exploit it.
Error Group
N
mean
sd
rms 1
AGR
294
0.937
0.174
0.185
AGR + Spell
61
0.8
0.239
0.312
LEX
279
0.929
0.17
0.185
LEX + Spell
138
0.867
0.199
0.239
Verbal
98
0.907
0.197
0.218
Spell + Verbal
52
0.833
0.209
0.268
Syntax
354
0.942
0.131
0.143
Spell
515
0.773
0.22
0.316
other
240
0.859
0.211
0.0
No error
12893
0.959
0.12
0.126
Table 6.7:
Mean,
Standard Deviation and Root Mean Square deviation from unity
for the MElt probability distributions
A final
test was performed on the sentence as a whole,
the averaged probability of its
tags from MElt was compared with the number of
error-tagged words divided by the
length of the sentence, as shown in Figure 6.6 for both arithmetic and geometric means.
The geometric mean adds a greater spread,
but neither seems to have a lower mean
probability for a sentence with a greater error density.
65
6.3
Variables for Classification
In this section we present the current range of variables which have been implemented
with their distributions by learner level in the CEFLE corpus, along with commentary
and interpretation.
6.3.1
General Structure Variables
Following the ‘Descriptive’ indices of Coh-Metrix, we extract 11 general structure vari-
ables,
for the CEFLE corpus,
their inter-correlations are shown in Table 6.8.
Syllables
per word are calculated using Lexique (see section 5.3) values, in the case of a word not
appearing in Lexique the number of syllables taken is the number of vowel groups except
for any ‘e’ at the end of a word.
In order to show correlations between the variables and
the level,
the level is interpretted as follows:
A1 = 1,
A2 = 2,
B1 = 3,
B2 = 4,
native
= 6.
This of course is somewhat arbitrary, and as such the correlation values should be
taken as purely qualitative.
Paragraphs divisions are currently estimated via line breaks
after multiple-sentence sections.
From the table we can see that the strongest correlations between variables are between
the number of
words and the number of
sentences,
and between means and standard
deviations of particular measures.
The number of letters in a word and the number of
syllables in it are also unsurprisingly correlated and these are both similarly correlated
with level, suggesting that either one of these variables without the other would suffice.
In terms of level predictors we can see that as for L1 essays, the number of words written
is a strong predictor of level.
It should be noted that since the native responses were
on average shorter than the highest level non-native responses, this correlation is most-
likely underestimated when considering only L2 speakers.
The other variable of interest
here is the number of
words per sentence,
which shows a weak correlation with level.
Some students included conversations in their narratives and this would most likely
affect their words per sentence scores as conversations often involve shorter phrases and
possibly segmented interjections (for example ‘um ...’)
and as such this variable may
require further investigation.
Most likely the nature of
the task also has an influence
on the number of words per sentence so this may not be a strong variable to use in the
most general case.
Figure 6.7 shows box plot distributions for these variables on the CEFLE corpus with
the exception of paragraph-related variables as the the number of paragraphs in all texts
in the CEFLE corpus is one.
Clearly for each variable there is a large degree of variation
within each level, however most variables show a general trend.
Outliers in the number
n Para.
n Sent.
n Words
Sent.
per Para.
Words per Sentence
Letters per Word
Syllables per Word
Level
mean
sd
mean
sd
mean
sd
mean
sd
n Paragraphs
1.00
n Sentences
1.00
0.68
1.00
-0.31
-0.20
-0.04
0.03
-0.22
-0.05
0.10
n Words
0.68
1.00
0.68
0.29
0.19
0.20
0.29
0.19
-0.05
0.56
sent/Para
1.00
0.68
1.00
-0.31
-0.20
-0.04
0.03
-0.22
-0.05
0.10
sent/Para(σ)
1.00
word/Sent
-0.31
0.29
-0.31
1.00
0.87
0.32
0.32
0.40
0.06
0.41
word/Sent(σ)
-0.20
0.19
-0.20
0.87
1.00
0.17
0.12
0.15
0.07
0.13
Lett/Word
-0.04
0.20
-0.04
0.32
0.17
1.00
0.59
0.66
0.34
0.50
Lett/Word(σ)
0.03
0.29
0.03
0.32
0.12
0.59
1.00
0.50
0.44
0.54
Syll/Word
-0.22
0.19
-0.22
0.40
0.15
0.66
0.50
1.00
0.38
0.51
Syll/Word(σ)
-0.05
-0.05
-0.05
0.06
0.07
0.34
0.44
0.38
1.00
-0.02
Level
0.10
0.56
0.10
0.41
0.13
0.50
0.54
0.51
-0.02
1.00
Table 6.8:
General
Structure Variables and their intercorrelations on the CEFLE corpus,
the number of
paragraphs is 1 for each text so its
corelations and standard deviation are no shown, the number of sentences per paragraph is also equivalent to the number of sentences
67
of sentences plot are indicative of occasional
problems in sentence segmentation.
It is
currently unclear why the standard deviations of
the number of
letters per word and
the number of
syllables per word show different trends,
most likely this is statistical,
however if it is not it could possibly be caused by differences in syllable complexity or
the heuristic used to calculate syllables.
An interesting cross-check might be to look at
the number of
phonemes per word and per syllable,
calculating this however may be
difficult to estimate for unknown words.
In Figure 6.8) we show examples of the distributions of the number of words per sentence.
The distribution appears Gaussian, though the statistics are insufficient to draw strong
conclusions,
the level
B2 estimation of
the standard deviation is strongly affected by
a single outlier of about 80 words per phrase,
the result of an incorrect segmentation.
The high standard deviation for the other plots is typical
of
the variables extracted,
this means there is no single ‘smoking gun’
variable which can be used in isolation to
estimate the language level of the writer.
6.3.2
Vocabulary variables
We extract several vocabulary measures, all using lemmas rather than word forms.
All
vocabulary variables,
as with the general variables,
seem to follow a gaussian distribu-
tion, for example the distribution of HD-D is shown in Figure 6.13.
For lexical diversity
we extract the variables VOCD, HD-D, and MTLD as shown in Figure 6.9.
To estimate
D and M T LD we use the Lingua::Diversity::VOCD and Lingua::Diversity::MTLD perl
modules [128] which uses the same algorithm as in [129].
HD-D is estimated using our
own implementation in python.
MTLD may be most effective only at low levels as it
appears to plateau or rise only slowly with level, while VOCD and HD-D show a more
steady increase with level.
We can see from the correlations between variables (Table 6.9) that VOCD and HD-D
measures are very similar, though the difference between them is larger than in previous
work,
this may be due to the short text lengths accentuating statistical
noise in the
vocd calculation.
We have included the number of words (i.e.
text length) in the table
to show how strongly it correlates with these variables.
As expected HD-D (and vocd)
are slightly correlated with text length,
while MTLD is not.
However,
it is perhaps
surprising that MTLD is such a poor indicator of
level.
It is important to note that
this correlation measure tests just text length,
not sample size and since text length
is correlated with level,
we expect a good indicator to at least partially correlate with
length.
We can conclude that,
while HD-D may be a good indicator of
level,
further
investigation of its relation with text length will be required.
68
Figure 6.7:
General Variable distributions by level on the CEFLE corpus, sd is stan-
dard deviation.
Some outliers may have been cut to make the main distributions more
visible
69
Figure 6.8:
Distributions by level of the number of words per sentence for the CEFLE
corpus
For lexical
sophistication,
we extract PLex and S (see Figure 6.11) and also a Lexical
Frequency Profile with bins of width 1000 lemmas up to rank 4000 then a bin of width
4000 lemmas to rank 8000 and finally ‘High’
meaning anything over rank 8000,
and
‘Other’
meaning any unrecognised words,
these are all
divided by the text length to
normalise them (see Figure 6.12 ).
The reference frequency distributions are based
on the French Wikipedia,
and in the calculation of PLex we chose a ‘high’
rank limit
of
500 based on trials with a small
number of
samples.
Neither PLex nor S seems
to be particularly useful
in their current forms.
PLex is dominated by noise and the
calculation, and perhaps might be improved with a better sampling method or a change
in the rank limit.
S appears to give very poor results at low levels,
in fact with higher
vocabularies for low level students than natives.
Closer inspection reveals that the fits
are very poor, as can be seen when considering the unbinned version of S. Figure
6.10
shows the fit of S for a high level
student for which S is well
estimated,
and also two
70
Figure 6.9:
Lexical Diversity Variables distributions by level on the CEFLE corpus,
sd is standard deviation.
vocd
hdd
mtld
PLex
S
vocab1k
vocab2k
vocab3k
vocab4k
vocab8k
vocabHigh
vocabOth
level
nWords
vocd
1.00
0.86
0.63
0.13
-0.12
0.03
-0.03
0.09
-0.11
0.08
0.04
-0.12
0.44
0.38
hdd
0.86
1.00
0.55
0.21
-0.22
0.10
0.07
0.14
0.02
0.13
-0.04
-0.29
0.58
0.41
mtld
0.63
0.55
1.00
0.14
-0.05
-0.06
0.01
0.08
-0.04
-0.02
0.14
-0.08
0.24
0.21
PLex
0.13
0.21
0.14
1.00
0.37
-0.54
0.34
0.25
0.33
0.24
0.54
-0.24
0.17
0.05
S
-0.12
-0.22
-0.05
0.37
1.00
-0.82
-0.17
-0.04
0.17
0.12
0.69
0.62
-0.38
-0.25
vocab1k
0.03
0.10
-0.06
-0.54
-0.82
1.00
-0.15
-0.21
-0.21
-0.16
-0.67
-0.60
0.35
0.27
vocab2k
-0.03
0.07
0.01
0.34
-0.17
-0.15
1.00
0.12
0.07
-0.12
-0.20
-0.16
0.01
-0.06
vocab3k
0.09
0.14
0.08
0.25
-0.04
-0.21
0.12
1.00
0.10
0.08
-0.11
-0.07
0.11
0.03
vocab4k
-0.11
0.02
-0.04
0.33
0.17
-0.21
0.07
0.10
1.00
0.15
-0.02
-0.13
0.19
0.16
vocab8k
0.08
0.13
-0.02
0.24
0.12
-0.16
-0.12
0.08
0.15
1.00
-0.08
-0.09
0.32
0.21
vocabHigh
0.04
-0.04
0.14
0.54
0.69
-0.67
-0.20
-0.11
-0.02
-0.08
1.00
0.21
-0.26
-0.18
vocabOth
-0.12
-0.29
-0.08
-0.24
0.62
-0.60
-0.16
-0.07
-0.13
-0.09
0.21
1.00
-0.55
-0.38
level
0.44
0.58
0.24
0.17
-0.38
0.35
0.01
0.11
0.19
0.32
-0.26
-0.55
1.00
0.56
nWords
0.38
0.41
0.21
0.05
-0.25
0.27
-0.06
0.03
0.16
0.21
-0.18
-0.38
0.56
1.00
Table 6.9:
Correlations of vocabulary variables on the CEFLE corpus
72
(a) A high level student with a
good fit for S
(b) A low level
student with a
poor fit for S
(c)
the same student
without
binning in the histogram
Figure 6.10:
The difficulty in extracting S at low levels.
Blue crosses denote the
observed cumulative text coverage while the dashed red line shows the fit
plots of
a low level
student’s cummulative text coverage distribution showing a clear
finite-vocabulary-limited shape and consequent poor fit of S. It is clear that the Zipfian
model
does not hold for low level
learners (who have a finite vocabulary and minimal
morphemic knowledge to expand it derivationally).
Most likely incorporation of power
law in the S fit could help improve things.
An alternative would be to calculate a
probability of
the frequency distribution of
words in the text having come from the
corpus distribution.
The Lexical Frequency Profile elements are more informative, the most common words
show an increase in use with level,
perhaps indicative of a wider range of grammatical
words being used or a greater confidence in using them as level increases.
At vocabularies
between 1000 and perhaps 2500 there is little difference between levels.
However in the
2500 to around 8000 range there is a difference between native and non-native usage.
Highly rare and non-recognised words both show a decrease with level, this is most-likely
due to spelling errors leading to misinterpretation as rare words,
or the use of foreign
words.
Interestingly the correlations between different ranks are not large,
with the
exception of the anticorrelation of the frequent (<1k) words with highly infrequent and
unrecognised ones.
Overall
the correlations with level
are not strong once the effects
of
unknown/misspelt words are removed,
which suggests that an approach combining
information from multiple LFP bins such as an improved PLex or S may still be more
useful than the individual bin values.
6.3.3
Error counting variables
We currently include 7 variables in this group, including one which is redundant.
From
the frmg parse we take the average weight assigned to each tree according to the machine
learnt weights (rather than the hand-created ones).
We also take the fraction of sentences
73
Figure 6.11:
Lexical
Sophistication Variable distributions by level
on the CEFLE
corpus
P melt
meltdiff
ok
corr.
rob.
w/word
spell
level
nWords
P melt
1.00
-0.70
0.39
-0.10
-0.40
0.53
-0.57
0.51
0.36
meltdiff
-0.70
1.00
-0.50
0.23
0.45
-0.58
0.63
-0.53
-0.47
parsed ok
0.39
-0.50
1.00
-0.53
-0.86
0.52
-0.65
0.56
0.39
parsed corr
-0.10
0.23
-0.53
1.00
0.02
-0.15
0.30
-0.29
-0.10
parsed rob
-0.40
0.45
-0.86
0.02
1.00
-0.53
0.58
-0.49
-0.40
weightperword
0.53
-0.58
0.52
-0.15
-0.53
1.00
-0.62
0.76
0.52
spell corr
-0.57
0.63
-0.65
0.30
0.58
-0.62
1.00
-0.65
-0.51
level
0.51
-0.53
0.56
-0.29
-0.49
0.76
-0.65
1.00
0.56
nWords
0.36
-0.47
0.39
-0.10
-0.40
0.52
-0.51
0.56
1.00
Table 6.10:
Correlations of error-related variables in the CEFLE corpus.
P melt is
the average POS probability,
meltdiff is the number of
POS disagreements between
MElt and FRMG per word,
ok,
corr.
and rob.
refer to the fraction of sentences that
were parsed ok, with corrections and in robust mode respectively.
w/word is the parser
average weight per word and spell is the number of spelling corrections per word
which are parsed without problem, with correction and in robust mode (the sum of these
three must be 1 so one of these variables is redundant).
From sxpipe we take the number
of spelling corrections used in the final frmg parse.
From MElt we take the (geometrical)
average tag confidence per word and also the number of tags which disagree with the
FRMG analysis of the phrase.
This required alligning the MElt and FRMG tagsets, the
alignment used is shown in Appendix D.
These variables all have in common that they are designed to capture the rate of errors
that the learner produces and Figure 6.14 shows their distributions by level.
The cor-
relations of these variables are detailed in Table 6.10,
and as expected,
the number of
disconcordances between MElt and FRMG is strongly correlated with the average POS
MElt probability.
Likewise the weight per word of the parser (which decreases with a
more unusual parse tree) is negatively correlated with the MElt/FRMG differences, this
suggests that unusual trees being used in the parsing of students are indeed due to gram-
matical errors.
The frmg parser weight per word is in fact the strongest of all variables
74
Figure 6.12:
Lexical Frequency Profile component distributions by level for the CE-
FLE corpus
75
Figure 6.13:
Distributions by level of HD-D for the CEFLE corpus
considered in this analysis and a very good predictor of level.
The number of spelling
corrections per word also provides a strong correlation with level,
and perhaps would
be stronger yet if our spelling corrector was more specifically adapted to L2 mistakes.
Both of these are correlated with the number of words and further examination will be
required to discern whether this is a sampling correlation (and hence a problem to worry
about) or simply an incidental
consequence of the value of text length in level
assess-
ment.
Of the three parsing success variables,
it seems that the fraction of successfully
parsed sentences and the fraction of robust-mode parsed sentences are the most linearly
related to level,
while the rate of
sentences parsed with corrections is the most prone
to statistical
variation.
Thus far we have not included a count of the total
number of
parser corrections used per sentence and this might provide slightly more information.
Figure 6.15 shows the kind of distributions obtained for the MElt-FRMG tag differences
per word
76
Figure 6.14:
Error variable distributions by level on the CEFLE corpus, sd is standard
deviation.
Some outliers may have been cut to make the main distributions more visible
77
Figure 6.15:
Distributions by level of the rate of MElt-FRMG POS disconcordances
per word for the CEFLE corpus
6.3.4
Verb and Clause variables
FRMG provides an interface to the lexical information contained in Lefff
in its results,
in particular we can extract the mood and tense values of
a verbal
form.
In the case
of
ambiguity between forms,
we take the least-marked form by default (for example
with subjunctive and indicative ambiguity,
the indicative is taken).
Currently we take
7 groups from these,
with some overlap between them as some are tenses while others
are moods, namely:
indicative, conditional, subjunctive, imperfect, future, present and
notense,
the last including both infinitives and gerunds.
Future here refers to the sim-
ple future which is realised morphologically,
the near future (combining the verb with
‘aller’) is counted as a multi-verb VP.
Other mood and tense values (for example the
simple past or imperative) can easily be extracted,
however they are rarer and more
prone to having accidental uses in learner texts than real ones,
and so for the moment
78
they are not considered.
The distributions of these variables are shown in Figure 6.16.
Subjunctives,
conditionals and future uses are all
rare regardless of level,
this will
ob-
viously be influenced by the nature of the task:
in this case a narrative sees them used
rarely.
Imperfect use is probably more elevated than in other tasks for the same reason.
The high usage rate for B2 level writers is perhaps explained by cases of overuse of the
imperfect it when a perfect tense should have been used.
Given that the present tense is
learnt earlier than all others, it is not surprising that it tends to be overused by learners
with perhaps some evidence of a U-shaped distribution with level.
Indicative use and
Untensed use are perhaps the most surprising plot here with a general
trend amongst
learners which moves away from the native standard.
It is possible that the missing
C1 and C2 levels reverse the trend leading to U-shaped distributions,
however for the
moment we have not had time to investigate this unusual phenomenon.
In FRMG, verb groups can be with auxiliaries or include control and raising verbs, thus
‘j’aime aller `a la plage’ will have the information for ‘aime’ and ‘aller’ grouped together.
As such we can count the fraction of auxiliary verb groups, single verb groups and com-
plex/compound verb groups (having multiple non-auxilliary verbs).
The distributions of
these with respect to level are shown in Figure 6.17.
Clearly there are some differences
between native and non-native distributions,
in particular high-level
learners seem to
be using more auxiliaries and more multiple-verb VPs than natives.
For the moment
we can only speculate on the origin of
this difference,
most likely it is an overuse by
learners since the spread of native values is relatively small.
In particular the question
of which structures are used by the native writers as alternatives to auxiliaried/complex
verb groups should be explored.
Examples of the auxiliary distributions are shown in
Figure 6.19.
We also extract the ‘xarg’ (external argument) information which contains information
on non-main clauses in a sentence.
This gives us four variables, the number of relative
clauses per sentence and the numbers of nominative, accusative and locative sub clauses
per sentence.
The distributions of
clauses are shown in Figure 6.18.
Relative and
Nominative clauses show a nice correlation with level,
while accusative and locative
clauses show a weaker correlation due to their infrequence.
Generally, with verb and clause variables, while some exhibit threshold-like distributions
for learners in that they are not used at all for lower levels and then gradually increasingly
included at higher levels, most however seem to be introduced gradually.
It is not clear
how task-dependent these distributions are.
Should they be found to vary with task, one
way around this might be to take ratios of different tense types (for example imperfect to
other tensed verbs), or to treat the variables as binary presence-absence values (though
this will
introduce some text length dependence).
Another alternative would be to
79
define degrees of markedness for the flection of verbs and use them as scores rather than
using variables for particular tenses and/or moods.
Since these variables are (with the
exception of
the error variables) the easiest for students to be able to make use of
in
feedback, they merit a deeper study than what is presented here.
Correlations between these variables are shown in table 6.11.
We can see that the
use of
compound or single verb structures are relatively good predictors of
level
as is
the use of
the present tense.
For clause-related variables,
relatives and locatives give
the strongest correlations with level
in the sample.
As for the correlations between
variables,
since most verbs are either single or with auxiliary,
these two variables are
almost mirror images of each other, likewise, the narrative genre means that imperfect
use and auxiliary use will often be interrelated.
The anti-correlation between indicative
and tenseless verbs is similar to that between single and auxiliary verbs, in that most of
the time the choice is one or the other.
As expected the use of relative clauses provides
some estimation of the level
as do locative clauses,
though nominative and accusative
clauses seem less important perhaps because they are easier to form.
6.3.5
Syntactic Complexity variables
FRMG’s metagrammar allows us to extract the elementary trees used in each sentence,
we thus propose new variables to estimate their diversity.
Firstly we can count the
number of types of trees used per sentence, for example a sentence with two Determiner
+ Noun combinations will
only see that basic tree count once.
We take the mean and
standard deviation of this for each text.
We can also apply lexical
diversity measures to syntax,
in particular we use HDD and
Yule’s K
1
measures,
as MTLD is not obviously defined in a tree structure (though
linearisation of
tree ordering is possible and perhaps warrants further research) and
VOCD requires re-optimisation in its calculation methods since its fit is optimal
for
vocabulary-like distributions which have a much greater diversity than syntactic ones.
Following Coh-Metrix,
we also estimate roughly the number of words before the main
verb by taking the number of tokens before it,
this then includes punctuation and re-
flexive pronouns (for example ‘m’ in m’appelle) along with auxiliaries, modals and con-
trol/raising verbs (j’aime aller will
have a score of
2 words before the main verb for
example).
We define the main verb as that having the largest span in the FRMG parse,
1
Yule defined his K measure of lexical diversity as
1000 ×
M
2
− M
1
M
2
1
where M
1
is the number of word forms and M
2
is the sum over all tokens of its frequency count squared
80
Figure 6.16:
fractions of VPs in various moods and tenses grouped by level
for the
CEFLE corpus
81
Figure 6.17:
fractions of VPs which are with auxiliaries, single-verbed and multiple-
verbed (note that currently multiple verbs include only goups with 2 verbs)
Figure 6.18:
numbers of
relative,
nominative,
accusative and locative clauses per
sentence, as grouped by level on the CEFLE corpus
82
Figure 6.19:
Distributions by level
of the rate of auxilliary verb use per VP for the
CEFLE corpus,
a gaussian plot is included though should largely be ignored as these
distributions are more Poissonian.
vsingle
vaux
vcomp
vindic
vcond
vsubj
vimp
vfut
vpres
vnotense
crel
cnom
cacc
cloc
level
vsingle
1.00
-0.92
-0.43
-0.08
-0.11
0.05
-0.50
0.05
0.25
0.06
-0.35
-0.26
-0.24
-0.22
-0.35
vaux
-0.92
1.00
0.07
0.07
0.13
-0.06
0.49
-0.07
-0.23
-0.04
0.24
0.15
0.14
0.17
0.23
vcompound
-0.43
0.07
1.00
0.14
-0.00
-0.01
0.20
0.04
-0.10
-0.10
0.42
0.37
0.35
0.18
0.43
vindicative
-0.08
0.07
0.14
1.00
-0.15
-0.10
0.02
-0.13
0.62
-0.94
-0.12
-0.05
-0.07
-0.15
-0.16
vconditional
-0.11
0.13
-0.00
-0.15
1.00
0.11
0.34
0.07
-0.14
-0.08
0.08
0.04
0.01
-0.00
0.05
vsubjunctive
0.05
-0.06
-0.01
-0.10
0.11
1.00
-0.03
0.03
0.04
-0.01
-0.01
-0.01
-0.03
-0.00
0.02
vimperfect
-0.50
0.49
0.20
0.02
0.34
-0.03
1.00
-0.03
-0.64
-0.05
0.21
0.07
0.28
0.13
0.26
vfuture
0.05
-0.07
0.04
-0.13
0.07
0.03
-0.03
1.00
-0.14
0.13
0.12
0.05
0.20
0.07
0.23
vpresent
0.25
-0.23
-0.10
0.62
-0.14
0.04
-0.64
-0.14
1.00
-0.64
-0.29
-0.09
-0.27
-0.31
-0.42
vnotense
0.06
-0.04
-0.10
-0.94
-0.08
-0.01
-0.05
0.13
-0.64
1.00
0.16
0.08
0.10
0.19
0.23
crel
-0.35
0.24
0.42
-0.12
0.08
-0.01
0.21
0.12
-0.29
0.16
1.00
0.82
0.44
0.56
0.46
cnom
-0.26
0.15
0.37
-0.05
0.04
-0.01
0.07
0.05
-0.09
0.08
0.82
1.00
0.12
0.11
0.25
cacc
-0.24
0.14
0.35
-0.07
0.01
-0.03
0.28
0.20
-0.27
0.10
0.44
0.12
1.00
0.25
0.28
cloc
-0.22
0.17
0.18
-0.15
-0.00
-0.00
0.13
0.07
-0.31
0.19
0.56
0.11
0.25
1.00
0.44
level
-0.35
0.23
0.43
-0.16
0.05
0.02
0.26
0.23
-0.42
0.23
0.46
0.25
0.28
0.44
1.00
Table 6.11:
Correlations between verb and clause variables.
vsingle represents VPs containing only one verb,
vaux represents those containing
auxiliaries,
vcomp other compound verb phrases.
indic,
cond,
subj,
imp,
fut and pres are short for indicative,
conditional,
subjunctive,
imperfect,
future and present respectively, vnotense is for infinitives and gerunds.
rel, nom, acc and loc are short for clause types relative, nominative, accusative,
and locative respectively.
84
in the case of a robust parse, we take the first verb in the sentence.
Distributions of this
variable are shown in Figure 6.21 As a part of this variable’s extraction, we also calculate
the number of sentences without a main verb,
this doesn’t neccessarily imply a poorly
constructed sentence,
for example ‘Salut!’
is not considered to have a verb.
Sentences
with no verb are not included in the average words before the main verb calculation,
unless there is no verb in any sentence in which case the average is taken as zero.
In
fact the number of verb-less sentences is often indicative of poor sentence segmentation,
though this in turn is often a reflection of misuse of punctuation or unusual structures,
which explains why it still correlates with level.
The distributions of these variables are shown in Figure 6.20.
The number of tree types
per sentence shows a nice correlation as does the plot of
the number of
words before
the main verb.
Estimations using lexical diversity based measures show different results,
with HDD more or less constant for learners and falling for natives, while Yule’s K shows
a slight increasing trend with level.
To further investigate this,
implementing other
diversity variables could well
be worthwhile.
The fraction of
sentences without verbs
show the expected decrease with level, it could probably be improved by distinguishing
cases where a verb should have been used from cases of correct usage involving discourse
markers such as ‘Salut’ or ‘voila’.
The correlations of these variables are shown in Table 6.12.
Of note is that Yule’s K,
when applied to syntactic trees, is strongly correlated with the number of words, and as
such should be avoided.
The number of words before the main verb seems a surprisingly
strong variable, this might be due to being a combination of measuring adverbial phrase
use and noun complexity or may be due to the linear effects of memory,
with a more
advanced language level
making it easier to stock more complex structures efficiently.
The number of words before the main verb is correlated with the mean number of tree
types used per sentence,
but has a stronger correlation with level,
this may be due to
it being a better indicator or as a result of
incorrectly parsed sentences diluting the
quality of
the tree types extracted.
In either case this suggests that better metrics
of syntactic complexity may well
be available either by improving parsing or studying
elements involved in or related to the number of words before the main verb.
The mean
number of
tree types used per sentence is a little less correlated with level,
perhaps
improving parsing accuracy would strengthen the correlation.
While the variables we
have extracted are not as strong indicators as some of the others, there is clearly some
potential with this type of variable.
85
TT/S
σ TT/S
HDD
Yule K
verbless
WBMW level
nWords
mean TT/Sent
1.00
0.63
0.34
-0.09
-0.19
0.68
0.34
0.18
sd TT/Sent
0.63
1.00
0.30
-0.06
-0.10
0.38
0.05
0.13
TT HDD
0.34
0.30
1.00
-0.49
0.23
0.05
-0.33
-0.20
TT Yule K
-0.09
-0.06
-0.49
1.00
-0.06
0.01
0.36
0.71
verbless
-0.19
-0.10
0.23
-0.06
1.00
-0.21
-0.47
-0.31
WBMW
0.68
0.38
0.05
0.01
-0.21
1.00
0.43
0.16
level
0.34
0.05
-0.33
0.36
-0.47
0.43
1.00
0.56
nWords
0.18
0.13
-0.20
0.71
-0.31
0.16
0.56
1.00
Table 6.12:
correlations of syntactic complexity variables, TT is ‘Tree Types’, TT/s
is ‘Tree Types per Sentence’ and WBMW is for ‘words before the main verb’
6.3.6
Cohesion variables
We take the French wikipedia as a basis for the word2vec word embeddings using gensim.
Several
subcategories of unknown words based on their structure rather than a single
term were included as unknown words are common in L2 texts, these included elements
like numbers,
dates,
urls and so on above those which are recognised by sxpipe.
Lines
containing filenames were also removed as often these were partially tokenised.
Cur-
rently, we extract four variables, all are the cosθ between the relevant vectors averaged
over all possibilities in the text, these being:
• between subsequent words
• between a word and its sentence
• between subsequent sentences
• between a sentence and the text
The distributions of
these variables are shown in Figure 6.22.
We can see that while
these measures of
cohesion all
increase with level,
there is a great deal
of
variation.
Most likely we are indirectly measuring the number of poor lexical choices by the writer.
The correlations of
these variables are shown in Table 6.13.
The word-to-word and
word-to-sentence variables seem to be essentially measuring the same thing, likewise for
the pair of
sentence-to-sentence and sentence-to-text variables.
The word-to-sentence
cohesion is the most strongly correlated with level,
however perhaps some care should
be taken with it as the length of a sentence may affect its variation within a text because
longer sentences will tend to have more neutral vectors.
A possible improvement might
be to consider word-to-clause cohesion.
The word-to-word distributions are shown in
more detail
in Figure 6.23,
and show seemingly Gaussian distributions.
Clearly this
is the tip of
the iceberg in terms of
possible cohesion variables,
in particular we have
86
Figure 6.20:
Syatactic complexity variables as grouped by level on the CEFLE corpus
not considered discourse connectors,
and it might be interesting to try and associate
them with particular semantic contrasts in word embedding vector space, however this
is beyond the scope of the present study.
6.4
Preliminary results:
Classification
In total
we have 55 variables,
though many are intercorrelated,
and some are much
stronger predictors of level than others.
For the CEFLE corpus,
some of these are not
relevant, in particular the number of paragraphs and related variables since there is only
87
Figure 6.21:
Distributions by level of the number of words before the main verb for
the CEFLE corpus
WWct
WSct
SSct
STct
level
nWords
mean Word to Word cosθ
1.00
0.95
0.49
0.48
0.46
0.20
mean Word to Sent cosθ
0.95
1.00
0.41
0.39
0.56
0.29
mean Sent to Sent cosθ
0.49
0.41
1.00
0.95
0.28
0.16
mean Sent to Text cosθ
0.48
0.39
0.95
1.00
0.26
0.11
level
0.46
0.56
0.28
0.26
1.00
0.56
nWords
0.20
0.29
0.16
0.11
0.56
1.00
Table 6.13:
Correlations between cohesion variables,
in column titles,
W = word,
S
= Sent, T = text and ct is cosθ
one paragraph in all
the texts,
so these are excluded from the following classifications.
All classification here is carried out using the Weka software.
We first perform a Naive-Bayes classification with 10-fold cross-validation, the confusion
matrix of results for which are shown in Table 6.14.
The classification is largely diagonal
with only 6 texts misclassified by two or more levels.
F-scores for each level are 0.67, 0.55,
88
Figure 6.22:
Cohesion variables as grouped by level on the CEFLE corpus
classification
true
A1
A2
B1
B2
Native
A1
20
3
0
1
0
A2
13
21
8
0
0
B1
3
11
21
7
0
B2
0
0
8
32
2
Native
0
0
2
4
24
Table 6.14:
Naive Bayes classification of learner texts from the CEFLE corpus, with
text length included as a feature
0.52, 0.74 and 0.86 for levels A1, A2, B1, B2 and native respectively.
The classifier has
the most difficulty distinguishing the A2 and B1 levels, but also has difficulties with A1
classification.
This is a little surprising as the lower levels of language learning usually
see the most rapid expansion in grammtical
capabilities and so might be expected to
be more easily distinguished by many of the variables used.
On the other hand these
levels are traversed quicker than the latter ones and perhaps it is possible that they
are objectively less distinct.
Hopefully further study of the misclassified texts and their
analysis will reveal the key variables to better separate these levels.
If we remove variables strongly correlated with text length,
in particular:
the number
of
sentences,
the number of
words and Yule’s K for syntactic complexity,
we are left
with 48 variables,
and with the same classification we obtain the results in Table 6.15.
89
Figure 6.23:
Distributions by level
of cosθ between subsequent words,
as extracted
by word2vec, for the CEFLE corpus
classification
true
A1
A2
B1
B2
Native
A1
21
3
0
0
0
A2
13
21
7
1
0
B1
3
13
20
6
0
B2
0
2
7
31
2
Native
1
1
0
4
24
Table 6.15:
Naive Bayes classification of learner texts from the CEFLE corpus without
text length correlated variables as features
90
classification
true
A1
A2
B1
B2
Native
A1
19
4
1
0
0
A2
3
22
16
1
0
B1
1
12
18
8
3
B2
0
1
7
29
5
Native
0
0
6
4
20
Table 6.16:
Classification via automated Bayesian Inference Network construction
using only presence/absence of meta-grammar trees
The number of texts misclassified by two levels or more increases to 8 and F-scores by
level
are 0.68,
0.51,
0.53,
0.74 and 0.86 again for A1 through Native respectively.
So
the degradation in performance is small,
though collectively other variables with mild-
correlations in text length may still
be influencing the result and further investigation
is required.
As an alternative approach we took as features binary variables indicating only the
presence or absence of
a particular FRMG tree type in the text.
Across the CEFLE
corpus, 194 different tree types are observed so this gives us an equal number of features.
Using only these features we construct a Bayesian inference Network automatically.
The results of
such a classification are shown in Table 6.16.
Here we have 10 texts
misclassified by 2 levels or more and F-scores of 0.81,
0.4,
0.54,
0.69,
0.69 and 0.64 for
the same level ordering as before.
Notably though the A1 level classification is improved
compared to the variable-based classifier.
The similarity in results between the two
approaches suggests that we syntactic information is the strongest indicator of language
level.
Most likely the classification succeeds due to a mix of use of more-difficult trees and
from student errors which lead to mis-parsing and unusual
trees,
however the relative
importances of these two factors requires further investigation, as does the structure of
the learnt network.
The result is nevertheless a very interesting one and suggests that
further research in this direction could be both revealing and helpful from a classification
point of view.
In particular examination of the generated network may give us insight
into the hierarchy of stages in interlanguage development.
Although thus far we have focused on classification, it seems more natural to imagine a
regression of some kind to a language level value ranging from zero to some maximum
value (or range of values) indicating native-like quality of language.
Doing this requires
the projection of level classes onto a real-valued scale, the most natural might be to have
the levels equally spaced, thus A1=1, A2=2, B1=3 ...
and so on.
This is not necessarily
the best option though as the time required to acquire each level
increases with level,
so a single ‘language quality’ metric may not be linear in learner level.
Further it is not
clear how much of a gap there is between Natives and high-level non-natives, or if they
91
should even be considered on the same scale (the reader will recall that some language
features such as indicative-mood usage show differences between native and non-native
speakers regardless of level).
Despite these concerns, we perform a linear regression using
the assignments A1 = 1, A2 = 2, B1 = 3, B2 = 4 and Native = 6 on the CEFLE corpus.
The result gives a correlation coefficient of 0.82,
and a mean absolute error of 0.70.
If
we consider the nearest value as the class assigned,
we obtain F-scores of
1 0.49,
0.4,
0.48, 0.65 and 0.73, with levels in standard order.
This sees 23 elements misclassified by
2 levels or more.
Although generally a positive result, non-linear elements will probably
need to be better included to optimise such a regression.
A final
alternative not yet implemented is to break the classification process up into
stages,
for example native v non-native then A v B v C then sublevels within thus al-
lowing different variables to play different roles at each point.
In effect a pre-constrained
decision tree.
Weka includes a wide ranges of
classification methods,
but for the mo-
ment they offer only slight improvements on those displayed here, suggesting that new
variables or improvements on existing ones may be required to improve classification.
Further any dependence on text length, L1 and task-type needs to be examined.
Overall
though the results are encouraging and suggest that an automatic analysis of
learner
level is a realisable objective.
Chapter 7
Conclusions and Prospects
7.1
The Alpage treatment chain
As we have seen,
the existing NLP tools from Alpage show sufficient robustness and
fortitude to handle the the kind of non-standard language common amongst language
learners.
In fact, this language has proven a fertile testing ground with a large number
of obscure bugs coming to light thanks to the kind of unusual combinations of characters
and words encountered in the Learner Corpora analysed.
Despite this, there are several possible paths to take to improve coverage and accuracy
in the analysis of learner texts.
For tokenisation and sentence segmentation,
it is clear
that punctuation cannot be relied upon to follow orthodox French rules as it seems
particularly prone to L1 transfer.
In one case, a low-level learner composed a phrase of
77 words which lead to parser to completely fill the machine memory, it turned out to be
a sequence of nominal
groups with only comma separators and occasional
capitals.
In
another case an English transfer of placing an apostrophe before a year ( ’89 for example)
caused the entire paragraph to be unsegmented.
A simple counter to this would be to
introduce heuristic limits on sentence length.
Since L2 sentences are reliably shorter than
the formal language often used as training data for NLP systems, this should help avoid
such problematic cases.
A more advanced solution might be extend the DAG philosophy
to ambiguous sentence boundaries.
So long as such boundaries are infrequent, this would
not impose a heavy burden.
After tokenisation and segmentation,
learner spelling errors obviously pose problems
to the system,
and L2-specific modules should be designed to deal
with them,
ideally
using both word-internal
and context information.
The use of phonetic rules adjusted
to L2 learners, along with edit distances that reflect misapplied morphological rules - as
92
93
used in FipsOrtho should help from the word-internal
perspective.
From the context,
although MElt does not seem to be able to strongly predict error locations, a language
model via neural networks or N-grams may be enough to fulfill the role.
If not then the
FipsOrtho route of correction-in-the-case-of-parsing-failure may be the only option.
Syntactically, the FRMG parser performed far beyond expectations on L2 texts, in some
sense it might be considered to be performing too well, as successful parsing may hide the
learner errors which one is seeking to find.
A first step would be to implement a grammar
checker, most likely using regular expression rules in a pre-parsing stage.
An alternative
is to try and integrate a comparison of MElt POS assignments and those of the parser,
with a mismatch indicating a possible error location.
Eric de la Clergerie is currently
working on including just such an option in FRMG via a MElt-based enrichment of the
sxpipe DAG. Another option is to integrate word-frequency information into Lefff
and
so the parser, thus a noun that is misspelt as an obscure verb would be more likely to be
corrected than to be parsed as is.
Finally, the current error-correction module of FRMG
is relatively small
and lightly developed.
An interface could be made to enable easy
control over which errors are considered in which circumstances, as well as the addition
of new error corrections.
For example a learner-specific correction would be to replace
‘qui’
by ‘que’
or vice-versa which is a relatively common mistake in the FipsOrtho
corpus.
Semantic level
errors are the most difficult to spot though,
in particular the
assessment of correct pronoun use will probably require anaphora resolution.
Although
not examined in this study, it is possible that this task is easier in L2 texts due to their
relative simplicity.
Semantic level information is also pertinent for FRMG’s corrections,
currently the noun gender dominates and in practice, other elements of a nominal group
are made to match it.
However learners do commit errors where the noun should be
modified rather than the determiner or adjective.
The tools developed in the growing field of
L1 identification could be used to help
integrate L1 specific analysis into the chain of treatment.
7.2
Language Analysis
We have extracted a diverse range of over 50 variables which to varying degrees quantify
the quality of
writing of
learners covering general
text properties,
vocabulary,
error
rates,
syntactic diversity,
verb and clause usage and cohesion.
Of these,
the syntactic
weight assigned by the FRMG parser and various error-related variables seem to be the
strongest measures of
language level.
Some measures such as PLex and S have been
found wanting, however may be improved relatively easily.
Other measures such as the
number of words before the main verb have been shown to be surprisingly strong even
94
when compared with perhaps more theoretically justified features.
Yet other features
such as the rate of use of the indicative mood may hint at differences between native
and learner writing which remain even at high learner levels.
Among these we have also
included several new measures of syntactic diversity.
With the variables extracted, the resulting classification is promising but perhaps needs
a little more information,
particularly to differentiate the lowest levels.
The purely
tree-based classification hints at providing important information on interlanguage hi-
erarchies,
which could be very interesting to study further.
In our analysis of
learner
language we have currently limited ourselves to the study of the CEFLE corpus mainly
due to its L1 and task consistency.
This then naturally leads to a question of general-
isability,
the CHY-FLE corpus offers a way to check this with groups of texts written
by the same person but with different tasks, along with different L1s from the Swedish
CEFLE corpus.
There are a large number of avenues to further progress in terms of written analysis.
A
first step is to better analyse low level texts for extractable traits that can help distinguish
among the levels better.
The inclusion of language model information (via N-grams or a
LSTM neural network) will probably provide useful information on the ‘naturalness’ of
the written texts.
Our cohesion variables are only a few among the many possible and
including analysis of connecting words and more semantically oriented variables could
possibly improve the rating significantly.
One way to achieve this would be to modify
the open source Light Side environment to incorporate the Alpage treatment chain.
An
alternative way forward is to construct a website so as to start accumulating a wider and
larger corpus of learner texts.
As yet we have also not included any attempts at genre
recognition of
the text nor L1 detection,
the former could well
aid level
classification
while the latter could improve error detection.
Most textual
analysers use upwards of
two hundred variables,
and our approximately 50 is small
in comparison so there is a
great deal of room for addition of new variables and new information.
Although the possible ways to advance are numerous, this does provide encouragement
that potentially significant improvements can be made, and with the strong base already
developed, provide some optimism for the potential of this system.
Appendix A
CEFLE prompts
The prompts for the three tasks other than the voyage en italie (ref) are shown here:
(taken from ref A thesis)
95
96
Figure A.1:
the ‘l’homme sur l’ˆıle’ task
97
Figure A.2:
the ‘Un souvenir de voyage’ task
98
Figure A.3:
the ‘Moi, ma famille et mes amis’ task
Appendix B
HELLAS prompts
The prompts for the B2 and C1 tasks are shown here:
(pers comm)
99
100
Figure B.1:
HELLAS B2 activities
101
Figure B.2:
HELLAS C1 activity 1
102
Figure B.3:
HELLAS C1 activity 2
Appendix C
detailed CEFR descriptors by
genre
C2
Can write clear,
smoothly flowing,
and fully engrossing stories
and descriptions of experience in a style appropriate to the genre
adopted.
C1
Can write clear,
detailed,
well-structured and developed descrip-
tions and imaginative texts in an assured, personal, natural style
appropriate to the reader in mind.
B2.2
Can write clear,
detailed descriptions of real or imaginary events
and experiences,
marking the relationship between ideas in clear
connected text, and following established conventions of the genre
concerned.
B2.1
Can write clear,
detailed descriptions on a variety of subjects re-
lated to his/her field of
interest.
Can write a review of
a film,
book or play.
B1
Can write straightforward, detailed descriptions on a range of fa-
miliar subjects within his/her field of interest.
Can write accounts
of
experiences,
describing feelings and reactions in simple con-
nected text.
Can write a description of an event,
a recent trip –
real or imagined.
Can narrate a story.
A2.2
Can write about everyday aspects of
his/her environment,
e.g.
people, places, a job or study experience in linked sentences.
Can
write very short,
basic descriptions of events,
past activities and
personal experiences.
A2.1
Can write a series of
simple phrases and sentences about their
family, living conditions, educational background, present or most
recent job.
Can write short,
simple imaginary biographies and
simple poems about people.
A1
Can write simple phrases and sentences about
themselves and
imaginary people, where they live and what
Table C.1:
Creative writing skill descriptions in the CEFR (p.62 of [9])
103
104
C2
Can produce clear, smoothly flowing, complex reports, articles or
essays which present a case,
or give critical
appreciation of
pro-
posals or literary works.
Can provide an appropriate and effective
logical structure which helps the reader to find significant points.
C1
Can write clear,
well-structured expositions of complex subjects,
underlining the relevant salient issues.
Can expand and support
points of view at some length with subsidiary points, reasons and
relevant examples.
B2.2
Can write an essay or report which develops an argument sys-
tematically with appropriate highlighting of significant points and
relevant supporting detail.
Can evaluate different ideas or solu-
tions to a problem.
B2.1
Can write an essay or report which develops an argument, giving
reasons in support of or against a particular point of view and ex-
plaining the advantages and disadvantages of various options.
Can
synthesise information and arguments from a number of sources.
B1.2
Can write short,
simple essays on topics of
interest.
Can sum-
marise,
report and give his/her opinion about accumulated fac-
tual
information on familiar B1 routine and non-routine matters
within his/her field with some confidence
B1.1
Can write very brief reports to a standard conventionalised format,
which pass on routine factual
information and state reasons for
actions.
A2
No descriptor available
A1
No descriptor available
Table C.2:
Essay/Report writing skill descriptions in the CEFR (p.62 of [9])
C.1
Range/Complexity descriptors
105
C2
Can exploit a comprehensive and reliable mastery of a very wide
range of language to formulate thoughts precisely, give emphasis,
differentiate and eliminate ambiguity ...
No signs of
having to
restrict what he/she wants to say.
C1
Can select an appropriate formulation from a broad range of lan-
guage to express him/herself
clearly,
without having to restrict
what he/she wants to say.
B2.2
Can express him/herself clearly and without much sign of having
to restrict what he/she wants to say.
B2.1
Has a sufficient range of language to be able to give clear descrip-
tions,
express viewpoints and develop arguments without much
conspicuous
searching for
words,
using some complex sentence
forms to do so.
B1.2
Has a sufficient range of language to describe unpredictable situa-
tions, explain the main points in an idea or problem with reason-
able precision and express thoughts on abstract or cultural topics
such as music and films
B1.1
Has enough language to get by,
with sufficient vocabulary to ex-
press him/herself
with some hesitation and circumlocutions on
topics such as family,
hobbies and interests,
work,
travel,
and
current events,
but lexical
limitations cause repetition and even
difficulty with formulation at times.
A2.2
Has a repertoire of basic language which enables him/her to deal
with everyday situations with predictable content, though he/she
will
generally have to compromise the message and search for
words.
A2.1
Can produce brief
everyday expressions in order to satisfy sim-
ple needs
of
a concrete type:
personal
details,
daily routines,
wants and needs,
requests for information.
Can use basic sen-
tence patterns and communicate with memorised phrases, groups
of a few words and formulae about themselves and other people,
what they do, places, possessions etc.
Has a limited repertoire of
short memorised phrases covering predictable survival situations;
frequent breakdowns and misunderstandings occur in non-routine
situations.
A1
Has a very basic range of simple expressions about personal details
and needs of a concrete type.
Table C.3:
General Linguistic Range descriptors in the CEFR (p.110 of [9])
Appendix D
POS tag alignement between
MElt and FRMG
currently the alignment used is as follows in Table D.1, this is rough and no doubt could
be improved in some of the more obscure cases, however should work well for the most
common ones.
106
107
MElt tag
FRMG tag(s)
V
aux, v
VINF
v, aux, vmodprep
VIMP
v, aux
VS
aux, v
VPP
v, aux
PPR
v, aux, vmodprep
NC
nc, ncpred, ncpred2
NPP
np, title
CC
coo
CS
csu, que, pri
ADJ
adj, number
ADJWH
adj
ADV
advneg, adv, clneg, predet
ADVWH
adv
CLS
cln, pro, ilimp, caimp, ce
CLO
cld, cla, cll, clr, caimp, clg, cll
CLR
cla, clr
P
prep
P+D
det, prep
P+PRO
prep, xpro, pri, prel
I
pres, adv, np, nc
PONCT
“ ”, poncts, ponctw, incise
ET
ETR
PROWH
pri
PROREL
prel, pri
PRO
xpro
DETWH
det”
DET
det, number
Table D.1:
Alignment of MElt tags and FRMG tags
Bibliography
[1]
Rod Ellis. “Second language acquisition”. In: The United States: Oxford (1997).
[2]
Muriel
Saville-Troike.
Introducing second language acquisition.
Cambridge Uni-
versity Press, 2012.
[3]
Rod Ellis. “Second Language Learning and Second Language Learners : Growth
and Diversity”. In: Tesl
Canada Journal
7.I (1989), pp. 74–94.
[4]
Larry Selinker.
“Interlanguage”.
In:
IRAL-International
Review of Applied Lin-
guistics in Language Teaching 10.1-4 (1972), pp. 209–232.
[5]
Manfred Pienemann.
“Is language teachable? Psycholinguistic experiments and
hypotheses”. In: Applied linguistics 10.1 (1989), pp. 52–79.
[6]
Freiderikos Valetopoulos and Efi Lamprou. “Evaluation des besoins langagiers des
apprenants chypriotes hell´enophones en fran¸
cais langue ´etrang`ere”. In: Languages
for Intercultural
Dialogue. 2008, pp. 128–138.
[7]
Valetopoulos Freiderikos. “R´elexions(s) sur l’exploitation d’un corpus d’apprenants”.
In: Le langage et ses niveaux d’analyse : cognition, production de formes, produc-
tion du sens.
Ed.
by Jean Chuquet.
Universitaires de Rennes,
2011.
Chap.
8,
pp. 125–137.
[8]
Conseil de l
0
Europe. Cadre europ´een commun de r´ef´erence pour les langues. Paris:
Didier, 2001.
[9]
Council
of
Europe.
“the Common European Framework of
Reference for Lan-
guages : Learning, Teaching, Assessment”. In: Council
of Europe (2001), pp. 1–
273.
url:
http://www.coe.int/t/dg4/linguistic/CADRE1%5C_EN.asp%5C#
TopOfPage.
[10]
Jean-Claude Beacco and R´emy Porquier. Niveau A1. 1 pour le fran¸cais. R´ef´erentiel
et certification (DILF) pour les premiers acquis en fran¸cais. 2005.
[11]
Jean-Claude Beacco and R´emy Porquier. “Niveau A2 pour le fran¸
cais: Un r´ef´erentiel
[Level B2 for French language: Reference descriptions]”. In: (2008).
108
Bibliography
109
[12]
Jean-Claude Beacco et al. “Niveau B1 pour le fran¸cais: Un r´ef´erentiel [Level B2
for French language: Reference descriptions]”. In: (2011).
[13]
Jean-Claude Beacco,
Simon Bouquet,
and R´emy Porquier.
Niveau B2 pour le
fran¸cais:
Un r´ef´erentiel
[Level
B2 for French language:
Reference descriptions].
2004.
[14]
Centre International. Centre International d’
´
Etudes P´edagogiques Rapport d’activit´e
2013. Tech. rep. 2013.
[15]
Eli
Hinkel.
“What Research on Second Language Writing Tells Us and What
it Doesn’t”.
In:
Handbook of research in second language teaching and learning,
Volume 2.
2011,
pp.
523–538.
url:
http://www.tandfebooks.com/doi/abs/
10.4324/9780203836507%5C#page=338.
[16]
Scott a.
Crossley and Danielle S.
McNamara.
“Understanding expert ratings of
essay quality: Coh-Metrix analyses of first and second language writing”. In: In-
ternational Journal of Continuing Engineering Education and Life-Long Learning
21.2/3 (2011), p. 170. issn: 1560-4624. doi: 10.1504/IJCEELL.2011.040197.
[17]
Suzanne Schlyter.
“Stades de d´eveloppement en fran¸cais L2”.
In:
Ms.
Institut
d
0
´etudes romanes de Lund. Universit´e de Lund. Disponible en ligne: http://www.
rom. lu. se/durs/STADES DE DEVELOPPEMENT EN FRANCAIS L2. pdf
(2003).
[18]
Hollis S Scarborough. “Index of productive syntax”. In: Applied psycholinguistics
11.01 (1990), pp. 1–22.
[19]
Km Rheinhardt.
“The Developmental
Sentence Scoring Procedure”.
In:
Inde-
pendent Studies and Capstones (1972),
p.
314.
url:
http://digitalcommons.
wustl.edu/cgi/viewcontent.cgi?article=1330%5C&amp;context=pacs%5C_
capstones.
[20]
Catherine Mimeau.
“Mesure et ´etiologie des habilet´es morphosyntaxiques des
enfants francophones d
0
ˆage scolaire”. PhD thesis. Universit´e LAVAL, 2015.
[21]
Aj S¨okmen. “Current trends in teaching second language vocabulary”. In: Vocab-
ulary: Description, acquisition and pedagogy. Ed. by Norbert Schmitt and Michael
McCarthy. Cambridge university press Cambridge, 1997.
[22]
Scott Jarvis. “Defining and measuring Lexical Diversity”. In: Vocabulary knowl-
edge: human ratings and automated measures. Ed. by Jarvis, S., Daller, M. John
Benjamins B.V., 2013.
[23]
David Daniel
Malvern and Brian Richards.
Measures of
lexical
richness.
2012.
doi: 10.1002/9781405198431.wbeal0755. url: http://centaur.reading.ac.
uk/30414/.
Bibliography
110
[24]
David Malvern and Brian Richards.
“Investigating accommodation in language
proficiency interviews using a new measure of
lexical
diversity”.
In:
Language
Testing 19.1 (2002), pp. 85–104. issn: 02655322. doi: 10.1191/0265532202lt221oa.
[25]
P.
M.
McCarthy and S.
Jarvis.
“vocd:
A theoretical
and empirical
evaluation”.
In:
Language Testing 24.4 (2007),
pp.
459–488.
issn:
0265-5322.
doi:
10.1177/
0265532207080767.
[26]
C Udny Yule. The statistical
study of literary vocabulary. Cambridge University
Press, 1944.
[27]
Jeanine Treffers-Daller. “Measuring lexical diversity among L2 learners of French:
an exploration of the validity of D,
MTLD and HD-D as measures of language
ability”. In: Vocabulary knowledge: human ratings and automated measures. Ed.
by Jarvis,
S.,
Daller,
M.
2013.
isbn:
9789027241887.
url:
http : / / centaur .
reading . ac . uk / 28712 / 1 / Chapter % 206 % 20(Treffers - Daller ) final % 5C _
25June2012.docx.
[28]
Batia Laufer and Paul
Nation.
“Vocabulary size and use:
Lexical
richness in
L2 written production”.
In:
Applied Linguistics 16.3 (1995),
pp.
307–322.
issn:
01426001. doi: 10.1093/applin/16.3.307.
[29]
Robin (Open University); Goodfellow, Glyn (City & Guilds College); Jones, and
Lamy Marie-No¨elle (Open University). “Assessing Learners’ Texts using the Lex-
ical Frequency Profile”. In: ReCALL 14.01 (2002), pp. 1–14. url: http://www.
lextutor.ca/vp/fr/recall01.htm%20http://journals.cambridge.org/
action / displayAbstract ? fromPage = online % 5C & aid = 108845 % 5C & fileId =
S0958344002001118.
[30]
Batia Laufer. Lexical
Frequency Profiles. doi: 10.1002/9781405198431.
[31]
Paul
Meara and Huw Bell.
“P Lex:
a simple and effective way of
describing
the lexical
characteristics of shortL2 texts”.
In:
Prospect
16.3 (2001),
pp.
5–19.
issn:
08147094.
url:
http://www.lognostics.co.uk/vlibrary/meara%5C&
bell2001.pdf.
[32]
F.
Tidball
and J.
Treffers-Daller.
“Analysing lexical
richness in French learner
language:
What frequency lists and teacher judgements can tell
us about basic
and advanced words”.
In:
Journal
of
French Language Studies 18.Special
Issue
03 (2008),
pp.
299–313.
issn:
0959-2695.
doi:
10 . 1017 / S0959269508003463.
url: http://journals.cambridge.org/action/displayAbstract?fromPage=
online%5C&aid=2370068.
[33]
Roderick Edwards and Laura Collins.
“Lexical
Frequency Profiles and Zipf’s
Law”.
In:
Language Learning 61.March (2011),
pp.
1–30.
issn:
00238333.
doi:
10.1111/j.1467-9922.2010.00616.x.
Bibliography
111
[34]
Scott a.
Crossley,
Tom Cobb,
and Danielle S.
McNamara.
“Comparing count-
based and band-based indices of word frequency: Implications for active vocabu-
lary research and pedagogical
applications”.
In:
System 41.4 (2013),
pp.
965–
981.
issn:
0346251X.
doi:
10 . 1016 / j . system . 2013 . 08 . 002.
url:
http :
//dx.doi.org/10.1016/j.system.2013.08.002.
[35]
Masumi Kojima and Junko Yamashita. “Reliability of lexical
richness measures
based on word lists in short second language productions”. In: System 42 (2014),
pp. 23–33. issn: 0346251X. doi: 10.1016/j.system.2013.10.019. url: http:
//dx.doi.org/10.1016/j.system.2013.10.019.
[36]
Josep Ginebra and Xavier Puig.
“On the measure and the estimation of
even-
ness and diversity”. In: Computational
Statistics and Data Analysis 54.9 (2010),
pp.
2187–2201.
issn:
01679473.
doi:
10 . 1016 / j . csda . 2010 . 04 . 001.
url:
http://dx.doi.org/10.1016/j.csda.2010.04.001.
[37]
Mark S Seidenberg.
“The time course of
phonological
code activation in two
writing systems”. In: Cognition 19.1 (1985), pp. 1–30.
[38]
Yves Bestgen and Sylviane Granger.
“Categorising spelling errors to assess L2
writing”.
In:
International
Journal
of
Continuing Engineering Education and
Life-Long Learning 21.2/3 (2011), p. 235. issn: 1560-4624. doi: 10.1504/IJCEELL.
2011.040201.
[39]
Claudia Leacock and Martin Chodorow. “Automated Grammatical Error Detec-
tion for Language Learners”. In: (2010).
[40]
Vivian J Cook. “L2 users and English spelling”. In: Journal
of Multilingual
and
Multicultural
Development 18.6 (1997), pp. 474–488.
[41]
Roger Mitton and Takeshi Okada. “The adaptation of an English spellchecker for
Japanese writers”. In: (2007).
[42]
Linda Bebout. “An error analysis of misspellings made by learners of English as
a first and as a second language”.
In:
Journal
of
psycholinguistic research 14.6
(1985), pp. 569–593.
[43]
Thomas K Landauer,
Peter W.
Foltz,
and Darrell
Laham.
“An introduction to
latent semantic analysis”.
In:
Discourse Processes
25.2-3 (1998),
pp.
259–284.
issn: 0163-853X. doi: 10.1080/01638539809545028.
[44]
Yves Bestgen, Guy Lories, and Jennifer Thewissen. “Using latent semantic anal-
ysis to measure coherence in essays by foreign language learners ?” In:
Corpus
(2010), pp. 1–12.
Bibliography
112
[45]
Marlene Scardamalia and Carl Bereiter. “Knowledge telling and knowledge trans-
forming in written composition”. In: Advances in Applied Psycholinguistics: Vol-
ume 2,
Reading,
Writing,
and Language Learning.
Ed.
by Sheldon Rosenberg.
1987.
[46]
Rolf A Zwaan and Gabriel A Radvansky. “Situation models in language compre-
hension and memory.” In: Psychological
bulletin 123.2 (1998), p. 162.
[47]
Danielle S McNamara et al. “Are good texts always better? Interactions of text
coherence,
background knowledge,
and levels of understanding in learning from
text”. In: Cognition and instruction 14.1 (1996), pp. 1–43.
[48]
Inge Bartning and Nathalie Kirchmeyer.
“Le d´eveloppement de la comp´etence
textuelle `a travers les stades acquisitionnels en fran¸cais L2”.
In:
Acquisition et
interaction en langue ´etrang`ere 19 (2003), pp. 9–39.
[49]
Sylviane Granger.
“a bird’s eye view of learner corpus research”.
In:
Computer
Learner Corpora, Second Language Acquisition and Foreign Language Teaching.
Ed. by Sylviane Granger, J. Hung, and S. Petch-Tyson. Amsterdam & Philadel-
phia, 2002, pp. 3–33.
[50]
Malin
˚
A gren.
“
`
A La Recherche De La Morphologie Silencieuse”.
PhD thesis.
Lund, 2008.
[51]
Freiderikos Valetopoulos. personal communication. June 3, 2015.
[52]
Valetopoulos Freiderikos. “La progression de la comp´etence grammaticale : L’ordre
des constituants dans un corpus d’apprenants ´ecrit”. In: Les comp´etences en pro-
gression : Un d´efi pour la didactique des langues. Ed. by Valetopoulos, Freiderikos
and Jolanta Zajac. 2012, pp. 351–365.
[53]
S´ebastien L’Haire.
“Traitement automatique des langues et apprentissage des
langues assist´e par ordinateur: bilan, r´esultats et perspectives”. PhD thesis. Uni-
versity of Geneva, 2011.
[54]
S´ebastien L
0
haire.
“FipsOrtho:
A spell
checker for learners of
French”.
In:
Re-
CALL 19.02 (2007), pp. 137–161.
[55]
Lucile CADET and Fanny RINCK. “APPEL A CONTRIBUTIONS”. In: ().
[56]
F.
Rinck,
M.-P.
Jacques,
and F.
Boch.
LittAvanceWeb.
2015.
url:
http : / /
lidilem.u- grenoble3.fr/ressources/corpus- du- labo/article/corpus-
litteracie-avancee (visited on 06/03/2015).
[57]
Mark D Shermis and Jill Burstein. Handbook of automated essay evaluation: Cur-
rent applications and new directions. Routledge, 2013.
Bibliography
113
[58]
David M Williamson, Isaac I Bejar, and Anne S Hone. “‘Mental Model
0
Comparison
of Automated and Human Scoring”. In: Journal of Educational Measurement 36.2
(1999), pp. 158–184.
[59]
Ellis B Page.
“The imminence of...
grading essays by computer”.
In:
Phi
Delta
Kappan (1966), pp. 238–243.
[60]
Rich Haswell
and Maja Wilson.
Professionals Against Machine Scoring Of Stu-
dent Essays In High-Stakes Assessment. 2013. url: http://humanreaders.org/
petition/ (visited on 06/06/2015).
[61]
Yongwei; Yang et al. “A Review of Strategies for Validating Computer-Automated
Scoring”.
In:
Applied Measurement in Education 15.4 (2010),
pp.
391–412.
doi:
10.1207/S15324818AME1504.
[62]
Mark D.
Shermis and Ben Hamner.
“Contrasting State-of-the-Art Automated
Scoring of Essays”. In: Handbook of Automated Essay Evaluation: Current Appli-
cations and New Directions. Ed. by Mark D. Shermis and Jill Burstein. Routledge,
2013. Chap. 19, pp. 313–346.
[63]
Ellis Batten Page. “Project essay grade: PEG”. In: Automated essay scoring: A
cross-disciplinary perspective (2003), pp. 43–54.
[64]
Measurement Incorporated.
Project
Essay Grade (PEG
R
).
2014.
url:
http :
//www.measurementinc.com/products/peg (visited on 06/06/2015).
[65]
Peter W Foltz, Darrell Laham, and Thomas K Landauer. “The intelligent essay
assessor: Applications to educational technology”. In: ().
[66]
Pearson.
Pearson Assessment.
2015.
url:
http://www.pearsonassessments.
com (visited on 06/06/2015).
[67]
Matthew T. Schultz. “The IntelliMetricTM Automated Essay Scoring Engine - A
Review and an Application to Chinese Essay Scoring”. In: Handbook of Automated
Essay Evaluation:
Current
Applications and New Directions.
Ed.
by Mark D.
Shermis and Jill Burstein. Routledge, 2013. Chap. 6, pp. 89–98.
[68]
Vantage Learning.
2015.
url:
http://www.vantagelearning.com/products/
intellimetric/ (visited on 06/06/2015).
[69]
Yigal
Attali
and Jill
Burstein.
“Automated essay scoring with e-rater
R
V.
2”.
In: The Journal
of Technology, Learning and Assessment 4.3 (2006).
[70]
ETS.
About the e-rater
R
Scoring Engine.
2015.
url:
https://www.ets.org/
erater/about (visited on 06/06/2015).
[71]
Lm Rudner and Tahung Liang. “Automated essay scoring using Bayes’ theorem”.
In: The Journal of Technology, Learning and . . .
1.2 (2002). issn: 1540-2525. url:
http://napoleon.bc.edu/ojs/index.php/jtla/article/view/1668.
Bibliography
114
[72]
Lawrence M Rudner,
Graduate Management,
and Admission Council.
“Scoring
and Classifying Examinees Using Measurement Decision Theory”.
In:
Practical
Assessment, Research & Evaluation 14.8 (2009), pp. 1–12. issn: 15317714.
[73]
Robert Williams and Heinz Dreher. “Automatically Grading Essays with Markit
c
”.
In:
Issues in Informing Science and Information Technology 1 (2004),
pp.
693–
700.
[74]
Robert Francis Williams.
“The theory,
design,
development and evaluation of
the MarkIT automated essay grading system”.
PhD thesis.
Curtin University
Australia, 2011.
[75]
Blue Wren. Blue Wren. 2015. url: http://www.essaygrading.com/ (visited on
06/07/2015).
[76]
Elija Mayfield and Carolyn Penstein Ros´e.
“LightSIDE Open Source Machine
Learning for Text”.
In:
Handbook of
Automated Essay Evaluation:
Current Ap-
plications and New Directions. Ed. by Mark D. Shermis and Jill Burstein. Rout-
ledge, 2013. Chap. 8, pp. 124–135.
[77]
TurnItIn.
LigntSide Revision Assistant.
2015.
url:
http : / / lightsidelabs .
com/what/research/ (visited on 06/07/2015).
[78]
Philip Gianfortoni, David Adamson, and Carolyn P. Rose. “Modeling of stylistic
variation in social media with stretchy patterns”. In: DIALECTS ’11 Proceedings
of the First Workshop on Algorithms and Resources for Modelling of Dialects and
Language Varieties (2011), pp. 49–59. url: http://dl.acm.org/citation.cfm?
id=2140533.2140539.
[79]
CTB McGraw Hill.
Bookette.
2015.
url:
http : / / www . ctb . com / ctb . com /
control/servicesDetailAction?serviceId=23556&p=services (visited on
06/07/2015).
[80]
Lexile.
The Lexile framework for reading.
2015.
url:
https : / / lexile . com/
(visited on 06/07/2015).
[81]
Colleen Lennon and Hal Burdick. the Lexile Framework As an Approach for Read-
ing.
Tech.
rep.
April.
2004.
url:
https : / / cdn . lexile . com / m / resources /
materials/Lennon__Burdick_2004.pdf.
[82]
Malbert III Smith. The Reading-Writing Connection. Tech. rep. 2007. url: https:
//d1jt5u2s0h3gkt.cloudfront.net/m/uploads/positionpapers/TheReading-
WritingConnection.pdf.
[83]
Pacific Metrics. CRASE. 2015. url: http://www.pacificmetrics.com/ (visited
on 06/07/2015).
Bibliography
115
[84]
Mark D.
Shermis.
“State-of-the-art automated essay scoring:
Competition,
re-
sults,
and future directions from a United States demonstration”.
In:
Assessing
Writing 20 (2014),
pp.
53–76.
issn:
10752935.
doi:
10 . 1016 / j . asw . 2013 .
04 . 001.
url:
http : / / www . sciencedirect . com / science / article / pii /
S1075293513000196.
[85]
Les Perelman.
“When ”the state of
the art” is counting words”.
In:
Assessing
Writing 21 (2014), pp. 104–111. issn: 10752935. doi: 10.1016/j.asw.2014.05.
001. url: http://dx.doi.org/10.1016/j.asw.2014.05.001.
[86]
Mark D Shermis.
“The challenges of
emulating human behavior in writing as-
sessment”.
In:
Assessing Writing 22 (2014),
pp.
91–99.
issn:
1075-2935.
doi:
10.1016/j.asw.2014.07.002.
url:
http://dx.doi.org/10.1016/j.asw.
2014.07.002.
[87]
Doug McCurry. “Can machine scoring deal with broad and open writing tests as
well
as human readers?” In:
Assessing Writing 15.2 (2010),
pp.
118–129.
issn:
10752935.
doi:
10.1016/j.asw.2010.04.002.
url:
http://dx.doi.org/10.
1016/j.asw.2010.04.002.
[88]
Marie Stevenson and Aek Phakiti. “The effects of computer-generated feedback
on the quality of
writing”.
In:
Assessing Writing 19 (2014),
pp.
51–65.
issn:
10752935.
doi:
10.1016/j.asw.2013.11.007.
url:
http://dx.doi.org/10.
1016/j.asw.2013.11.007.
[89]
Elena Cotos. “Automated Writing Evaluation for non-native speaker English aca-
demic writing : The case of IADE and its formative feedback”. PhD thesis. Iowa
State University, 2010.
[90]
Elena Cotos. Genre-based Automated Writing Evaluation for L2 Research Writ-
ing: From Design to Evaluation and Enhancement. Palgrave Macmillan, 2014.
[91]
Mark Warschauer and Paige Ware. “Automated writing evaluation: defining the
classroom research agenda”. In: Language Teaching Research 10.2 (2006), pp. 157–
180. issn: 13621688. doi: 10.1191/1362168806lr190oa.
[92]
Trude Heift.
“Corrective feedback and learner uptake in CALL”.
In:
ReCALL
16.02 (2004), pp. 416–431. issn: 0958-3440. doi: 10.1017/S0958344004001120.
[93]
Jonas Granfeldt,
Pierre Nugues,
and Emil
Persson.
“Direkt Profil:
un syst`eme
d
0
´evaluation de textes d
0
´el`eves de fran¸
cais langue ´etrang`ere fond´e sur les itin´eraires
d’acquisition”.
In:
Conf´erence Traitement Automatique des Langues Naturelles.
Vol. 2005. Association pour le traitement automatique des langues. 2005, pp. 113–
122.
Bibliography
116
[94]
Jonas Granfeldt and Pierre Nugues. “CEFLE and Direkt Profil: A new computer
learner corpus in French L2 and a system for grammatical profiling”. In: LREC.
2006, pp. 565–570.
[95]
Lund university Department of Romance languages and Lund Institute of Tech-
nology the Department of
computer science.
Direkt
Profil
2.2.
url:
http : / /
profil.sol.lu.se:8080/profil/about.do.
[96]
Sebastien L’Haire and Anne Vandeventer Faltin. “Error diagnosis in the FreeText
project”. In: Calico Journal
20.3 (2003), pp. 481–495.
[97]
S´ebastien L’haire and Anne Vandeventer Faltin.
“Diagnostic d’erreurs dans le
projet FreeText”. In: Apprentissage des Langues et Syst`emes d’Information et de
Communication 6.2 (2003), pp. 21–37.
[98]
Sylviane Granger.
“Error-tagged learner corpora and CALL:
A promising syn-
ergy”. In: CALICO journal
20.3 (2013), pp. 465–480.
[99]
Wajdi
Zaghouani.
“Auto-´eval
:
vers un mod`ele d’´evaluation automatique des
textes”. In: CESLA colloquium. Montr´eal: Universit´e du Qu´ebec `a Montr´eal, 2002.
[100]
Suzanne Bertrand-Gastaldya et al.
“LE TRAITEMENT DES TEXTES PRI-
MAIRES ET SECONDAIRES POUR LA CONCEPTION ET LE FONCTION-
NEMENT D’UN PROTOTYPE DE SYST
`
EME EXPERT D
0
AIDE
`
A L
0
ANALYSE
DES JUGEMENTS”. In: Actes du colloque Traitement automatique du fran¸cais
´ecrit:
d´eveloppements th´eoriques et
applications.
62e congr`es de l’Acfas.
1996,
pp. 241–276.
[101]
Danielle S McNamara et al.
Automated evaluation of
text
and discourse with
Coh-Metrix. Cambridge University Press, 2014.
[102]
Coh-Metrix. url: http://cohmetrix.com/.
[103]
Coh-Metrix Text Easability Assessor. url: http://tea.cohmetrix.com/.
[104]
Benoˆıt Sagot.
“The Lefff,
a freely available and large-coverage morphological
and syntactic lexicon for French”.
In:
7th international
conference on Language
Resources and Evaluation (LREC 2010). 2010.
[105]
Benoˆıt Sagot and Pierre Boullier.
“SxPipe 2:
architecture pour le traitement
pr´e-syntaxique de corpus bruts”.
In:
Traitement Automatique des Langues 49.2
(2008), pp. 155–188.
[106]
Pascal
Denis and Benoˆıt Sagot.
“Coupling an annotated corpus and a lexicon
for state-of-the-art POS tagging”.
In:
Language Resources and Evaluation 46.4
(2012),
pp.
721–736.
doi:
10.1007/s10579-012-9193-0.
url:
https://hal.
inria.fr/inria-00614819.
[107]
Alpage Linguistic Workbench. MElt. url: http://lingwb.gforge.inria.fr/.
Bibliography
117
[108]
Benoit Crabb´e and Marie Candito. “Exp´eriences d’analyse syntaxique statistique
du fran¸cais”.
In:
15`eme conf´erence sur le Traitement Automatique des Langues
Naturelles-TALN’08. 2008, pp–44.
[109]
Anne Abeill´e, Lionel Cl´ement, and Fran¸cois Toussenel. “Building a treebank for
French”. In: Treebanks. Springer, 2003, pp. 165–187.
[110]
´
Eric De La Clergerie et al.
“FRMG: ´evolutions d’un analyseur syntaxique TAG
du fran¸cais”. In: Journ´ee de l’ATALA sur: Quels analyseurs syntaxiques pour le
fran¸cais? 2009.
[111]
Marie-H´el`ene Candito. “Organisation modulaire et param´etrable de grammaires
´electroniques lexicalis´ees”. In: ().
[112]
´
Eric Villemonte De La Clergerie.
“Acquisition et validation de connaissances `a
partir de sorties syntaxiques”.
In:
(2015).
url:
http : / / alpage . inria . fr /
~
clerger/SLIDES/LIRMM15.pdf.
[113]
Alpage.
FRMGWiki.
2014.
url:
http://alpage.inria.fr/frmgwiki/wiki/
performances-de-frmg (visited on 05/30/2015).
[114]
Radim
ˇ
Reh˚uˇrek and Petr Sojka. “Software Framework for Topic Modelling with
Large Corpora”.
English.
In:
Proceedings of
the LREC 2010 Workshop on New
Challenges for NLP Frameworks.
http://is.muni.cz/publication/884893/
en. Valletta, Malta: ELRA, May 22, 2010, pp. 45–50.
[115]
Tomas Mikolov et al.
“Efficient Estimation of
Word Representations in Vector
Space”.
In:
CoRR abs/1301.3781 (2013).
url:
http://arxiv.org/abs/1301.
3781.
[116]
Sepp Hochreiter. “Untersuchungen zu dynamischen neuronalen Netzen”. In: Mas-
ter’s thesis, Institut fur Informatik, Technische Universitat, Munchen (1991).
[117]
Sepp Hochreiter and J¨
urgen Schmidhuber. “Long short-term memory”. In: Neural
computation 9.8 (1997), pp. 1735–1780.
[118]
Daniel
Soutner and Ludˇek M¨
uller.
“Continuous Distributed Representations of
Words as Input of LSTM Network Language Model”. In: ().
[119]
Tomas
Mikolov.
Google Groups post.
Oct.
7,
2013.
url:
https : / / groups .
google . com / forum / # ! searchin / word2vec - toolkit / c - bow / word2vec -
toolkit/NLvYXU99cAM/E5ld8LcDxlAJ (visited on 05/24/2015).
[120]
Tomas Mikolov et al. “Distributed representations of words and phrases and their
compositionality”. In: Advances in Neural Information Processing Systems. 2013,
pp. 3111–3119.
[121]
Ronan Collobert et al. “Natural language processing (almost) from scratch”. In:
The Journal
of Machine Learning Research 12 (2011), pp. 2493–2537.
Bibliography
118
[122]
Boris New. “Lexique 3: Une nouvelle base de donn´ees lexicales”. In: Actes de la
Conf´erence Traitement Automatique des Langues Naturelles (TALN 2006). 2006.
[123]
Boris New and Christophe Pallier.
Manuel
de Lexique 3.
Tech.
rep.
2005.
url:
http://www.lexique.org/docLexique.php (visited on 05/24/2015).
[124]
Christophe Pallier.
Syllabation des repr´esentations phon´etiques de Brulex et de
Lexique. Tech. rep. 2004. url: http://www.pallier.org/ressources/syllabif/
syllabation.pdf (visited on 05/24/2015).
[125]
Sophie Dufour et al. “VoCoLex: Une base de donn´ees lexicales sur les similarit´es
phonologiques entre les mots fran¸cais”. In: L’Ann´ee psychologique 102.4 (2002),
pp. 725–745.
[126]
Fiammetta Namer. Morphologie, Lexique et Traitement Automatique des Langues.
TIC et sciences cognitives. ISBN 978-2-7462-2363-9. Herm`es-Lavoisier, 2009, p. 444.
url: https://hal.archives-ouvertes.fr/hal-00413337.
[127]
Fiammetta Namer.
D´eriF (D´erivation en Fran¸cais).
2012.
url:
http://www.
cnrtl.fr/outils/DeriF/ (visited on 05/24/2015).
[128]
Aris Xanthos. Lingua::Diversity::VOCD. 2011. url: http://search.cpan.org/
~
axanthos/Lingua- Diversity/lib/Lingua/Diversity/VOCD.pm (visited on
06/15/2015).
[129]
G McKee,
D Malvern,
and B Richards.
“Measuring vocabulary diversity using
dedicated software”. In: Literary and Linguistic Computing 15.3 (2000), pp. 323–
338.
issn:
0268-1145.
doi:
10 . 1093 / llc / 15 . 3 . 323.
url:
http : / / llc .
oxfordjournals.org/content/15/3/323.abstract.
