Vol.:(0123456789)
1 3
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531 
DOI 10.1007/s13042-017-0661-0
ORIGINAL ARTICLE
A hybrid-based method for Chinese domain lightweight ontology 
construction
Jing Qiu
1
· Lin Qi
1
· Jianliang Wang
1
· Guanghua Zhang
1
Received: 16 July 2016 / Accepted: 11 March 2017 / Published online: 28 March 2017 
© Springer-Verlag Berlin Heidelberg 2017
to extract taxonomic relationships and refine the taxonomy. 
Finally, we evaluate our method using gold-standard evalu-
ation on domain of football games. In our evaluation, we 
compare our method with several classical algorithms. The 
experimental results show the effectiveness of our method.
Keywords 
Ontology learning · Concept extraction · 
Taxonomic relationships extraction · Hybrid-based method
1 Introduction
Ontology is defined as an explicit specification of a con-
ceptualization, and is consisted of concepts, relations, axi-
oms which describe a certain domain [21]. As the “back-
bone” of the Semantic Web [2], domain ontology acts as 
fundamental component for sharing and reasoning over 
knowledge domains, and plays an important role in tasks 
of information retrieval [23], question answering [5, 14], 
knowledge searching and classification [34, 64]. At the 
same time, it becomes more and more helpful for bioinfor-
matics [36].
However, the process of ontology construction is not 
trivial. It is time and cost consuming to construct an 
ontology manually. To reduce these costs, methods in 
the fields of Natural Language Processing (NLP) and 
machine learning (ML) are often used to making the pro-
cess more automatic. And the corresponding research 
area is called ontology learning (OL) which aims to auto-
matically or semi-automatically create ontology. Differ-
ent data sources can be used for ontology learning, such 
as structured data, semi-structured data, non-structured 
data, and even existing dictionaries. Over the years, with 
the development of Internet, it becomes easy to capture 
a huge amount of digital textual documents. Ontology 
Abstract 
This paper proposes a framework to automati-
cally construct lightweight ontology from a corpus of Chi-
nese domain Web documents. A hybrid-based method was 
used for domain lightweight ontology learning. Rule-based 
method, statistics-based method and cluster-based method 
were combined to complete two sub-tasks: concept extrac-
tion and taxonomic relationships extraction. Firstly, multi-
word terms were identified based on a set of rules as well 
as a Named Entity Module. Three statistic methods were 
employed jointly to rank the order of domain concepts. 
Secondly, clustering and subsumption methods were joined 
to construct taxonomy. Concepts were clustered into sev-
eral groups through clustering method. Three similarity 
measures were defined to compute similarities between 
concepts, which aims at capturing semantic, spatial, and co-
occurrence information. Subsumption method was adopted 
to construct taxonomic structure for each concept group, 
since taxonomic relations only existed between similar con-
cepts. Thirdly, the definitions of the concepts extracted in 
the first step are collected from online Chinese Encyclope-
dia. On this collection of concept definitions, the rule-based 
method and a set of lexico-syntactic patterns were applied 
* 
Jing Qiu 
qiujing.ch@gmail.com
Lin Qi 
qilin_126@126.com
Jianliang Wang 
wjlheb@126.com
Guanghua Zhang 
xian_software@163.com
1
Department of Information Science and Engineering, Hebei 
University of Science and Technology, Shijiazhuang 050018, 
China
1520
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531
1 3
learning from text is becoming one of the most promising 
research area. In particular, text mining and Information 
Extraction (IE) techniques, and NLP tools can be applied 
to the documents analysis, which is helpful for concepts 
and relationships extraction. Natural language texts con-
tain rich semantic information, the contexts surrounding 
the concepts are often considered to improve the perfor-
mances of concepts and relationships extraction by using 
syntactic patterns and grammatical classes.
There are already many ontology learning tools and 
systems available now [12, 33, 38, 47, 49, 59]. IE tech-
niques, ML techniques, and NLP techniques are often 
adopted 
combinedly 
to 
solve 
ontology 
learning 
prob-
lems. All the methods can be classified into three types: 
statistics-based 
method, 
linguistic-based 
method, 
and 
hybrid-based method. Statistic methods, such as cluster-
ing and Latent Semantic Analysis, are often exployed to 
extract concepts as well as taxonomic relationships [6]. 
Linguistic-based methods can be used in all sub-tasks of 
OL [13], since Part of Speech (POS) tagging and syntac-
tic structure analysis are the fundamental works of docu-
ments processing. In recent years, Hybrid-based methods 
are widely used in OL, particular in the complex tasks 
such as non-taxonomic relationships extraction and axi-
oms learning [19, 62, 63].
In this paper, we propose a new hybrid-based method 
for lightweight ontology construction from Chinese domain 
Web news documents. Lightweight domain ontology con-
tains concepts and taxonomic relationships in a specific 
domain. Three sub-sequential processes are included: (1) 
multiword terms identification (2) domain concepts filter-
ing (3) taxonomic relationships extraction. Statistic meth-
ods, rules-based methods, clustering methods are combin-
edly used to learn Chinese domain lightweight ontology.
The contributions presented in this paper are: (1) Rule-
based method and varied statistical methods were com-
bined to selected domain terms, and to extract concepts. 
Three statistic methods were combined to rank the order 
of the domain concepts. (2) Clustering and subsumption 
algorithm were combined to filter taxonomic relationships. 
Three knowledge components were combined to qualify 
the concept clustering: vector-based similarity, co-occur-
rence-based 
similarity, 
and 
Word2Vec-based 
similarity. 
Subsumption method is only used for taxonomy construc-
tion, not for taxonomy identification. Taxonomy structures 
are built for each cluster, but not for all concepts. (3) A 
rule-based method was employed on online Chinese Ency-
clopedia corpus for taxonomy refinement.
The rest of the paper is organized as follows. Section 2 
reviews related work in the area of OL. Section 3 intro-
duces the workflow of our method and its implementation. 
Then, Sect. 4 evaluates the performance of our method, 
which includes the performance of concept extraction and 
taxonomic 
relationships 
extraction. 
Finally, 
concluding 
remarks and future work are given in Sect. 5.
2 Related work
2.1 Term identification and concept filtering techniques
For 
domain 
ontology 
concept 
extraction, 
most 
of 
the 
research works have two steps. First, possible candidate 
terms are identified from texts. Second, domain concepts 
are filtered from candidate terms with a more complex 
method. Methods for this field generally fall into three cate-
gories: rule-based methods, statistical methods, and hybrid 
methods.
Rule-based 
methods 
require 
experts 
to 
define 
hand-
coded rules or syntactic patterns for the extraction task. 
This category method is based on NLP tools, such as POS 
tagging [3] and syntactic structure analysis [24]. Lexico-
syntactic patterns [7, 22] are often used for concept extrac-
tion. Rule-based methods can always obtain high preci-
sions, however, the shortcomings are also obvious: the 
recall rates are not good enough; the rules cannot fit for 
different domains; and the methods are time-consuming 
because rules have to be created by hand.
Statistical 
methods 
use 
probabilistic 
techniques 
to 
extract terms and concepts. A classic statistical method 
is 
Term 
Frequency 
Inverse 
Document 
Frequency 
(TF-
IDF) [45] which computes the weights for terms in the 
document. The more important terms will receive higher 
weights. 
Domain 
Relevance 
and 
Domain 
Consensus 
(DR&DC) are used in Ontolearn system for filtering “true” 
terminology [37]. Colace et al. [13] extracted terms based 
on the Latent Dirichlet Allocation (LDA) model. Statistical 
machine learning methods are often used for domain con-
cept extraction, and these methods are belonged to IE tech-
niques. Lv et al. [32] proposed an instance-based transfer 
learning method for domain concept extraction. Maximum 
Entropy Model (MEM), Support Vector Machine (SVM), 
and Conditional Random Fields (CRF) are combined to 
extract ten types of clinical entities from text documents 
[61]. CRF and SVM are used separately for clinical named 
entity extraction to show the different performances of 
these two machine learning methods [29]. However, most 
of the machine learning methods are based on labeled cor-
pus, which is not fit for Web document sets.
Hybrid 
methods 
combine 
rules-based 
and 
statistical 
methods together for term and concept extraction. Fortuna 
et al. [20] proposed a method named OntoTermExtraction 
for semi-automated ontology construction. The documents 
were clustered together, then terms were extracted from 
document clusters. Concepts were extracted after the term 
populating and keyword extraction. Sclano and Velardi 
1521
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531 
1 3
[48] designed a high-performance technique to extract the 
shared terminology from domain documents. Experimen-
tal results show its usefulness by test it in several domains. 
In the research work of Doing-Harris et al. [16], the term 
extraction is based on domain dictionary, and the synonym 
extraction combined the TFIDF and pattern-based method. 
In the work of Knijff et al. [26], terms were extracted using 
POS parser, then domain concepts were filtered out using 
several statistical based filters.
2.2 Taxonomic relationships extraction techniques
Techniques 
used 
for 
taxonomic 
relationships 
extraction 
can be classified into three categories: the methods based 
on linguistic approaches, the methods based on lexical pat-
terns, and the methods based on clustering and statistic.
The linguistic approach build concept taxonomy based 
on the properties of the language, however, it is difficult to 
use this approach for different domains [18, 57].
Lexical pattern based methods extract taxonomic rela-
tions for concepts based on patterns which are manually 
predefined (e.g., ‘such as’ and ‘is a’). One of the first and 
most influential approaches is proposed by Hearst [22], 
more patterns are learned from some lexico-syntactic pat-
terns predefined with bootstrapping algorithm. In Suchanek 
et al. [53], deep linguistic structures are used instead of 
surface text patterns for concept relations extraction, and 
a theoretical analysis of the pattern matching approach is 
provided as well. In Ponzetto and Strube [41], a concep-
tual network is built based on category system in Wikipe-
dia. Concept relations are extracted based on connectivity 
in the network and lexical patterns. In Rios-Alvarado et al. 
[43], lexical patterns, clustering algorithm and contextual 
information extracted from the web are combined together 
to extract concepts and their taxonomic relationships from 
unstructured texts. In Pennacchiotti and Pantel [40], lexical 
patterns are learned based on a small set of seed instances, 
and a weakly-supervised iterative algorithm is used for 
semantic relation extraction. A novel lexico-syntactic pat-
tern probabilistic methods is proposed in Fallucchi and 
Zanzotto [17], two probabilistic models are defined for 
taxonomy learning. In Specia and Motta [52], a pipeline 
method is used for relation extraction, and the results show 
the usefulness of knowledge intensive strategies. In Ciara-
mita et al. [10], an unsupervised model is proposed for 
learning arbitrary relations between concepts.
Method 
based 
on 
clustering 
and 
statistic 
is 
often 
achieved based on IR and ML for taxonomy construction. 
In Cimiano et al. [11], a new method is proposed based 
on Formal Concept Analysis, and the vector space model 
is used to model the context of a certain term. In Snow 
et al. [51], a clustering approach is proposed for taxonomy 
learning, and the evidence from multiple classifiers is used 
to optimize the entire taxonomy structure. In Snchez and 
Moreno [50], an automatic and unsupervised method is 
used to obtain taxonomic relationships. Search engines are 
used to retrieve domain documents from web. In Koza-
reva et al. [28], a weakly supervised bootstrapping algo-
rithm, starting with two seed words, is proposed for tax-
onomy learning. In Velardi et al. [56], a semi-automated 
strategy 
is 
proposed 
to 
extract 
taxonomic 
relationships 
from domain web documents. In another study, a Bayesian 
approach is proposed to build taxonomy for a set of key-
word phrases [31]. In Dietz et al. [15], a semantic-based 
hierarchical clustering is used for taxonomy learning. An 
approach based on probabilistic topic models is proposed 
for automatic learning of terminological ontology [60]. In 
Knijff et al. [26], a framework is proposed to automatically 
construct taxonomies from text documents. Subsumption 
method 
and 
hierarchical 
clustering 
algorithm 
are 
com-
bined for taxonomy learning. In Paukkeri et al. [39], a new 
method is proposed for taxonomy learning. The hierarchi-
cal structure is obtained by clustering the concept definition 
documents with a hierarchical approach to the Self-Organ-
izing Map. In Kozareva and Hovy [27], a graph based algo-
rithm is proposed for taxonomy learning, and the experi-
mental results are compared with WordNet. In another 
study, a new method named OntoLearn Reloaded is pro-
posed for taxonomy learning, and graph based algorithm is 
also used in this work [58]. In Kang et al. [25], a different 
graph-based approach is proposed, it measures the associa-
tive strength between concepts using the combination of 
co-occurrence of concepts, the distance between sentences, 
and the semantic similarity between those sentences.
Extracting semantic relations between entities is an 
important task in Information Extraction (IE). Machine 
learning techniques are often used in this task. Zelenko 
et al. [66] proposed extracting relations by computing 
kernel functions between shallow parse trees. Bunescu 
and Mooney [8] proposed a shortest path dependency 
kernel which uses an even smaller part of the dependency 
structure to increase the performance of the system. Zhou 
et al. [68] also presented a feature-based approach, and 
they incorporated the base phrase chunking information 
to improve the effectiveness of relation extraction while 
using parse tree features. Zhang [67] proposed a compos-
ite kernel to effectively capture both flat and structured 
features for relation extraction. However, these works are 
all based on supervised methods which require a large 
among of manually labeled relation instances which is 
not always easy to acquire. In order to minimize the huge 
manual effort, researchers have been investigating unsu-
pervised 
(or 
weakly 
supervised) 
learning 
algorithms. 
Such as Active Learning [55], Bootstrapping algorithm 
[1], co-training algorithm [4], and so on. Recently, with 
the development of deep learning, many studies employ 
1522
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531
1 3
automatic feature learning via neural network (NN) based 
models for relation extraction. Such as CNN-based mod-
els [46], and RNN-based models [54]. Xu et al. [65] pro-
posed a novel neural network SDP-LSTM which leverage 
the shortest dependency path for relation classification. 
Li et al. [30] compared separate sequence-based and tree-
structured LSTM-RNNs on relation classification to show 
when and why recursive models can outperform simpler 
models.
3 The lightweight ontology learning workflow
With the development of Internet, it is easy to obtain a 
large amount of domain text documents online. For the 
data set preparation stage, several different domain Web 
document sets were created for ontology learning, such 
as football games field, military field, and culture field. In 
this paper, lightweight ontology is constructed on football 
games domain. Therefore, football games is the target 
domain, and others are merged as background domain. 
Our Chinese domain lightweight ontology learning work-
flow consists of two steps:
1. 
Domain Concept Extraction In order to ensure the 
quality of concepts that are extracted, multiword terms 
must be identified firstly. POS parser combined with 
rule-based method is used for multi-word terms identi-
fication.
Then three statistic methods are employed jointly for 
domain concept extraction: TF-IDF, improved DR&DC, 
and Log-likelihood ratio (LLR). Statistical methods are 
good for finding the importance of a word, which is fit for 
this task.
2. 
Concept taxonomy Construction Hierarchical cluster-
ing and subsumption are combined for taxonomic rela-
tionships extraction. Firstly, concepts are clustered into 
several groups depend on the similarities between con-
cepts. Three similarity calculation methods are defined 
and joined to compute the similarities between con-
cepts. Then subsumption method is used to build tax-
onomy for each cluster. We believe not all the concepts 
have taxonomic relationships, and taxonomic relation-
ships only exist in similar concepts. Finally, rule-based 
method is used to refine taxonomic structure, since 
rule-based method has high extraction precision. Web 
semantic data source Baidu Encyclopedia which is the 
biggest Chinese Encyclopedia is used in this phase to 
provide more semantic information.
3.1 Domain concepts extraction
Statistical or probabilistic methods are generally good for 
finding the importance of a word. TF-IDF, a popular sta-
tistical algorithm, is often used to extract domain terms 
and concepts. However, TF-IDF is designed primarily for 
document retrieval, and it can only compute values for 
single-word terms. Therefore, for better use of this type 
of method and enrich the learned ontology, linguistic fil-
ters are often defined to extract multiword terms firstly.
In this paper, a set of POS tag based rules are defined 
for multiword terms extraction. Then TF-IDF, improved 
DR&DC, and LLR method are used to rank the order 
of candidate domain terms. Only the top 
k
terms are 
extracted as the domain concepts.
3.1.1 Rule‑based multiword terms extraction
The process of pretreatment includes Chinese word seg-
mentation, POS tagging, and stop words deletion. Differ-
ent from English text analysis, word segmentation is the 
first step for Chinese text analysis. Because there is no 
obvious mark between Chinese words. In this paper, Lan-
guage Technology Platform (LTP)
1
is used for Chinese 
text analysis [9]. LTP provides several core functions of 
Chinese text analysis, such as word segmentation, POS 
tagging, 
dependency 
parsing, 
semantic 
role 
labeling, 
named entity recognition, and so on.
Named entities can be easily extracted with the use of 
LTP Named Entity Module. Other multiword terms are 
identified by the following POS tag based rules.
1. 
General noun sequence (n*)
2. 
Other noun-modifier + location noun (b + nl)
3. 
Other proper noun + general noun sequence (nz + n*)
4. 
Organization name + person name (ni + nh)
5. 
Abbreviation + general noun sequence (j + n*)
6. 
Geographical name + other proper noun (ns + nz)
7. 
Geographical name + general noun sequence (ns + n*)
8. 
Direction noun + general noun sequence (nd + n*)
9. 
Verb + general noun sequence (v + n*)
10. 
Verb + direction noun (v + nd)
After using the above rules, a set of multiword terms 
can be identified. All these multiword terms are added 
into dictionary to do segmentation again. Then three sta-
tistical methods are used to extract domain concepts, and 
1
LTP is provided by Research Center for Social Computing and 
Information Retrieval, Harbin Institute of Technology.
1523
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531 
1 3
only nouns and above multiword terms are considered as 
the candidates.
3.1.2 Statistic‑based concept extraction
TF-IDF and DR&DC method are often used to find high 
relevance domain terms. TF-IDF is a traditional statisti-
cal method, which is applied to evaluate the importance 
of words. The TF-IDF value is higher for terms that with 
high frequency in one document, but with lower frequency 
in multiple documents. The higher value indicates the cor-
responding term is more important in this domain. For term 
t
in a document set 
D
, the value of Term Frequency (
TF
), 
Inverse Document Frequency (
IDF
), and final 
TFIDF
are 
computed as follows.
TF-IDF is designed for IR field, it can’t give a high value 
for domain concept which maybe appears in almost every 
document. To overcome this deficiency, other domain doc-
ument sets are merged as background corpus.
In 
the 
OntoLearn 
system, 
DR&DC 
method 
is 
used 
to 
identify 
domain 
concepts 
[37]. 
Given 
a 
corpus 
D = { D
1
,D
2
, ⋯ , D
n
}
, 
where 
D
i
= { d
1
,d
2
, ⋯ , d
m
}
is 
the 
ith
particular domain document set, 
d
j
is the 
jth
document 
in 
D
i
. The value of Domain Relevance (
DR
) and Domain 
Consensus (
DC
) of a term 
t
in domain 
D
i
are computed as 
follows.
where 
ft(t ∈ D
i
)
is the frequency of term 
t
appears in 
domain 
D
i
.
The original DR&DC only considers the term’s fre-
quency in certain domain, however, the domain con-
cepts 
should 
not 
only 
have 
high 
frequency, 
but 
also 
should widely appear in domain documents. Also, there 
will obtain an unreasonable conclusion by using above 
DC formula. When two terms appear in the same count 
(1)
TF(t) =
number of term t in D
number of terms in D
,
(2)
IDF(t) = log
2
the size of D
number of documents where t appears in
,
(3)
TFIDF(t) = TF(t) × IDF(t).
(4)
DR(t, D
i
) =
ft(t ∈ D
i
)
∑
n
i=1
ft(t ∈ D
i
)
,
(5)
DC(t, D
i
) =
∑
d
j
∈D
i
p(t, d
j
) × log
2
(
1
p(t, d
j
)
),
(6)
p(t, d
j
) =
ft(t ∈ d
j
)
∑
d
j
∈D
i
ft(t ∈ d
j
)
.
of documents with the same frequency rate, they will 
obtain the same DC value. For example, term 
t
1
appears 
in two documents with frequency 1 and 3, respectively. 
Term 
t
2
appears in two documents with frequency 2 and 
6, respectively. Using the original DC formula, we’ll find 
DC(t
1
) = DC(t
2
)
. This conclusion is obviously not appro-
priate, since term 
t
2
has higher frequency should obtain 
higher value.
In this paper, we design a new DR&DC formula called 
DRd&DCd by considering the count of documents that 
term 
t
appears in domain 
D
i
, besides the frequency of 
term 
t
in domain 
D
i
.
where 
fd(t ∈ D
i
)
is the count of documents that term 
t
appears in domain 
D
i
. 
|D
i
|
is the size of domain 
D
i
.
where 
ft(t ∈ d
j
)
is the frequency of term 
t
appears in docu-
ment 
d
j
= { t
1
,t
2
, ⋯ , t
l
}
.
The DRd&DCd value 
TW
for term 
t
is a linear combi-
nation of DRd and DCd.
Likelihood ratio is often used to verify hypothesis by 
comparing the probabilities of two hypotheses. To extract 
domain concepts, we divided corpus 
D
into two groups. 
One is target domain document set 
D
t
, the other is back-
ground document set 
B
which is consisted by the rest of 
domain document sets. Two hypotheses is that: if term 
t
is a concept of 
D
t
, then the probabilities of 
t
in 
D
t
and 
B
are different, since domain concepts will have higher fre-
quencies in domain document set than background docu-
ment set; if term 
t
is not a concept of 
D
t
, then the prob-
abilities of 
t
in 
D
t
and 
B
have the same value.
Hypothesis H
1
: 
p
D
t
= p(t|D
t
) p
B
= (t|B) p
D
t
≠ p
B
.
Hypothesis H
2
: 
p
D
t
= p(t|D
t
) = p
t
= p
B
= (t|B).
Depend on binomial distribution, the probabilities of 
H
1
and H
2
can be written as:
where 
N
t_D
t
and 
N
t_B
are the frequencies of 
t
in 
D
t
and 
B
, 
N
D
t
and 
N
B
are the total number of terms in 
D
t
and 
B
, 
(7)
DRd(t, D
i
) =
ft(t ∈ D
i
)
∑
n
i=1
ft(t ∈ D
i
)
×
fd(t ∈ D
i
)
�D
i
�
,
(8)
DCd(t, D
i
) =
∑
d
j
∈D
i
p(t, d
j
) × log
2
(
1
p(t, d
j
)
) ×
fd(t ∈ D
i
)
|D
i
|
,
(9)
p(t, d
j
) =
ft(t ∈ d
j
)
max
1⩽k⩽l
ft(t
k
∈ d
j
)
,
(10)
TW = 𝛼 × DRd + 𝛽 × DCd.
(11)
H
1
= p
D
t
N
t_D
t
(1 − p
D
t
)
N
D
t
−N
t_D
t
p
B
N
t_B
(1 − p
B
)
N
B
−N
t_B
,
(12)
H
2
= p
t
N
t_D
t
(1 − p
t
)
N
D
t
−N
t_D
t
p
t
N
t_B
(1 − p
t
)
N
B
−N
t_B
,
1524
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531
1 3
p
D
t
=
N
t_D
t
N
D
t
, 
p
B
=
N
t_B
N
B
, and 
p
t
=
N
t_D
t
+N
t_B
N
D
t
+N
B
. The likelihood 
ratio is computed by the following formula.
The above formulas are based on term’s occurrence 
frequency in document set. However, as we mentioned 
before, domain concept extraction should not only depend 
on occurrence frequency, but also depend on occurrence 
breadth. The formulas based on term’s occurrence breadth 
are written as:
where 
p
D
t_b
=
fd(t∈D
t
)
|D
t
|
, 
p
B_b
=
fd(t∈B)
|D
B
|
, 
and 
p
t_b
=
fd(t∈D
t
)+fd(t∈B)
|D
t
|+|B|
.
The corresponding log likelihood ratio formulas are pre-
sented as follows.
Final 
LLR
value 
is 
the 
average 
of 
LLR
f
(t|D
t
)
and 
LLR
b
(t|D
t
)
.
Terms with high 
LLR
values tend to have higher capabil-
ity to separate the corpus 
D
t
and 
B
. However, 
LLR
value 
cannot be used only to determine which term is concept 
of domain 
D
t
, since this high 
LLR
value term maybe one 
of the keywords in 
B
. Finally, we combine the TF-IDF, 
DRd&DCd and 
LLR
together to get the score for term 
t
. 
Because only the terms that have high 
LLR
value, and high 
relevance with domain 
D
t
will act as the concepts. The final 
value is computed by:
(13)
LR
f
(t|D
t
) =
H
1
H
2
=
p
D
t
N
t_D
t
(1 − p
D
t
)
N
D
t
−N
t_D
t
p
B
N
t_B
(1 − p
B
)
N
B
−N
t_B
p
t
N
t_D
t
(1 − p
t
)
N
D
t
−N
t_D
t
p
t
N
t_B
(1 − p
t
)
N
B
−N
t_B
.
(14)
LR
b
(t|D
t
) =
H
1
H
2
=
p
D
t_b
fd(t∈D
t
)
(1 − p
D
t_b
)
|D
t
|−fd(t∈D
t
)
p
B_b
fd(t∈B)
(1 − p
B_b
)
|B|−fd(t∈B)
p
t_b
fd(t∈D
t
)
(1 − p
t_b
)
|D
t
|−fd(t∈D
t
)
p
t_b
fd(t∈B)
(1 − p
t_b
)
|B|−fd(t∈B)
,
(15)
LLR
f
(t|D
t
) = log
H
1
H
2
=
1
2
log
p
D
t
N
t_D
t
(1 − p
D
t
)
N
D
t
−N
t_D
t
p
B
N
t_B
(1 − p
B
)
N
B
−N
t_B
p
t
N
t_D
t
(1 − p
t
)
N
D
t
−N
t_D
t
p
t
N
t_B
(1 − p
t
)
N
B
−N
t_B
,
(16)
LLR
b
(t|D
t
) = log
H
1
H
2
=
1
2
log
p
D
t_b
fd(t∈D
t
)
(1 − p
D
t_b
)
|D
t
|−fd(t∈D
t
)
p
B_b
fd(t∈B)
(1 − p
B_b
)
|B|−fd(t∈B)
p
t_b
fd(t∈D
t
)
(1 − p
t_b
)
|D
t
|−fd(t∈D
t
)
p
t_b
fd(t∈B)
(1 − p
t_b
)
|B|−fd(t∈B)
.
(17)
LLR(t|D
t
) =
LLR
f
(t|D
t
) + LLR
b
(t|D
t
)
2
.
3.2 Taxonomic relationships extraction
Hierarchical clustering and subsumption are two popular 
methods for taxonomy construction. Bottom-up cluster-
ing method is a natural way for constructing a hierarchy. 
At the start of the algorithm, all concepts are individual 
clusters. The nearest clusters are progressively clustered 
until one cluster remains. We think this method is not 
appropriate for building taxonomic relations. Because the 
final cluster tree created by hierarchical clustering com-
prises all concepts. However, as we know, not all the con-
cepts have taxonomic relationships, since part of the con-
cepts have non-taxonomic relationships between them. 
Subsumption is a simple method for taxonomy construc-
tion. A given concept subsumes another one if the extent 
of the given concept is more general than later one. The 
conditional probabilities of the occurrence of concepts in 
documents are used to extract taxonomic relationships. 
However, this method just depend on frequency of co-
occur, less semantic information is considered.
In this paper, a new method is proposed for concept 
taxonomic relationship extraction by combine the cluster-
based method and subsumption method together. Firstly, all 
concepts are clustered into several groups based on similar-
ities between concepts. Then subsumption method is used 
to build taxonomic relations for each cluster. Finally, rule-
based method is used to refine the taxonomic relationships.
3.2.1 Taxonomic relationships identification
As we mentioned above, hierarchical clustering method 
finally creates one cluster which comprises all items by 
progressively merge the two nearest clusters into one 
cluster. However, not all the concepts have taxonomic 
relationships, since taxonomic relations just exist in simi-
lar concepts group. Therefore, all concepts are clustered 
into several semantic groups based on their similarities, 
the similarity threshold value is 
𝜆
1
. The algorithm can be 
described as follows.
1. 
Build 
|C|
clusters for concept set 
C = { c
1
,c
2
, ⋯ ,c
n
} .
2. 
Compute the distances between clusters.
3. 
Find the two nearest clusters, the value is
Sim_neareat.
(18)
Score(t,D
t
) =
TFIDF(t)
max (TFIDF(t))
×
TW
(t,D
t
)
max (TW(t,D
t
))
×
LLR(t|D
t
) − min(LLR(t|D
t
))
max (LLR(t|D
t
)) − min(LLR(t|D
t
))
.
1525
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531 
1 3
4. 
If
Sim_nearest > 𝜆
1
5. 
While
Sim_nearest > 𝜆
1
6. 
Merge the two nearest clusters into one cluster.
7. 
Return to step 2.
8. 
Else stop the clustering.
The average distance is used as the distance between 
clusters. To calculate the distances between the clusters, 
several similarity measures are defined here: vector-based 
similarity, co-occurrence-based similarity, and word2vec-
based similarity.
Vector‑based 
similarity 
Vector 
Space 
Model 
(VSM) 
is widely used in text classification. For tasks of OL, 
context 
information 
of 
concepts 
can 
be 
expressed 
into 
vectors, 
and 
the 
similarities 
between 
concepts 
can 
be 
calculated by vectors similarities. In this paper, all the sen-
tences which contain concept 
c
are collected firstly. Then 
for each sentence 
S
i
, we create a vector for concept 
c
as 
V
S
i
(c) = {t
2
∕w
2
,t
1
∕w
1
,t
−1
∕w
−1
,t
−2
∕w
−2
,c
1
∕w
c1
,c
2
∕w
c2
, ⋯ ,
c
n
∕w
cn
}
. Where 
t
2
, 
t
1
, 
t
- 1
and 
t
- 2
are two words (only nouns 
and verbs are considered here) before and after concept 
c
in 
S
i
respectively, 
{ c
1
,c
2
, ⋯ ,c
n
}
are all other concepts co-
occurrence with 
c
in sentence 
S
i
, 
w
j
is the frequency of the 
corresponding word. The final vector of concept 
c
is the 
union of all the 
V
S
i
(c)
. When do the merge operation, if 
the coming word is new, then we do insert operation; if the 
coming word is already exist, then the word frequency is 
updated by the sum of the frequencies. For example:
After concept vectors can be created, cosine measure is 
used to compute the similarities between two concept vec-
tors 
V(c
1
)
and 
V(c
2
)
. Item 
word∕0
will be added into 
V(c
1
)
or 
V(c
2
)
, if 
word
only appears in 
V(c
2
)
or 
V(c
1
)
, to ensure 
two vectors have the same length.
1
(
"outside left forward")
{
" breakthrough "/ 1,
"defence"
" player "/ 1,
"striker "/ 1}
S
V
=
ᐖ䗩䬻
ケ⹤
䱢ᆸ
/ 1, ⨳ઈ
ࡽ䬻
2
(
"outside left forward")
{
" position "/ 1,
"defence"
"football "/ 1,
"game"/ 1}
S
V
=
ᐖ䗩䬻
ս㖞
䱢ᆸ
/ 1, 䏣⨳
∄䎋
The union of above two vectors is:
(
"outside left forward")
{
" breakthrough "/ 1,
"defence"
" player "/ 1,
"striker "/ 1,
" position "/ 1,
"football "/ 1,
"game"/ 1}
V
=
ᐖ䗩䬻
ケ⹤
䱢ᆸ
/ 2, ⨳ઈ
ࡽ䬻
ս㖞
䏣⨳
∄䎋
(19)
Sim
COS
(V(c
1
), V(c
2
)) =
∑
V(c
1
)
i
V(c
2
)
i
�
∑
V(c
1
)
i
2
�
∑
V(c
2
)
i
2
.
However, the similarity of two vectors will be zero, if 
there is no same word contained in these two vectors. This 
situation is what we don’t want to see. Therefore, we com-
bined other two similarities together with cosine similarity.
Co‑occurrence‑based similarity Two concepts have high 
similarity if they have high co-occurrence frequency. The 
similarity is defined as:
where 
n(c
1
, c
2
)
is the number of documents in which two 
concepts co-occur, 
n(c
1
)
and 
n(c
2
)
is the number of docu-
ments in which 
c
1
and 
c
2
appears respectively. However, 
this method only based on frequency, no semantic informa-
tion is considered.
Word2vec‑based similarity Word embedding is a map-
ping of a word to a high dimensional vector space where 
the real valued vector representation can capture semantic 
and 
syntactic 
features. 
The 
corresponding 
algorithm 
is 
described in Mikolov et al. [35]. The open source library 
Gensim [42] is used here to implement the open source tool 
Word2Vec
2
which can achieve the word embedding task, 
implement word vector comparison, and find the top N 
related terms to a query word efficiently. This similarity of 
concepts 
c
1
and 
c
2
based on Word2Vec is presented as 
Sim
Word2Vec
(c
1
, c
2
)
.
The final similarity of two concepts is the combination 
of above three similarity values.
3.2.2 Taxonomic relationships construction
Subsumption method is often used for discover concept 
taxonomic relationships. Concept 
c
1
is considered more 
specific than concept 
c
2
if the document set which 
c
1
occurs 
(20)
sim
coocur
(c
1
, c
2
) =
2n(c
1
, c
2
)
n(c
1
) + n(c
2
)
,
2
https://code.google.com/p/word2vec/.
(21)
Sim(c
1
,c
2
) = 𝛾
1
× Sim
COS
(V(c
1
),V(c
2
)) + 𝛾
2
× sim
coocur
(c
1
,c
2
) + 𝛾
3
× Sim
Word2Vec
(c
1
,c
2
).
1526
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531
1 3
in is the subset of the document set which 
c
2
occurs in. In 
this paper, we use subsumption method to build taxonomy 
structure for each cluster which is created in the previous 
subsection. Because we believe, taxonomy relations just 
exist in the semantic similar concepts groups.
Subsumption method is based on concept co-occurrences. 
The conditional probability of two concepts is defined as:
where 
n(c
i
c
j
)
is the number of documents in which con-
cepts 
c
i
and 
c
j
co-occur, 
n(c
i
)
is the number of documents in 
which concept 
c
i
occurs.
For each concept 
c
i
, 
c
j
is potential parent of concept 
c
i
if:
The hierarchy is created for cluster 
C
k
based on the con-
cept specificity 
Spec(c
i
)
, 
c
i
∈ C
k
. The function we applied to 
calculate 
Spec(c
i
)
is inspired by functions that are mentioned 
in Ryu and Choi [44].
where 
|C
k
|
is the size of cluster 
C
k
.
Concept is more specific with higher specificity value. 
Different levels are constructed depend on different speci-
ficity values. Higher specificity value corresponds to lower 
level, and lower specificity value corresponds to higher level. 
If concepts have same specificity values, they become sibling 
nodes in the same level. After different levels are separated, 
parent concept need to be detected for each concept 
c
i
. If 
c
i
is in level 
l
r
, and its directly upper level is 
l
s
. Then the par-
ent concept 
c
j
of a concept 
c
i
is detected by the following 
formula.
(22)
p(c
j
|c
i
) =
n(c
i
c
j
)
n(c
i
)
,
P(c
j
|c
i
) ⩾ 𝜆
2
and P(c
i
|c
j
) < 𝜆
2
.
(23)
Spec(c
i
) =
∑
1⩽j⩽�C
k
�
subsume(c
j
, c
i
)
�C
k
�
,
(24)
subsume(c
j
, c
i
) =
{
1,
if P(c
j
|c
i
) ⩾ 𝜆
2
and P(c
i
|c
j
) < 𝜆
2
0,
otherwise
,
(25)
c
j
= arg max
c
j
∈l
s
(P(c
j
|c
i
)) .
3.3 Rule-based taxonomic relationships refine
According to the observation of the data, and the character-
istics of Chinese. After all domain concepts are extracted, 
there is a simple rule can be used to find hypernym con-
cept for a given concept. If concept 
c
i
is the longest right 
substring of concept 
c
j
, then 
c
i
is the hypernym concept of 
c
j
. For example, “
䗩䬻
” (winger) is the right longest sub-
string of “
ਣ䗩䬻
” (outside right), so “
䗩䬻
” (winger) is 
the hypernym concept of “
ਣ䗩䬻
” (outside right).
At the same time, a set of lexico-syntactic patterns are 
defined to extract taxonomic relations. These patterns are not 
used on domain corpus, but on a Web document set which 
collected from Baidu Encyclopedia. Baidu Encyclopedia is 
the biggest online Chinese Encyclopedia, which can provide 
detailed description of the given concept, such as the defini-
tion of the concept, the development course of the concept, 
background and related culture, and so on. In this paper, all 
concepts are collected as the input of Baidu Encyclopedia, 
concept definitions are collected for further use. According 
to the observation of Baidu Encyclopedia data, the section of 
definition information is always at the very beginning of the 
Web page, directly follows the concept terms. Concept defi-
nition contains rich semantic description of concept, espe-
cially contains the description of the relationships between 
the given concept and other relative concepts. Therefore, the 
content of concept definition is very useful for finding hyper-
nym or hypornym concepts. Lexico-syntactic patterns are 
listed in Table 1. Where 
Hypernym(C
1
, C
0
)
means concept 
C
1
is the hypernym of concept 
C
0
.
All the extracted results in this subsection are used to 
refine the taxonomic relationships created by the previous 
steps. Because rule based method has higher precision.
4 Experiments and the results
4.1 Experimental setup
The following experiments have been performed on a set 
of News Web pages which collected from official website 
Table 1 The lexico-syntactic patterns used for taxonomic relationships extraction
1. 
0
C
ᱟ(is)
1
C
ᵟ䈝ѻа(one of the terms)
1
0
Hypernym(
,
)
C C
2. 
0
C
ᱟ(is)
1
C
Ⲵа⿽(a kind of)
1
0
Hypernym(
,
)
C C
3. 
0
C
ᱟа⿽(is a kind of) 
1
C
1
0
Hypernym(
,
)
C C
4. 
0
C
वਜ਼(including)
1
C
{
n
2
/
/
(
) 
C
and
C
L
ˈǃ
ˈǃ ઼
} 
0
Hypernym(
,
)
i
C C
5. 
0
NP
࠶Ѫ(divided into)
1
C
{
n
2
/
/
(
) 
C
and
C
L
ˈǃ
ˈǃ ઼
}
0
Hypernym(
,
)
i
C C
1527
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531 
1 3
of Sohu (http://www.sohu.com). The data includes several 
different domains: football news (1300 pages), military 
news (900 pages), culture news (800 pages), financial news 
(1200 pages), where football news is the target domain, 
and all other domain documents are merged as background 
corpus.
In this paper, we evaluate our OL method by compare 
our automatically created lightweight domain ontology 
O
C
with a lightweight ontology created manually 
O
G
(golden 
standard). The golden lightweight ontology is created by 
four undergraduate students and two graduate students. 
There are 537 concepts contained in 
O
G
. Precision, recall, 
and F-measure are used for evaluate the performance of 
concept and taxonomic relationships extraction.
The formulas used to compute bathe quality of domain 
concepts are defined as follows.
where 
C
C
is concept set of ontology 
O
C
, 
C
G
is concept 
set of ontology 
O
G
.
After defining a method to measure the domain con-
cept relevance, we need to define a method to measure the 
performance of taxonomic relationships extraction. The 
method considers only the <concept, hypernym concept 
>pairs in taxonomic structure.
4.2 Concept extraction results
LTP Named Entity Module combined with POS tag based 
rules are used for multiword term identification. TF-IDF, 
(26)
P
C
=
|C
C
∩ C
G
|
|C
C
|
,
R
C
=
|C
C
∩ C
G
|
|C
G
|
,
F
C
=
2 × P
C
× R
C
P
C
+ R
C
,
(27)
P
R
=
| Rel (O
C
) ∩ Rel (O
G
)|
| Rel (O
C
)|
, R
R
=
| Rel (O
C
) ∩ Rel (O
G
)|
| Rel (O
G
)|
,
F
R
=
2 × P
R
× R
R
P
R
+ R
R
.
Rel (O
C
) =
{
< c
i
,c
ip
> |c
ip
is hypernym concept of c
i
,c
i
∈ C
C
,c
ip
∈ C
C
}
.
DRd&DCd and LLR algorithm are three components of 
our 
concept 
extraction 
method. 
The 
parameter 
weights 
for DRd&DCd are 
𝛼 = 0.7
and 
𝛽 = 0.3
. The top 600 con-
cepts are extracted by our method and these three methods 
respectively. The results are presented in Table 2.
As shown in Table 2, TF-IDF, DRd&DCd and LLR 
method have similar performances in concept extraction. 
LLR has a bit better performance than other two methods, 
maybe because LLR method is more complex, and contains 
more statistical information. When we combine these meth-
ods progressively, the corresponding performance becomes 
better. This can be explained as these three methods cap-
ture different aspects of statistical information. Therefore, 
the combination of these methods can get better results.
However, there are also some disadventages of com-
bining methods together. Although each component has 
normalized before combination, values of different com-
ponents have different distributions. DRd&DCd method 
is designed based on DR&DC by considering the term’s 
occurrence breadth in domain document set. Term values 
distribution is not uniform depends on DRd&DCd, for 
example, the values of terms are either too big or too small. 
Therefore, when combine the different components’ value 
together, final value may not reflect the true importance of 
the term. In the future, the rank information of terms in dif-
ferent components will be considered.
The sample of the extracted concepts are shown in 
Fig. 1, where the font size shows the weight of the corre-
sponding concept. Part (a) shows the Chinese domain con-
cepts, and part (b) is the English expression corresponds to 
(a).
4.3 Taxonomic relationships extraction results
The process of taxonomic relationships extraction is sep-
arated with the process of concept extraction to avoid the 
impact of the errors that are created by previous step. All 
the 537 concepts in ontology 
O
G
are collected as input 
of this step. In 
O
G
there are 505 concepts have hyper-
nym concepts. The parameter weights are set as 
𝜆
1
= 0.6
, 
𝛾
1
= 0.2
, 
𝛾
2
= 0.3
, 
𝛾
3
= 0.5
and 
𝜆
2
= 0.75
. Where 
𝜆
1
is the 
Table 2 The performances of 
different methods in concept 
extraction
Method
#Top
#Correct
#Miss
Precision (%)
Recall (%)
F value (%)
TF-IDF
600
396
141
66.00
73.74
69.66
DRd&DCd
600
402
135
67.00
74.86
70.71
LLR
600
437
100
72.83
81.37
76.86
TF-IDF* DRd&DCd
600
432
105
72.00
80.45
75.99
TF-IDF* 
DRd&DCd*LLR (our 
method)
600
446
91
74.33
83.05
78.45
1528
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531
1 3
threshold employed for concepts clustering, 
{ 𝛾
1
,𝛾
2
,𝛾
3
}
are applied for combining three concept similarities, and 
𝜆
2
is the threshold used for subsumption algorithm to 
build taxonomy for each cluster. Performance of concept 
clustering according to different concept similarities are 
compared. Subsumption algorithm is used for taxonomy 
construction for each cluster. The results comparison is 
shown in Table 3.
When vector-based similarity is only considered for con-
cept clustering, there is a high precision, but low recall. It’s 
because vector-based similarity is depend on concept con-
text, higher similarity value will be obtained when more 
same words appear around two concepts. However, this 
constrain is difficult to achieve due to the complex language 
phenomenon. Therefore, the recall value cannot be high 
when use vector-based similarity only. The second concept 
similarity 
component 
is 
co-occurrence-based 
similarity. 
Fig. 1 The sample results of 
domain concept extraction
Table 3 The performances of 
concept taxonomic relationships 
extraction
𝜆
1
{ 𝛾
1
,𝛾
2
,𝛾
3
}
𝜆
2
#E
#Correct
#Miss
Precision (%)
Recall (%)
F value (%)
0.6
{1,0,0}
0.75
284
209
296
73.59
41.39
52.98
0.6
{0,1,0}
0.75
507
330
175
65.09
65.35
65.22
0.6
{0,0,1}
0.75
559
352
153
62.97
69.70
66.16
0.6
{0.2,0.3,0.5}
0.75
482
355
150
73.65
70.30
71.94
1529
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531 
1 3
Two concepts have higher similarity when they have higher 
co-occurrence frequency. This similarity value is computed 
totally based on frequency information. Therefore, the cor-
responding performance is at average level, both the preci-
sion and recall are not very high and not very low. When 
only use Word2Vec-based similarity for concepts cluster-
ing, more concept relationships are extracted. This maybe 
because the threshold value 
𝜆
1
= 0.6
is relatively low for 
Word2Vec method. Therefore, when only use Word2Vec-
based similarity we will obtain a relatively high recall 
value.
Our method for concept taxonomic relationships extrac-
tion combines the above three concept similarities for con-
cept clustering, then subsumption algorithm is used to find 
hypernym node for each concept. Word2Vec-based similar-
ity is given higher weight, because it reflects more semantic 
information, and when only use this similarity it shows rel-
atively good performance. After combining three category 
similarities the F value reaches 71.94%.
Rule based method also applied to taxonomic relation-
ships extraction. All taxonomic relations extracted by rule-
based method are employed to refine the results created in 
previous phases. Because rule-based method has higher 
precision. There are 263 taxonomic relationships extracted 
out by utilizing rule-based method, where 237 are correct, 
the precision is 90.11%. The final performance is shown in 
Table 4. F-value is improved from 71.94 to 76.65%. The 
taxonomic structure is shown in Fig. 2.
5 Conclusion
In this paper, a novel framework for domain lightweight 
ontology construction was proposed. Domain concepts and 
taxonomic relationships are extracted from Chinese domain 
Web news documents.
In order to improve the performance of concept extrac-
tion, multiword terms are identified firstly based on a set 
of rules and LTP Named Entity Module. Then TF-IDF, 
DRd&DCd, and LLR method are used to calculate the 
weights for each terms. Where TF-IDF depends on term 
frequency in document; DR&DC depends on term fre-
quency in domain, and DRd&DCd is an improved DR&DC 
by adding the consideration of term’s occurrence breadth; 
LLR depends on probability ratio of two hypotheses. These 
three statistical methods capture three different types of 
information. Therefore, a better extraction result can be 
obtained when combine these three methods together.
Hierarchical clustering method is often chose for con-
struct taxonomic structure. All concepts are finally clus-
tered into one big cluster using this method. However, 
not all concepts have taxonomic relationships. Therefore, 
a new method for taxonomic relationships extraction was 
proposed here. Firstly, all concepts are divided into sev-
eral clusters by using a clustering method. Vector-based 
similarity, co-occurrence-based similarity, and Word2Vec-
based similarity are joined to compute similarities between 
concepts. Secondly, subsumption method is used to con-
struct taxonomic structure for each concept cluster. Con-
cept that has higher specificity value locates on the lower 
level. Finally, a rule-based method is applied to extract 
taxonomic relationships from concept definitions which are 
collected from Baidu Encyclopedia. The results of this step 
are used to refine the taxonomy structures created in previ-
ous step. Extraction results are compared when using dif-
ferent types of similarities. And the final results show the 
effectiveness of rule-based refinement.
In the future, we intend to evaluate other concept extract-
ing 
techniques. 
Meanwhile, 
synonymous 
concepts 
are 
necessary to be considered. Now we just use the Chinese 
Synonym Dictionary to identify synonyms, however, there 
are many domain concepts that have the same meaning are 
Fig. 2 A taxonomy on domain of football games
Table 4 Final performance 
after taxonomic relationships 
refine
Method
#E
#Correct
#Miss
Precision (%)
Recall (%)
F value (%)
Before refine
482
355
150
73.65
70.30
71.94
After refine
497
384
121
77.26
76.04
76.65
1530
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531
1 3
not contained in. Therefore, method is need to be proposed 
to identify synonymous concepts. Future work could also 
focus on how to use external knowledge source, such as 
Baidu 
Encyclopedia, 
more 
effectively. 
Frequency 
infor-
mation, syntactic features, and statistical information will 
be try to combined with Deep Learning methods to learn 
parameters automatically. And we’d also like to apply our 
framework to other domains.
Acknowledgements 
This paper is supported by the National Natu-
ral Science Foundation of China (61300120), partially supported by 
the China Postdoctoral Science Foundation (2015M582622), Colleges 
of Science and Technology Research Foundation in Hebei Province 
(YQ2013032, YQ2014036), Science and technology department of 
Hebei province of china (15210338), and The Open Project of Bei-
jing Key Laboratory of IOT information security technology, Institute 
of Information Engineering. And we thank Harbin Institute of Tech-
nology Information Retrieval Laboratory for providing us with LTP 
modules.
References
1. 
Abney S (2004) Understanding the yarowsky algorithm. Comput 
Linguist 30(3):365–395
2. 
Berners-Lee T, Hendler J, Lassila O (2001) The semantic web: 
a new form of web content that is meaningful to computers will 
unleash a revolution of new possibilities. Sci Am 285(5):34–43
3. 
Bird S, Klein E, Loper E, Baldridge J (2008) Multi-disciplinary 
instruction with the natural language toolkit. In: Proceedings of 
the Third Workshop on Issues in Teaching Computational Lin-
guistics (TeachCL’08), 2008, pp 62–70
4. 
Blum A, Mitchell T (1998) Combining labeled and unlabeled 
data with co-training. In: Proceedings of the 11th annual confer-
ence on Computational learning theory, 1998, pp 92–100
5. 
Bradesko L, Dali L, Fortuna B et al (2010) Contextualized ques-
tion answering. In: ITI 2010, pp 73–78
6. 
Brewster C, Jupp S, Luciano J et al (2009) Issues in learning an 
ontology from text. BMC Bioinform 10(5):S1
7. 
Buitelaar P, Magnini B (2005) Ontology learning from text: an 
overview. In: Buitelaar P, Cimiano P, Magnini B (eds) Ontology 
learning from text: methods, applications and evaluation. IOS 
Press, The Netherlands, pp 3–12.
8. 
Bunescu RC, Mooney RJ (2005) A shortest path dependency 
kernels for relation extraction. In: Proceedings of EMNLP’2005, 
2005, pp 724–731
9. 
Che W, Li Z, Liu T (2010) LTP: a Chinese language technology 
platform. In: Coling, pp 13–16
10. 
Ciaramita M, Gangemi A, Ratsch E, Saric J, Rojas I (2005) 
Unsupervised learning of semantic relations between concepts 
of a molecular biology ontology. In: Proceedings of the 19th 
International Joint Conference on Artificial Intelligence, 2005, 
pp 659–664
11. 
Cimiano P, Hotho A, Staab S (2005) Learning concept hierar-
chies from text corpora using formal concept analysis. J Artif 
Intell Res 24:305–339
12. 
Cimiano P, Volker J (2005) Text2Onto: A framework for ontol-
ogy 
learning 
and 
data-driven 
change 
discovery. 
In: 
NLDB, 
pp 227–238
13. 
Colace F, Santo MD, Greco L et al (2014) Terminological 
ontology learning and population using latent Dirichlet alloca-
tion. J Visual Lang Comput 25:818–826
14. 
Curtis J, Matthews G, Baxter D (2005) On the effective use of 
Cyc in a question answering system. In: IJCAI Workshop on 
KRAQ’05, Edinburgh, Scotland, pp 61–71.
15. 
Dietz EA, Vandic D, Frasincar F (2012) TaxoLearn: a seman-
tic approach to domain taxonomy learning. In: Proceedings 
of IEEE/WIC/ACM International Conference on Web Intelli-
gence and Intelligent Agent Technology, 2012, pp 58–65
16. 
Doing-Harris 
K, 
Livnat 
Y, 
Meystre 
S 
(2015) 
Automated 
concept and relationship extraction for the semi-automated 
ontology 
management 
(SEAM) 
system. 
J 
Biomed 
Semant 
6(15):1–15
17. 
Fallucchi F, Zanzotto F M (2011) Inductive probabilistic taxon-
omy learning using singular value decomposition. Nat Lang Eng 
17(1):71–94
18. 
Faure D, Poibeau T (2000) First experiments of using seman-
tic knowledge learned by ASIUM for information extraction 
task using INTEX. In ECAI Workshop on Ontology Learning, 
pp 7–12
19. 
Ferreira V H, Lopes l, Vieira R, Finatto M J (2013) Automatic 
extraction 
of 
domain 
specific 
non-taxonomic 
relations 
from 
Portuguese Corpora. In: Proceedings of 12th IEEE/WIC/ACM 
International Joint Conferences on Web Intelligence and Intelli-
gent Agent Technology, 2013, pp 135–138
20. 
Fortuna B, Lavrac N, Velardi P (2008) Advancing Topic Ontol-
ogy Learning through Term Extraction. In: PRICAI, pp 626–635
21. 
Gruber T (1993) A translation approach to portable ontology 
specifications. Knowl Acquis 5:199–220
22. 
Hearst MA (1992) Automatic acquisition of hyponyms from 
large text corpora. In: COLING, vol 2, pp 539–545
23. 
Heflin J, Hendler J (2000) Dynamic ontologies on the Web. In: 
AAAI, pp 443–449
24. 
Hippisley A, Cheng D, Ahmad K (2005) The head-modifier 
principle 
and 
multilingual 
term 
extraction. 
Nat 
Lang 
Eng 
11(2):129–157
25. 
Kang Y, Haghigh PD, Burstein F (2016) TaxoFinder: a graph-
based approach for taxonomy learning. IEEE Trans Knowl Data 
Eng 28(2):524–536
26. 
Knijff JD, Frasincar F, Hogenboom F (2013) Domain taxonomy 
learning from text: the subsumption method versus hierarchical 
clustering. Data Knowl Eng 83(1):54–69
27. 
Kozareva 
Z, 
Hovy 
E 
(2010) 
A 
semi-supervised 
method 
to 
learn and construct taxonomies using the web. In: EMNLP, 
pp 1110–1118
28. 
Kozareva Z, Hovy E, Riloff E (2009) Learning and evaluat-
ing the content and structure of a term taxonomy. In: AAAI, 
pp 50–57
29. 
Li D, Kipper-Schuler K, Savova G (2008) Conditional random 
fields and support vector machines for disorder named entity 
recognition in clinical texts. In: Proceedings of the workshop on 
current trends in biomedical natural language processing, 2008, 
pp 94–95
30. 
Li J, Luong T, Jurafsky D, and Hovy E (2015) When are tree 
structures necessary for deep learning of representations? In: 
Proceedings of the 2015 EMNLP, 2015, pp 2304–2314
31. 
Liu X, Song Y, Liu S, Wang H (2012) Automatic taxonomy 
construction from keywords. In: ACM SIGKDD International 
Conference on Knowledge Discovery and Data Mining, 2012, 
pp 1433–1441
32. 
Lv X, Guan Y, Deng B (2014) Learning based clinical con-
cept extraction on data from multiple sources. J Biomed Inform 
52:55–64
33. 
Maedche A, Staab S (2000) The text-to-onto ontology learning 
environment. In: Proceedings of SoftwareDemonstration at the 
1531
Int. J. Mach. Learn. & Cyber. (2018) 9:1519–1531 
1 3
8th International Conference on Conceptual Structures, 2000, 
pp 14–18
34. 
Meijer 
K, 
Frasincar 
F, 
Hogenboom 
F 
(2014) 
A 
semantic 
approach for extracting domain taxonomies from text. Decis 
Support Syst 62:78–93
35. 
Mikolov T, Chen K, Corrado G, Dean J (2013) Efficient estima-
tion of word representations in vector space. In: ICLR Work-
shop, 2013
36. 
Milano M, Agopito G, Guzzi PH, Cannataro M (2016) An 
experimental 
study 
of 
information 
content 
measurement 
of 
gene ontology terms. Int J Mach Learn Cybern. doi:10.1007/
s13042-015-0482-y
37. 
Navigli R, Velardi P (2004) Learning domain ontologies from 
document warehouses and dedicated web sites. Comput Linguist 
30(2):151–179
38. 
Nedellec C (2000) Corpus-based learning of semantic relations 
by the ILP system, Asium. In: Proceeding of Learning Language 
in Logic, 2000, pp 259–278
39. 
Paukkeri MS, Garcia-Plaza AP, Fresno V et al (2012) Learn-
ing a taxonomy from a set of text documents. Appl Soft Comput 
12:1138–1148
40. 
Pennacchiotti M, Pantel P (2006) A bootstrapping algorithm for 
automatically harvesting semantic relations. In: Proceedings of 
Inference in Computational Semantics, 2006, pp 87–96
41. 
Ponzetto 
SP, 
Strube 
M 
(2011) 
Taxonomy 
induction 
based 
on 
a 
collaboratively 
built 
knowledge 
repository. 
Artif 
Intell 
75(9–10):1737–1756
42. 
Rehurek R, Sojka P (2010) Software Framework for Topic Mod-
elling with Large Corpora. In: Proceedings of the LREC 2010 
Workshop 
on 
New 
Challenges 
for 
NLP 
Frameworks, 
2010, 
pp 45–50
43. 
Rios-Alvarado 
AB, 
Lopez-Arevalo 
I, 
Sosa-Sosa 
VJ 
(2013) 
Learning concept hierarchies from textual resources for ontolo-
gies construction. Expert Syst Appl 40(15):5907–5915
44. 
Ryu P M, Choi K S (2006) Taxonomy learning using term speci-
ficity and similarity. In: Proceedings Workshop on Ontology 
Learning and Population, 2006, pp 41–48
45. 
Salton G, McGill MJ (1986) Introduction to modern information 
retrieval. In: McGraw-Hill Inc. New York, USA, pp 180–198
46. 
Santos CD, Xiang B, Zhou B (2015) Classifying relations by 
ranking with convolutional neural networks. In: Proceedings of 
the 53rd ACL and the 7th IJCNLP, 2015, pp 626–634
47. 
Schutz A, Buitelaar P (2005) RelExt: a tool for relation extrac-
tion from text in ontology extension. In: Proceedings of 4th 
International Semantic Web Conference, 2005, pp 593–606
48. 
Sclano F, Velardi P (2007) TermExtractor: a web application 
to learn the shared terminology of emergent web communities. 
Enterp Interoper. II. doi:10.1007/978-1-84628-858-6_32
49. 
Shamsfard M, Barforoush A (2004) Learning ontologies from 
natural language texts. Int J Hum Comput Stud 60(1):17–63
50. 
Snchez D, Moreno A (2005) Web-scale taxonomy learning. 
In: Proceedings of workshop on extending and learning lexical 
ontologies using machine learning, 2005, pp 53–60
51. 
Snow R, Jurafsky D, Ng A Y (2006) Semantic taxonomy induc-
tion from heterogenous evidence, In: ACL, pp 801–808
52. 
Specia L, Motta E (2006) A hybrid approach for extracting 
semantic relations from texts. In: Proceedings of 2nd Workshop 
on Ontology Learning and Population, 2006, pp 57–64
53. 
Suchanek FM, Ifrim G, Weikum G (2006) Combining linguistic 
and statistical analysis to extract relations from web documents. 
In: Proceeding of 12th ACM SIGKDD Int. Conf. Knowl. Dis-
covery Data Mining, 2006, pp 712–717
54. 
Tai KS, Socher R, Manning CD (2015) Improved semantic rep-
resentations from tree-structured long short-term memory net-
works. In: Proceedings of the 53rd ACL and the 7th IJCNLP, 
2015, pp 1556–1566
55. 
Thompson CA, Califf ME, Mooney RJ (1999) Active learn-
ing for natural language parsing and information extraction. In: 
Proceedings of the 16th International Conference on Machine 
Learning, Morgan Kaufmann, 1999, pp 406–414
56. 
Velardi P, Cucchiarelli A, Ptit M (2007) A taxonomy learning 
method and its application to characterize a scientific web com-
munity. IEEE Trans Knowl Data Eng 19:180–191
57. 
Velardi P, Fabriani P, Missikoff M (2001) Using text processing 
techniques to automatically enrich a domain ontology. In: Pro-
ceedings of the ACM Conference on Formal Ontologies in Infor-
mation Systems, 2001, pp 270–284
58. 
Velardi P, Faralli S, Navigli R (2013) OntoLearn reloaded: a 
graph-based algorithm for taxonomy induction. Comput Linguist 
39(3):665–707
59. 
Velardi P, Navigli R, Cucchiarelli A et al (2005) Evaluation of 
OntoLearn, a methodology for automatic learning of domain 
ontologies. In: Buitelaar P, Cimiano P, Magnini B (eds) Ontol-
ogy learning from text: methods, applications and evaluation. 
IOS Press, Amsterdam, pp 92–106
60. 
Wang W, Mamaani Barnaghi P, Bargiela A (2010) Probabilistic 
topic models for learning terminological ontologies. IEEE Trans 
Knowl Data Eng 22(7):1028–1040
61. 
Wang Y, Patrick J (2009) Cascading classifiers for named entity 
recognition in clinical notes. In: Proceedings of the workshop on 
biomedical information extraction, 2009, pp 42–49
62. 
Weichselbraun A, Wohlgenannt G, Scharl A (2010) Refining 
non-taxonomic relation labels with external structured data to 
support ontology learning. J Data Knowl Eng 69(8):763–778
63. 
Wong MK, Abidi SSR, Jonsen ID (2014) A multi-phase correla-
tion search framework for mining non-taxonomic relations from 
unstructured text. J Knowl Inf Syst 38(3):641–667
64. 
Wong 
W, 
Liu 
W, 
Bennanoun 
M 
(2012) 
Ontology 
learning 
from text: a look back and into the future. ACM Comput Surv 
44(4):20
65. 
Xu Y, Mou L, Li G, Chen Y, Peng H, Jin Z (2015) Classifying 
relations via long short term memory networks along shortest 
dependency paths. In: Proceedings of the 2015 EMNLP, 2015, 
pp 1785–1794
66. 
Zelenko D, Aone C, Richardella A (2003) Kernel methods for 
relation extraction. J Mach Learn Res 3(3/1/2003):1083–1106
67. 
Zhang 
Z 
(2008) 
Mining 
relational 
data 
from 
text: 
from 
strictly 
supervised 
to 
weakly 
supervised 
learning. 
Inf 
Syst 
33(3):300–314
68. 
Zhou G D, Su J, Zhang J, and Zhang M (2005) Exploring vari-
ous knowledge in relation extraction. In: Proceedings of the 
ACL’2005, 2005, pp 419–444
