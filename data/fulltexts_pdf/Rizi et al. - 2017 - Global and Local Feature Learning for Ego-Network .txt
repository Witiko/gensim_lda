Global
and Local
Feature Learning for Ego-Network
Analysis
Fatemeh Salehi
Rizi
Michael
Granitzer and Konstantin Ziegler
TIR Workshop
29 August 2017
(TIR Workshop)
University of Passau
29 August 2017
1 / 28
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
2 / 28
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
3 / 28
Graph Embedding
What is a Graph Representation?
● Anomaly Detection
● Attribute Prediction
● Clustering
● Link Prediction
● ...
|V|
d << |V|
Latent Dimensions
Adjacency Matrix
Bryan Perozzi
DeepWalk: Online Learning of Social Representations
We can also create features by transforming the 
graph into a lower dimensional latent 
representation.
Given graph
G
with a set of nodes
V
=
{v
1
, . . . , v
n
}
f
:
v
i
7→ y
i
∈ R
d
,
d  |V |
Methods based on eigen-decomposition of the Adjacency Matrix
Methods inspired by NLP and Deep Learning
(TIR Workshop)
University of Passau
29 August 2017
4 / 28
Graph Embedding
What is a Graph Representation?
● Anomaly Detection
● Attribute Prediction
● Clustering
● Link Prediction
● ...
|V|
d << |V|
Latent Dimensions
Adjacency Matrix
Bryan Perozzi
DeepWalk: Online Learning of Social Representations
We can also create features by transforming the 
graph into a lower dimensional latent 
representation.
Given graph
G
with a set of nodes
V
=
{v
1
, . . . , v
n
}
f
:
v
i
7→ y
i
∈ R
d
,
d  |V |
Methods based on eigen-decomposition of the Adjacency Matrix
Methods inspired by NLP and Deep Learning
(TIR Workshop)
University of Passau
29 August 2017
4 / 28
What is an Ego Network?
Social
graphs have been divided to several
subgraphs (ego-networks) [1]
extracting features for nodes
detecting distinct neighborhood patterns
study social
relationships
Ego-network [1]
ego
alters
social
circles
(TIR Workshop)
University of Passau
29 August 2017
5 / 28
Sport teammates
Friends 
Family members
Colleagues 
ego
alter
An ego-network with four social
circles
Local
Neighborhood Analysis
Neighborhood around each ego has a different pattern [2]
Finding a vector representation for each ego-network
Social
circle detection and prediction
(TIR Workshop)
University of Passau
29 August 2017
6 / 28
(a) Linked neighbors
(b) Strongly linked
(c) Dense
(d) Complete
(e) Powerful ego node
(f) Strong ego neighbor
(g) Less cohesive star
(h) Star
Figure 2:
Clustering results of ego graphs,
depicting the ego node in the middle (red color),
with connections to first and second-degree
neighbors. This study focuses on the automatic categorization of such ego graphs according to their graph structure. All in all, there exist
eight distinct trends. We label each graph according to its characteristics.
Cluster(f) (Strong ego neighbors): The ego node has
few immediate neighbors and the network is not
densely
connected.
However,
some nodes in the graph are highly
populated and more powerful than the ego.
The ego has a
high closeness and degree centrality,
but
low betweenness
centrality. The internal density is between 0.50 and 0.60.
Cluster(g) (Less cohesive star): The graph contains few
small structures. It contains few trivial triads and the remain-
ing neighbors form a star-shape network.
Cluster(h)
(Star):
Overall,
the network has
a sparse
structure.
The ego graph has a star structure and immedi-
ate neighbors of the ego are not connected. The number of
neighbors varies depending upon the size of the network;
the structure is small for smaller networks and large for big-
ger networks.
The overall density of the ego graph is very
low with no complete sub-graphs.
Normally,
such clusters
are identified by a high value of centrality measures. In such
cases, we found that the second order neighborhood of the
ego is even more powerful and denser in terms of structure.
Table 4 represents the clustering results.
We detect mis-
classified clusters using two sources:
i)
we select
clusters
with very low or negative Silhouette width as misclassified
instances,
ii)
we rely on visual inspection of the small clus-
ters. Similarly, we model Bluetooth and GPS modality from
the Nokia dataset. We further categorize the clusters in four
classes:
i)
linked structures (
C(a)
,
C(b)
),
ii)
dense struc-
tures (
C(c)
,
C(d)
),
iii)
informative ego (
C(e)
,
C(f )
), and
iv)
less dense (
C(g)
,
C(h)
).
One shall note that the inter-
pretation of the structures depends on many factors, such as
the environment of data collection, context data and so on.
In the later sections,
we analyze some networks based on
context information to give meaningful interpretation to the
clusters. Now, we discuss major trends from Table 4.
Network
C(a)
C(b)
C(c)
C(d)
C(e)
C(f )
C(g)
C(h)
M
Nokia BT
+*!
+*!
+!
+*!
+*
Nokia GPS
+*!
+*!
+*!
+*!
*
Fri& Fam
+*!
*
+*!
+*!
Social
+*!
+*!
+*!
+*!
Orange
+*
+*
+*
*
+*
+*
+*
+*!
Facebook
+*
+*
+*
+*
*
+*
*
+*!
Twitter
+*
+*
+*
+*
+*
+*
+*!
Foursquare
+*
+*
+*
+*
*
+*
+*
+*!
Philosophers
+*
+*
+*
+*
+*
+*
!
Hep-TH
*
+*
+*
+*
+*!
Table 4:
Clustering results for nine datasets using three algorithms.
The first row represents the datasets. It contains the results for the
features selected by FSFS. Similarly,
C(a)
,. . . ,
C(h)
represent the
short form of cluster (a),. . . ,
cluster (h).
The letter
M
represents
the misclassified clusters in the datasets.
We use three signs to
represent clustering algorithms. The +, * and ! signs represent the
k
-means, hierarchical clustering and affinity propagation (AP) re-
spectively.
There are some cells without entries representing that
for some combinations that particular shape is not detected.
Spatial-temporal clustering trends.
Overall,
we notice
that extraction of possible clusters is largely dependent on
the sample size and environment of data collection. The So-
cial
Evolution and the Friends & Family datasets are col-
lected in a certain environment (student dormitory and mar-
ried graduate students living in a campus facility) with peo-
ple well familiarized with each other. Their clustering results
illustrate that only certain clustering patterns are prominent.
For the Social Evolution dataset, people are mostly confined
within
C(a)
,
C(b)
,
C(b)
and
C(d)
that represent strongly
clustered structures. Similarly, the Friends & Family dataset
analysis shows that
C(a)
,
C(d)
and
C(g)
are prominent.
The remaining patterns hardly exist
in the data.
The Or-
ange dataset is gathered from a large sample of people liv-
273
Local
Neighborhood Analysis
Neighborhood around each ego has a different pattern [2]
Finding a vector representation for each ego-network
Social
circle detection and prediction
(TIR Workshop)
University of Passau
29 August 2017
6 / 28
(a) Linked neighbors
(b) Strongly linked
(c) Dense
(d) Complete
(e) Powerful ego node
(f) Strong ego neighbor
(g) Less cohesive star
(h) Star
Figure 2:
Clustering results of ego graphs,
depicting the ego node in the middle (red color),
with connections to first and second-degree
neighbors. This study focuses on the automatic categorization of such ego graphs according to their graph structure. All in all, there exist
eight distinct trends. We label each graph according to its characteristics.
Cluster(f) (Strong ego neighbors): The ego node has
few immediate neighbors and the network is not
densely
connected.
However,
some nodes in the graph are highly
populated and more powerful than the ego.
The ego has a
high closeness and degree centrality,
but
low betweenness
centrality. The internal density is between 0.50 and 0.60.
Cluster(g) (Less cohesive star): The graph contains few
small structures. It contains few trivial triads and the remain-
ing neighbors form a star-shape network.
Cluster(h)
(Star):
Overall,
the network has
a sparse
structure.
The ego graph has a star structure and immedi-
ate neighbors of the ego are not connected. The number of
neighbors varies depending upon the size of the network;
the structure is small for smaller networks and large for big-
ger networks.
The overall density of the ego graph is very
low with no complete sub-graphs.
Normally,
such clusters
are identified by a high value of centrality measures. In such
cases, we found that the second order neighborhood of the
ego is even more powerful and denser in terms of structure.
Table 4 represents the clustering results.
We detect mis-
classified clusters using two sources:
i)
we select
clusters
with very low or negative Silhouette width as misclassified
instances,
ii)
we rely on visual inspection of the small clus-
ters. Similarly, we model Bluetooth and GPS modality from
the Nokia dataset. We further categorize the clusters in four
classes:
i)
linked structures (
C(a)
,
C(b)
),
ii)
dense struc-
tures (
C(c)
,
C(d)
),
iii)
informative ego (
C(e)
,
C(f )
), and
iv)
less dense (
C(g)
,
C(h)
).
One shall note that the inter-
pretation of the structures depends on many factors, such as
the environment of data collection, context data and so on.
In the later sections,
we analyze some networks based on
context information to give meaningful interpretation to the
clusters. Now, we discuss major trends from Table 4.
Network
C(a)
C(b)
C(c)
C(d)
C(e)
C(f )
C(g)
C(h)
M
Nokia BT
+*!
+*!
+!
+*!
+*
Nokia GPS
+*!
+*!
+*!
+*!
*
Fri& Fam
+*!
*
+*!
+*!
Social
+*!
+*!
+*!
+*!
Orange
+*
+*
+*
*
+*
+*
+*
+*!
Facebook
+*
+*
+*
+*
*
+*
*
+*!
Twitter
+*
+*
+*
+*
+*
+*
+*!
Foursquare
+*
+*
+*
+*
*
+*
+*
+*!
Philosophers
+*
+*
+*
+*
+*
+*
!
Hep-TH
*
+*
+*
+*
+*!
Table 4:
Clustering results for nine datasets using three algorithms.
The first row represents the datasets. It contains the results for the
features selected by FSFS. Similarly,
C(a)
,. . . ,
C(h)
represent the
short form of cluster (a),. . . ,
cluster (h).
The letter
M
represents
the misclassified clusters in the datasets.
We use three signs to
represent clustering algorithms. The +, * and ! signs represent the
k
-means, hierarchical clustering and affinity propagation (AP) re-
spectively.
There are some cells without entries representing that
for some combinations that particular shape is not detected.
Spatial-temporal clustering trends.
Overall,
we notice
that extraction of possible clusters is largely dependent on
the sample size and environment of data collection. The So-
cial
Evolution and the Friends & Family datasets are col-
lected in a certain environment (student dormitory and mar-
ried graduate students living in a campus facility) with peo-
ple well familiarized with each other. Their clustering results
illustrate that only certain clustering patterns are prominent.
For the Social Evolution dataset, people are mostly confined
within
C(a)
,
C(b)
,
C(b)
and
C(d)
that represent strongly
clustered structures. Similarly, the Friends & Family dataset
analysis shows that
C(a)
,
C(d)
and
C(g)
are prominent.
The remaining patterns hardly exist
in the data.
The Or-
ange dataset is gathered from a large sample of people liv-
273
Social
Circle Prediction
Predicting the social
circle for a new added alter to the ego-network
(TIR Workshop)
University of Passau
29 August 2017
7 / 28
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
8 / 28
Our Contributions
We introduce local
vector representations for egos to capture
neighborhood structures
We apply local
vectors to the circle prediction problem
We replace global
representations by local
to improve the performance
(TIR Workshop)
University of Passau
29 August 2017
9 / 28
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
10 / 28
Vector Representation for Social
Graphs
DeepWalk [3]
walks globally over the graph and samples sequences of nodes
treats all
these sequences as an artificial
corpus
feeds the corpus to a Skip-Gram based Word2Vec [5]
Word2Vec:
Having the sequence of words
{w
1
, w
2
, . . . , w
t−
1
, w
t
,
w
t
+1
. . . , w
n
}
, language models aims to maximize
P
(
w
t
|w
1
, . . . , w
t−
1
)
.
given sequence of nodes
{v
1
, v
2
, . . . , v
t−
1
, v
t
,
v
t
+1
, . . . , v
n
}
it
maximizes:
P
n
t
=1
log
P r
(
v
t
|v
t
+
c
, . . . , v
t
1
, v
t
+1
, . . . , v
t−c
)
glo :
V → R
d
It walks globally and exceeds the ego-network
(TIR Workshop)
University of Passau
29 August 2017
11 / 28
Vector Representation for Social
Graphs
DeepWalk [3]
walks globally over the graph and samples sequences of nodes
treats all
these sequences as an artificial
corpus
feeds the corpus to a Skip-Gram based Word2Vec [5]
Word2Vec:
Having the sequence of words
{w
1
, w
2
, . . . , w
t−
1
, w
t
,
w
t
+1
. . . , w
n
}
, language models aims to maximize
P
(
w
t
|w
1
, . . . , w
t−
1
)
.
given sequence of nodes
{v
1
, v
2
, . . . , v
t−
1
, v
t
,
v
t
+1
, . . . , v
n
}
it
maximizes:
P
n
t
=1
log
P r
(
v
t
|v
t
+
c
, . . . , v
t
1
, v
t
+1
, . . . , v
t−c
)
glo :
V → R
d
It walks globally and exceeds the ego-network
(TIR Workshop)
University of Passau
29 August 2017
11 / 28
Input layer
Hidden layer
Output layer
x
1
x
2
x
3
x
k
x
V
y
1
y
2
y
3
y
j
y
V
h
1
h
2
h
i
h
N
W
V×N
={w
ki
}
W'
N×V
={w'
ij
}
Figure 1:
A simple CBOW model with only one word in the context
layers are fully connected.
The input is a one-hot encoded vector, which means for a given
input context word,
only one out of V units,
{x
1
, · · · , x
V
},
will
be 1,
and all
other units
are 0.
The weights between the input layer and the hidden layer can be represented by a
V × N matrix W.
Each row of
W is the N -dimension vector representation v
w
of
the
associated word of the input layer.
Formally, row i of W is v
T
w
.
Given a context (a word),
assuming x
k
= 1 and x
k
0
= 0 for k
0
6= k, we have
h = W
T
x = W
T
(k,·)
:= v
T
w
I
,
(1)
which is essentially copying the k-th row of W to h.
v
w
I
is the vector representation of the
input word w
I
.
This implies that the link (activation) function of the hidden layer units is
simply linear (i.e., directly passing its weighted sum of inputs to the next layer).
From the hidden layer to the output layer, there is a different weight matrix W
0
= {w
0
ij
},
which is an N × V matrix.
Using these weights, we can compute a score u
j
for each word
in the vocabulary,
u
j
= v
0
w
j
T
h,
(2)
where v
0
w
j
is the j-th column of
the matrix W
0
.
Then we can use softmax,
a log-linear
classification model, to obtain the posterior distribution of words, which is a multinomial
distribution.
p(w
j
|w
I
) = y
j
=
exp(u
j
)
P
V
j
0
=1
exp(u
j
0
)
,
(3)
where y
j
is the output of the j-th unit in the output layer.
Substituting (1) and (2) into
2
Vector Representation for Social
Graphs
DeepWalk [3]
walks globally over the graph and samples sequences of nodes
treats all
these sequences as an artificial
corpus
feeds the corpus to a Skip-Gram based Word2Vec [5]
Word2Vec:
Having the sequence of words
{w
1
, w
2
, . . . , w
t−
1
, w
t
,
w
t
+1
. . . , w
n
}
, language models aims to maximize
P
(
w
t
|w
1
, . . . , w
t−
1
)
.
given sequence of nodes
{v
1
, v
2
, . . . , v
t−
1
, v
t
,
v
t
+1
, . . . , v
n
}
it
maximizes:
P
n
t
=1
log
P r
(
v
t
|v
t
+
c
, . . . , v
t
1
, v
t
+1
, . . . , v
t−c
)
glo :
V → R
d
It walks globally and exceeds the ego-network
(TIR Workshop)
University of Passau
29 August 2017
11 / 28
Visual Example
Bryan Perozzi
DeepWalk: Online Learning of Social Representations
On Zachary’s Karate Graph:
Input
Output
Zachary’s karate club embedding [2]
Vector Representation for Social
Graphs
node2vec [4]
similar to DeepWalk with two additional
parameters
hyper-parameters
p ∈ R
+
and
q ∈ R
+
control
random walks
q >
1
and
p <
min(
q,
1)
walk locally (BFS)
p >
1
and
q <
min(
q,
1))
walk explorative (DFS)
Even the local
walk can exceed the ego-network
(TIR Workshop)
University of Passau
29 August 2017
12 / 28
could be organized based on communities they belong to (i.e., ho-
mophily);
in other cases,
the organization could be based on the
structural
roles of nodes in the network (i.e.,
structural
equiva-
lence) [7,
10,
36].
For instance,
in Figure 1,
we observe nodes
u
and
s
1
belonging to the same tightly knit community of nodes,
while the nodes
u
and
s
6
in the two distinct communities share the
same structural role of a hub node. Real-world networks commonly
exhibit a mixture of such equivalences. Thus, it is essential to allow
for a flexible algorithm that can learn node representations obeying
both principles:
ability to learn representations that embed nodes
from the same network community closely together,
as well as to
learn representations where nodes that share similar roles have sim-
ilar embeddings.
This would allow feature learning algorithms to
generalize across a wide variety of domains and prediction tasks.
Present work. We propose node2vec, a semi-supervised algorithm
for scalable feature learning in networks.
We optimize a custom
graph-based objective function using SGD motivated by prior work
on natural language processing [21].
Intuitively, our approach re-
turns feature representations that maximize the likelihood of pre-
serving network neighborhoods of nodes in a
d
-dimensional fea-
ture space.
We use a 2
nd
order random walk approach to generate
(sample) network neighborhoods for nodes.
Our key contribution is in defining a flexible notion of a node’s
network neighborhood.
By choosing an appropriate notion of a
neighborhood,
node2vec can learn representations that
organize
nodes based on their network roles and/or communities they be-
long to.
We achieve this by developing a family of biased random
walks, which efficiently explore diverse neighborhoods of a given
node. The resulting algorithm is flexible, giving us control over the
search space through tunable parameters, in contrast to rigid search
procedures in prior work [24, 28].
Consequently, our method gen-
eralizes prior work and can model the full spectrum of equivalences
observed in networks.
The parameters governing our search strat-
egy have an intuitive interpretation and bias the walk towards dif-
ferent network exploration strategies.
These parameters can also
be learned directly using a tiny fraction of labeled data in a semi-
supervised fashion.
We also show how feature representations of individual
nodes
can be extended to pairs of nodes (i.e., edges). In order to generate
feature representations of edges,
we compose the learned feature
representations of the individual nodes using simple binary oper-
ators.
This compositionality lends node2vec to prediction tasks
involving nodes as well as edges.
Our experiments focus on two common prediction tasks in net-
works:
a multi-label
classification task,
where every node is as-
signed one or more class labels and a link prediction task, where we
predict the existence of an edge given a pair of nodes.
We contrast
the performance of node2vec with state-of-the-art feature learning
algorithms [24,
28].
We experiment with several real-world net-
works from diverse domains, such as social networks, information
networks, as well as networks from systems biology.
Experiments
demonstrate that node2vec outperforms state-of-the-art methods by
up to 26.7% on multi-label classification and up to 12.6% on link
prediction.
The algorithm shows competitive performance with
even 10% labeled data and is also robust
to perturbations in the
form of noisy or missing edges. Computationally, the major phases
of node2vec are trivially parallelizable,
and it
can scale to large
networks with millions of nodes in a few hours.
Overall our paper makes the following contributions:
1.
We propose node2vec,
an efficient
scalable algorithm for
feature learning in networks that efficiently optimizes a novel
network-aware, neighborhood preserving objective using SGD.
2.
We show how node2vec is in accordance with established
u 
s
3 
s
2 
s
1 
s
4 
s
8 
s
9 
s
6 
s
7 
s
5 
BFS 
DFS 
Figure 1: BFS and DFS search strategies from node
u
(
k = 3
).
principles in network science, providing flexibility in discov-
ering representations conforming to different equivalences.
3.
We extend node2vec and other feature learning methods based
on neighborhood preserving objectives, from nodes to pairs
of nodes for edge-based prediction tasks.
4.
We empirically evaluate node2vec for multi-label classifica-
tion and link prediction on several real-world datasets.
The rest of the paper is structured as follows.
In Section 2,
we
briefly survey related work in feature learning for networks.
We
present
the technical
details for feature learning using node2vec
in Section 3.
In Section 4,
we empirically evaluate node2vec on
prediction tasks over nodes and edges on various real-world net-
works and assess the parameter sensitivity,
perturbation analysis,
and scalability aspects of our algorithm.
We conclude with a dis-
cussion of the node2vec framework and highlight
some promis-
ing directions for future work in Section 5.
Datasets and a refer-
ence implementation of node2vec are available on the project page:
http://snap.stanford.edu/node2vec.
2.
RELATED WORK
Feature engineering has been extensively studied by the machine
learning community under various headings. In networks, the con-
ventional paradigm for generating features for nodes is based on
feature extraction techniques which typically involve some seed
hand-crafted features based on network properties [8, 11].
In con-
trast, our goal is to automate the whole process by casting feature
extraction as a representation learning problem in which case we
do not require any hand-engineered features.
Unsupervised feature learning approaches typically exploit the
spectral properties of various matrix representations of graphs, es-
pecially the Laplacian and the adjacency matrices. Under this linear
algebra perspective, these methods can be viewed as dimensional-
ity reduction techniques.
Several linear (e.g., PCA) and non-linear
(e.g., IsoMap) dimensionality reduction techniques have been pro-
posed [3,
27,
30,
35].
These methods suffer from both computa-
tional and statistical performance drawbacks. In terms of computa-
tional efficiency, eigendecomposition of a data matrix is expensive
unless the solution quality is significantly compromised with ap-
proximations,
and hence,
these methods are hard to scale to large
networks. Secondly, these methods optimize for objectives that are
not robust to the diverse patterns observed in networks (such as ho-
mophily and structural equivalence) and make assumptions about
the relationship between the underlying network structure and the
prediction task.
For instance,
spectral
clustering makes a strong
homophily assumption that graph cuts will be useful for classifica-
tion [29].
Such assumptions are reasonable in many scenarios, but
unsatisfactory in effectively generalizing across diverse networks.
Recent advancements in representational learning for natural lan-
guage processing opened new ways for feature learning of discrete
objects such as words. In particular, the Skip-gram model [21] aims
to learn continuous feature representations for words by optimiz-
ing a neighborhood preserving likelihood objective. The algorithm
Vector Representation for Social
Graphs
node2vec [4]
similar to DeepWalk with two additional
parameters
hyper-parameters
p ∈ R
+
and
q ∈ R
+
control
random walks
q >
1
and
p <
min(
q,
1)
walk locally (BFS)
p >
1
and
q <
min(
q,
1))
walk explorative (DFS)
Even the local
walk can exceed the ego-network
(TIR Workshop)
University of Passau
29 August 2017
12 / 28
could be organized based on communities they belong to (i.e., ho-
mophily);
in other cases,
the organization could be based on the
structural
roles of nodes in the network (i.e.,
structural
equiva-
lence) [7,
10,
36].
For instance,
in Figure 1,
we observe nodes
u
and
s
1
belonging to the same tightly knit community of nodes,
while the nodes
u
and
s
6
in the two distinct communities share the
same structural role of a hub node. Real-world networks commonly
exhibit a mixture of such equivalences. Thus, it is essential to allow
for a flexible algorithm that can learn node representations obeying
both principles:
ability to learn representations that embed nodes
from the same network community closely together,
as well as to
learn representations where nodes that share similar roles have sim-
ilar embeddings.
This would allow feature learning algorithms to
generalize across a wide variety of domains and prediction tasks.
Present work. We propose node2vec, a semi-supervised algorithm
for scalable feature learning in networks.
We optimize a custom
graph-based objective function using SGD motivated by prior work
on natural language processing [21].
Intuitively, our approach re-
turns feature representations that maximize the likelihood of pre-
serving network neighborhoods of nodes in a
d
-dimensional fea-
ture space.
We use a 2
nd
order random walk approach to generate
(sample) network neighborhoods for nodes.
Our key contribution is in defining a flexible notion of a node’s
network neighborhood.
By choosing an appropriate notion of a
neighborhood,
node2vec can learn representations that
organize
nodes based on their network roles and/or communities they be-
long to.
We achieve this by developing a family of biased random
walks, which efficiently explore diverse neighborhoods of a given
node. The resulting algorithm is flexible, giving us control over the
search space through tunable parameters, in contrast to rigid search
procedures in prior work [24, 28].
Consequently, our method gen-
eralizes prior work and can model the full spectrum of equivalences
observed in networks.
The parameters governing our search strat-
egy have an intuitive interpretation and bias the walk towards dif-
ferent network exploration strategies.
These parameters can also
be learned directly using a tiny fraction of labeled data in a semi-
supervised fashion.
We also show how feature representations of individual
nodes
can be extended to pairs of nodes (i.e., edges). In order to generate
feature representations of edges,
we compose the learned feature
representations of the individual nodes using simple binary oper-
ators.
This compositionality lends node2vec to prediction tasks
involving nodes as well as edges.
Our experiments focus on two common prediction tasks in net-
works:
a multi-label
classification task,
where every node is as-
signed one or more class labels and a link prediction task, where we
predict the existence of an edge given a pair of nodes.
We contrast
the performance of node2vec with state-of-the-art feature learning
algorithms [24,
28].
We experiment with several real-world net-
works from diverse domains, such as social networks, information
networks, as well as networks from systems biology.
Experiments
demonstrate that node2vec outperforms state-of-the-art methods by
up to 26.7% on multi-label classification and up to 12.6% on link
prediction.
The algorithm shows competitive performance with
even 10% labeled data and is also robust
to perturbations in the
form of noisy or missing edges. Computationally, the major phases
of node2vec are trivially parallelizable,
and it
can scale to large
networks with millions of nodes in a few hours.
Overall our paper makes the following contributions:
1.
We propose node2vec,
an efficient
scalable algorithm for
feature learning in networks that efficiently optimizes a novel
network-aware, neighborhood preserving objective using SGD.
2.
We show how node2vec is in accordance with established
u 
s
3 
s
2 
s
1 
s
4 
s
8 
s
9 
s
6 
s
7 
s
5 
BFS 
DFS 
Figure 1: BFS and DFS search strategies from node
u
(
k = 3
).
principles in network science, providing flexibility in discov-
ering representations conforming to different equivalences.
3.
We extend node2vec and other feature learning methods based
on neighborhood preserving objectives, from nodes to pairs
of nodes for edge-based prediction tasks.
4.
We empirically evaluate node2vec for multi-label classifica-
tion and link prediction on several real-world datasets.
The rest of the paper is structured as follows.
In Section 2,
we
briefly survey related work in feature learning for networks.
We
present
the technical
details for feature learning using node2vec
in Section 3.
In Section 4,
we empirically evaluate node2vec on
prediction tasks over nodes and edges on various real-world net-
works and assess the parameter sensitivity,
perturbation analysis,
and scalability aspects of our algorithm.
We conclude with a dis-
cussion of the node2vec framework and highlight
some promis-
ing directions for future work in Section 5.
Datasets and a refer-
ence implementation of node2vec are available on the project page:
http://snap.stanford.edu/node2vec.
2.
RELATED WORK
Feature engineering has been extensively studied by the machine
learning community under various headings. In networks, the con-
ventional paradigm for generating features for nodes is based on
feature extraction techniques which typically involve some seed
hand-crafted features based on network properties [8, 11].
In con-
trast, our goal is to automate the whole process by casting feature
extraction as a representation learning problem in which case we
do not require any hand-engineered features.
Unsupervised feature learning approaches typically exploit the
spectral properties of various matrix representations of graphs, es-
pecially the Laplacian and the adjacency matrices. Under this linear
algebra perspective, these methods can be viewed as dimensional-
ity reduction techniques.
Several linear (e.g., PCA) and non-linear
(e.g., IsoMap) dimensionality reduction techniques have been pro-
posed [3,
27,
30,
35].
These methods suffer from both computa-
tional and statistical performance drawbacks. In terms of computa-
tional efficiency, eigendecomposition of a data matrix is expensive
unless the solution quality is significantly compromised with ap-
proximations,
and hence,
these methods are hard to scale to large
networks. Secondly, these methods optimize for objectives that are
not robust to the diverse patterns observed in networks (such as ho-
mophily and structural equivalence) and make assumptions about
the relationship between the underlying network structure and the
prediction task.
For instance,
spectral
clustering makes a strong
homophily assumption that graph cuts will be useful for classifica-
tion [29].
Such assumptions are reasonable in many scenarios, but
unsatisfactory in effectively generalizing across diverse networks.
Recent advancements in representational learning for natural lan-
guage processing opened new ways for feature learning of discrete
objects such as words. In particular, the Skip-gram model [21] aims
to learn continuous feature representations for words by optimiz-
ing a neighborhood preserving likelihood objective. The algorithm
Social
Circle Prediction by McAuley et al.
[1]
A Probabilistic Classifier
Time Complexity
O
(
n
3
)
(TIR Workshop)
University of Passau
29 August 2017
13 / 28
4:12
J. McAuley and J. Leskovec
ALGORITHM 2: Update memberships node x and circle k.
Data: node x whose membership to circle C
k
is to be updated
Result: updated membership for node x
initialize 
k
x
(0) := 0, 
k
x
(1) := 0;
construct a dummy node x
0
with the communities and features of x but with x /
∈ C
k
;
construct a dummy node x
1
with the communities and features of x but with x ∈ C
k
;
for (c, f ) ∈ dom(types) do
// c = community string,
f = feature string
n := types(c, f );
// n = number of nodes of this type
if S(x) = c ∧ Q(x) = f then
// avoid including a self-loop on x
n := n − 1;
end
construct a dummy node y with community memberships c and features f ;
// first compute probabilities assuming all pairs (x, y)
are non-edges
k
x
(0) := 
k
x
(0) + n log p((x
0
, y) /
∈ E);
k
x
(1) := 
k
x
(1) + n log p((x
1
, y) /
∈ E);
end
for (x, y) ∈ E do
// correct for edges incident on x
k
x
(0) := 
k
x
(0) − log p((x
0
, y) /
∈ E) + log p((x
0
, y) ∈ E);
k
x
(1) := 
k
x
(1) − log p((x
1
, y) /
∈ E) + log p((x
1
, y) ∈ E);
end
// update membership to circle k
types(S(x), Q(x)) := types(S(x), Q(x)) − 1;
z ← U(0, 1);
if z < exp {T ( 
k
x
(1) − 
k
x
(0))} then
S(x)[k] := 1
else
S(x)[k] := 0
end
types(S(x), Q(x)) := types(S(x), Q(x)) + 1;
In terms of Big-O notation, our MCMC algorithm computes updates in worst case
O(|N|
2
) per iteration, whereas our pseudo-boolean optimization algorithm of Section 3
(which is based on maximum flow) requires O(|N|
3
). These are both loose, worst-case
upper bounds:
our MCMC algorithm is much faster if
many nodes share common
community affiliations,
and maximum flow is typically much faster than cubic time.
In practice, our pseudo-boolean optimization procedure is limited by quadratic mem-
ory requirements (energies must be stored for all
pairs of nodes);
this limitation is
circumvented by our MCMC algorithm.
The entire procedure is demonstrated in Algorithm 2.
We also exploit the same observation when computing partial derivatives of the log-
likelihood—that is, we first efficiently compute derivatives under the assumption that
the graph contains no edges and then correct the result by summing over all edges in
E.
6.
DATASET DESCRIPTION
Although our method is unsupervised, we require labeled ground-truth data in order
to evaluate its performance.
We expended significant time,
effort,
and resources to
ACM Transactions on Knowledge Discovery from Data, Vol. 8, No. 1, Article 4, Publication date: February 2014.
4:14
J. McAuley and J. Leskovec
Fig. 3.
Feature construction. Profiles are tree structured, and we construct features by comparing paths in
those trees. Examples of trees for two users x (blue) and y (pink) are shown at top. Two schemes for construct-
ing feature vectors from these profiles are shown at bottom. (1) (bottom left) We construct binary indicators
measuring the difference between leaves in the two trees—for example, ‘work→position→Cryptanalyst’ ap-
pears in both trees. (2) (bottom right) We sum over the leaf nodes in the first scheme, maintaining the fact
that the two users worked at the same institution but discarding the identity of that institution.
7.
CONSTRUCTING FEATURES FROM USER PROFILES
Profile information in all of our datasets can be represented as a tree where each level
encodes increasingly specific information (Figure 3, left). In other words, user profiles
are organized into increasingly specific categories. For example, a user’s profile might
have an education category,
which would be further separated into categories such
as name,
location,
and type.
The leaves of the tree are then specific values in these
categories,such as Princeton,
Cambridge,
and Graduate School.
Several
works deal
with automatically building features from tree-structured data [Haussler 1999; Vish-
wanathan and Smola 2002], but in order to understand the relationship between circles
and user profile information, we shall design our own feature representation scheme.
We propose two hypotheses for how users organize their social circles:
either they
may form circles around users who share some common property with each other, or
they may form circles around users who share some common property with themselves.
For example, if a user has many friends who attended Stanford, then they may form a
ACM Transactions on Knowledge Discovery from Data, Vol. 8, No. 1, Article 4, Publication date: February 2014.
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
14 / 28
Local
Representations using Paragraph Vector
walking locally over an ego-network to generate sequence of nodes
treating this sequence as an artificial
paragraph
applying Paragraph Vector [6] to learn vector representation
given an artificial
paragraph
v
1
, v
2
, v
3
, . . . , v
t
,
. . . , v
l
for ego
u
i
, it maximizes
the average log probability:
l
X
t
=1
log
P r
(
v
t
|u
i
, v
t
+
c
, . . . , v
t−c
)
loc :
U → R
d
(TIR Workshop)
University of Passau
29 August 2017
15 / 28
Local
Representations using Paragraph Vector
walking locally over an ego-network to generate sequence of nodes
treating this sequence as an artificial
paragraph
applying Paragraph Vector [6] to learn vector representation
given an artificial
paragraph
v
1
, v
2
, v
3
, . . . , v
t
,
. . . , v
l
for ego
u
i
, it maximizes
the average log probability:
l
X
t
=1
log
P r
(
v
t
|u
i
, v
t
+
c
, . . . , v
t−c
)
loc :
U → R
d
(TIR Workshop)
University of Passau
29 August 2017
15 / 28
Social
Circle Prediction
Setting
social
network
G
= (
V, E
)
with egos
U
and alters
V \ U
profile information
(
v.
feat
1
, . . . , v.
feat
f
)
for every
v ∈ V
Input/Output
predict social
circles
c
:
V \ U → {C
1
, . . . , C
k
}
∗
given several
samples
Approach
Feature selection (users’ profile information, graph embeddings)
A Neural
Network Classifier
(TIR Workshop)
University of Passau
29 August 2017
16 / 28
Incorporating Profile Information
Similarity of ego’s and alter’s profile as a feature
ego’s profile feature:
u.
feat
1
, . . . , u.
feat
f
alter’s profile feature:
v.
feat
1
, . . . , v.
feat
f
sim(
u, v
) = (
b
1
, . . . , b
f
)
, where
b
i
=
(
1
if
u.
feat
i
=
v.
feat
i
,
0
otherwise
.
(1)
(TIR Workshop)
University of Passau
29 August 2017
17 / 28
Social
Circle Prediction
A feed-forward neural
network classifier
Predicting social
circle for alter
v
which
belongs to ego-network of ego
u
Input layer:
locglo:
loc(
u
)
⊕
glo(
v
)
gloglo:
glo(
u
)
⊕
glo(
v
)
locgloglo:
loc(
u
)
⊕
glo(
u
)
⊕ glo
(
v
)
locglosim:
loc(
u
)
⊕
glo(
v ⊕
sim(
u, v
)
gloglosim:
glo(
u
)
⊕
glo(
v
)
⊕
sim(
u, v
)
locgloglosim:
loc(
u
)
⊕
glo(
u
)
⊕
glo(
v
)
⊕
sim(
u, v
)
Hidden layer:
a single dense layer with ReLU activation units
Output layer:
softmax units (same number as circles)
Ground-truth
alter’s circle label
(family, colleagues, etc)
(TIR Workshop)
University of Passau
29 August 2017
18 / 28
Hidden layer
Input layer
Output layer
.
.
.
.
.
.
.
.
.
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
19 / 28
Dataset
Table 1: Statistics of Social
Network Datasets [7]
Facebook
Twitter
Google+
nodes
|V |
4,039
81,306
107,614
edges
|E|
88,234
1,768,149
13,673,453
egos
|U |
10
973
132
circles
|C|
46
100
468
features
f
576
2,271
4,122
(TIR Workshop)
University of Passau
29 August 2017
20 / 28
Experimental
Results
Table 2: Performance (
F
1
-score) of different embeddings for circle prediction on
three dataset.
Standard deviation is less than
0
.
02
for all
experiments.
Approach
Facebook
Twitter
Google+
gloglo
0.37
0.46
0.49
locglo
0.42
0.50
0.52
locgloglo
0.37
0.44
0.48
gloglosim
0.40
0.49
0.51
locglosim
0.45
0.53
0.55
locgloglosim
0.38
0.46
0.47
Φ
1
, McAuley & Leskovec [1]
0.38
0.54
0.59
(TIR Workshop)
University of Passau
29 August 2017
21 / 28
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
22 / 28
Future Work
Using embeddings to approximate more complex measures (e.g.
shortest-path distance)
Using embedding to find similar egos
Learning embedding for directed graphs
(TIR Workshop)
University of Passau
29 August 2017
23 / 28
Outline
1
Introduction
Graph Embedding
Ego-Network Analysis
2
Contributions
3
State-of-the-art
4
Approach
5
Experimental
Results
6
Future Work
7
References
(TIR Workshop)
University of Passau
29 August 2017
24 / 28
References
McAuley, Julian J., and Jure Leskovec. ”Learning to Discover Social
Circles in Ego Networks.” In NIPS, vol. 2012, pp.
548-56. 2012.
Muhammad, Syed Agha, and Kristof Van Laerhoven. ”DUKE: A Solution for Discovering Neighborhood Patterns in Ego
Networks.” In ICWSM, pp. 268-277. 2015.
Perozzi, Bryan, Rami
Al-Rfou, and Steven Skiena. ”DeepWalk:
Online learning of social
representations.” In Proceedings
of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining, pp. 701-710. ACM, 2014.
Grover, Aditya, and Jure Leskovec. ”node2vec:
Scalable feature learning for networks.” In Proceedings of the 22nd ACM
SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 855-864. ACM, 2016.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and
their compositionality. In NIPS, 2013
Le, Quoc V., and Tomas Mikolov. ”Distributed Representations of Sentences and Documents.” In ICML, vol. 14, pp.
1188-1196. 2014.
https://snap.stanford.edu/data/
(TIR Workshop)
University of Passau
29 August 2017
25 / 28
Word2vec [5]
Language Modeling
Distributional
Hypothesis in the natural
languages:
semantically similar
words dispose to appear in similar word neighborhoods
Having the sequence of words
{w
1
, w
2
, ..., w
t−
1
, w
t
, ..., w
n
}
, language
models aims to maximize
P
(
w
t
|w
1
, ..., w
t−
1
)
.
In word2vec [2], they defined a fix context length surrounding each word
with length context c and the sequence of words
{w
1
, w
2
, ..., w
t−
1
, w
t
, ..., w
n
}
the goal
is to word2vec is to maximize:
P
n
t
=1
log
P
(
w
t
|w
t
+
c
, ..., w
t−c
)
The neural
network which learns word representations:
One hidden layer
The number of input layer entries is equal
to the vocabulary size of the
text
The number of units in the hidden layer determines dimensionality of
the vectors
(TIR Workshop)
University of Passau
29 August 2017
26 / 28
Word2vec [5]
Having a sequence of words
{w
1
, w
2
, ..., w
t−
1
, w
t
, ..., w
n
}
and context
window with length one
Predict one target word, given one context word
P
(
w
t
|w
t−
1
)
.
Input layer
Hidden layer
Output layer
x
1
x
2
x
3
x
k
x
V
y
1
y
2
y
3
y
j
y
V
h
1
h
2
h
i
h
N
W
V×N
={w
ki
}
W'
N×V
={w'
ij
}
Figure 1:
A simple CBOW model with only one word in the context
layers are fully connected.
The input is a one-hot encoded vector, which means for a given
input context word,
only one out of V units,
{x
1
, · · · , x
V
},
will
be 1,
and all
other units
are 0.
The weights between the input layer and the hidden layer can be represented by a
V × N matrix W.
Each row of
W is the N -dimension vector representation v
w
of
the
associated word of the input layer.
Formally, row i of W is v
T
w
.
Given a context (a word),
assuming x
k
= 1 and x
k
0
= 0 for k
0
6
= k, we have
h = W
T
x = W
T
(k,·)
:= v
T
w
I
,
(1)
which is essentially copying the k-th row of W to h.
v
w
I
is the vector representation of the
input word w
I
.
This implies that the link (activation) function of the hidden layer units is
simply linear (i.e., directly passing its weighted sum of inputs to the next layer).
From the hidden layer to the output layer, there is a different weight matrix W
0
= {w
0
ij
},
which is an N × V matrix.
Using these weights, we can compute a score u
j
for each word
in the vocabulary,
u
j
= v
0
w
j
T
h,
(2)
where v
0
w
j
is the j-th column of
the matrix W
0
.
Then we can use softmax,
a log-linear
classification model, to obtain the posterior distribution of words, which is a multinomial
distribution.
p(w
j
|w
I
) = y
j
=
exp(u
j
)
P
V
j
0
=1
exp(u
j
0
)
,
(3)
where y
j
is the output of the j-th unit in the output layer.
Substituting (1) and (2) into
2
Each input is a one-hot encoding vector
The probability is computed using the softmax function:
h
=
W
T
× x,
u
=
W
0T
× h,
P
(
w
t
|w
t−
1
) =
y
t
=
e
u
t
P
V
i
=1
e
u
j
After several
iterations matrix
W
will
not change
(TIR Workshop)
University of Passau
29 August 2017
27 / 28
Paragraph Vector
Given a sequence of paragraphs
p
1
, p
2
, ..., p
q
and training words
w
1
, w
2
, w
3
, . . . , w
t
, . . . w
n
, the idea of Paragraph Vector [6] is to maximize
p
(
w
t
|p
j
, w
t−c
..., w
t
+
c
))
For example consider 2 paragraphs and window size of 3
P
1
:
The cat sat
on the mat
P
2
:
I ate potato crisps for evening snack
p
(
on|P
1
, T he, cat, sat
)
Distributed Representations of Sentences and Documents
example, “powerful” and “strong” are close to each other,
whereas “powerful” and “Paris” are more distant.
The dif-
ference between word vectors also carry meaning.
For ex-
ample,
the word vectors can be used to answer analogy
questions using simple vector algebra:
“King” - “man” +
“woman” = “Queen” (Mikolov et al., 2013d). It is also pos-
sible to learn a linear matrix to translate words and phrases
between languages (Mikolov et al., 2013b).
These properties make word vectors attractive for many
natural language processing tasks such as language mod-
eling (Bengio et
al.,
2006;
Mikolov,
2012),
natural
lan-
guage understanding (Collobert
& Weston,
2008;
Zhila
et al., 2013), statistical machine translation (Mikolov et al.,
2013b;
Zou et
al.,
2013),
image understanding (Frome
et al., 2013) and relational extraction (Socher et al., 2013a).
2.2. Paragraph Vector: A distributed memory model
Our approach for learning paragraph vectors is inspired by
the methods for learning the word vectors.
The inspiration
is that the word vectors are asked to contribute to a predic-
tion task about the next word in the sentence.
So despite
the fact that the word vectors are initialized randomly, they
can eventually capture semantics as an indirect result of the
prediction task. We will use this idea in our paragraph vec-
tors in a similar manner.
The paragraph vectors are also
asked to contribute to the prediction task of the next word
given many contexts sampled from the paragraph.
In our Paragraph Vector framework (see Figure 2),
every
paragraph is mapped to a unique vector,
represented by a
column in matrix
D
and every word is also mapped to a
unique vector, represented by a column in matrix
W
.
The
paragraph vector and word vectors are averaged or concate-
nated to predict the next word in a context.
In the experi-
ments, we use concatenation as the method to combine the
vectors.
More formally,
the only change in this model
compared
to the word vector framework is in equation 1, where
h
is
constructed from
W
and
D
.
The paragraph token can be thought of as another word.
It
acts as a memory that remembers what is missing from the
current
context
– or the topic of the paragraph.
For this
reason,
we often call this model the Distributed Memory
Model of Paragraph Vectors (PV-DM).
The contexts are fixed-length and sampled from a sliding
window over the paragraph. The paragraph vector is shared
across all contexts generated from the same paragraph but
not across paragraphs.
The word vector matrix
W
,
how-
ever, is shared across paragraphs. I.e., the vector for “pow-
erful” is the same for all paragraphs.
The paragraph vectors and word vectors are trained using
stochastic gradient descent and the gradient is obtained via
backpropagation.
At every step of stochastic gradient de-
scent, one can sample a fixed-length context from a random
paragraph, compute the error gradient from the network in
Figure 2 and use the gradient to update the parameters in
our model.
At prediction time, one needs to perform an inference step
to compute the paragraph vector for a new paragraph. This
is also obtained by gradient descent.
In this step,
the pa-
rameters for the rest of the model, the word vectors
W
and
the softmax weights, are fixed.
Suppose that
there are
N
paragraphs in the corpus,
M
words in the vocabulary,
and we want to learn paragraph
vectors such that
each paragraph is mapped to
p
dimen-
sions and each word is mapped to
q
dimensions,
then the
model
has the total
of
N × p + M × q
parameters (ex-
cluding the softmax parameters).
Even though the number
of parameters can be large when
N
is large,
the updates
during training are typically sparse and thus efficient.
Figure 2. A framework for learning paragraph vector. This frame-
work is similar to the framework presented in Figure 1; the only
change is the additional paragraph token that is mapped to a vec-
tor via matrix
D
.
In this model, the concatenation or average of
this vector with a context
of three words is used to predict
the
fourth word.
The paragraph vector represents the missing infor-
mation from the current context and can act as a memory of the
topic of the paragraph.
After being trained,
the paragraph vectors can be used as
features for the paragraph (e.g.,
in lieu of or in addition
to bag-of-words).
We can feed these features directly to
conventional machine learning techniques such as logistic
regression, support vector machines or K-means.
In summary,
the algorithm itself has two key stages:
1)
training to get word vectors
W
, softmax weights
U, b
and
paragraph vectors
D
on already seen paragraphs;
and 2)
“the inference stage” to get paragraph vectors
D
for new
paragraphs (never seen before) by adding more columns
in
D
and gradient descending on
D
while holding
W, U, b
fixed. We use
D
to make a prediction about some particular
labels using a standard classifier, e.g., logistic regression.
(TIR Workshop)
University of Passau
29 August 2017
28 / 28
