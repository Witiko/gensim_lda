Modeling Taxi Drivers’ Behaviour for the Next
Destination Prediction
Alberto Rossi
1
, Gianni Barlacchi
2,4
, Monica Bianchini
3
, and Bruno Lepri
4
1
University of Florence, Florence, Italy alberto.rossi@unifi.it
2
University of Trento, Trento, Italy gianni.barlacchi@gmail.com
3
University of Siena, Siena, Italy monica@diism.unisi.it
4
Fondazione Bruno Kessler, Trento, Italy lepri@fbk.eu
Abstract.
Taxi destination prediction is a very important task for op-
timizing the efficiency of
electronic dispatching systems,
thus allowing
relevant advantages for both taxi companies and customers. In fact, dur-
ing periods of high demand,
there should be a taxi
whose current ride
will
end near a requested pick up location from a new customer.
If an
electronic dispatcher is able to know in advance where all taxi drivers will
end their current ride, it will also be able to better allocate its resources,
identifying which taxi to assign to each call. Moreover, automatic systems
for the taxi mobility monitoring collect data that, integrated with other
information sources, can help in understanding daytime human mobility
routines. In this paper, we introduce a novel approach for addressing the
taxi destination prediction problem, based on Recurrent Neural Networks
(RNNs) applied to a regression setting. RNNs are trained based on the
individual
drivers’
history and on geographical
information (i.e.,
points
of interest),
using only the starting point of each ride (with no knowl-
edge about the whole trajectory). The proposed approach was tested on
the dataset of the ECML/PKDD Discovery Challenge 2015 - based on
the city of Porto - obtaining better results with respect to the competi-
tion winner,
whilst using less information,
and on Manhattan and San
Francisco datasets.
1
Introduction
World population is increasingly moving from rural areas to urban centers, mak-
ing large cities densely populated. Actually, approximately 54% of people world-
wide live in cities,
especially in metropolis,
which offer both greater working
and leisure opportunities.
In fact,
in urban centers there is greater access to
work, a wide variety of options for education and training, ease of transport and
abundance of attractive places within a few kilometers. Across huge cities people
tend to move more and have to do it faster than in the past. On the other hand,
heavy traffic (e.g.,
traffic jams,
severe traffic congestions,
etc.) can cause noise
and atmospheric pollution (i.e., smog, hydrocarbon concentration, and exhaust
gas emissions). Optimizing the public transportation system can therefore help
in improving the quality of
citizens’
lives,
both by facilitating their mobility
arXiv:1807.08173v1 [cs.AI] 21 Jul 2018
and ensuring their health. The research in this field can also support transport
companies to give a better service, in terms of waiting times for the customers,
fuel
saving,
traffic reduction,
and ease of
mobility.
Indeed,
in a smart city en-
vironment,
where real-world urban data,
captured by sensors,
actuators,
and
mobile devices,
are analyzed based on network infrastructures,
there is a great
demand for effective,
efficient and more sustainable transportation services.
In
this way,
the ability to forecast and manage urban flows is improved,
integrat-
ing the dimensions of the physical, digital and institutional spaces of a regional
agglomeration [1].
In particular,
interesting research has been carried out in recent years for
the taxi
mobility prediction problem.
Current advances in technology have,
in
fact, made available a variety of low-cost electronic devices, which can be of great
support to rapidly move in the city traffic. Inside taxis, electronic GPS terminals
are installed, able to grab the vehicle position or the taximeter status, and send
this information to the dispatcher unit.
In addition to GPS data,
there are
novel sources of information, e.g., data from mobile phones and Location Based
Social
Networks (LBSNs),
that can be exploited to model
the human mobility
behaviour and to optimize the city traffic.
Common LBSNs,
like Foursquare
5
,
for instance,
can provide the number and type of activities present in a target
area (e.g., Arts & Entertainment, Nightlife Spot, etc.), giving an insight on how
many people can converge to that zone. All the collected information can then
be used to infer the taxi destinations and the average travel time.
In human mobility, the most straightforward way to predict the next location
is to build a grid over an area of interest, then treating the problem as a multiclass
classification,
where the aim is to predict the next visited cell.
For instance,
in
the winning approach [2]
of
the ECML/PKDD 2015 challenge
6
,
the next taxi
destination is obtained by employing a Multi-Layer Perceptron (MLP) network
trained on the taxi trajectory, represented as a variable-length sequence of GPS
points, and on diverse associated meta-information, such as the departure time,
the driver identity and the client information.
Some of
the limitations of
previous approaches [2,3]
resides in the need of
using the whole trajectory and in the possibility of performing an instantaneous
prediction of the destination only when most of the trajectory is available.
In this paper, we study models for improving the performance in predicting
the next taxi
destination.
For this purpose,
a Long-Short-Term-Memory net-
work [4],
equipped with an attention module, is trained to predict the drop-off
latitude and longitude,
based on a regression framework.
Our model
grounds
on the individual
driver’s history,
intended as the sequence of
the last visited
points,
i.e.,
pick-up or drop-off points,
and not on the whole GPS trajectory.
To each of these points,
the geographical
information coming from Foursquare
is attached. We test our model in three different cities, namely Porto, as in the
ECML/PKDD 2015 challenge,
New York City (more specifically,
Manhattan),
and San Francisco.
We compare our methods with the winning model
of
the
5
www.foursquare.com
6
https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i
ECML/PKDD 2015 challenge under various settings.
Our regression approach
outperforms the winning models by more than 1.357 km in average error dis-
tance, giving an improvement of 42%.
The rest of the paper is organized as follows. Section 2 provides the state of
the art on the taxi mobility and destination prediction tasks, whereas Section 3
presents an in-depth description of the problem.
In Section 4,
we describe the
used data and our methodological
approach.
Section 5 delineates the proposed
predictive model,
while Section 6 illustrates the experimental
setup,
together
with the obtained results. Finally, conclusions and future research are drawn in
Section 7.
2
Related Work
A particularly interesting problem for taxi companies is the optimization of the
total profit guaranteed by the entire fleet. This comprises the definition of a com-
prehensive mobility pattern able,
for instance,
to help drivers to decide which
of
two potential
rides will
be more lucrative.
Two interesting case studies are
reported in [5]
and [6],
with respect to fare data related to Bangkok (collected
by the Department of Land Transport of Thailand) and to New York City,
re-
spectively.
Moreover,
in [7]
a model
to approach this task was proposed,
using
reinforcement learning, whereas, in [8], an approach for quantifying the benefits
of sharing a taxi ride based on a shareability network is illustrated, which trans-
lates spatio-temporal
sharing problems into a graph framework with efficient
solutions. In [9], it is shown that not always the shortest path between the origin
and the destination is the best option for a taxi. Indeed, it is found that drivers
choose a sub-optimal path in 90% of the cases, instead of the optimal one.
In a follow-up study of
[8],
Vazifeh et
al.
[10]
provide a network-based so-
lution to a “minimum fleet problem”,
namely that of
searching the minimum
number of vehicles needed to serve all the trips without incurring any delay to
the passengers.
The method was tested on a data set of 150 million taxi
trips
taken in the city of New York over one year.
The real-time implementation of
this approach with near-optimal
service levels,
allows a 30% reduction in the
fleet size compared to current taxi operations. These improvements follow from
a mere re-organization of the taxi dispatching, without assuming ride sharing or
requiring changes to regulations.
Investigating the taxi mobility is also useful to suggest where to move in order
to pick up the next passenger. Indeed, in [11], taxi trajectories are employed to
predict if it is preferable to wait in an ad hoc parking area or to travel without
passengers, in order to increase the probability of finding new customers. Also in
[12], a Random Forest regression approach is proposed to predict the taxi pickup
zones in New York City (NYC), based only on start/end points of trajectories,
and on geographical
and Twitter data.
A recommendation model
for both the
driver and the customer is implemented in [11], in order to reduce the distance
covered without any passenger for the former and the waiting time for the latter.
In [13], an auto-regressive integrated moving average (ARIMA) method is built
to forecast the spatio-temporal variation of passengers in a hotspot, discovering
patterns of pick-up quantity (PUQ) for urban hotspots.
Many mobility approaches are based on Markov models and their variations.
For instance, in [14,15], the probability of the future displacement is computed
through a transition matrix between city hotspots,
obtained by the past tra-
jectories. By applying trajectory pattern mining, Monreale et al. [16] predicted
the next location at a certain level
of
accuracy by using GPS data.
Finally,
a
rich literature exists exploiting the usage of neural networks to model the prob-
lem of next location prediction,
for instance [17,18,3,19],
sometimes showing a
high computational complexity and some applicability limitations but, neverthe-
less, proposing interesting solutions to capture the regularities present in human
mobility based on taxi
rides.
In order to perform taxi
trajectory prediction,
in
[20] trajectories are modeled as two-dimensional images, and then a Multi-Layer
Convolutional neural networks is used to predict the next destination point.
3
Background
In this section,
we provide a formal
definition of
the taxi
destination predic-
tion problem,
introducing the necessary notation and background information.
Then,
we briefly sketch the main characteristics of
the neural
network model,
namely Long-Short-Term-Memory (LSTM), employed to face the problem, also
explaining how LSTMs can be equipped with an attention module.
3.1
Notation and problem definition
The general
idea behind this research is to predict the destination of
a taxi
trip, relying on some (partial) information on the trajectory, and on the starting
point information about the trip (e.g., timestamp, call id, etc.). More precisely,
the definition of trajectory can be stated as follows.
Definition 1.
Given a trip u, the trajectory tr
u
, followed by the taxi during u,
is the list of n tuples composed by the location of the taxi - defined by the couple
(latitude, longitude) - at time t:
tr
u
= q
1
, q
2
, q
3
, . . . , q
n
,
where q
t
= (location(lat, long)
t
, t), t = 1, . . . , n.
Then, the taxi destination prediction task can be formalized as the problem of
predicting q
n
, the destination of the current trip u, based on its partial trajectory
tr
u
= q
1
, q
2
, q
3
, . . . , q
n−1
.
Nonetheless, in the present study we aim at predicting the destination as soon
as the trip is started, therefore having no information on the current trajectory,
except for its initial
point.
Instead,
we use some “historical
data”,
namely the
pick-up and drop-off locations visited by a certain driver during his/her recent
trips.
Thus,
we reformulate the concept of
trajectory in a more general
and
driver-based sense.
Definition 2.
Given a driver d, a d-trajectory dt is the list of 2m tuples com-
posed by the location of d’s taxi at the pick-up and drop-off points (indicated with
P and D, respectively) relative to the last m trips:
dt
d
= q
1P
, q
1D
, q
2P
, q
2D
, . . . , q
mP
, q
mD
,
where
q
iL
= (location(lat, long)
i
L, i), i = 1, . . . , m, L ∈ {P, D}.
Then,
given a driver d,
the taxi
destination prediction task can be formalized
as the problem of predicting q
mD
,
the destination of the current trip m,
based
on dt
d
= q
1P
, q
1D
, q
2P
, q
2D
, . . . , q
(m−1)P
, q
(m−1)D
∪ q
mP
(i.e.,
based on the last
m − 1 trips and on the starting point of the current ride).
3.2
Recurrent Neural
Networks and Long-Short-Term-Memory
architectures
Recurrent Neural Networks (RNNs) are equipped with feedback connections that
produce internal loops. Such loops induce a recursive dynamics within the net-
works and thus introduce delayed activation dependencies across the processing
elements. In doing so, RNNs develop a kind of memory, that makes them partic-
ularly tailored to process temporal (or even sequential) data, e.g., coming from
written text, speech, or genome and protein sequences. Unfortunately, properly
training RNNs is hard,
due to both the vanishing and the exploding gradient
pathologies, which introduce the long-term dependency problem, making it dif-
ficult the processing of long sequences.
Long-Short-Term-Memory networks
-
introduced in [21]
and usually just
called LSTMs - are a special
kind of
RNN,
capable of
learning long-term de-
pendencies.
All
these networks have the form of
a chain of
repeated modules
but, if in standard architectures such module has a very simple structure, like a
single tanh layer, in LSTMs there are four interacting modules with very different
functions.
In particular, the forget layer f
t
selects the part of the cell state h
t−1
which
is responsible for removing the information no longer required for the LSTM to
carry on its task. This allows the optimization of the LSTM performances. The
forget gate processes also x
t
, i.e., the input at the current time step. Instead, the
input gate i
t
, whose elaboration depends on h
t−1
and x
t
, is responsible for the
addition of information to the cell
state at time t;
next,
a tanh layer creates a
vector of new candidate values,
˜
C
t
, possibly added to the cell state. Then, these
two sources of
information are combined to create an updated state.
Finally,
the output gate o
t
selects those parts of the cell state that must be produced in
output.
The complete computing algorithm for LSTMs can be summarized as follows:
f
t
= σ(W
f
[h
t−1
, x
t
] + b
f
)
i
t
= σ(W
i
[h
t−1
, x
t
] + b
i
)
˜
C
t
= tanh(W
C
[h
t−1
, x
t
] + b
C
)
C
t
= f
t
∗ C
t−1
+ i
t
∗
˜
C
t
o
t
= σ(W
o
[h
t−1
, x
t
] + b
o
)
h
t
= o
t
tanh(C
t
)
(1)
3.3
The attention mechanism
One of the most interesting human cognitive processes is the attention mecha-
nism,
which is well
studied in neuroscience.
In fact,
just looking at an image,
humans are able to focus on certain regions with high resolution, while perceiv-
ing the surrounding parts as background, and then adjusting the focal point over
time.
A very similar approach can be attached to deep learning tasks - even if
attention in neural
networks is,
indeed,
loosely related to the visual
attention
mechanism found in humans - and it has been applied to a very wide range of
applications, such as speech recognition [22], machine translation [23], text sum-
marization [24], and image description [25]. Moreover, attention mechanisms are
widely used also in few-shot learning [26][27]
in order to do a comparison be-
tween an unknown sample and a set of labeled samples called the support set,
indeed focusing the attention on the most similar support samples to the queried
one.
The main idea behind attention is to produce a score for each element of the
current input (in the sequence case, for instance, a score for each timestamp of the
sequence itself). The implementation of this mechanism is quite straightforward
and suited to this case study.
In fact,
formally speaking,
an attention model
is a method that takes n arguments y
1
, . . . , y
n
and a context C,
and returns
a vector a which is supposed to be the “summary” of
the inputs,
focused on
particular information dictated by the context C.
This can be obtained as a
weighted arithmetic mean of y
i
, i = 1, . . . , n, with the weights chosen according
to the relevance of each y
i
, given the context C.
4
Data
Taxi
trajectory data are of
great interest for the observation,
evaluation,
and
optimization of transportation infrastructures and policies. For example, major
problems of modern cities,
such as traffic jams,
frequently occurring accidents,
and unsatisfied public transport services, are caused by an improper road plan-
ning, maintenance, and control. Actually, the emerging taxi trajectory data pro-
vide useful
information for modeling and understanding urban transportation
patterns and human mobility behaviors.
Moreover,
taxi
trajectories are avail-
able for several large cities [28,8,29]. In our study, we focus on Porto, Manhat-
tan (New York City)
7
, and San Francisco [30]. In particular, the Porto dataset
was released in the context of the ECML/PKDD 2015 challenge and hosted as
a Kaggle competition
8
,
which allows us to easily compare our model
with the
winner’s approach.
Finally, it is worth noting that the proposed methodology is based also on ge-
ographical data sources, like the points-of-interest obtained from the Foursquare
API. For the sake of reproducibility, data, preprocessing tools and models eval-
uated in this paper were made freely availble
9
.
In the following subsections we
describe in details the two different types of data.
4.1
Taxi mobility data
Pickup intensities
0.0003 
0.0027 
0.0071 
0.0117 
0.0171 
0.0244 
0.0333 
0.0502 
0.0799 
1.0000 
Legenda
Dropoﬀ intensities
0.0003 
0.0027 
0.0071 
0.0117 
0.0171 
0.0244 
0.0333
0.0502 
0.0799 
1.0000 
Legenda
Fig. 1.
Pick–up (left) and drop–off (right) intensities in the city of
Porto.
Data are
linearly scaled between 0 and 1.
The Porto taxi
ride dataset is composed by 1.7 million records,
coming from
442 taxis running in Porto, for a period ranging from 2013-07-01 to 2014-06-30.
Figure 1 reports the pick-up and drop-off distributions in the city. For each ride,
a polyline, describing the complete taxi trajectory (as stated in Definition 1), is
recorded:
in particular,
the first point represents the pick-up location,
whereas
the last one is the drop-off place. The trajectory is sampled every 15 seconds, but
not all the samples are provided. In addition, other metadata, like, for instance,
the taxi
id,
the type and origin of the call,
the day type (i.e.,
holiday,
working
day, weekend), and the timestamp of the beginning of the ride, are attached to
each trip. Such information is useful in order to detect some recurrent patterns,
such as the flow of people who go to work or return home in specific time slots
on working days.
7
https://chriswhong.com/open-data/foil nyc taxi/
8
https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i
9
https://bitbucket.org/alberto91c/taxidestinationprediction
The Manhattan dataset collects data related to 13426 taxis, driven by 32224
drivers, during the entire 2013. Each month accounts for approximately 15 mil-
lion trips.
We selected only the first three months,
considering the most 5994
active drivers,
and gathering 9362829 trips.
Only the start and the end-points
are provided for each trajectory.
The last dataset employed in our experiments is related to 536 taxis of the
San Francisco Bay Area.
It collects the GPS trajectory coordinates,
sampled
every 10 seconds and obtained over 30 days, during 2008, accounting for 464019
trips.
In addition to the trajectory data,
both these datasets
also provide the
driver’s id and the starting time of the ride.
TIME STEP
SAMPLE
FEATURE
P
t-4
D
t-4
P
t-3
D
t-3
P
t-2
D
t-2
P
t-1
D
t-1
P
t
Fig. 2. The driver’s trajectory fed into our model (P marks the pick–up points and D
marks the drop–off points).
In order to construct the specific driver trajectory (following Definition 2), we
group together all the trips of a driver, sorting them in an ascending way, based
on their timestamp.
Then,
a trajectory (Def.
2) is created based on a sequence
of five trips, separated by less than three hours from each other, and consists of
five pick-up/drop-off couples of
points,
where the first nine points are used as
training data, whereas the last arrival point represents the target. Each trip can
be contained in a unique trajectory (Def. 2). We set the length of the sequence
to five trips just to guarantee a significant number of generated trajectories, thus
allowing a good set of examples for the training.
Following this procedure,
we
obtain 260600 trajectories for Porto, 87548 for San Francisco, and more than a
million for Manhattan.
In the last case,
we select 600 drivers out of the initial
5994, for computational reasons, obtaining 184000 sequences.
For a detailed explanation of the trajectories (Def. 2) generation process see the
Algorithm 1.
Algorithm 1 Driver’s trajectory generation
1:
trj ← [ ]
2:
driver trips ← dataset.groupby([taxi id])
3:
for group in driver trips do
4:
group.sort([t stmp], ascending)
5:
group len ← len(group)
6:
if group len > 4 then
7:
i ← 0
8:
while i < (group len − 5) do
9:
flag time ← 0
10:
for j in range(i+1,i+5) do
11:
diff ← |(group.t stmp[j]-group.t stmp[j-1])|
12:
if diff > 3 hour then
13:
flag time ← 1
14:
break
15:
end if
16:
end for
17:
if flag time==0 then
18:
trj.append(group[i:(i+5)]
19:
i ← i+5
20:
else
21:
i ← i+1
22:
end if
23:
end while
24:
end if
25:
end for
4.2
Points Of Interest
Points-Of-Interest (POIs) indicates significant places present in a specific area.
They are organized in a hierarchical structure, where each category has a finite
number of subcategories. Moving from the root to the leaves, the description of
the particular point of interest becomes gradually more refined. We focused on
the following ten macro-categories for the POIs:
1.
Arts and Entertainment,
2.
College and University,
3.
Event,
4.
Food,
5.
Nightlife Spot,
6.
Outdoors and Recreation,
7.
Professional
and Other Places,
8.
Residence,
9.
Shop and Service,
10.
Travel
and Transport.
The presence of a concentration of POIs in a particular urban area generally
accounts for a huge number of trips both moving towards and starting from that
area.
We extracted 8928 POIs for Porto,
72567 for Manhattan,
and 30059 for
San Francisco.
5
The model
Previous works have mainly focused on a fine-grained single taxi trajectory (Def.
1) to predict the next destination [2].
However,
this approach has several
limi-
tations such as the huge amount of data to be stored, i.e., all the GPS points of
a trip,
and,
more important,
the need to have most of the trajectory available
in order to predict the next destination.
On the other hand,
the main advantage of
our method resides in using a
less amount of
memory to store data - two GPS points (pick-up and drop-off
locations) instead of the whole trajectory obtained at a certain sampling rate -
and in performing an instantaneous prediction of the destination as soon as the
trip start.
Moreover our architecture is suited to deal with temporal information, while a
standard MLP is not. In our framework, we also define an attention mechanism
in order to increase the focus on the most informative part of
the trajectory.
Finally,
in most of the mobility models the next location prediction is set as a
classification problem, where the goal is to classify to which of the seen locations
our user is moving [3,19].
The main drawback of
this approach is that many
locations will
never be produced by the model,
i.e.,
unseen locations in the
training set.
To overcome this limitation,
we propose to model
the problem as
a regression task on latitude and longitude data by approximating directly two
different functions (one for the latitude and one for the longitude, respectively).
5.1
General framework
The proposed neural network architecture is illustrated in Figure 3. Our frame-
work consists of three main components: (i) Feature Extraction and Embedding,
(ii) Recurrent Module with Attention, and (iii) Prediction. First, we model each
location with an embedding layer in order to capture all the factors that influ-
ence the human mobility, such as time, day and the semantic characterization of
each place. In addition, we represent each location as a single word and we apply
Word2Vec [31] on the resulting sequences in order to obtain a dense representa-
tion. Then, with a recurrent module, we capture all the sequential information
we can derive from the past visited locations. We use an LSTM architecture as
the basic recurrent unit, because of its effectiveness in modeling human mobility
[17]. Following the idea of [19], we apply the attention mechanism on the input
sequence in order to capture mobility regularities from previous visited locations.
Finally, a prediction module composed by a softmax layer, followed by a linear
layer,
is designed to estimate the latitude and the longitude values of the next
taxi destination.
Cluster
embedding
BOC 
Attention Module 
LSTM 
Softmax 
Dropout
Lat
Long
Centroid
longitude 
inizialization
Centroid 
latitude 
inizialization
Embedding Layer
Feature extraction 
and embedding
Recurrent model 
and attention
Prediction 
Taxi_ID Stand_ID Hour Weekday Day_type
user
time
location
Location Cluster
dim=1
dim=1
dim=1
dim=1
dim=1
dim=1
dim=10
dim=20
dim=10
dim=10
dim=10
dim=10
dim=10
Fig. 3. The architecture of our model with the three submodules: (i) Feature Extraction
and Embedding, (ii) Recurrent Model and Attention, and (iii) Prediction.
5.2
Feature extraction and embedding
Mobility transitions are governed by multiple factors, such as the day-time and
the user preferences. Thus, we design a multimodal embedding module to jointly
embed the spatio-temporal features and the personal features into dense repre-
sentations,
to help modeling such complicated transitions.
We embed different
categories of features,
namely user,
time,
and location characteristics.
In prac-
tice, we represent the user by means of the driver and the taxi IDs. The time is
represented by using the hour of the day h ∈ [0, 23],
the week-day,
wd ∈ [0, 6],
and the day-type,
dt ∈ [0, 2]
(i.e.
workday,
pre-holiday,
and holiday).
All
these
categorical features are fed into the multimodal embedding module to obtain the
dense representation,
which has been demonstrated to be much more powerful
compared to the simple one-hot representation [31].
Differently from all
previous works,
we introduce the idea of
including the
semantic of
each visited place.
Instead of
using directly the latitude and the
longitude position,
we use a predefined set of
location clusters C,
calculated
with a clustering algorithm, i.e., K-means, on the destinations of all the training
trajectories. Thus, each point of the trajectory is assigned to the closest centroid
point c
i
.
By using the same set of clusters C,
we apply the same approach to
the POIs introduced in Section 4.2.
Since we are working with latitude and
longitude values,
the distance between points is computed with the haversine
distance, defined as follows:
d
haversine
(x, y) = 2R
s
a(x, y)
a(x, y) − 1
!
(2)
where R is the earth radius, λ
x
is the longitude of x, φ
x
is the latitude of x, and
a(x, y) is calculated as:
a(x, y) = sin
2

φ
y
− φ
x
2

+
cos(φ
x
)cos(φ
y
)sin
2

λ
y
− λ
x
2

(3)
We assign a textual label to each cluster and we translate each trajectory in
a sequence of words.
This allows us to apply Word2Vec [31]
in order to obtain
a dense representation for each cluster label. Word2Vec is a well-known textual
embedding technique,
coming from Natural
Language Processing (NLP) and
commonly used for word embedding.
For each trajectory point,
we use the embedding of
the textual
label
as-
sociated to that point.
Then,
we include geographical
information of
the area
around the point,
by using the Bag-Of-Concepts (BOCs) representation pre-
sented in [32].
Every venue is hierarchically categorized (e.g.,
Professional
and
Other Places → Medical
Center → Doctor’s office) and the categories are used
to produce an aggregated representation of the area. The BOC representation is
then built by aggregating all the POIs that are associated to the trajectory points
(i.e.,
cluster points),
by counting the macro-categories described in Section 4.2
(e.g.,
Food ).
In other words,
Bag-Of-Concept (BOC) features are generated by
counting the number of
activities for each category in a cluster.
All
the sin-
gle dense representations that we obtain from the embedding module are then
jointly fed into a Recurrent Neural Network.
5.3
Recurrent module with attention
Recurrent Neural Networks are particularly suited to work with sequential data
such as GPS traces [17,3,19].
For this reason, we model our problem using Re-
current Neural Networks with an LSTM as a basic unit, having the hyperbolic
tangent as the activation function. First, we apply the attention mechanism to
the input spatio-temporal
vector sequence,
generated by the embedding layer.
The implementation structure is presented in Figure 4.
The aim of the attention module is that of learning which part of the trajec-
tory is more important to focus on.
This is done by transposing the input and
feeding it to a softmax, which estimates the weight distributions, that are then
combined with the input sequence. Thus the recurrent layer is focused on specific
parts of the sequence,
to capture mobility regularities from the current trajec-
tory.
Finally,
dropout is applied before the softmax layer,
in order to prevent
overfitting.
Fig. 4. The attention mechanism applied on the input trajectory.
5.4
Prediction
The prediction module represents the last component that combines the output
of the previous modules and thus completes the prediction task. In particular, it
consists of a softmax layer and a linear layer. First, the softmax layer takes the
representation generated by the recurrent module as its input.
Then,
since the
network must evaluate the coordinates of the destination point, we add an addi-
tional output layer with two neurons, representing the latitude and the longitude
coordinates, respectively. It is worth noting that this operation is equivalent to
adding a simple linear output layer, whose adjustable weight matrix is initialized
with the cluster centers. Thus, the output of this layer is defined as:
˜
y =
C
X
i=1
p
i
c
i
p
i
=
exp(e
i
)
P
C
j=i
exp(e
j
)
(4)
where p
i
is the softmax probability associated to each cluster point.
6
Experimental results
The following experiments aim at demonstrating the effectiveness of our model
for predicting the next destination in a taxi
journey.
We experimented our ap-
proach on three different taxi datasets for the cities of (i) Porto, (ii) Manhattan
(New York City), and (iii) San Francisco, also enabling comparisons with state-
of-the-art models [18].
6.1
Experimental setup
We performed experiments in Porto,
Manhattan and San Francisco by using
260.600, 184.000 and 87.500 sequences (Def. 2). For the three cities we used the
whole datasets, randomly splitting the data into 65%-15%-20% for the training,
validation,
and test sets,
respectively.
For the city of Porto and San Francisco
the available dataset is composed by the complete trajectory of the trip (Def. 1),
while for Manhattan the trajectories are only composed by pick-up and drop-
off point (Def.
2).
To measure the performance of our model
as well
as of the
baselines,
we used the Error Distance Score (EDS),
which is defined as the
Haversine distance between the predicted point and the actual
destination of
the trip:
EDS = d
haversine
(˜
y, y)
(5)
where ˜
y is the predicted point and y is the correct destination. As for the base-
line methods, (i) Nearest Neighbors (NN), (ii) MMLP, and (iii) MMLP-
SEQ have been considered.
In particular,
with respect to NN,
we evaluate its
performances based on the distance between the pick-up point and the closest
cluster centroid (distinct from the starting one).
On the other hand,
MMLP is
the Multi-Layer Perceptron (MLP) implementation that won the ECML/PKDD
2015 challenge [2]. In this model, the input layer receives a representation of the
taxi trajectory (Def. 1), composed by the first five and the last five GPS points,
and its associated metadata. Then, a standard hidden layer, containing 500 Rec-
tifier Linear Units (ReLUs) [33],
is used.
As in our approach,
each coordinate
is assigned to a given set of
cluster centroids.
Thus,
the output layer predicts
˜
y, which is a weighted average of the predefined destination cluster points. The
optimization problem was modeled as a multi-class classification,
having the
cross-entropy as the loss function. The network is trained with Stocastic Gradi-
ent Descent with a batch size of 200, a learning rate of 0.001 and a momentum
term of 0.9.
To perform a fair comparison,
we trained the MMLP model
on the last
trip of
our trajectory (Def.
2),
thus obtaining the same number of
training,
validation and test patterns.
As in in [2],
we use 10 GPS points.
In particular,
the first five and last five GPS points of the trajectory (Def. 1). The evaluation
was performed on the same trip used for testing our approach based on the
LSTM model.
It is worth noting that,
in [2],
the Kaggle private test set was
used, composed only by approximately 300 trips. Moreover, it was shown, that
such model performs worse than other techniques, e.g, Bidirectional Long-Short-
Term-Memory Networks (BLSTM) [34],
when evaluated on a bigger test set
obtained by slicing the training set.
Finally, MMLP-SEQ is very similar to MMLP and it is trained following the
same approach,
with the unique difference that,
instead of
using the first five
and the last five GPS points as inputs, we have linearized the driver’s trajectory
(Def. 2).
6.2
Training and Hyperparameters
The parameters of our deep learning model have been chosen for each city based
on a trail
and error procedure,
and their values are reported in Table 1.
For
the embedding of user and time features, we adopted a layer size of 10, for each
single feature, as in [2]. The size of the location embedding is 20, while the BOC
representation has 10 components, equal to the number of the macro-categories
present in Foursquare. The input dimension of the LSTM (with hyperbolic tan-
gent activation functions) is equal to the size of the vector obtained after concate-
nating all the previous representations. To train the network, we used the Adam
optimizer [35]. We approximate latitude and longitude values separately, sharing
the underlying structure:
embeddings,
attention module and LSTM layer.
The
network is trained using early stopping.
This means that we stop the training
procedure if no update respect to EDS has been made on the validation set for
at least 10 epochs. During the test phase, we use the parameters of the network
that produced the best EDS score on the validation set. We compute the EDS
score after each epoch and save the network parameters if a new best EDS score
is obtained.
The dropout rate is set to p = 0.5.
The word embeddings for the
textual label of each cluster is done by using the Gensim
10
implementation [36].
We opt for a Continuous Bag-of-Words (CBOW) model with a window size of 5
and without filtering words on frequency. The dimensionality of this embedding
is set to 20, as before.
City
LSTM Neurons Learning Rate Batch Size
Porto
200
10
−5
32
San Francisco
100
10
−3
32
Manhattan
200
10
−4
32
Table 1. Training parameters for each city.
6.3
Results
The performances of the compared approaches are listed (per column) in Table
2.
In particular,
regarding our models,
we indicate with LSTM (fifth row) the
simple approach that uses the input representation only constituted by the user
and time features, and basic location embedding. Instead, for LSTM (BOC), also
the BOC representation is used (see Section 5.2);
finally,
LSTM (BOC+W2V)
represents the Recurrent Neural
Network that includes both the BOC features
and the representation of the cluster textual labels (W2V).
10
https://radimrehurek.com/gensim/
Table 2. Error Distance Score (km) for Porto, San Francisco and Manhattan.
(∗)
The
trajectory is composed by the pickup and the dropoff points only.
Model
Porto San Francisco Manhattan
NN
3.215
3.023
2.375
MMLP
3.211
1.994
2.543
(∗)
MMLP-SEQ
3.003
2.762
2.554
LSTM
2.192
0.663
1.297
LSTM
(BOC)
1.957
0.621
1.303
LSTM
(BOC + W2V)
1.853
0.582
1.053
LSTM
LSTM (POI)
LSTM (POI + W2V)
Model
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Error distance (km)
Porto
Regression
Classification
LSTM
LSTM (POI)
LSTM (POI + W2V)
Model
0.0
0.5
1.0
1.5
2.0
2.5
Error distance (km)
San Francisco
Regression
Classification
LSTM
LSTM (POI)
LSTM (POI + W2V)
Model
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Error distance (km)
Manhattan
Regression
Classification
Fig. 5. Classification vs. Regression in Porto, San Francisco and Manhattan.
Based on Table 2, we notice that our models reduce the EDS for all the three
cities, with a relative improvement, with respect to the MMLP, that equals 42%
for Porto, 78% for San Francisco, and 58% for Manhattan.
A significant improvement is also obtained with the introduction of semantic
information about places (BOC+W2V).
Indeed,
we have a relative increase in
performance of 0.221 km, on average.
It is worth highlighting that all
the models reported in Table 2 have been
trained on the same set of trajectories.
In Figure 5 we report the results of our models both in the regression and in
the multi-class classification settings. We trained the classification models using
categorical cross-entropy as loss function and removing the two output neurons.
In this way, the output locations are limited to the list of cluster centroids again
weighted through the probability of the softmax layer. The experimental results
show that our deep learning models, that approximate the exact location - based
on a regression procedure on the latitude/longitude values - clearly outperform
all the other models, which realize a multi-class classification on a predefined set
of locations.
7
Conclusion
We have introduced a new way of looking at the problem of the next destination
prediction in a taxi
journey.
Instead of
using the complete taxi
trajectory,
we
rely on the individual
driver’s history,
namely the sequence of
the last visited
points by the driver,
i.e.,
pick-up or drop-off points.
Indeed,
we are able to
obtain an instantaneous prediction,
just after the starting of
the trip,
which
results in information of paramount relevance for the taxi company dispatcher.
Moreover,
to the best of
our knowledge,
we have proposed for the first time
a model
that treats this task as a regression problem,
instead of
a multi-class
classification one [3,19].
Finally,
we have also demonstrated how using (i) geo-
located semantic information, like the one related to points of interest [37], and
(ii) a richer data representations, based on Word2Vec, may strongly improve the
prediction accuracy.
Moreover,
as a matter of
future work,
different data sources can be inte-
grated and injected into the model.
In particular,
the representation of
POIs
can be enriched using a greater number of categories, whereas the existing links
among POIs, provided by Foursquare, suggest considering POI-graphs instead of
isolated points. Also land use information, which shows the prevalent functions
of particular urban areas (e.g. Commercial, Residential, and Industrial), can be
relevant to this task.
Finally,
our approach can be extended to the more general
task of the hu-
man mobility prediction with respect to both the individual
and the collective
behaviour,
using a deterministic neural
network approach instead of
a proba-
bilistic model, as in [38].
References
1.
Novotn`
y, R., Kuchta, R., Kadlec, J.: Smart city concept, applications and services.
Journal of Telecommunications System & Management 3(2) (2014)
2.
De Br´ebisson, A., Simon,
´
E., Auvolat, A., Vincent, P., Bengio, Y.: Artificial neural
networks applied to taxi
destination prediction.
arXiv preprint arXiv:1508.00021
(2015)
3.
Yao, D., Zhang, C., Huang, J., Bi, J.:
Serm: A recurrent model for next location
prediction in semantic trajectories. In: Proceedings of the 2017 ACM on Conference
on Information and Knowledge Management, ACM (2017) 2411–2414
4.
Gers, F.A., Schmidhuber, J., Cummins, F.:
Learning to forget: Continual predic-
tion with lstm.
(1999)
5.
Phiboonbanakit,
T.,
Horanont,
T.:
How does taxi
driver behavior impact their
profit? discerning the real
driving from large scale gps traces.
In:
Proceedings
of
the 2016 ACM International
Joint Conference on Pervasive and Ubiquitous
Computing: Adjunct, ACM (2016) 1390–1398
6.
Antoniades, C., Fadavi, D., Amon Jr, A.F.:
Fare and duration prediction: A study
of new york city taxi rides.
(2016)
7.
Gao, Y., Jiang, D., Xu, Y.: Optimize taxi driving strategies based on reinforcement
learning.
International Journal of Geographical Information Science (2018) 1–20
8.
Santi, P., Resta, G., Szell, M., Sobolevsky, S., Strogatz, S.H., Ratti, C.: Quantifying
the benefits of
vehicle pooling with shareability networks.
Proceedings of
the
National Academy of Sciences 111(37) (2014) 13290–13294
9.
Li,
L.,
Wang,
S.,
Wang,
F.Y.:
An analysis of taxi
driver’s route choice behavior
using the trace records.
IEEE Transactions on Computational Social Systems 5(2)
(2018) 576–582
10.
Vazifeh, M., Santi, P., Resta, G., Strogatz, S., Ratti, C.:
Addressing the minimum
fleet problem in on-demand urban mobility.
Nature 557(7706) (2018) 534–538
11.
Yuan, J., Zheng, Y., Zhang, L., Xie, X., Sun, G.: Where to find my next passenger.
In:
Proceedings of
the 13th international
conference on Ubiquitous computing,
ACM (2011) 109–118
12.
Smith,
A.W.,
Kun,
A.L.,
Krumm,
J.:
Predicting taxi
pickups in cities:
which
data sources should we use? In: Proceedings of the 2017 ACM International Joint
Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017
ACM International Symposium on Wearable Computers, ACM (2017) 380–387
13.
Li, X., Pan, G., Wu, Z., Qi, G., Li, S., Zhang, D., Zhang, W., Wang, Z.:
Prediction
of urban human mobility using large-scale taxi traces and its applications. Frontiers
of Computer Science 6(1) (2012) 111–121
14.
Chen, M., Liu, Y., Yu, X.:
Nlpmm: A next location predictor with markov mod-
eling.
In:
Pacific-Asia Conference on Knowledge Discovery and Data Mining,
Springer (2014) 186–197
15.
Gambs,
S.,
Killijian,
M.O.,
del
Prado Cortez,
M.N.:
Next place prediction using
mobility markov chains.
In: Proceedings of the First Workshop on Measurement,
Privacy, and Mobility, ACM (2012)
3
16.
Monreale,
A.,
Pinelli,
F.,
Trasarti,
R.,
Giannotti,
F.:
Wherenext:
a location pre-
dictor on trajectory pattern mining.
In:
Proceedings of the 15th ACM SIGKDD
international
conference on Knowledge discovery and data mining,
ACM (2009)
637–646
17.
Liu, Q., Wu, S., Wang, L., Tan, T.: Predicting the next location: A recurrent model
with spatial and temporal contexts.
In: AAAI. (2016) 194–200
18.
Du,
N.,
Dai,
H.,
Trivedi,
R.,
Upadhyay,
U.,
Gomez-Rodriguez,
M.,
Song,
L.:
Re-
current marked temporal point processes: Embedding event history to vector.
In:
Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge
Discovery and Data Mining, ACM (2016) 1555–1564
19.
Feng,
J.,
Li,
Y.,
Zhang,
C.,
Sun,
F.,
Meng,
F.,
Guo,
A.,
Jin,
D.:
Deepmove:
Predicting human mobility with attentional
recurrent networks.
In:
Proceedings
of the 2018 World Wide Web Conference on World Wide Web, International World
Wide Web Conferences Steering Committee (2018) 1459–1468
20.
Lv, J., Li, Q., Sun, Q., Wang, X.: T-conv: A convolutional neural network for multi-
scale taxi
trajectory prediction.
In:
Big Data and Smart Computing (BigComp),
2018 IEEE International Conference on, IEEE (2018) 82–89
21.
Hochreiter,
S.,
Schmidhuber,
J.:
Long short-term memory.
Neural
computation
9(8) (1997) 1735–1780
22.
Chorowski,
J.K.,
Bahdanau,
D.,
Serdyuk,
D.,
Cho,
K.,
Bengio,
Y.:
Attention-
based models for speech recognition. In: Advances in neural information processing
systems. (2015) 577–585
23.
Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning
to align and translate.
arXiv preprint arXiv:1409.0473 (2014)
24.
Rush,
A.M.,
Chopra,
S.,
Weston,
J.:
A neural
attention model
for abstractive
sentence summarization.
arXiv preprint arXiv:1509.00685 (2015)
25.
Xu,
K.,
Ba,
J.,
Kiros,
R.,
Cho,
K.,
Courville,
A.,
Salakhudinov,
R.,
Zemel,
R.,
Bengio,
Y.:
Show,
attend and tell:
Neural
image caption generation with visual
attention.
In: International conference on machine learning. (2015) 2048–2057
26.
Mishra,
N.,
Rohaninejad,
M.,
Chen,
X.,
Abbeel,
P.:
A simple neural
attentive
meta-learner.
(2018)
27.
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.:
Matching networks for
one shot learning.
In: Advances in Neural Information Processing Systems. (2016)
3630–3638
28.
Zheng, Y., Liu, Y., Yuan, J., Xie, X.: Urban computing with taxicabs. In: Proceed-
ings of the 13th international
conference on Ubiquitous computing,
ACM (2011)
89–98
29.
Alfeo,
A.L.,
Cimino,
M.G.,
Egidi,
S.,
Lepri,
B.,
Gigliola,
V.:
A stigmergy-based
analysis of city hotspots to discover trends and anomalies in urban transportation
usage.
IEEE Transactions on Intelligent Transportation Systems 19(7) (2018)
2258–2267
30.
Piorkowski,
M.,
Sarafijanovic-Djukic,
N.,
Grossglauser,
M.:
CRAWDAD dataset
epfl/mobility (v. 2009-02-24).
https://crawdad.org/epfl/mobility/20090224
31.
Mikolov, T., Chen, K., Corrado, G., Dean, J.:
Efficient estimation of word repre-
sentations in vector space.
arXiv preprint arXiv:1301.3781 (2013)
32.
Barlacchi, G., Rossi, A., Lepri, B., Moschitti, A.:
Structural semantic models for
automatic analysis of
urban areas.
In:
Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, Springer (2017) 279–291
33.
Glorot,
X.,
Bordes,
A.,
Bengio,
Y.:
Deep sparse rectifier neural
networks.
In:
Proceedings of the fourteenth international conference on artificial intelligence and
statistics. (2011) 315–323
34.
Thireou, T., Reczko, M.:
Bidirectional long short-term memory networks for pre-
dicting the subcellular localization of
eukaryotic proteins.
IEEE/ACM Transac-
tions on Computational Biology and Bioinformatics 4(3) (2007) 441–446
35.
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
36.
Rehurek,
R.,
Sojka,
P.:
Software framework for topic modelling with large cor-
pora.
In: In Proceedings of the LREC 2010 Workshop on New Challenges for NLP
Frameworks, Citeseer (2010)
37.
Tong, Y., Chen, Y., Zhou, Z., Chen, L., Wang, J., Yang, Q., Ye, J., Lv, W.:
The
simpler the better:
a unified approach to predicting original
taxi
demands based
on large-scale online platforms.
In:
Proceedings of
the 23rd ACM SIGKDD In-
ternational
Conference on Knowledge Discovery and Data Mining,
ACM (2017)
1653–1662
38.
Calabrese,
F.,
Di
Lorenzo,
G.,
Ratti,
C.:
Human mobility prediction based on
individual and collective geographical preferences.
(2010)
