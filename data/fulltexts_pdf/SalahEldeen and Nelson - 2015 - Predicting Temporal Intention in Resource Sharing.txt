Predicting Temporal Intention in Resource Sharing
Hany M. SalahEldeen
Old Dominion University
Norfolk, VA, USA
hany@cs.odu.edu
Michael L. Nelson
Old Dominion University
Norfolk, VA, USA
mln@cs.odu.edu
ABSTRACT
When users post links to web pages in Twitter there is a time
delta between when the post was shared (t
tweet
) and when it
was read (t
click
).
Ideally, when this time delta is small there
is often no change in the page’s state.
However upon reading
shared content in the past and due to the dynamic nature
of the web, the page’s state could change and the intention
of the author need to be inferred.
In this work, we enhance
a prior temporal
intention model
and tackle its shortcom-
ings by incorporating extended linguistic feature analysis,
replacing the prior textual
similarity measure with seman-
tic similarity one based on latent topic detection trained
on Wikipedia English corpus,
and finally by enriching and
balancing the training dataset.
We uncovered three differ-
ent intention behaviors in respect to time:
Stable Intention,
Changing Intention from current to past, and Undefined in-
tention.
Using these classes and only the information avail-
able at posting time from the tweet and the current state
of the resource, we correctly predict the temporal intention
classification and strength with 77% accuracy.
1.
INTRODUCTION
When a link to a webpage is shared in social
media,
au-
thors have an implicit expectation as to what people in their
social network would read when they click on that link.
Au-
thors might want the readers to view the exact same version
of the page they saw when they posted (at time t
tweet
);
or
they might want the readers to examine the most current
version of
the resource at the time of
clicking on the link
(t
click
).
If the time difference is small
(t
click
− t
tweet
≈ 0),
then the reader will
not notice any discrepancy or shift in
what the author intended for them to see.
Unfortunately,
due to the highly dynamic nature of the web it is highly likely
that the intentions diverge through time either by change or
loss of resources.
This divergence could be highly problem-
atic upon archiving and curating social media posts that nar-
rate public events like the Arab Spring, Occupy Wall Street
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page.
Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
JCDL’15, June 21–25, 2015, Knoxville, Tennessee, USA.
Copyright
c
2015 ACM 978-1-4503-3594-2/15/06 ...$15.00.
DOI: http://dx.doi.org/10.1145/2756406.2756921.
movement, natural disasters like Hurricane Sandy and oth-
ers.
(a) The intention is towards the past version at t
tweet
.
(b) The intention is towards the latest version at t
click
.
Figure 1:
Tweet
examples
for different
intention
classes.
Consider the tweets shown in Figure 1.
In Figure 1(a) we
can see that the author’s intention is for a specific informa-
tion resource and thus the intention is t
tweet
.
In Figure 1(b)
the author wants the reader to see the latest information,
so the intention is t
click
.
In this work,
we build a predic-
tive model
which can effectively differentiate between each
intention class at tweet-authoring time.
The ability to dif-
ferentiate the intention in real-time can be used to push a
copy of the linked resource into a web archive (e.g., webcita-
tion.org, archive.today, archive.org) at t
tweet
so the link is to
an archived version instead of a web version,
thus ensuring
what readers see is consistent with the author’s intention.
In our prior work, we analyzed the temporal intention con-
cept and were able to successfully model temporal intention
by breaking it down into two simpler tasks of
content rel-
evance and change,
and named it the Temporal
Intention
Relevance Model (TIRM) [17].
Relevance describes whether
the content of
the resource still
matches the tweet in top-
icality and context,
while Change means that the resource
have changed in content from the initial
state at the time
of tweeting.
The subproblems of relevance and change are
more straightforward to measure and there is an abundance
of prior literature addressing them in regards to textual con-
tent relevance, as well as linguistic and semantic similarity.
To illustrate TIRM,
we examine two points in the life of
a tweet as described earlier:
1)t
tweet
when the author of
the tweet posted it,
2) t
click
when the reader clicks on the
link to examine the resource at current time.
Table 1 shows
that if the resource has changed and no longer relevant, then
the intention is for the past (e.g., in Figure 1(a) the author
intends for readers to access the WHO page as it was at
t
tweet
), while if the resource changed but still relevant, then
the intention is for the current (e.g., in Figure 1(b) the au-
thor intends for readers to access latest news page as it will
be at t
click
).
The model was trained using 39 different social,
archival, and textual features extracted at t
click
.
Resource’s State
Tweet and resource are:
Relevant
Not Relevant
Changed
t
click
t
tweet
Not Changed
t
tweet
either or undefined
Table
1:
Temporal
Intention Relevancy
Model
(TIRM).
When we first started using TIRM in prediction we no-
ticed that our model in its simplistic form had some short-
comings.
One problem in judging relevance between a tweet
and a linked resource is the tweet is limited to 140 characters
while the resource could span thousands of characters.
This
highlights the need to find a better similarity measure based
on the semantic similarity rather than just textual overlap.
Second problem is that the model is more aggressive towards
relevance due to the bias in the training dataset extracted
from Mechanical
Turk (80% Relevant class and 20% Non-
Relevant).
Therefore, prior to the prediction analysis steps
we extend the model to address those problems and enhance
the model.
Following the enhancement of
the model,
we wanted to
estimate the confidence of
this probabilistic classification.
We are able to tell the relevance class and if the content is
changed or not,
but not a collective classification measure
of confidence.
We propose a formulation of intention based
on the relevance measure from the classifier and the change
measure obtained by calculating similarity between the re-
source’s versions.
We call
this formulation the Intention
Strength Measure.
With this measure, for each resource in our dataset we cal-
culate the intention measure at 12 points in the time period
spanning 3.5 years from t
click
to t
tweet
.
Plotting those in-
tention strength across time We devised a model to identify
the intention progression from t
click
to t
tweet
.
This modeling
will enable us to predict this temporal intention at an early
stage,
and provide the authors with enough information to
perform an educated assessment on how to successfully con-
vey their intention to the readers at any future t
click
.
For
example, in the tweet shown in Figure 1, at t
tweet
when the
author is posting a tweet with a link to the WHO Disease
Outbreak News page,
if it was predicted that the intention
would change in the future they might push a copy of the
current state of the page into one of the public archives and
link to that snapshot instead of the live version.
2.
RELATED WORK
Researchers also explored the problem of web content chang-
ing through time and how to predict this change [1, 4, 5, 8].
Radinsky et al.
introduced a new method incorporating not
only the analysis of
the page changes in the past but also
the usefulness of this change and how related this page un-
der observation is to other pages and how similar they are
in the types of changes they undergo [14].
Latent Dirichlet Allocation (LDA) has been used exten-
sively in detecting latent relations between phrases and words
in corpora.
Rus et al.
investigated LDA and Latent Seman-
tic Analysis (LSA) in deriving meaning on word level
and
phrase level in large corpora [16, 12].
In conversational anal-
ysis, and perhaps in Twitter, Oliva et al.
explored the con-
cept of semantic similarity in relatively short-text and built
their method based on the semantic meaning behind words
along with the structural relation between those words in the
short text [13].
The semantic understanding of text and the
meaning behind it could be utilized in further more than tex-
tual similarity but rather in identifying usefulness of content
posted in social media [11].
Beyond merely textual analysis,
Chen et al.
have focused on analyzing the tweets that are
associated with other resources,
namely image tweets [3].
In their search for content relevancy between the associated
images and the text,
they explored various textual,
visual,
and social context features.
They utilized LDA in modeling
and detecting textual and visual hidden topics.
LDA has been also utilized in other context as well
like
tweet
classification.
The concept
of
retweeting was ana-
lyzed by Wang et al.
in their search for the linguistic and
functional
factors leading to retweeting [20].
In their work
they analyzed a multitude of factors in the tweet’s structure
and text ranging from sentiment analysis,
emoticons exis-
tence,
part-of-speech (POS) tagging,
named entity extrac-
tion (NE),
hashtag analysis,
verb tense analysis,
discourse
relations and phrase similarity.
In our work, we adopt their
approach in tweet analysis and extract similar features aid-
ing the enhancement of our model.
3.
ENHANCING TIRM
To enhance TIRM we focused on the shortcomings of the
model in regards to linguistic analysis, similarity measures,
and dataset balancing while utilizing the same dataset used
in TIRM spanning 1,124 instances.
3.1
Linguistic Feature Analysis
Previously,
39 different features were extracted from the
tweet-resource pair in regards to similarity, URL structure,
social
and archival
existence.
The results were promising
but we needed a deeper analysis and understanding of the
linguistic properties of the tweet-resource pair.
3.1.1
Tweet structural analysis
After filtering out the linked resource we checked the ex-
istence of user mentions,
hashtags,
question marks “?” (in-
dicating a question tweet), and exclamation marks “!” (indi-
cating an expression of strong feelings).
Furthermore we uti-
lized regular expressions, adopted from Ritter et al.’s work,
in detecting emoticons in the tweets [15].
We deduced that
along with the extracted sentiment from the prior exper-
iment we will
be able to capture the emotional
state the
author.
Finally, we also checked if the tweet was a re-tweet.
These simple features proved to be highly effective as six of
which are present among the top 13 ranked features in infor-
mation gain of the retrained model as shown by table 3 (Key:
FB=Facebook, Twt=Tweet, Sim=Similarity, Cur=Current,
Len=Length, Celeb=Celebrities, Pct=Percent, Init=Initial,
Pos=Positive, Neg=Negative, Neu=Neutral).
3.1.2
POS tagging and Named Entity Extraction
In the prior TIRM,
Wikipedia was harvested for lists of
artists, actors, and singers from the English speaking world
to use in detecting the existence of celebrities in the tweets.
This feature proved to be highly valuable due to its cor-
responding high information gain.
This observation led us
to believe we need to further investigate named entities in
tweets.
In tweet analysis, due to the rather small size of text with
the lack of
context and the informality in writing,
tasks
like Part-of-speech (POS) tagging,
sentence chunking,
and
named entity recognition are quite challenging.
Ritter et
al.
developed a distantly supervised approach that is tai-
lored for tweet based analysis overcoming those challenges
[15].
We adopted their labeled LDA-based POS tagger and
chunker which have performed effectively against standard
POS taggers on tweet datasets.
Ten different types of enti-
ties are defined by Ritter’s tagger as shown in table 2 along
with the number of identified entities in each class across the
training dataset of 1,124 instances.
Furthermore,
with the
extracted POS tags and chunks we were able to determine if
the most prominent tense in a tweet is present or past and
used it as a feature too.
The rationale behind this analysis
is to also identify the intention of the author in discussing
contemporaneous events or past ones.
3.1.3
Tweet Classification
Users tweet to convey an opinion,
update a status,
ask
for information, sarcasm, spread jokes, and many other rea-
sons [7].
In our search for the author’s temporal
intention
we utilized Wang et al.
work in classifying tweeting motive
[20].
We adopted the first level of their two-tiered classifica-
tion:
Opinion, Update, Interaction, Fact, Deals, News, Oth-
ers.
Furthermore, and for the sake of simplicity, we utilized
the largest classes of
Opinion,
Update,
Interaction,
Others
which collectively comprised 94% of the instances in Wang
et al.’s datset.
Interaction
Update
Opinion
Relevant
69.67%
59.28%
36.99%
Non-Relevant
30.33%
40.72%
63.01%
Table 4:
Tweet classification across relevancy classes
As shown in table 4,
for class Interaction,
the Relevance
class is significantly higher than the other while in class
Opinion the instances are more biased towards the Non-
Relevant.
This indicates the relation between tweet class
and relevance thus we use it as a feature.
3.2
Semantic Similarity Analysis Using Latent
Topic Modeling
In the prior TIRM,
similarity measures were based on
word overlap either by using SimHash or n-grams cosine
similarity.
This technique proved to be lacking upon calcu-
lating the similarity between a tweet (140 characters) and a
resource which could be virtually unlimited.
Similarly be-
tween two versions of a resource, where a change in the html
design could be interpreted as a low similarity while in fact
the content itself remained unchanged.
It is worth mention-
ing that in this experiment we only account for non-missing
resources (i.e.,
gives 200 OK response).
In their prior ex-
periment, we attempted to overcome the latter problem by
introducing a boiler plate removal algorithm to remove the
effect of change in styling and extract the main content.
To address the former we employed topic detection,
as
we would consider a tweet and a resource to be similar if
they were both mentioning the same topic, or discussing the
same point.
Thus we measure similarity based on collec-
tive semantics or “aboutness” of the pair rather than textual
overlap.
We use both LSA and LDA in calculating the similarities
between the tweet-resource and resource-resource pairs ac-
cordingly.
We considered both techniques as LSA is much
faster to train while LDA has higher accuracy.
We also con-
sidered utilizing Twitter based LDA models from the works
of Mehrotra et al.
and Zhao et al.
which are more fitted to
handle tweeted textual content with its embedded hashtags
[10,
21].
Since we were not performing topic modeling on
tweets only and we are calculating similarities between the
tweet and the resource,
which is written formally in most
cases, traditional LDA-LSI models trained on a diverse cor-
pus like Wikipedia seemed more suitable.
Furthermore, we
calculate the similarities between the resource versions (at
t
tweet
or past version,
and at t
click
or current version) and
the tweet.
To prepare these models we utilize the Wikipedia Cor-
pus in extracting the topics and features.
We downloaded
4,295,020 documents spanning the English Wikipedia docu-
ments in January 2014
1
.
We chose Wikipedia for training as
it spans a wide variety of topics and the latent relations be-
tween them.
Next we built the LDA and LSA models with
100,000 features, 672,235,199 non-zero entries in the sparse
TF-IDF matrix.
The LDA model
in this case is an online
learning LDA model
developed by Hoffman et al.
[6].
We
get the current version of the resource, the past version, and
the tweet.
We convert each to latent vector space and using
the model
we calculate the cosine similarity.
The result is
1
http://download.wikimedia.org/enwiki/
Person
GeoLoc
TVShow
Movie
Facility
Company
Product
SportsTeam Band
Other
233
81
18
37
19
115
42
10
62
96
Tweets with Named Entities
543
Tweets without Named Entities
581
Table 2:
Named Entities instances in the dataset
#
Feature Name
Type
Extraction Method
Availability
Gain
Rank
Min
1
ShortURLLen
Structural
Analyzing resource’s URL
At t
tweet
0.1709
4
X
The original TIRM Model with 39 Features
2
NumArchives
Archival
Analyzing resource’s timemap
After Archival
0.1663
5
X
3
URLDepth
Structural
Analyzing resource’s URL
At t
tweet
0.1569
10
X
4
CelebInTwts
Linguistic
Mining Wiki+Text analysis
At t
tweet
0.1203
11
X
5
CelebInTwt
Linguistic
Mining Wiki+Text analysis
At t
tweet
0.0917
22
X
6
CosTwtPast
Similarity
Cosine Similarity+BoilerPlt
After Archival
0.0877
23
X
7
CosCurTwt
Similarity
Cosine Similarity+BoilerPlt
At t
tweet
0.0864
24
X
8
CosCurPast
Similarity
Cosine Similarity+BoilerPlt
After Archival
0.0862
25
X
9
CelebPctInTwt
Linguistic
Mining Wiki+Text analysis
At t
tweet
0.0861
26
X
10
TwtSimCur
Similarity
Similarity+BoilerPlt
At t
tweet
0.0846
27
X
11
URLLen
Structural
Analyzing resource’s URL
At t
tweet
0.0846
286
12
ReductionRate
Structural
Analyzing resource’s URL
At t
tweet
0.0845
29
13
CelebPctInTwts
Linguistic
Mining Wiki+Text analysis
After being retweeted
0.0835
30
14
InfluTwtsCount
Social
Mining Topsy API
After being retweeted
0.0835
31
15
SimhashCurPast
Similarity
Simhash Similarity+BoilerPlt
After Archival
0.0799
33
16
MementoCount
Archival
Analyzing resource’s timemap
After Archival
0.0774
34
17
FBClicks
Social
Mining FB API
After being posted on FB
0.074
35
18
CosCurTwts
Similarity
Cosine Similarity+BoilerPlt
After being retweeted
0.0695
36
19
FBLikes
Social
Mining FB API
After being posted on FB
0.0689
37
20
FBComments
Social
Mining FB API
After being posted on FB
0.0668
38
21
TwtLen
Structural
Text analysis
At t
tweet
0.0662
39
22
CosTwtsPast
Similarity
Cosine Similarity+BoilerPlt
After Archival+retweeted
0.0569
41
23
SimhashCurTwts
Similarity
Simhash Similarity+BoilerPlt
After being retweeted
0.0569
42
24
FBShares
Social
Mining FB API
After being posted on FB
0.0538
44
25
InitContentLen
Structural
Mining Bitly API
After being Bitly Shortened
0.0481
46
26
NeuSentiment
Linguistic
NLTK Sentiment Analysis
At t
tweet
0.048
47
27
TwtSimPast
Similarity
Similarity+BoilerPlt
After Archival
0.0475
48
28
BitlyClicks
Social
Mining Bitly API
After being Bitly Shortened
0.0463
49
29
SimhashCurTwt
Similarity
Simhash Similarity+BoilerPlt
At t
tweet
0.0438
52
30
CloseMemTime
Archival
Analyzing resource’s timemap
After Archival
0.0434
53
31
SimhashTwtpast
Similarity
Simhash Similarity+BoilerPlt
After Archival
0.0411
55
32
PastCurSim
Similarity
Similarity+BoilerPlt
After Archival
0.0376
56
33
PosSentiment
Linguistic
NLTK Sentiment Analysis
At t
tweet
0.0356
57
34
SimhashTwtsPast
Similarity
Simhash Similarity+BoilerPlt
After Archival+retweeted
0.0353
58
35
TwtsSimCur
Similarity
Similarity+BoilerPlt
At t
tweet
0.0351
59
36
RetrievedTwts
Social
Mining Topsy API
After being retweeted
0.0233
60
37
NegSentiment
Linguistic
NLTK Sentiment Analysis
At t
tweet
0.0215
62
38
TotalTwtCount
Social
Mining Topsy API
After being retweeted
0.0202
63
39
TwtsSimPast
Similarity
Similarity+BoilerPlt
After Archival
0
65
40
UserMention
Linguistic
Text analysis
At t
tweet
0.2254
1
X
The enhancing extended features
41
IsRetweet
Linguistic
Text analysis
At t
tweet
0.2015
2
X
42
Has!
Linguistic
Text analysis
At t
tweet
0.1845
3
X
43
GEO-LOC
Linguistic
Named Entity Extraction
At t
tweet
0.1653
6
X
44
Has?
Linguistic
Text analysis
At t
tweet
0.1643
7
X
45
PERSON
Linguistic
Named Entity Extraction
At t
tweet
0.1612
8
X
46
HashtagCount
Linguistic
Counting Hashtags
At t
tweet
0.1602
9
X
47
COMPANY
Linguistic
Named Entity Extraction
At t
tweet
0.1186
12
X
48
HasEmoticon
Linguistic
Text analysis
At t
tweet
0.1106
13
X
49
MOVIE
Linguistic
Named Entity Extraction
At t
tweet
0.1085
14
X
50
TVSHOW
Linguistic
Named Entity Extraction
At t
tweet
0.1065
15
X
51
OTHER
Linguistic
Named Entity Extraction
At t
tweet
0.1056
16
X
52
BAND
Linguistic
Named Entity Extraction
At t
tweet
0.1016
17
X
53
SPORTSTEAM
Linguistic
Named Entity Extraction
At t
tweet
0.0985
18
X
54
LDATwtsSimCur
Similarity
LDA Similarity+BoilerPlt
After being retweeted
0.0945
19
X
55
PRODUCT
Linguistic
Named Entity Extraction
At t
tweet
0.0922
20
X
56
LSATwtSimCur
Similarity
LSA Similarity+BoilerPlt
At t
tweet
0.092
21
X
57
LSATwtsSimCur
Similarity
LSA Similarity+BoilerPlt
After being retweeted
0.0819
32
58
LSATwtSimPast
Similarity
LSA Similarity+BoilerPlt
After Archival
0.0591
40
59
LSATwtsSimPast
Similarity
LSA Similarity+BoilerPlt
After Archival+retweeted
0.0548
43
60
LDATwtSimCur
Similarity
LDA Similarity+BoilerPlt
At t
tweet
0.0522
45
61
TweetClass
Linguistic
LDA Tweet Classification
At t
tweet
0.0453
50
62
LDATwtSimPast
Similarity
LDA Similarity+BoilerPlt
After Archival
0.0452
51
63
LDATwtsSimPast
Similarity
LDA Similarity+BoilerPlt
After Archival+retweeted
0.0429
54
64
Tense
Linguistic
POS tagging
At t
tweet
0.0223
61
65
FACILITY
Linguistic
Named Entity Extraction
At t
tweet
0
64
Table 3:
All the features used in TIRM and after enhancement, ranked by Information Gain Ratio.
a number ranging from 0 (indicating no similarity) and 1
(identical
similarity).
Gensim by
ˇ
Reh˚uˇrek et al.
was used
in our LDA and LSA modeling and similarity calculations
[19].
3.3
Dataset Balancing
From the prior experiment,
the dataset used in training
and cross validation was collected using five different Me-
chanical Turk voters for 1,124 instances.
The instances were
classified by the majority of
voters to Relevant and Non-
Relevant classes.
Unfortunately, but yet matching intuition,
the dataset collected is biased towards Relevancy (with 930
Relevant Vs.
194 Non-Relevant).
This undersampling of
the class Non-Relevant is causing the trained model
to be
more aggressive towards the Relevant class as shown in the
class-based recall, precision and F-measure in table 6.
The problem of imbalanced training datasets in classifica-
tion is a well-known problem.
In a multitude of cases,
one
of
the classes is significantly lower in training points than
the other class(es).
This causes the classifier to be overly
sensitive towards one class than the other.
In our analysis,
the Relevant class is almost five times in magnitude than
the Non-Relevant class resulting in a reduced precision and
recall in the minor class.
Possible solutions to this problem
is by undersampling the major class (Relevant) to be nearly
the same size of the minor class (Non-Relevant).
This ap-
proach has a downside as we purposely dispose of good data
points that could enhance the classifier.
Also it reduces the
size of the training dataset for the collective classes gravely.
Another approach is the Synthetic Minority Over-sampling
Technique or (SMOTE) introduced by Chawla et al.
[2].
By
synthesizing balancing datapoints via over-sampling the mi-
nor class in the dataset and utilizing the k-nearest neighbors
algorithm, they were able to enrich the training dataset iter-
atively by oversampling the minor class until the two classes
were close in size.
Their technique proved to achieve better
classifier performance (in ROC space) than undersampling
the major class.
Given so,
we utilized SMOTE with five
nearest neighbors in balancing our Relevant-NonRelevant
dataset iteratively then we randomized the dataset uniformly.
Table 5 shows the results of retraining the classifier after en-
hancement.
3.4
Feature Minimization
To this point,
we have collected 65 different features (39
original
+ 26 new) to train TIRM.
Due to the associated
high cost of calculating all
the features,
we investigate the
effect of feature minimization on the trained classifier.
For each feature,
there are two important factors:
cost
(computational power and time) and effectiveness (informa-
tion gain ratio).
We will assume a uniform cost and optimize
in regards to information gain.
We use ranker algorithm
in extracting the top 25 features (as shown in table 3) in
terms of information gain to retrain TIRM.
Table 6 shows
the
60% reduced TIRM classifier has a performance reduc-
tion of about 2%.
4.
INTENTION STRENGTH
To indicate the intention class,
we use the resulting rel-
evance from the model
along with change in TIRM (as il-
lustrated in table 1).
This mapping model
is effective,
but
Precision
Recall
F-measure
Prior TIRM
Relevant
0.863
0.971
0.914
Non-Relevant
0.654
0.263
0.375
Weighted Avg.
0.827
0.849
0.821
TIRM after Enhancement
Relevant
0.880
0.932
0.905
Non-Relevant
0.928
0.873
0.900
Weighted Avg.
0.904
0.903
0.903
TIRM after Minimization
Relevant
0.849
0.939
0.892
Non-Relevant
0.932
0.834
0.880
Weighted Avg.
0.890
0.886
0.886
Table 6:
Results from the previous TIRM,
TIRM
after enhancement, and minimization with Random
Forrest Classifier
unfortunately,
although we can deduce the intention class
(being past or current) but there is no quantification of this
intention strength.
To overcome this,
we devise a formula-
tion of calculating the intention strength in terms of change
and relevance as follows.
For each resource r,
the similarity σ
past−current
is cal-
culated using LDA similarity illustrated earlier between the
two versions, the past version (at t
tweet
) and the current ver-
sion (at t
click
).
Thus, δ
past−current
is the normalized change
ranging from 0.0 to 1.0.
From the classifier we extract the
relevance measure ρ(r) ranging from 0.0 -
1.0,
where 0.0
being completely Non-Relevant,
and 1.0 being completely
Relevant.
Referring back to the TIRM model
table 1 we
define the intention class χ(r) in terms of change δ(r) and
relevance ρ(r) as follows:
χ(r) =











Current,
if ρ(r) > 0.5 & δ(r) > 0.5
Past,
if
(
ρ(r) < 0.5 & δ(r) > 0.5
ρ(r) > 0.5 & δ(r) < 0.5
Unknown,
otherwise
(1)
After identifying the intention class χ(r), we calculate the
intention magnitude or strength |χ(r)|.
From Figure 2 we
can deduce that the point (ρ(r
s
), δ(r
s
)) = (1,1) means it is
most relevant and completely changed which indicates the
strongest “decided current intention” or |χ(r
s
)| = 1.
Point (ρ(r
c
),
δ(r
c
)) = (0.5,0.5) is considered the point of
confusion as it illustrates peak uncertainty of intention,
or
|χ(r
c
)|
= 0.
The further the new resource (ρ(r),
δ(r)) =
(x,y) from the point of confusion the stronger the intention
certainty is.
The furthest distance is the distance from the
confusion point (ρ(r
c
), δ(r
c
)) = (0.5,0.5) to certainty point
(ρ(r
s
), δ(r
s
)) = (1,1).
This distance S will be used for nor-
malization.
Also the distances are Euclidean.
So to calculate |χ(r)| for the new resource (ρ(r), δ(r)) =
(x,y) we follow equation 2.
|χ(r)| =
L
L
0
=
p
(ρ(r) − ρ(r
c
))
2
+ (δ(r) − δ(r
c
))
2
p
(ρ(r
s
) − ρ(r
c
))
2
+ (δ(r
s
) − δ(r
c
))
2
(2)
10-Fold Cross-Validation Testing
Mean
Absolute Error
Relative
Absolute Error
Kappa
Statistic
Incorrectly
Classified %
Correctly
Classified %
Prior TIRM
0.22
75.77%
0.31
15.12%
84.88%
Enhanced TIRM
0.20
39.69
0.81
9.73%
90.27%
Table 5:
Results of 10-fold cross-validation for TIRM prior and after the three-staged enhancement process.
Figure 2:
Intention Strength Mapping.
Or to simplify:
strengthsInstances
|χ(r)| =
q
(ρ(r) −
1
2
)
2
+ (δ(r) −
1
2
)
2
q
(1 −
1
2
)
2
+ (1 −
1
2
)
2
(3)
|χ(r)| =
r
2[(ρ(r) −
1
2
)
2
+ (δ(r) −
1
2
)
2
]
(4)
Finally by merging the intention class χ(r) and the inten-
tion strength |χ(r)| we get this formula:
|χ(r)| =





|χ(r)|
if
χ(r) = Current
−|χ(r)|
if
χ(r) = Past
U ndef ined
if
χ(r) = Unknown
(5)
Equation 5 summarizes |χ(r)| to be a value ranging from
-1.0 to 1.0, with -1.0 being the strongest Past intention and
1.0 being the strongest Current
intention.
For the 1,124
instances in the dataset we calculate the corresponding in-
tention strengths |χ(r
1−1,124
)|.
Figure 3 shows a histogram
of the instances in each intention strength bin ranging from
-1.0 to 1.0 and Figure 4 shows the sorted instances in terms
of intention strength.
5.
MODELING INTENTION ACROSS TIME
At this point, we are able to calculate the intention strength
at the current time, and whether it is current or past, given
Figure 3:
Histogram of the 1,124 instances in each
intention strength bin.
the tweet and the resource’s state at t
tweet
and at t
click
.
The
next logical question was:
Did the intention strength through
the life span of the resource between t
tweet
and t
click
change
at one point during these three and half years?
Answering this question will
put us on track of
answer-
ing the ultimate question of this paper:
Would the study of
how intention strength change through time give us a way
Figure 4:
Intention strength across all 1,124 instances
Behavior
Example Tweet
Steady-Current
Check out our latest news at http://bit.ly/1xC7MhK #PolyU
Changing
@heathermeeker The media just lost interest, the WHO has been releasing regular flu A(H1N1)
updates, latest is #47 http://bit.ly/whodu
Steady-Past
The Real Secret to Becoming a Popular Blogger http://bit.ly/16OY7q via @FreelanceSw
Table 7:
Tweet examples of the behavior classes.
to predict this change at an earlier point in time, namely at
posting time t
tweet
?
In the dataset used in building the model, the 1,124 tweets
were extracted from the Stanford Network Analysis Project
(SNAP) Twitter dataset [9] which spanned around 476 mil-
lion tweets.
These instances were collected in the period
from June to December of 2009.
The Mechanical
Turk ex-
periment
was
conducted and the current
snapshots
were
captured in January 2013 after about three and half years.
We utilized the public archives through the Memento Frame-
work to extract the state of the linked resource at the time
of
the tweet [18].
To get the past version of
the resource,
we extracted the closest memento to the time of posting the
tweet t
tweet
.
To gauge the validity of the extracted past ver-
sion, we measured the time difference between t
tweet
and the
creation time of that closest memento.
The time difference
between t
tweet
and t
closest memento
was ranging from 3.07
minutes to 56.04 hours averaging 25.79 hours.
For the sake
of
simplicity,
we assumed these time deltas are negligible
and t
closest memento
≈ t
tweet
.
Following the same paradigm
we extracted 10 mementos from the period between t
tweet
and t
click
:
t
snapshot
(i) =





t
tweet
f or
i = 0
t
memento
(i)
∀i = 1...10
t
click
f or
i = 11
(6)
Where i = 0 means the first snapshot which is at time of
the tweet and i = 11 means the last snapshot at the current
end time of the experiment.
The ten downloaded mementos
are at i = 1 → 10.
The next step is to calculate the intention strength at
each of those 12 points in time.
Since we need to simulate
the state at each time t
snapshot
(i) we need to download the
state of
the resource,
get the Bitly clicklogs and the sum-
mation of
the posted tweets up to this time.
We mined
the Bit.ly API to extract the clicks count to that moment
t
snapshot
(i).
We extracted the tweets posted till t
snapshot
(i)
from Topsy.com API. This is another rationale behind using
Topsy API instead of
the Twitter API as the latter does
not enable searching further than the indexing period (two
weeks).
Furthermore,
we calculated all
the applicable fea-
tures for each snapshot as shown in Figure 5.
Finally, using
our prior trained model
and the strength formulation we
calculated |χ(r
i
)| for each snapshot and plotted them across
time as shown in examples in Figure 6.
6.
PREDICTING TEMPORAL INTENTION
AT TWEET TIME
In Figure 6, the blue points indicate the intention strength
at this point in time.
We noticed a steady behavior with re-
spect to time in some cases and a changing behavior in oth-
ers.
This matches our intuition that users intended for the
(a) Steady Current intention:∼ 0 slope.
(b) Changing intention:
Current to Past,
-ve slope.
(c) Steady Past intention:∼ 0 slope.
Figure 6:
The resources’ intention strength across time for different behavior categories.
Figure 5:
Intention Strength calculation per snap-
shot
readers to see the version at t
click
for the first short period
of time, but upon changing and updating of the resource the
intention deviated to the t
tweet
version as shown in Figure
6(b).
To further analyze this phenomena,
and to differentiate
the steady state from the changing one, we fitted with blue
intention strength points in the graphs with the closest linear
regression line (red line) to measure its progression through
its slope as shown as well
in Figure 6.
Evidently,
if
the
slope was negative this indicate the intention has changed
from current
to past.
We use both the slope of
the fitted
regression line and the fitting error to cluster the plots into
three different categories:
Steady, Changing, and Unknown.
The Steady Intentional
behavior
means the slope is small
and the fitting error is small, this indicates a resource where
the intention did not change across time.
The Changing
Intentional
behavior means the slope is negative indicating
a change in intention from current to past across time, with
a moderate fitting error.
Finally the Unknown Intentional
behavior is where the regression line fitting error is too high
or the 4th class of
our TIRM model
where the resource is
not relevant and did not change.
Given the slope,
intercept,
and fitting error along with
the other features,
we were able to successfully train a re-
gression classifier to automatically categorize the behavior
of a resource across time into either one of these three cat-
egories.
We performed a 10-fold cross-validation and the
classifier correctly classified 89% of the dataset as shown in
table 9.
We were able to identify the behavioral class of in-
tention given the knowledge of the state of the resource and
the social
network around it through time;
The next step
is to validate the viability of identifying these classes given
only the information available at tweet time t
tweet
.
With our model
from the previous stage,
we filtered out
all the longitudinal temporal features and kept only the fea-
tures extracted from the tweet and the current version of the
resource at t
tweet
.
We retrained the classifier using these lim-
ited features and it correctly classified 77% of the dataset.
This percentage is lower than the prior percentage with the
full
knowledge of
the resource in time as expected.
More-
over,
it is still
high enough indicating the viability of
pre-
dicting the temporal
intention progression given only the
knowledge of the tweet at posting time and the state of the
resource at t
tweet
as shown also in tables 8 and 9.
In other words,
at
the time of
authoring a tweet,
and
given only the information about the resource and the tweet
available at the current time, we can predict for the author
whether the intention conveyed to the readers will be consis-
tent or will
it change with 77% accuracy.
Returning to our tweet examples,
in the tweet in Fig-
ure 1(a),
the model
predicted a change in intention from
current to past with 60% probability.
While in the tweet
in Figure 1(b),
the model
predicted a 60% probability of
steady-current intention.
Furthermore in table 7,
for the
third tweet our model
predicted a steady behavior with a
40% probability.
This prediction will give the author sufficient information
to choose to just post the tweet or take a snapshot of
the
resource and push it into one of the public archives and link
to that snapshot instead to maintain the consistency.
This
prediction will
have implication on maintaining the consis-
tency of the conveyed information on the web and will help
enrich the archived content of
a multitude of
resources by
crowdsourcing the preservation task.
Precision
Recall
F-measure
Steady Intention
0.680
0.715
0.697
Changing Intention
0.912
0.897
0.904
(Current to Past)
Undefined Intention
0.713
0.688
0.700
Weighted Avg.
0.768
0.767
0.767
Table 8:
Intention behavior prediction classifier
10-Fold Cross-Validation Testing
Predicting Intention
Mean
Absolute Error
Relative
Absolute Error
Kappa
Statistic
Incorrectly
Classified %
Correctly
Classified %
With all Features
0.15
34.11%
0.84
10.94%
89.06%
With the tweet and
the resource at t
tweet
0.22
50.57%
0.65
23.32%
76.68%
Table 9:
Results of 10-fold cross-validation for predicting intention behavior strength across time.
Figure 7:
Intention Oracle API service
7.
INTENTION ORACLE API
We built a proof-of-concept class prediction service which
implements the prediction model described in section 6.
The
service takes a tweet with a URL shortened via Bit.ly and
extracts the necessary features after downloading content
and then predicts the behavioral class of the tweet.
For the
time being it classifies if
the resource is more likely to be
in a steady state of intention or a changing state of inten-
tion.
The service interface is shown in Figure 7 and a sam-
ple JSON-encoded response obtained in correspondence to
the three tweet examples in table 7 respectively are demon-
strated in Figure 8.
8.
CONCLUSIONS
In this work,
we analyze the problem of temporal
inten-
tion in sharing resources in social media.
We adopt a prior
preliminary model
in analyzing users’
temporal
intention.
This model enabled us to detect and classify relevance and
map it along with the resource’s change to extract the in-
tention class of the tweet in relation to the linked resource
in it.
In this paper,
our first contribution is by enhancing
this model
and addressing the short comings in regards to
linguistic features analysis,
balancing the training dataset,
and finally using latent semantics in measuring similarity
instead of
merely textual
resemblance.
With these three
stages, we were able to enhance the model considerably, es-
pecially in the Non-Relevant class, with a 0.5 improvement
in F-measure and a 6% increase in total
classification from
the prior model upon utilizing a Random Forrest based clas-
sifier.
Our second contribution is quantifying this temporal
in-
tention based on the enhanced model.
We formulate a com-
bination of the new semantic change measure and the rele-
vance prediction from the enhanced classifier to produce a
normalized quantifiable intention strength measure ranging
from -1.0 to 1.0 (past to current intention, respectively).
The third contribution is analyzing the progression of in-
tention through time.
We simulate the intention analysis
over the period of 3.5 years from June 2009 to January 2013
to observe the intention strength change across time.
We ob-
serve three different classes of behavior:
a steady intention
either in the past or current, a changing intention gradually
with time from the current to past, and undefined intention.
We use these observations to fit regression lines to calculate
the slopes and intercepts of intention to detect the progres-
sion scheme through time.
Our fourth contribution is in predicting the temporal in-
tention at the time of
authoring a tweet.
We incorporate
{
"Tweet Analyzed": "Check out our latest news
at http://bit.ly/1xC7MhK #PolyU",
"Bitly Extracted": "http://bit.ly/1xC7MhK",
"Original Resource URL": "http://www.fb.
polyu.edu.hk/content/10505/index.html",
"State": "Steady, Not changing",
"Prediction": "Predicted Steady intention
for the resource with 60.0% confidence",
"Confidence": "60.0"
}
{
"Tweet Analyzed": "@heathermeeker The media
just lost interest, the WHO has been
releasing regular lu A(H1N1) updates,
latest is #47 http://bit.ly/whodu",
"Bitly Extracted": "http://bit.ly/whodu",
"Original Resource URL": "http://www.who.
int/csr/don/en/",
"State": "Unsteady, Changing",
"Prediction": "Predicted Unsteady intention
observed for the resource, recommend
preservation with 60.0% confidence",
"Confidence": "60.0"
}
{
"Tweet Analyzed": "The Real Secret to
Becoming a Popular Blogger http://bit.ly/
16OY7q via @FreelanceSw",
"Bitly Extracted": "http://bit.ly/16OY7q",
"Original Resource URL": "http://www.
copyblogger.com/popular-blogger/",
"State": "Steady, Not changing",
"Prediction": "Predicted Steady intention
for the resource with 50.0% confidence",
"Confidence": "50.0"
}
Figure 8:
JSON Objects resulting from the Inten-
tion Oracle API
these features to our previous dataset across time and model
the change of
intention with a success of
89%.
Finally we
predict this change or steadiness of intention at t
tweet
by us-
ing only the features that are readily available at t
tweet
from
both the tweet and the resource and were able to success-
fully predict this intention with 77% accuracy.
Giving the
authors enough information to aid them to either re-write
the tweet with the knowledge of change or push a snapshot
of the resource to one of the public archives and link to it
instead, maintaining temporal consistence and enriching the
archives at the same time.
Finally we demonstrate a prototype API service imple-
mentation of
our intention prediction model
to be utilized
on tweet with links shortened via Bitly.com service.
9.
ACKNOWLEDGMENT
This work was supported in part by the Library of Congress
and NSF IIS-1009392.
We thank the anonymous reviewers
for their suggestions in regards to the strength formulation.
10.
REFERENCES
[1]
E. Adar, J. Teevan, S. T. Dumais, and J. L. Elsas.
The Web Changes Everything:
Understanding The
Dynamics Of Web Content. In WSDM ’09:
Proceedings Of The Second ACM International
Conference On Web Search and Data Mining, pages
282–291, 2009.
[2]
N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P.
Kegelmeyer. SMOTE: Synthetic Minority
Over-Sampling Technique. Journal
Of Artificial
Intelligence Research, 16(1):321–357, June 2002.
[3]
T. Chen, D. Lu, M.-Y. Kan, and P. Cui.
Understanding and Classifying Image Tweets. In
Proceedings Of The 21st ACM International
Conference On Multimedia, MM ’13, pages 781–784,
New York, NY, USA, 2013. ACM.
[4]
J. Cho and H. Garcia-Molina. Estimating Frequency
Of Change. ACM Transactions On Internet
Technology, 3(3):256–290, Aug. 2003.
[5]
D. Fetterly, M. Manasse, M. Najork, and J. Wiener. A
Large-Scale Study Of The Evolution Of Web Pages. In
WWW ’03:
Proceedings Of The 12th International
Conference On World Wide Web, pages 669–678,
2003.
[6]
M. D. Hoffman, D. M. Blei, and F. R. Bach. Online
Learning For Latent Dirichlet Allocation. In NIPS,
pages 856–864, 2010.
[7]
A. Java, X. Song, T. Finin, and B. Tseng. Why We
Twitter:
Understanding Microblogging Usage and
Communities. In Proceedings Of The 9th WebKDD
and 1st Sna-Kdd 2007 Workshop On Web Mining and
Social
Network Analysis, WebKDD/SNA-KDD ’07,
pages 56–65, New York, NY, USA, 2007. ACM.
[8]
M. Klein and M. L. Nelson. Investigating The Change
Of Web Pages’ Titles Over Time. Technical Report
arXiv:0907.3445, 2009.
[9]
J. Leskovec and A. Krevl. Snap Datasets:
Stanford
Large Network Dataset Collection.
http://snap.stanford.edu/data, June 2014.
[10]
R. Mehrotra, S. Sanner, W. Buntine, and L. Xie.
Improving Lda Topic Models For Microblogs Via
Tweet Pooling and Automatic Labeling. In
Proceedings Of The 36th International
ACM SIGIR
Conference On Research and Development In
Information Retrieval, SIGIR ’13, pages 889–892, New
York, NY, USA, 2013. ACM.
[11]
E. Momeni, K. Tao, B. Haslhofer, and G.-J. Houben.
Identification Of Useful User Comments In Social
Media:
A Case Study On Flickr Commons. In
Proceedings Of The 13th ACM/IEEE-CS Joint
Conference On Digital
Libraries, JCDL ’13, pages
1–10, New York, NY, USA, 2013. ACM.
[12]
N. Niraula, R. Banjade, D. Stefanescu, and V. Rus.
Experiments With Semantic Similarity Measures
Based On Lda and Lsa. Statistical
Language and
Speech Processing, 7978:188–199, 2013.
[13]
J. Oliva, J. I. Serrano, M. D. Del Castillo, and
A. Iglesias. Symss:
A Syntax-Based Measure For
Short-Text Semantic Similarity. Data & Knowledge
Engineering, 70(4):390–405, 2011.
[14]
K. Radinsky and P. N. Bennett. Predicting content
change on the web. In Proceedings of the Sixth ACM
International
Conference on Web Search and Data
Mining, WSDM ’13, pages 415–424, New York, NY,
USA, 2013. ACM.
[15]
A. Ritter, S. Clark, Mausam, and O. Etzioni. Named
Entity Recognition In Tweets:
An Experimental
Study. In Proceedings Of The Conference On
Empirical
Methods In Natural
Language Processing,
EMNLP ’11, pages 1524–1534, Stroudsburg, PA, USA,
2011. Association for Computational Linguistics.
[16]
V. Rus, N. Niraula, and R. Banjade. Similarity
Measures Based On Latent Dirichlet Allocation.
Computational
Linguistics and Intelligent Text
Processing, 7816:459–470, 2013.
[17]
H. M. SalahEldeen and M. L. Nelson. Reading The
Correct History?:
Modeling Temporal Intention In
Resource Sharing. In Proceedings Of The 13th
ACM/IEEE-CS Joint Conference On Digital
Libraries, JCDL ’13, pages 257–266, New York, NY,
USA, 2013. ACM.
[18]
H. Van de Sompel, M. L. Nelson, R. Sanderson,
L. Balakireva, S. Ainsworth, and H. Shankar.
Memento:
Time Travel For The Web. Technical
Report arXiv:0911.1112, 2009.
[19]
R.
ˇ
Rehuˇrek and P. Sojka. Software Framework For
Topic Modelling With Large Corpora. In Proceedings
Of The LREC 2010 Workshop On New Challenges
For NLP Frameworks, pages 45–50, Valletta, Malta,
May 2010. ELRA.
[20]
A. Wang, T. Chen, and M.-Y. Kan. Re-Tweeting
From A Linguistic Perspective. In Proceedings Of The
Second Workshop On Language In Social
Media, LSM
’12, pages 46–55, Stroudsburg, PA, USA, 2012.
Association for Computational Linguistics.
[21]
W. X. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim,
H. Yan, and X. Li. Comparing Twitter and
Traditional Media Using Topic Models. In Proceedings
Of The 33rd European Conference On Advances In
Information Retrieval, ECIR’11, pages 338–349,
Berlin, Heidelberg, 2011. Springer-Verlag.
