Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 298–302,
Gothenburg, Sweden, 23-24 May 2017.
c
2017 Link
¨
oping University Electronic Press
Wordnet extension via word embeddings:
Experiments on the Norwegian Wordnet
Heidi Sand and Erik Velldal
and Lilja Øvrelid
University of Oslo
Department of Informatics
{heidispe,erikve,liljao}@ifi.uio.no
Abstract
This paper describes the process of auto-
matically adding synsets and hypernymy
relations
to an existing wordnet
based
on word embeddings computed for POS-
tagged lemmas in a large news corpus,
achieving exact
match attachment
accu-
racy of over 80%.
The reported experi-
ments are based on the Norwegian Word-
net,
but the method is language indepen-
dent and also applicable to other wordnets.
Moreover,
this study also represents the
first documented experiments of the Nor-
wegian Wordnet.
1
Introduction
This paper
documents experiments with an un-
supervised method for extending a wordnet with
new words and automatically identifying the ap-
propriate hypernymy relations.
Using word em-
beddings trained on a large corpus of news text
(~330 million tokens),
candidate hypernyms for
a given target
word are identified by computing
nearest neighbors lists towards the wordnet and re-
trieving the ancestors of the neighbors.
The can-
didate hypernyms are then scored according to a
combination of distributional
similarity and dis-
tance in the wordnet graph.
While the particular
experimental results reported here are for the Nor-
wegian Wordnet
(NWN),
and using vectors esti-
mated on POS tagged lemmas of the Norwegian
news corpus, the methodology is generally appli-
cable and not specific to neither the language nor
the particular language resources used.
2
Background
Due to the coverage limitations of manually con-
structed semantic resources,
several
approaches
have attempted to enrich various taxonomies with
new relations and concepts. The general approach
is to attempt
to insert
missing concepts into a
taxonomy based on distributional
evidence.
In
their probabilistic formulation, Snow et al. (2006)
maximize the conditional probability of hyponym-
hypernym relations
based on observed lexico-
syntactic patterns in a corpus.
Jurgens and Pile-
hvar (2015) extend the existing WordNet
taxon-
omy using an additional resource,
Wiktionary,
to
extract sense data based on information (morphol-
ogy/lexical overlap) in the term glosses.
The work which is most directly relevant to our
study is that of Yamada et al. (2009). They extend
an automatically generated Wikipedia-taxonomy
by inserting new terms based on various similar-
ity measures calculated from additional web doc-
uments.
As the approach described in the current
paper
will
abstractly adapt
the approach of
Ya-
mada et al. (2009), we will devote some space to
elaborate how the algorithm works, glossing over
details that does not pertain to our setting. The in-
sertion mechanism works as follows:
For a given
target
word w we first
find the k similar words
that are already present in the hierarchy,
accord-
ing to some distributional similarity measure sim
and with the constraint that the similarity is greater
than some cutoff m.
Secondly the hypernyms of
each of these k similar words are assigned scores,
based on a combination of the similarity measure
and a depth penalty d.
The latter is a function of
the distance r in the hierarchy between the neigh-
bor and the hypernym, as the hypernym candidates
also include ancestor nodes beyond the immediate
parents. Finally, the hypernym h
↑
with the highest
score will be selected for attaching w.
The function used for scoring hypernym candi-
dates is defined as follows (paraphrased here ac-
cording to the notation of the current paper):
score
(
h
↑
) =
∑
h
↓
∈
desc
(
h
↑
)∩
ksim
(
w
)
d
r
(
h
↑
,
h
↓
)−
1
×
sim
(
w
,
h
↓
)
(1)
298
ksim
(
w
)
picks out the k nearest neighbors of the
target word w according to the distributional sim-
ilarity measure sim,
with the constraint
that
the
similarity is greater than a cutoff m.
The function
desc
(
h
↑
)
picks out the descendants (hyponyms) of
the candidate hypernym h
↑
. The term d
r
(
h
↑
,
h
↓
)−
1
is
the depth penalty where the parameter d can have
a value between 0 and 1, r
(
n
↑
,
n
↓
)
is the difference
in hierarchy depth between h
↑
and h
↓
.
For sim,
two distributional similarity measures
are tested by Yamada et al.
(2009),
based on re-
spectively 1) raw verb-noun dependencies and 2)
clustering of verb-noun dependencies.
Yamada et
al. (2009) also apply a baseline approach, selecting
the hypernym of the most similar hyponym (base-
line approach 1), which essentially is the same as
specifying k=1 when computing the nearest neigh-
bors in the approach outlined above.
Manually
evaluating a random sample of 200 of the high-
est scoring insertions, Yamada et al. (2009) report
an attachment accuracy of up to 91.0% among the
top 10,000,
and 74.5% among the top 100,000,
when using the clustering based similarity – a re-
sult
which substantially improves over the base-
line (yielding scores of ~55%).
Although Yamada et al. (2009) work with a se-
mantic taxonomy based on the Wikipedia struc-
ture,
the scoring function in Equation 1 which
forms the pivot of the approach is general enough
to be adopted for other settings as well.
Notably,
no assumptions are made about the particular sim-
ilarity function instantiating sim
(
w
i
,
w
j
)
.
In the
work reported in the current paper we experiment
with instead using a similarity function based on
word embeddings computed from a large unanno-
tated corpus, and apply this for extending NWN.
3
The Norwegian Wordnet
The Norwegian Wordnet (NWN) was created by
translation from the Danish Wordnet
(DanNet)
(Pedersen et
al.,
2009).
DanNet
encodes both
relations found in Princeton Wordnet
(Fellbaum,
1998)
and EuroWordNet
(Vossen,
1998),
some
of which are also employed in NWN.
There are
no prior
publications
documenting NWN,
and
the current
study provides the first
reported ex-
periments on this resource.
Before commenc-
ing our experiments it was therefore necessary to
pre-process NWN in order
to (i)
correct
errors
in the format
and/or the structure of NWN,
and
(ii) remove named entities and multiword expres-
POS
Lemmas
Synsets
Senses
Monos.
Polys.
Noun
38,440
43,112
48,865
31,957
6,483
Verb
2,816
4,967
5,580
1,612
1,204
Adj
2,877
3,179
3,571
2,413
464
Total
44,133
51,258
58,016
35,982
8,151
Table 1:
Number of lemmas,
synsets,
senses and
monosemous/polysemous words for nouns,
verbs
and adjectives in NWN.
sions. The resulting wordnet is summarized in Ta-
ble 1, showing the number of lemmas, synsets and
monosemous/polysemous terms broken down by
their part-of-speech (nouns, adjectives and verbs).
The modified NWN, which forms the basis for our
experiments, is made freely available.
1
4
Word embeddings for tagged lemmas
This section describes how we generate the se-
mantic context
vectors representing both unseen
target
words
and the words
already present
in
the existing wordnet synsets.
These vectors form
the basis of our distributional similarity measure,
used both for computing nearest neighbors within
NWN for unclassified target words and for scor-
ing candidate hypernyms according to Equation 1
(where the similarity function will correspond to
the cosine of word vectors).
Our semantic vectors
are given as word2vec-based word embeddings
(Mikolov et
al.,
2013a;
Mikolov et
al.,
2013b)
estimated on the Norwegian Newspaper Corpus
2
.
This is a corpus of
Norwegian newspaper
texts
from the time period 1998–2014. We used approx-
imately 25% of this corpus (due to some technical
issues), which amounts to 331,752,921 tokens and
3,014,704 types.
Rather than estimating word embeddings from
raw text,
we use POS-tagged lemmas in order to
have embeddings that more closely correspond to
the word representations found in NWN.
In or-
der to extract
lemmas and their parts of speech,
we pre-processed the data using the Oslo-Bergen
Tagger
3
(Johannessen et
al.,
2012),
a rule-based
POS-tagger for Norwegian which also performs
tokenization and lemmatization.
The tagger was
accessed through the Language Analysis Portal
(LAP
4
) which provides a graphical web interface
1
https://github.com/heisand/NWN
2
http://www.nb.no/sprakbanken/repositorium
3
http://www.tekstlab.uio.no/obt-ny
4
https://lap.hpc.uio.no
299
to a wide range of language technology tools (Lap-
poni et al., 2013).
Word2vec implements two model
types:
con-
tinuous
bag-of-words
(CBOW)
and skip-gram.
These differ in the prediction task they are trained
to solve:
prediction of target words given context
words (CBOW) or the inverse, prediction of con-
text
words given target
words (skip-gram).
We
used the word2vec implementation provided in the
free python library gensim (
ˇ
Reh
˚
u
ˇ
rek and Sojka,
2010),
using the default parameters to train skip-
gram models.
The defaults are a minimum of 5
occurrences in the corpus for the lemmas and an
embedding dimension of size 100.
Five iterations
over the corpus was made.
5
The attachment process
In this section we detail the steps involved in clas-
sifying a new word w in the wordnet hierarchy.
5.1
Selecting nearest neighbors
The first step in the process is to compute the list
of k nearest neighbors of w according to the distri-
butional similarity sim
(
w
,
w
0
)
, a measure which in
our case corresponds to the cosine of word embed-
dings described in Section 4. Candidate neighbors
are words that
are (a) already defined in NWN,
(b) have a hypernym in NWN, (c) have the same
part
of speech as the target
and (d) occur in the
news corpus with a sufficient frequency to have a
word embedding in the model
described in Sec-
tion 4.
In addition we discard any neighbors that
have a similarity less than some specified thresh-
old m.
As described in Section 6 we tune both k
and m empirically.
5.2
Selecting candidate hypernyms
Hypernymy is a relation between synsets repre-
senting word senses, which means that the process
of selecting candidate hypernyms for scoring has
the following steps: First we (a) identify the list of
k nearest neighbors for a given target word w, and
then (b) for each neighbor word retrieve all synsets
that encode a sense for that word, before we finally
(c) retrieve all hypernym synsets, including all an-
cestor nodes, for those synsets.
Each candidate hypernym synset h
↑
will in turn
be assigned a score according to Equation 1.
The
synset with the highest score is finally chosen as
the hypernym synset for the target word to be at-
tached in NWN. Note that a given target word will
only be assigned a single hypernym.
5.3
Evaluation
There are several ways one could choose to evalu-
ate the quality of the words that are automatically
inserted into the hierarchy.
For example, Yamada
et al. (2009) chose to manually evaluate a random
sample of 200 unseen words,
while Jurgens and
Pilehvar (2015) treat
the words already encoded
in the hierarchy as gold data and then try to re-
attach these.
We here follow the latter approach.
However,
while Jurgens and Pilehvar (2015) re-
strict their evaluation to monosemous words,
we
also include polysemous words in order to make
the evaluation more realistic.
For evaluation and tuning we split the wordnet
into a development
set
and a test
set,
with 1388
target words in each.
Potential targets only com-
prise words that have a hypernym encoded (which,
in fact,
are not
that
many,
as NWN is relatively
flat) and occur in the news corpus sufficiently of-
ten (
≥
5) to be represented by a word embedding.
We evaluate hypernym selection according to
both accuracy and attachment.
While accuracy
reflects the percentage of target words added that
were correctly placed under the right
hypernym,
the attachment
score is the percentage of target
words that
actually were inserted into NWN.
A
candidate target word might end up not getting at-
tached if it has no neighbors fulfilling the require-
ments described in Section 5.1.
Computing accuracy based only on exactly cor-
rect insertions is rather strict.
Intuitively, a hyper-
nymy relation can be right or wrong with varying
degrees.
We therefore also include a soft
accu-
racy measure that aims to take account of this by
counting how many hyponym or hypernym edges
that
separates a lemma from its correct
position.
Each edge will weight the count by a factor of 0
.
5,
partly based on the accuracy measure of Jurgens
and Pilehvar (2015), who, instead of weighting the
score,
only measures accuracy as the number of
edges away that a lemma is placed from its orig-
inal position.
We defined the formula for soft ac-
curacy as:
count
(
correct
) +
∑
count
(
misplaced
)
0
1
∗
0
.
5
edges
count
(
attached
)
(2)
6
Experiments and results
The parameters that need to be empirically tuned
are:
the depth penalty d, the number of k nearest
300
≥
Freq.
Dev. set
#Words
Att.
Acc.
Soft
5
1388
1337
96.33
55.80
63.25
100
854
840
98.36
61.67
68.08
500
461
448
97.18
62.50
68.89
1000
316
304
96.20
64.47
70.85
Table 2:
Accuracy restricted to target words with
a frequency higher
than some given threshold.
#Words shows the number of attached words.
neighbors to consider,
and the minimum thresh-
old m for the similarity of neighbors towards the
target.
After an initial round of experiments that
determined the approximate appropriate range of
values for these parameters, we performed an ex-
haustive grid search for the best parameter combi-
nation among the following values:
k
∈ [
1
,
12
]
in increments of 1.
m
∈ [
0
.
5
,
0
.
9
]
in increments of 0
.
05.
d
∈ [
0
.
05
,
0
.
5
]
in increments of 0
.
05.
Optimizing for attachment accuracy,
the best pa-
rameter configuration after tuning on the develop-
ment set was found to be k=6, m=0.5, and d=0.05,
yielding an accuracy of 55.80% and a degree of
attachment of 96.33%.
As one might
expect
that
the embeddings are
more reliable for
high-frequent
words than for
low-frequent words, we also computed the dev-set
accuracy relative to frequency of occurrence in the
corpus used for estimating the embeddings.
The
results are shown in Table 2.
We indeed see that
the accuracy goes up when enforcing a higher fre-
quency cutoff,
reaching 64.47% when setting the
cutoff to 1000,
though per definition this means
sacrificing coverage.
We also evaluated the effect
of only inserting
words that had hypernyms with a score higher than
a given cutoff,
which again naturally leads to a
lower degree of attachment. Table 3 shows the ac-
curacies over the development set when enforcing
different cutoffs,
showing an increased accuracy.
We see that the best performance is when the cut-
off on the hypernym score is set to 4.6, with a cor-
responding attachment accuracy of 83.26%.
Held-out results
Applying the model
configu-
ration (without cutoffs) to the held-out test words
of NWN yields an attachment of 95.97% and an
accuracy of 59.91% (soft
acc.
= 66.04%).
We
see that there is a slight increase in the accuracy
≥
Hyp. score
#Words
Att.
Acc.
Soft
0.2
1337
96.33
55.80
63.25
1.0
1185
85.38
59.41
66.85
1.8
958
69.02
65.66
73.20
2.6
720
51.87
72.92
80.16
3.4
505
36.38
78.22
85.00
4.6
239
17.22
83.26
89.33
Table 3:
Accuracy restricted to hypernyms with
a score higher than some given threshold,
com-
puted over the 1388 words in the development set.
#Words shows the number of attached words, e.g.
1337 is 96.33% of 1388.
for the insertions performed with the word embed-
dings when moving from the development data to
the held-out data.
As a baseline approach we also
tried attaching each target word to the hypernym
of its 1-nearest-neighbor.
(When there are several
candidate hypernyms available,
we simply pick
the first candidate in the retrieved list.)
Yielding
an accuracy of 47.61%, it is clear that we improve
substantially over the baseline when instead per-
forming insertion using the scoring function.
Applying the scoring function to the test
set
using the cutoff with the highest
accuracy from
Table 3,
yields an accuracy of
84.96% (soft
=
90.38%), though at the cost of a lower attachment
rate (16.28%).
7
Summary and further work
This paper has demonstrated the feasibility of us-
ing word embeddings for automatically extending
a wordnet
with new words and assigning hyper-
nym relations to them.
When scoring candidate
hypernyms we adopt the scoring function of Ya-
mada et al. (2009) and show that this yields high
accuracy even-though we apply it with a different
type of taxonomic hierarchy and different
types
of
distributional
similarity measures.
We com-
pute distributional
similarity based on word em-
beddings estimated from the Norwegian news cor-
pus,
using this as our basis for automatically at-
taching new words into hypernym relations in the
Norwegian Wordnet, with exact-match accuracies
of over 80%.
For immediate follow-up work we
plan to let the parameter tuning be optimized to-
wards a combination of attachment and accuracy,
rather than just accuracy alone.
301
References
Christiane Fellbaum, editor.
1998.
WordNet: An Elec-
tronic Lexical
Database.
MIT Press,
Cambridge,
MA.
Janne
Bondi
Johannessen,
Kristin
Hagen,
Andr
´
e
Lynum, and Anders Nøklestad.
2012.
Obt+stat:
A
combined rule-based and statistical tagger.
In Gisle
Andersen,
editor,
Exploring Newspaper Language:
Using the web to create and investigate a large cor-
pus of modern Norwegian. John Benjamins, Amster-
dam, The Netherlands.
David Jurgens and Mohammad Taher Pilehvar.
2015.
Reserating the awesometastic:
An automatic exten-
sion of the wordnet taxonomy for novel terms.
In
Proceedings of
the 2015 Conference of
the North
American Chapter of
the Association for Compu-
tational Linguistics Human Language Technologies
(NAACL HLT 2015).
Emanuele Lapponi, Erik Velldal, Nikolay Aleksandrov
Vazov, and Stephan Oepen.
2013.
HPC-ready lan-
guage analysis for human beings.
In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013).
Tomas Mikolov,
Kai
Chen,
Gregory S.
Corrado,
and
Jeffrey Dean.
2013a.
Efficient estimation of word
representations in vector space.
In Proceedings of
the International Conference on Learning Represen-
tations (ICLR).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean.
2013b.
Distributed rep-
resentations of words and phrases and their com-
positionality.
Advances in Neural Information Pro-
cessing Systems.
Bolette Sandford Pedersen,
Sanni
Nimb,
Jørg As-
mussen, Nicolai Hartvig Srensen, Lars Trap-Jensen,
and Henrik Lorentzen.
2009.
DanNet:
the chal-
lenge of compiling a WordNet for Danish by reusing
a monolingual dictionary.
Language Resources and
Evaluation, 43:269–299.
Radim
ˇ
Reh
˚
u
ˇ
rek and Petr
Sojka.
2010.
Software
Framework for
Topic Modelling with Large Cor-
pora.
In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.
Semantic taxonomy induction from heterogenous
evidence.
In Proceedings of the 21st International
Conference on Computational
Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics.
Piek Vossen,
editor.
1998.
EuroWordNet:
A Multi-
lingual Database with Lexical
Semantic Networks.
Kluwer, Dordrecht, The Netherlands.
Ichiro Yamada,
Kentaro Torisawa,
Jun’ichi
Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida.
2009.
Hypernym dis-
covery based on distributional similarity and hierar-
chical structures.
In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing.
302
