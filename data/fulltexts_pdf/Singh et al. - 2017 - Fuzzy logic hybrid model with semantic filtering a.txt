Fuzzy LogicHybrid Model with Semantic Filtering 
Approach for Pseudo Relevance Feedback-based
Query Expansion
Jagendra Singh
1,*
, Mukesh Prasad
2
, Yousef Awwad Daraghmi
3
, Prayag Tiwari
4
, Pranay Yadav
5
,
Neha Bharill
6
, Mahardhika Pratama
7
, Amit Saxena
8
1
Department of Computer Science, National Institute of Technology, Kerala, India
2
Centre for Artificial Intelligence, School of Software, FEIT, University of Technology Sydney, Australia
3
Department of Electrical and Computer Engineering, Palestine Technical University, Palestine
4
Information Engineering, University of Padua, Italy
5
School of Information Technology, RGPV, Bhopal, India 
6
Department of Computer Science and Engineering, Indian Institute of Technology Indore, Indore, India
7
School of Computer Science and Engineering, Nanyang Technological University, Singapore 
8
Department of Computer Science & IT, Guru Ghasidas University, Bilaspur, Chhatishgarh, India
1,*
jagendrasngh@gmail.com
Abstract
—
Individual query expansion term selection methods 
have been widely investigated in an attempt to improve their 
performance. Each expansion term selection method has its own 
weaknesses and strengths. To overcome the weaknesses and utilize 
the strengths of individual methods, this paper combined multiple 
term selection methods. In this paper, initially the possibility of 
improving 
the 
overall 
performance 
using 
individual 
query 
expansion (QE) term selection methods are explored. Secondly,
some 
well-known 
rank 
aggregation 
approaches 
are 
used 
for 
combining multiple QE term selection methods. Thirdly, a new 
fuzzy logic-
based QE approach that considers the relevance score 
produced by different rank aggregation approaches is proposed. 
The proposed fuzzy logic approach combines different weights of 
each term using fuzzy rules to infer the weights of the additional 
query 
terms. 
Finally, 
Word2vec 
approach 
is 
used 
to 
filter 
semantically irrelevant terms obtained after applying the fuzzy 
logic approach.
The experimental results demonstrate that the
proposed approaches achieve significant improvements over each 
individual term selection method, aggregated method and related 
state-of-the
-art method.
Keywords
—
Query 
expansion 
term 
selection; 
information 
retrieval; semantic filtering; rank aggregation; score combination; 
fuzzy inference system
I.
I
NTRODUCTION
Current information retrieval (IR) systems attempt to find 
relevant information or documents with respect to user needs, 
modeled 
through a query 
from 
a large 
data corpus 
in an 
appropriate time interval
[1]. IR systems are used in several 
application domains such as web search, digital library search, 
blog search, information filtering, recommender system and 
social search. A user query to an IR system contains multiple 
topic-specific keywords that describe the user’s information 
needs, and ideally, the system returns the relevant documents. 
However, this is not always the case because the user queries are 
usually 
short 
and 
because 
natural 
language 
is 
inherently 
ambiguous. 
The 
most 
critical 
language 
issue
for 
retrieval 
effectiveness is the term mismatch problem: the indexers and the 
users often do not use the same words. This is known as the 
vocabulary problem [2]. To address the vocabulary problem, 
one of the most natural and successful techniques is to expand 
the original query with additional words that best capture the 
user’s intent. 
Query expansion (QE) is one of the important research topics 
in IR. To improve the performance of IR systems, various QE 
techniques have been proposed [3]-[11]. Berardi et al. [3] used 
association rules to mine QE terms and presented techniques to 
filter out redundant association rules. Chen et al. [4] used 
association rules to discover the degrees of similarity between 
terms and constructed a hierarchical-tree structure to select QE 
terms. Chang et al. [5] presented a QE method based on fuzzy 
rules. Cui et al. [6] presented a method for probabilistic QE 
using query logs. Jin et al. [7] presented a method for QE based 
on the term similarity tree model. Kim et al. [8] presented a 
method for query term expansion and term reweighting using 
the 
term 
co-occurrence 
similarity 
and 
fuzzy 
inference 
techniques. Latiri et al. [9], [10] considered the relationship 
between terms and documents as a fuzzy binary relation, based 
on the closure of the extended fuzzy Galois connection, and used 
fuzzy association rules to determine real correlated terms as QE 
terms. Lin et al. [11] presented a method for mining additional 
query terms for QE.
This paper focuses on pseudo-relevance feedback (PRF)-
based automatic query expansion (AQE). PRF-based QE is an 
effective technique for boosting the overall performance of IR 
systems. This technique assumes that top ranked documents in 
the first-pass retrieval are relevant and then uses feedback 
documents as a source for selecting additional potentially related 
terms. Although PRF-based QE has been shown to be effective 
in improving IR performance [12]-[16] in a number of IR tasks, 
there remains substantial room for improvement. 
978-1-5386-2726-6/17/$31.00 ©2017 IEEE
The 
main 
problem 
of 
AQE 
is 
that 
it 
cannot 
perform 
efficiently due to the inherent sparseness of the user query terms 
in the high-dimensional corpus. Another problem is that not all 
the terms of the top retrieved documents (feedback documents) 
are important for QE. Some of the QE terms may be redundant 
or 
irrelevant, 
and 
some 
may 
even 
misguide 
the 
results, 
especially when there are more irrelevant QE terms than relevant 
terms. QE selection attempts to remove redundant and irrelevant 
terms from the term pool (top retrieved documents as feedback 
documents for selecting user QE terms), and the selected QE 
term set should contain sufficient and reliable information about 
the original document. Thus, QE term selection should not only 
reduce the high dimensionality of the feedback document corpus 
(term pool) but also provide a better understanding of the 
documents to improve the AQE result. Various feedback-based 
QE term selection methods have been widely used in AQE, and 
it has been reported that QE term selection methods can improve 
the efficiency and accuracy of IR models. 
The performance of the proposed model is shown on two 
well-known benchmark datasets: FIRE and TREC-3. For the 
performance evaluation, the proposed model is compared with 
Okapi-BM25 [20], Aguera and Araujo’s model [18] and Tomiye 
et al.’s model [21]. The proposed method increases the precision 
and recall rates of IR systems for document retrieval and also 
achieves a significantly higher average recall rate, average 
precision rate and F-Measure on both datasets. 
The contributions of the proposed model issummarized as 
follows: 
• 
First,CHI, 
Co-occurrence, 
KLD 
and 
Robertson 
selection 
value 
(RSV) 
term 
selection 
methods 
for pseudo-
relevance feedback-based AQE. Then, the experimental analysis 
of these term selection methods is presented with an evaluation 
parameter score. 
• 
Second, 
aggregate 
the 
ranked 
list 
of 
QE 
terms 
suggested by different term selection methods discussed in Step 
1. Here, we combine these ranks using the most popular rank 
aggregation methods: Borda, Condorcet and reciprocal. 
• 
Third, a new fuzzy logic approach to obtain an optimal 
combination of expansion terms obtained from Step 2 with user 
query 
terms. 
Using 
this 
approach, 
we 
find 
a 
set 
of 
best 
performing QE terms. 
• 
Fourth, a new Word2Vec based semantic similarity 
approach approach used to filter the irrelevant and redundant 
expansion terms in the context of the user query obtained from 
Step 3. Next, additional expansion terms are used after applying 
the reweighting approach. 
• 
Finally, 
pair 
t-tests 
are 
conducted 
between 
our 
proposed approaches and the other model considered as a 
baseline model. 
The organization of this paper is as follows. In section II 
explains the proposed model and its algorithm with the fuzzy 
logic-based approach and the semantic filtering-based approach. 
Section III presents experimental results and comparison with 
baseline approaches in terms of precision, recall and F-measure 
on both the FIRE and TREC datasets. Finally, conclusion is 
shown in section IV. 
II.
PROPOSED
RANK
AGGREGATION
WITH
FUZZY
LOGIC
AND
SEMANTIC
FILTERING-BASED
MODEL 
In the traditional PRF-based QE method, the candidate terms 
for expanding the user query are selected from the initially 
retrieved set of documents. The main concept behind the PRF-
based QE method is the proper selection of a similarity measure 
for selecting an initial set of documents and an appropriate 
criterion for selecting expansion terms. The block diagrams of 
our proposed model are depicted in Figures 1 and 2. 
Initially, the proposed model uses a co-occurrence approach, 
in which words present around the query term in the top 
feedback documents are used to select expansion terms; it is 
called co-occurrence-based query expansion (CBQE). In this 
approach, high co-occurrence value terms selected from a co-
occurrence context window form a term pool of candidate terms. 
Further, the chi-square method [21] is used to score terms of the 
term pool, and some high-scoring terms are used as the QE 
terms; 
this 
is 
called 
chi-square-based 
query 
expansion 
(CHIBQE). Next, the concepts behind the KLD and the RSV are 
used to score the term pool terms, and high-scoring terms are 
used to expand the original user query; this is called KLD-based 
query expansion (KLDBQE) and RSV-based query expansion 
(RSVBQE). 
Further, 
rank 
aggregation 
methods, 
namely, 
Borda, 
Condorcet and reciprocal, are used to combine multiple term 
ranks obtained from the co-occurrence, CHI, KLD and RSV 
methods. These rank aggregation methods produce the Borda 
weight (w
B
), Condorcet weight (w
C
) and reciprocal weight (w
R
) 
for each candidate term. Some highly weighted candidate terms 
selected under these rank aggregation methods are used to 
reformulate the original query. These rank aggregation methods 
are called Borda-based query expansion (BBQE), Condorcet-
based query expansion (CNBQE) and reciprocal-based query 
expansion (RBQE). 
After applying rank aggregation methods, the FIS is used to 
combine the Borda weight (w
B
), Condorcet weight (w
C
) and 
reciprocal weight (w
R
) of the candidate terms produced by 
BBQE, CNBQE and RBQE. The FIS produces a fuzzy weight 
(w
F
) for each candidate expansion term. Some high-fuzzy-score 
terms are used to expand the user query, which is called fuzzy 
logic-based query expansion (FLBQE). Then, the concept of 
semantic similarity is used to filter semantically irrelevant 
terms 
obtained 
from 
FLBQE 
for 
query 
reformulation 
or 
expansion, For applying semantic similarity we have used 
Word2Vec [22]. We used the publicly available Gensim library 
provided in [23] to train a Word2Vec distributional word 
representation using the TREC-
CDS scientific article corpus. 
Word2Vec 
is 
a 
highly 
sophisticated 
deep-learning 
neural 
network architecture which learns how to represent words as 
multi-dimensional vectors [24]. The model operates without 
human 
supervision 
by 
considering 
the 
textual 
context 
surrounding words. These contexts may be represented in a 
variety of ways, although in this work we utilize the Skip-gram 
model 
which 
is 
able 
to 
capture 
discontinuous 
multi-word 
sequences. Word2Vec attempts to project words onto a multi-
dimension vector space such that the proximity between two 
vectors 
indicates 
the 
semantic 
similarity 
between 
their 
associated words. We used this property by associated each 
key-phrase with its vector representation and using the ten most 
similar words in the vocabulary for expansion purpose. 
Finally, 
the 
reformulated 
query 
with 
the 
reweighted 
expansion terms is submitted to the searching engine, and a list 
of ranked documents [25] is retrieved as a final result of the user 
query. The steps of the proposed model are shown in Table I. 
Fig. 1. A diagram of the proposed rank aggregation, fuzzy logic and semantic filtering-based AQE model.
TABLE I. ALGORITHM DEVELOPED FOR PROPOSED AQE MODEL. 
1.
Apply okapi-BM25 similarity function to retrieve ranked relevant documents with respect to the user query. 
2.
All the unique terms of the top 
N
retrieved documents obtained from step no. 1 are selected to form the term pool. 
3.
The following methods are used to score the unique terms of the term pool to form candidate terms: 
a)
Calculate the CHI score. 
b)
Calculate the Co-occurrence score. 
c)
Calculate the KLD score. 
d)
Calculate the RSV score. 
The top-scoring candidate terms obtained from substeps (i) to (iv) of step 3 are used to expand the user query, called CHIBQE, CBQE, 
KLDBQE, and RSVBQE, respectively. 
4.
Different rank aggregation methods are used to combine different candidate term ranks obtained from substeps (i) to (iv) of step 3. 
Combination using candidate term rank position. 
a)
Borda rank aggregation produces the Borda weight (w
B
) for each selected candidate term. 
b)
Condorcet rank aggregation produces the Dondorcet weight (w
C
) for each selected candidate term. 
c)
Reciprocal rank aggregation produces the reciprocal weight (w
R
) for each selected candidate term. 
Some top-scoring candidate terms obtained from substeps (i) to (iii) of step 4a are selected to expand the user query, called the BBQE, 
CNBQE, and RBQE approaches, respectively. 
5.
A fuzzy logic controller (FLCcombine) is used to combine the Borda weight (w
B
), Condorcet weight (w
C
) and reciprocal weight 
(w
R
) of the expansion terms suggested from BBQE, CNBQE and RBQE, respectively. Some high-fuzzy-weight (w
F
) expansion 
terms obtained under the FLCcombine approach are used to expand the query. This fuzzy logic-based approach is called FLBQE. 
6.
A semantic filtering approach is used to filter out semantically irrelevant expansion terms from the expansion term set obtained 
from the FLBQE approach. After applying semantic filtering, this fuzzy and semantic-based approach is called FLSBQE.
Fig. 2. The structure of the FIS or FLC used in the proposed model. 
III.
E
XPERIMENTAL 
R
ESULTS
To conduct fair comparisons of the proposed model with 
other popular, relevant and state-of-the-art models, we use 
Okapi-BM25 models (probabilistic-based model) [20], Aguera 
Araujo’s model (combining multiple term selection methods) 
[18] and Tomiye 
et 
al.’s 
model, 
which 
uses 
a 
fuzzy 
and 
semantic-based PRF approach (using the WordNet ontology) to 
QE for improving IR performance) [21]. Two well-known 
datasets are used to show the performance of the proposed 
model. Detailed descriptions of both datasets are given in Table 
II. 
T
ABLE 
II. S
UMMARY OF USED DATASETS AND QUERY NUMBERS
Data sets 
Task 
Queries 
Docs 
No. of 
unique terms 
Average 
document 
length 
TREC-3 
(disk 
1&2)
ad hoc 
151–
200 
7,41,856 
14,83,71,200 
349 
FIRE
ad hoc 
126-175
4,56,329
6,27,56,468
273
A.
Parameter tuning 
To investigate the optimal setting of the parameters for fair 
comparisons, we used the training method explained in Diaz and 
Metzler [26] for our proposed model, which is very popular in 
the IR field. First, for the parameters in PRF models, we used 
different 
numbers 
of 
top 
feedback 
documents 
in 
both 
the 
baseline and proposed approaches (5, 10, 15, 25, and 50) to find 
the optimal number of feedback documents for generating a 
proper collection of expansion terms that may improve the 
performance of the IR system. However, we found that our 
proposed 
model 
performs 
best 
for 
the 
top 
15 
feedback 
document. This is why we use the top 15 feedback documents to 
generate the term pool in our experiment. Second, we select 
different numbers of top candidate terms from the ranked 
candidate terms based on a similarity value with query terms as 
expansion terms (10, 20, 30, 50, and 75) for both the baseline 
and 
proposed 
method 
to 
find 
the 
optimal 
number 
of 
top 
expansion terms used for reformulating the query. However, our 
proposed model performs best with the top 30 candidate terms, 
which is why we used the top 30 candidate terms to reformulate 
the original user query in our experiment. 
B.
Evaluation parameters 
Recall (
R
), precision (
P
) and F-measure are three parameters 
that are used to evaluate the performance of IR systems. Recall 
and 
Precision 
are 
defined 
as 
shown 
in 
Eq. 
(1) 
and 
(2), 
respectively. 
Recall = 
|
R
|
|
S
|
(1) 
where 
R
r
is the set of relevant documents retrieved and 
S
arel
is the set of all relevant documents. 
Precision = 
|
R
|
|
S
|
(2) 
where 
S
ret
is the retrieved document set. 
The Average Precision (AP) is used as a standard measure to 
determine the quality of a search system in IR. The precision of 
a document 
d
is defined as the fraction of relevant documents 
within the set of retrieved documents. The AP of a relevant 
document sets is obtained as the mean precision of all these 
documents and it is shown in Eq. (3). 
AP = 
1
n
Precision
(
P
)
(3) 
where 
R
i
is the relevant document set. 
The F-measure is a harmonic combination of the precision 
(
P
i
) and recall (
R
i
) values of the 
i
th
document set used in IR. 
The F-measure is calculated by Eq. (4) as follows: 
= 
2
+ 
(4) 
We use these evaluation metrics as the primary single 
summary performance metric in our experiments, which is also 
the main official evaluation metric in the corresponding TREC 
and FIRE evaluation forum. To further confirm the superiority 
of our proposed method’s results, we used fixed-level the 
Interpolated Precision, Recall (the PR curve) curve to facilitate 
the basic comparisons of our proposed method with other 
methods. 
Tables III and IV show the retrieval performance of fuzzy 
logic and semantic filtering-based methods in terms of average 
precision and recall on both the FIRE and TREC datasets and 
compared with Tomiye et al.’s model, where Tomiye’s model is 
the 
state-of-the-art 
fuzzy 
and 
semantic-based 
(using 
the 
WordNet ontology) QE model (PRF-based) [19] for improving 
IR performance [21]. In our experiment, Tables III and IV show 
that the performance of our proposed fuzzy logic-based QE 
approach, FLBQE, alone and with semantic filtering, FLSBQE, 
achieved 
significant 
improvements 
over 
the 
Okapi
-BM25 
model, KLDBQE (best individual expansion term selection 
method), BBQE (best rank aggregation method) and Tomiye et 
al.’s methods. 
Figure 3 shows the significant improvement obtained by our 
proposed FLBQE and FLSBQE techniques over Okapi-BM25 
and Tomiye et al.’s model in terms of Recall, Precision and F-
measures on both the FIRE and TREC datasets. 
The 
11-point 
precision 
recall 
curves 
of 
the 
proposed 
approaches, namely, FLBQE and FLSBQE, and the baseline 
approaches, Okapi-BM25 and Tomiye et al.’s model, are shown 
in Figure 4. The 11-point precision-recall curve is a graph 
plotting the interpolated precision of an IR system at 11 standard 
recall levels: {0.0, 0.1, 0.2,...,1.0}. The graph is widely used to 
evaluate IR systems that return ranked documents, which are 
c
ommon in modern search systems. Figure 4 also shows the 
significant improvement obtained by both of our proposed 
approaches over the baseline approaches. This indicates that the 
combination of fuzzy logic and semantic filtering have a positive 
effect on improving the quality of expansion terms. 
A.
Statistical Analysis 
After observing that our proposed approach achieves better 
performance than the best of the individual similarity measures 
considered, a t-test was applied to show that the improvement is 
statistically significant. This pair t-test compares one set of 
measurements with a second set from the same sample. Given 
two paired sets 
X
i
and 
Y
i
of n measured values, the paired t-test 
determines whether they differ from each other in a significant 
way under the assumptions that the paired differences are 
independent and identically normally distributed. 
Table V shows pair t-test values between our proposed 
approaches and the other proposed model. The tables contain 
only the proposed approaches that pass the pair t-test. In our 
experiment, we compared both of our proposed approaches with 
Tomiye 
et 
al.’s 
model. Table 
V 
clearly 
indicates 
that the 
improvement 
of 
our 
proposed 
approaches, 
FLBQE 
and 
FLSBQE, over Tomiye et al.’s model is statistically significant 
at a = 0.05 (
p
is almost zero on both the FIRE and TREC 
dataset). 
IV.
C
ONCLUSION
In this paper, we presented a new fuzzy logic-based AQE 
method for document retrieval based on PRF techniques by 
mining additional QE terms, where our proposed fuzzy logic-
based approach combined the Borda, Condorcet and reciprocal 
weights of candidate expansion terms and produced a single 
fuzzy weight for every candidate expansion term. The proposed 
method calculates the degrees of importance of relevant terms 
to find additional query terms. Higher the degree of importance 
of a relevant term, higher the chance that the relevant term is 
selected as an additional query term. The proposed method uses 
fuzzy rules to infer the weights of the additional query terms 
and then uses these additional query terms together with the 
original query terms to retrieve documents to improve the 
performance 
of 
IR 
systems. 
Fuzzy 
logic-based 
semantic 
similarity 
algorithms 
are 
used 
to 
filter 
out 
semantically 
irrelevant terms from candidate expansion terms obtained after 
applying the fuzzy logic-based QE approach.
Fig. 3.
Recall, Precision and F-measure values of both proposed approaches on both the FIRE and TREC datasets (for top 10 retrieved documents, discussed in 
Tables III and IV).
Fig. 4. 
Precision recall curve of both proposed approaches on both the FIRE and TREC datasets.
T
ABLE 
III.
C
OMPARISON OF BOTH OF OUR PROPOSED APPROACHES WITH 
T
OMIYE ET AL
.’
S MODEL IN TERMS OF AVERAGE PRECISION AND RECALL FOR 
50
QUERIES USING TOP 
15
FEEDBACK DOCUMENTS AND TOP 
30
EXPANSION TERMS ON THE 
FIRE
DATASET
.
Methods
Top 10 retrieved documents 
Top 25 retrieved documents 
Top 50 retrieved documents 
Average precision 
Average recall 
Average precision 
Average recall 
Average precision 
Average recall 
Okapi-BM25 
0.2217 
0.1043 
0.2175 
0.1871 
0.1839 
0.2957 
KLDBQE 
0.2517 
0.1299 
0.2481 
0.2150 
0.2397 
0.3268 
BBQE 
0.2925 
0.1446 
0.2772 
0.2510 
0.2640 
0.3418 
Tomiye et al.’s model 
0.2932 
0.1413 
0.2669 
0.2518 
0.2556 
0.3459 
FLBQE (Proposed) 
0.3250 
0.1574 
0.2798 
0.2712 
0.2723 
0.3606 
FLSBQE (Proposed) 
0.3299
0.1620
0.2829
0.2762
0.2799
0.3613
T
ABLE 
IV.
C
OMPARISON OF BOTH OF OUR PROPOSED APPROACHES WITH 
T
OMIYE ET AL
.’
S MODEL IN TERMS OF AVERAGE PRECISION AND RECALL FOR 
50
QUERIES USING TOP 
15
FEEDBACK DOCUMENTS AND TOP 
30
EXPANSION TERMS ON THE 
TREC
DATASET
.
Methods
Top 10 retrieved documents 
Top 25 retrieved documents 
Top 50 retrieved documents 
Average precision 
Average recall 
Average precision 
Average recall 
Average precision 
Average recall 
Okapi-BM25 
0.2378 
0.1172 
0.2204 
0.1911 
0.1955 
0.3012 
KLDBQE 
0.2536 
0.1304 
0.2594 
0.2191 
0.2413 
0.3315 
BBQE 
0.2983 
0.1450 
0.2785 
0.2601 
0.2693 
0.3452 
Tomiye et al.’s model 
0.2928 
0.1499 
0.2795 
0.2585 
0.2696 
0.3467 
FLBQE (Proposed) 
0.3293 
0.1587 
0.2851 
0.2799 
0.2729 
0.3644 
FLSBQE (Proposed) 
0.3310
0.1635
0.2867
0.2818
0.2803
0.3663
T
ABLE 
V.
P
AIRED T
-
TEST RESULTS BETWEEN BOTH PROPOSED APPROACHES AND 
T
OMIYE ET AL
.’
S MODEL ON THE 
FIRE
AND 
TREC
DATASETS
. 
Proposed approaches
Dataset
Tomiye et al.’s model
h-Value
p-Value
CI
FLBQE 
FIRE 
1 
0.0005 
[-0.1712, -0.1010] 
TREC 
1 
0.0010 
[-0.1137, -0.0694] 
FLSBQE 
FIRE 
1 
0.0000 
[-0.1610, -0.0842] 
TREC
1 
0.0011
[-0.1385, -0.0473]
R
EFERENCES
[1]
R. B. Yates, R. Berthier, Modern Information Retrieval. Addisson 
Wesley, Harlow, UK, 1999. 
[2]
G. W. Furnas, T. K. Landauer, L. M. Gomez, S. T. Dumais, “The 
vocabulary 
problem 
in 
human-system 
communication,” 
Commun. ACM, vol. 30, no. 11, pp. 964-971, Nov. 1987. 
[3]
M. Berardi, M. Lapi, P. Leo, D. Malerba, C. Marinelli, G. 
Scioscia, “A data mining approach to PubMed query refinement,” 
in 
Proc. 
15th 
Int. 
Workshop 
Database 
Expert 
Syst. 
Appl., 
Zaragoza, Spain, 2004, pp. 401–405. 
[4]
H. Chen, J. X. Yu, K. Furuse, N. Ohbo, “Support IR query 
refinement by partial keyword set,” in Proc. 2nd Int. Conf. Web 
Inf. Syst. Eng., Vol. 1, Singapore, 2001, pp. 245–253. 
[5]
Y. C. Chang, S. M. Chen, C. J. Liau, “A new query expansion 
method based on fuzzy rules,” in Proc. 7th Joint Conf. AI Fuzzy 
Syst. Grey Syst., Taipei, Taiwan, Republic of China, 2003. 
[6]
H. Cui, J. R. Wen, J. Y. Nie, W. Y. Ma, “Probabilistic query 
expansion using query logs,” in Proc. 11th Int. Conf. World Wide 
Web, Honolulu, Hawaii, 2002, pp. 325–332. 
[7]
Q. Jin, J. Zhao, B. Xu, “Query expansion based on term similarity 
tree model,” in Proc. 2003 Int. Conf. Nat. Lang. Process. Knowl. 
Eng., Beijing, China, 2003, pp. 400–406. 
[8]
B. M. Kim, J. Y. Kim, J. Kim, “Query term expansion and 
reweighting 
using 
term 
co-occurrence 
similarity 
and 
fuzzy 
inference,” in Proc. Joint 9th IFSA World Congr. 20th NAFIPS 
Int. Conf., Vol. 2, Vancouver, Canada, 2001, pp. 715–720. 
[9]
C. C. Latiri, S. Elloumi, J. P. Chevallet, A. Jaoua, “Extension of 
fuzzy Galois connection for information retrieval using a fuzzy 
quantifier,” in Proc. 2003 ACS/IEEE Int. Conf. Comput. Syst. 
Appl., Tunis, Tunisia, 2003, pp. 109-118. 
[10]
C. C. Latiri, S. B. Yahia, J. P. Chevallet, A. Jaoua, 2003, “Query 
expansion using fuzzy association rules between terms,” in Proc. 
2003 4th JIM Int. Conf. Knowl. Discov. Discret. Math., France: 
Mets. 
[11]
H. C. Lin, L. H. Wang, S. M. Chen, “A new query expansion 
method for document retrieval by mining additional query terms,” 
in Proc. 2005 Int. Conf. Bus. Inf., Hong Kong, China, 2005. 
[12]
Y. 
Bade, 
R. 
Bhat, 
P. 
Borate, 
“Optimization 
techniques 
for 
improving the performance of information retrieval system,” Int. 
J. Res. Advent Technol., vol. 2, no. 2, pp. 263–267, Feb. 2014. 
[13]
K. C. Thompson, “Reducing the risk of query expansion via 
robust constrained optimization,” in CIKM ’09 Proc. 18th ACM 
Conf. Inf. Knowl. Manag., New York, NY, 2009, pp. 837–846. 
[14]
K. Raman, R. Udupa, P. Bhattacharya, A. Bhole, “On improving 
pseudo-relevance feedback using pseudo-irrelevant documents,” 
in Advances in Information Retrieval., C. Gurrin, Y. He, G. 
Kazai, U. Kruschwitz, S. Little, T. Roelleke, S. Rüger, K. van 
Rijsbergen, Eds. Berlin, Germany: Springer Berlin Heidelbert, 
2010, pp. 573–576. 
[15]
R. W. White, G. Marchionini, “Examining the effectiveness of 
real-time query expansion,” Inf. Process. Manag., vol. 43, no. 3, 
pp. 685–704, May 2007. 
[16]
Z. Ye, J. X. Huang, H. Lin, “Finding a good query-related topic 
for boosting pseudo-relevance feedback,” J. Am. Soc. Inf. Sci. 
Technol., vol. 62, no. 4, pp. 748-760, Apr. 2011. 
[17]
A. Adekpedjou, K. D. Zamba, “A chi-squared goodness of Fit test 
for recurrent event data,” J. Stat. Theory Appl., vol. 11, no. 2, pp. 
97-119, 2012. 
[18]
J. R. P. Aguera, L. Araujo, “Comparing and combining methods 
for automatic query expansion,” Adv. Nat. Lang. Process. Appl. 
Res. Comput. Sci., vol. 33, pp. 177-188, Apr. 2008. 
[19]
X. Zhang, S. Wang, G. Huang, “Query expansion based on 
associated semantic space,” J. Comput., vol. 6, no. 2, pp. 172-177, 
Jan. 2011. 
[20]
S. E. Robertson, S. Walker, S. Jones, M. M. H. Beaulieu, M. 
Gatford, “Okapi at TREC-3,” in Proc. 3rd Text Retr. Conf., 1995, 
pp. 109-126. 
[21]
A. C. Tomiye, A. B. Samuel, A. B. Ijesunor, I. Udo, “A fuzzy-
ontology 
based 
information 
retrieval 
system 
for 
relevance 
feedback,” Int. J. Comput. Sci. Issues, vol. 18, no. 1, pp. 382-389, 
Jan. 2011. 
[22]
Y. Goldberg, and 
O. Levy, “word2vec Explained: Deriving 
Mikolov et al.’s Negative-Sampling Word-Embedding Method,” 
In: arXiv preprint arXiv:1402.3722, 2014. 
[23]
R. Rehurek, and P. Sojka, “Software Framework for Topic 
Modelling with Large Corpora,” In: Proceedings of the LREC’10 
Workshop on New Challenges for NLP Frameworks, pp. 45-50, 
2010. 
[24]
17. T. Goodwin, and S. M. Harabagiu, “UTD at TREC 2014: 
Query 
Expansion 
for 
Clinical 
Decision 
Support,” 
In: 
The 
Twenty-Third 
Text 
REtrieval 
Conference, 
TREC’14, 
NIST 
Special Publication, 2014. 
[25]
Z. Wei, W. Gao, T. E. Ganainy, W. Magdy, K. F. Wong, 
“Ranking model selection and fusion for effective micro blog 
search,” in Proc. 1st Int. Workshop Soc. Media Retr. Anal. New 
York, NY: ACM, 2014, pp. 21-26. 
[26]
F. Diaz, D. Metzler, “Improving the estimation of relevance 
models using large external corpora,” in Proc. 29th Annu. Int. 
ACM SIGIR Conf. Res. Dev. Inf. Retr. New York, NY: ACM, 
2006, pp. 154-161 
