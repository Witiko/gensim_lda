Document Classification by Inversion of
Distributed Language Representations
Matt Taddy
University of Chicago Booth School of Business
taddy@chicagobooth.edu
Abstract
There have been many recent
advances
in the structure and measurement
of dis-
tributed language models:
those that map
from words to a vector-space that is rich in
information about word choice and com-
position.
This
vector-space is
the dis-
tributed language representation.
The
goal
of
this
note
is
to point
out
that any distributed representation can be
turned into a classifier through inversion
via Bayes rule.
The approach is simple
and modular,
in that
it
will
work with
any language representation whose train-
ing can be formulated as
optimizing a
probability model.
In our application to 2
million sentences from Yelp reviews,
we
also find that it performs as well as or bet-
ter than complex purpose-built algorithms.
1
Introduction
Distributed, or vector-space, language representa-
tions
V
consist
of a location,
or embedding,
for
every vocabulary word in
R
K
, where
K
is the di-
mension of the latent representation space.
These
locations are learned to optimize, perhaps approx-
imately, an objective function defined on the origi-
nal text such as a likelihood for word occurrences.
A popular example is the Word2Vec machin-
ery of
Mikolov et
al.
(2013b).
This trains the
distributed representation to be useful as an input
layer for prediction of words from their neighbors
in a Skip-gram likelihood. That is, to maximize
t+b
X
j 6
=t, j=t−b
log p
V
(w
sj
| w
st
)
(1)
summed across all words
w
st
in all sentences
w
s
,
where
b
is the skip-gram window (truncated by the
ends of the sentence) and
p
V
(w
sj
|w
st
)
is a neural
network classifier that takes vector representations
for
w
st
and
w
sj
as input (see Section 2).
Distributed language representations have been
studied since the early work on neural
networks
(Rumelhart
et
al.,
1986) and have long been ap-
plied in natural
language processing (Morin and
Bengio,
2005).
The models are generating much
recent interest due to the large performance gains
from the newer systems, including Word2Vec and
the Glove model of Pennington et al. (2014),
ob-
served in,
e.g.,
word prediction,
word analogy
identification, and named entity recognition.
Given the success
of
these new models,
re-
searchers have begun searching for ways to adapt
the representations for use in document classifica-
tion tasks such as sentiment prediction or author
identification.
One naive approach is to use ag-
gregated word vectors across a document (e.g.,
a
document’s average word-vector location) as input
to a standard classifier (e.g.,
logistic regression).
However,
a document is actually an ordered path
of locations through
R
K
, and simple averaging de-
stroys much of the available information.
More sophisticated aggregation is proposed in
Socher et al.
(2011; 2013),
where recursive neu-
ral networks are used to combine the word vectors
through the estimated parse tree for each sentence.
Alternatively,
Le and Mikolov’s Doc2Vec (2014)
adds document labels to the conditioning set in (1)
and has them influence the skip-gram likelihood
through a latent input vector location in
V
. In each
case,
the end product is a distributed representa-
tion for every sentence (or document for Doc2Vec)
that can be used as input to a generic classifier.
1.1
Bayesian Inversion
These approaches all add considerable model and
estimation complexity to the original
underlying
distributed representation.
We are proposing a
simple alternative that turns fitted distributed lan-
guage representations
into document
classifiers
arXiv:1504.07295v3 [cs.CL] 24 Jul 2015
without any additional modeling or estimation.
A typical
language model
is trained to max-
imize the likelihoods of
single words and their
neighbors.
For example, the skip-gram in (1) rep-
resents conditional
probability for a word’s con-
text
(surrounding words),
while the alternative
CBOW Word2Vec specification (Mikolov et
al.,
2013a) targets the conditional probability for each
word given its context.
Although these objectives
do not
correspond to a full
document
likelihood
model, they can be interpreted as components in a
composite likelihood
1
approximation.
Use
w = [w
1
. . . w
T
]
0
to denote a sentence:
an
ordered vector of words.
The skip-gram in (1)
yields the pairwise composite log likelihood
2
log p
V
(w) =
T
X
j=1
T
X
k=1
1
[1≤|k−j|≤b]
log p
V
(w
k
|w
j
).
(2)
In another example, Jernite et al. (2015) show that
CBOW Word2Vec corresponds to the pseudolike-
lihood for a Markov random field sentence model.
Finally,
given a sentence likelihood as in (2),
document
d = {w
1
, ...w
S
}
has log likelihood
log p
V
(d) =
X
s
log p
V
(w
s
).
(3)
Now suppose that your training documents are
grouped by class label,
y ∈ {1 . . . C}
.
We can
train separate distributed language representations
for
each set
of
documents as partitioned by
y
;
for example, fit Word2Vec independently on each
sub-corpus
D
c
= {d
i
:
y
i
= c}
and obtain the
labeled distributed representation map
V
c
.
A new
document
d
has probability
p
V
c
(d)
if we treat it as
a member of class
c
, and Bayes rule implies
p(y|d) =
p
V
y
(d)π
y
P
c
p
V
c
(d)π
c
(4)
where
π
c
is our prior probability on class label
c
.
Thus
distributed
language
representations
trained
separately
for
each
class
label
yield
directly a document
classification rule via (4).
This approach has a number of attractive qualities.
Simplicity: The inversion strategy works for any
model of language that can (or its training can) be
1
Composite likelihoods are a common tool in analysis of
spatial
data and data on graphs.
They were popularized in
statistics by Besag’s (1974;
1975) work on the pseudolike-
lihood –
p(w) ≈
Q
j
p(w
j
|w
−j
)
– for analysis of Markov
random fields. See Varin et al. (2011) for a detailed review.
2
See Molenberghs and Verbeke (2006) for similar pair-
wise compositions in analysis of longitudinal data.
interpreted as a probabilistic model.
This makes
for
easy implementation in systems that
are al-
ready engineered to fit
such language represen-
tations,
leading to faster
deployment
and lower
development
costs.
The strategy is also inter-
pretable: whatever intuition one has about the dis-
tributed language model can be applied directly to
the inversion-based classification rule.
Inversion
adds a plausible model
for reader understanding
on top of any given language representation.
Scalability: when working with massive corpora
it is often useful to split the data into blocks as part
of distributed computing strategies.
Our model of
classification via inversion provides a convenient
top-level partitioning of the data. An efficient sys-
tem could fit separate by-class language represen-
tations,
which will
provide for document
classi-
fication as in this article as well as class-specific
answers for NLP tasks such as word prediction or
analogy.
When one wishes to treat a document as
unlabeled, NLP tasks can be answered through en-
semble aggregation of the class-specific answers.
Performance: We find that,
in our examples,
in-
version of Word2Vec yields lower misclassifica-
tion rates than both Doc2Vec-based classification
and the multinomial inverse regression (MNIR) of
Taddy (2013b).
We did not
anticipate such out-
right performance gain.
Moreover, we expect that
with calibration (i.e.,
through cross-validation)
of the many various tuning parameters available
when fitting both Word and Doc 2Vec the perfor-
mance results will change. Indeed, we find that all
methods are often outperformed by phrase-count
logistic regression with rare-feature up-weighting
and carefully chosen regularization.
However, the
out-of-the-box performance of
Word2Vec inver-
sion argues for its consideration as a simple default
in document classification.
In
the
remainder,
we
outline
classification
through inversion of a specific Word2Vec model
and illustrate the ideas in classification of
Yelp
reviews.
The implementation requires
only a
small
extension of the popular
gensim
python
library
(Rehurek
and
Sojka,
2010);
the
ex-
tended
library
as
well
as
code
to
reproduce
all
of
the
results
in this
paper
are
available
on
github
.
In
addition,
the
yelp
data
is
publicly
available
as
part
of
the
correspond-
ing data mining contest
at
kaggle.com
.
See
github.com/taddylab/deepir
for detail.
2
Implementation
Word2Vec trains
V
to maximize the skip-gram
likelihood based on (1).
We work with the Huff-
man softmax specification (Mikolov et al., 2013b),
which includes a pre-processing step to encode
each vocabulary word in its representation via a
binary Huffman tree (see Figure 1).
Each individual probability is then
p
V
(w|w
t
) =
L(w)−1
Y
j=1
σ

ch [η(w, j + 1)] u
>
η(w,j)
v
w
t

(5)
where
η(w, i)
is the
i
th
node in the Huffman tree
path, of length
L(w)
, for word
w
;
σ(x) = 1/(1 +
exp[−x])
; and
ch(η) ∈ {−1, +1}
translates from
whether
η
is a left or right child to +/- 1.
Every
word thus has both input and output vector coor-
dinates,
v
w
and
[u
η(w,1)
· · · u
η(w,L(w))
]
. Typically,
only the input space
V = [v
w
1
· · · v
w
p
]
,
for a
p
-
word vocabulary, is reported as the language rep-
resentation – these vectors are used as input
for
NLP tasks.
However, the full representation
V
in-
cludes mapping from each word to both
V
and
U
.
We apply the
gensim
python implementation
of Word2Vec, which fits the model via stochastic
gradient
descent
(SGD),
under default
specifica-
tion.
This includes a vector space of dimension
K = 100
and a skip-gram window of size
b = 5
.
2.1
Word2Vec Inversion
Given Word2Vec trained on each of
C
class-
specific corpora
D
1
. . . D
C
,
leading to
C
distinct
language representations
V
1
. . . V
C
,
classification
for new documents is straightforward.
Consider
Figure 1:
Binary Huffman encoding of a 4 word
vocabulary,
based upon 18 total
utterances.
At
each step proceeding from left
to right
the two
nodes with lowest count are combined into a par-
ent node.
Binary encodings are read back off of
the splits moving from right to left.
the
S
-sentence document
d
:
each sentence
w
s
is
given a probability under each representation
V
c
by applying the calculations in (1) and (5).
This
leads to the
S×C
matrix of sentence probabilities,
p
V
c
(w
s
)
, and document probabilities are obtained
p
V
c
(d) =
1
S
X
s
p
V
c
(w
s
).
(6)
Finally,
class
probabilities
are
calculated
via
Bayes rule as in (4).
We use priors
π
c
= 1/C
, so
that classification proceeds by assigning the class
ˆ
y = argmax
c
p
V
c
(d).
(7)
3
Illustration
We consider a corpus of reviews provided by Yelp
for a contest on
kaggle.com
.
The text is tok-
enized simply by converting to lowercase before
splitting on punctuation and white-space.
The
training data are 230,000 reviews containing more
than 2 million sentences.
Each review is marked
by a number
of
stars,
from 1 to 5,
and we fit
separate Word2Vec representations
V
1
. . . V
5
for
the documents at
each star
rating.
The valida-
tion data consist
of 23,000 reviews,
and we ap-
ply the inversion technique of Section 2 to score
each validation document
d
with class probabili-
ties
q = [q
1
· · · q
5
]
, where
q
c
= p(c|d)
.
The probabilities will be used in three different
classification tasks; for reviews as
a.
negative at 1-2 stars, or positive at 3-5 stars;
b.
negative 1-2, neutral 3, or positive 4-5 stars;
c.
corresponding to each of 1 to 5 stars.
In each case,
classification proceeds by sum-
ming across the relevant
sub-class probabilities.
For example,
in task
a
,
p(
positive
)
= q
3
+
q
4
+ q
5
.
Note that the same five fitted Word2Vec
representations are used for each task.
We consider a set
of related comparator tech-
niques.
In each case,
some document
repre-
sentation (e.g.,
phrase counts or
Doc2Vec vec-
tors) is used as input
to logistic regression pre-
diction of the associated review rating.
The lo-
gistic regressions are fit
under
L
1
regularization
with the penalties weighted by feature standard
deviation (which,
e.g.,
up-weights rare phrases)
and selected according to the corrected AICc cri-
teria (Flynn et al.,
2013) via the
gamlr
R pack-
age of Taddy (2014).
For multi-class tasks
b
-
c
,
we use distributed Multinomial regression (DMR;
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
0.0
0.2
0.4
0.6
0.8
1.0
word2vec inversion
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
0.0
0.2
0.4
0.6
0.8
1.0
phrase regression
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
0.0
0.2
0.4
0.6
0.8
1.0
doc2vec regression
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2
3
4
5
0.0
0.2
0.4
0.6
0.8
1.0
mnir
stars
probability positive
Figure 2:
Out-of-Sample fitted probabilities of a review being positive (having greater than 2 stars) as
a function of the true number of review stars.
Box widths are proportional to number of observations in
each class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars.
Taddy 2015) via the
distrom
R package.
DMR
fits multinomial logistic regression in a factorized
representation wherein one estimates independent
Poisson linear models for each response category.
Document representations and logistic regressions
are always trained using only the training corpus.
Doc2Vec is also fit via
gensim
,
using the same
latent space specification as for Word2Vec:
K =
100
and
b
= 5
.
As recommended in the doc-
umentation,
we apply repeated SGD over 20 re-
orderings of each corpus (for comparability,
this
was also done when fitting Word2Vec).
Le and
Mikolov provide two alternative Doc2Vec specifi-
cations: distributed memory (DM) and distributed
bag-of-words (DBOW).
We fit both.
Vector rep-
resentations for validation documents are trained
without updating the word-vector elements,
lead-
ing to 100 dimensional
vectors for
each docu-
ment
for
each of
DM and DCBOW.
We input
each,
as well
as the combined 200 dimensional
DM+DBOW representation, to logistic regression.
Phrase regression applies logistic regression of re-
sponse classes directly onto counts for short
1-2
word ‘phrases’.
The phrases are obtained using
gensim
’s phrase builder, which simply combines
highly probable pairings; e.g.,
first date
and
chicken wing
are two pairings in this corpus.
MNIR,
the
multinomial
inverse
regression
of
Taddy (2013a;
2013b;
2015)
is applied as im-
plemented in the
textir
package for R. MNIR
maps
from text
to
the
class-space
of
inter-
est
through a multinomial
logistic regression of
phrase counts onto variables relevant to the class-
space.
We apply MNIR to the same set
of 1-2
word phrases used in phrase regression.
Here, we
regress phrase counts onto stars expressed numeri-
cally and as a 5-dimensional indicator vector, lead-
a
(NP)
b
(NNP)
c
(1-5)
W2V inversion
.099
.189
.435
Phrase regression
.084
.200
.410
D2V DBOW
.144
.282
.496
D2V DM
.179
.306
.549
D2V combined
.148
. 284
.500
MNIR
.095
.254
.480
W2V aggregation
.118
.248
.461
Table 1:
Out-of-sample misclassification rates.
ing to a 6-feature multinomial logistic regression.
The MNIR procedure then uses the
6×p
matrix of
feature-phrase regression coefficients to map from
phrase-count
to feature space,
resulting in 6 di-
mensional ‘sufficient reduction’ statistics for each
document. These are input to logistic regression.
Word2Vec aggregation averages fitted word rep-
resentations for a single Word2Vec trained on all
sentences to obtain a fixed-length feature vector
for each review (
K = 100
, as for inversion).
This
vector is then input to logistic regression.
3.1
Results
Misclassification rates for each task on the valida-
tion set
are reported in Table 1.
Simple phrase-
count regression is consistently the strongest per-
former,
bested only by Word2Vec inversion on
task
b
. This is partially due to the relative strengths
of discriminative (e.g., logistic regression) vs gen-
erative (e.g.,
all
others here)
classifiers:
given
a large amount
of training text,
asymptotic effi-
ciency of logistic regression will start to work in
its favor over the finite sample advantages of a
generative classifier (Ng and Jordan, 2002; Taddy,
2013c).
However,
the comparison is also unfair
to Word2Vec and Doc2Vec:
both phrase regres-
sion and MNIR are optimized exactly under AICc
selected penalty,
while Word and Doc 2Vec have
only been approximately optimized under a sin-
gle specification.
The distributed representations
should improve with some careful engineering.
Word2Vec inversion outperforms the other doc-
ument
representation-based alternatives (except,
by a narrow margin,
MNIR in task
a
).
Doc2Vec
under
DBOW specification and MNIR both do
worse,
but
not
by a large margin.
In contrast
to
Le and Mikolov,
we find here that
the Doc2Vec
DM model
does much worse than DBOW.
Re-
gression onto simple within- document
aggrega-
tions of Word2Vec perform slightly better than any
Doc2Vec option (but not as well as the Word2Vec
inversion).
This again contrasts the results of Le
and Mikolov and we suspect that the more com-
plex Doc2Vec model would benefit from a careful
tuning of the SGD optimization routine.
3
Looking at the fitted probabilities in detail we
see that Word2Vec inversion provides a more use-
ful
document
ranking than any comparator
(in-
cluding phrase regression).
For example,
Figure
2 shows the probabilities of a review being ‘pos-
itive’ in task
a
as a function of the true star rat-
ing for each validation review.
Although phrase
regression does slightly better in terms of misclas-
sification rate, it does so at the cost of classifying
many terrible (1 star) reviews as positive. This oc-
curs because 1-2 star reviews are more rare than 3-
5 star reviews and because words of emphasis (e.g.
very
,
completely
,
and
!!!
)
are used both
in very bad and in very good reviews.
Word2Vec
inversion is the only method that yields positive-
document probabilities that are clearly increasing
in distribution with the true star rating. It is not dif-
ficult to envision a misclassification cost structure
that favors such nicely ordered probabilities.
4
Discussion
The goal of this note is to point out inversion as an
option for turning distributed language representa-
tions into classification rules.
We are not arguing
3
Note also that
the unsupervised document
representa-
tions – Doc2Vec or the single Word2Vec used in Word2Vec
aggregation – could be trained on larger unlabeled corpora. A
similar option is available for Word2Vec inversion: one could
take a single Word2Vec model trained on a large unlabeled
corpora as a shared baseline (prior) and update separate mod-
els with additional training on each labeled sub-corpora. The
representations will all be shrunk towards a baseline language
model,
but will differ according to distinctions between the
language in each labeled sub-corpora.
for the supremacy of Word2Vec inversion in par-
ticular, and the approach should work well with al-
ternative representations (e.g., Glove).
Moreover,
we are not even arguing that it will always outper-
form purpose-built classification tools.
However,
it is a simple, scalable, interpretable, and effective
option for classification whenever you are working
with such distributed representations.
References
[Besag1974]
Julian Besag.
1974.
Spatial
interaction
and the statistical analysis of lattice systems.
Jour-
nal of the Royal Statistical Society, Series B.
[Besag1975]
Julian Besag.
1975.
Statistical analysis of
non-lattice data.
The Statistician, pages 179–195.
[Flynn et al.2013]
Cheryl Flynn, Clifford Hurvich, and
Jefferey Simonoff.
2013.
Efficiency for Regular-
ization Parameter Selection in Penalized Likelihood
Estimation of Misspecified Models.
Journal of the
American Statistical Association, 108:1031–1043.
[Jernite et al.2015]
Yacine Jernite, Alexander Rush, and
David Sontag.
2015.
A fast
variational
approach
for learning Markov random field language models.
In Proceedings of the 32nd International Conference
on Machine Learning (ICML 2015).
[Le and Mikolov2014]
Quoc V. Le and Tomas Mikolov.
2014.
Distributed representations of sentences and
documents.
In Proceedings of
the 31 st
Interna-
tional Conference on Machine Learning.
[Mikolov et al.2013a]
Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean.
2013a.
Efficient estima-
tion of word representations in vector space.
arXiv
preprint arXiv:1301.3781.
[Mikolov et al.2013b]
Tomas Mikolov,
Ilya Sutskever,
Kai Chen, Greg S. Corrado, and Jeff Dean.
2013b.
Distributed representations
of
words
and phrases
and their compositionality.
In Advances in Neural
Information Processing Systems, pages 3111–3119.
[Molenberghs and Verbeke2006]
Geert
Molenberghs
and Geert
Verbeke.
2006.
Models for discrete
longitudinal
data.
Springer
Science & Business
Media.
[Morin and Bengio2005]
Frederic Morin and Yoshua
Bengio.
2005.
Hierarchical probabilistic neural net-
work language model.
In Proceedings of
the In-
ternational Workshop on Artificial Intelligence and
Statistics, pages 246–252.
[Ng and Jordan2002]
Andrew Y. Ng and Michael I. Jor-
dan.
2002.
On Discriminative vs Generative Clas-
sifiers:
A Comparison of Logistic Regression and
naive Bayes.
In Advances in Neural
Information
Processing Systems (NIPS).
[Pennington et al.2014]
Jeffrey
Pennington,
Richard
Socher, and Christopher D. Manning.
2014.
Glove:
Global vectors for word representation.
Proceedings
of the Empiricial Methods in Natural Language Pro-
cessing (EMNLP 2014), 12.
[Rehurek and Sojka2010]
Radim Rehurek and Petr So-
jka.
2010.
Software Framework for Topic Mod-
elling with Large Corpora.
In Proceedings of
the
LREC 2010 Workshop on New Challenges for NLP
Frameworks, pages 45–50.
[Rumelhart et al.1986]
David
Rumelhart,
Geoffrey
Hinton,
and Ronald Williams.
1986.
Learning
representations by back-propagating errors.
Nature,
323:533–536.
[Socher et al.2011]
Richard Socher, Cliff C. Lin, Chris
Manning,
and Andrew Y. Ng.
2011.
Parsing natu-
ral scenes and natural language with recursive neural
networks.
In Proceedings of the 28th international
conference on machine learning (ICML-11),
pages
129–136.
[Socher et al.2013]
Richard
Socher,
Alex
Perelygin,
Jean Y. Wu, Jason Chuang, Christopher D. Manning,
Andrew Y. Ng, and Christopher Potts.
2013.
Recur-
sive deep models for semantic compositionality over
a sentiment treebank.
In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.
[Taddy2013a]
Matt
Taddy.
2013a.
Measuring Polit-
ical
Sentiment
on Twitter:
Factor Optimal
Design
for Multinomial Inverse Regression.
Technometrics,
55(4):415–425, November.
[Taddy2013b]
Matt
Taddy.
2013b.
Multinomial
In-
verse Regression for Text Analysis.
Journal of the
American Statistical Association, 108:755–770.
[Taddy2013c]
Matt
Taddy.
2013c.
Rejoinder:
Effi-
ciency and structure in MNIR.
Journal of the Amer-
ican Statistical Association, 108:772–774.
[Taddy2014]
Matt
Taddy.
2014.
One-step estimator
paths for concave regularization.
arXiv:1308.5623.
[Taddy2015]
Matt Taddy.
2015.
Distributed Multino-
mial
Regression.
Annals of
Applied Statistics,
To
appear.
[Varin et al.2011]
Cristiano Varin,
Nancy Reid,
and
David Firth.
2011.
An overview of composite like-
lihood methods.
Statistica Sinica, 21(1):5–42.
