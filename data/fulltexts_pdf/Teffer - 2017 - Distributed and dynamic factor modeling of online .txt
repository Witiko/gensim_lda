Copyright
by
Dean Whitney Teffer
2017
The Dissertation Committee for Dean Whitney Teffer
certifies that this is the approved version of the following dissertation:
Distributed and Dynamic Factor Modeling of Online
Data
Committee:
Joydeep Ghosh, Supervisor
Sarfraz Khurshid
Christine Julien
Sujay Sanghavi
Deepayan Chakrabarti
Distributed and Dynamic Factor Modeling of Online
Data
by
Dean Whitney Teffer, B.S., M.A.
DISSERTATION
Presented to the Faculty of the Graduate School of
The University of Texas at Austin
in Partial Fulfillment
of the Requirements
for the Degree of
DOCTOR OF PHILOSOPHY
THE UNIVERSITY OF TEXAS AT AUSTIN
May 2017
Dedicated to my wife Lena.
Acknowledgments
I first of all wish to thank my advisor,
Professor Joydeep Ghosh,
who
has been a mentor,
and inspiration,
and a guide for me the past few years.
Being an older student, already working full-time at the University, he took a
risk in taking me on as a student.
We have worked on many things together,
but I could never repay him for the many lessons he has taught me; often the
most instructive ones were a single comment during review:
"did you think
about this...?" or "what about if we look at it this way ...
?"
I also wish to thank my committee members, Professors Sarfraz Khur-
shid,
Christine Julien,
and Sujay Sanghavi
from the Department of
Electri-
cal and Computer Engineering and Professor Deepayan Chakrabarti from the
Department of
Information,
Risk,
and Operations Management,
McCombs
School of Business.
To this day, I regularly bring up lessons from Prof.
Kur-
shid’s class on Verification in my job and with my funding program managers
in D.C.
I did not take classes from my other committee members,
but they
had a significant influence on my studies.
Prof.
Julien was Chair of the Soft-
ware Engineering track, and I met with her several times as I was navigating
requirements, especially prior to and during pre-qualification.
Her advice was
invaluable throughout.
Prof.
Sanghavi has a well-earned reputation for rigor,
and his feedback during my qualification helped to push me in a more suc-
v
cessful
direction in my research.
Plus,
he and I are now neighbors,
and I
value his friendship and our conversations whenever we can have them.
Prof.
Chakrabarti, in explaining his seminal research at Facebook, finally helped me
to understand message passing algorithms on large social networks, and some
of the limitations.
Although I was not engaged much with Prof.
Ghosh’s research group,
I did work early on with Dr.
Ayan Acharya,
while he was completing his
dissertation research,
which was on the GPPF model
expanded upon herein.
I learned a great deal about Bayesian models and Monte Carlo inference form
working with Ayan.
And I really enjoyed my time with him.
And,
Ayan,
I
finally bought a MacBook Pro to work on my dissertation!
I also wish to thank
the IDEAL group students who helped me with my qualificaion and defense
prepration, including Jette Henderson and Shalmali Joshi.
Since taking classes and working on my research has been in conjunc-
tion with working full
time at the UT Applied Research Labs,
I cannot fail
to acknowledge my colleagues at the "Lab." In particular my generous and
understanding supervisors over the span of years,
Mark Donnell,
and in par-
ticular Dr.
Cheryl
Martin.
Dr.
Martin strongly encouraged me to complete
my dissertation and gave me sound advice on how to accomplish this while
working.
It was advice I tried to heed, and I am grateful to her for her support.
Her supervisor and our lab Director Karl Fisher encouraged me to return for
my PhD years ago,
and he has supported me along the way.
And the Lab’s
Executive Director,
Clark Penrod,
many times asked for my progress,
often
vi
more probing questions, and encouraged me to keep at it, so at times I felt I
had to finish or disappoint him.
So I thank all
my "overhead" support and
encouragement.
Several
other of my colleagues gave me advice on managing my time,
scoping my work, and evaluating my ideas.
Dr.
Alex Liu (a former student of
Prof.
Ghosh) and Dr.
Ravi Srinivasan were particularly giving in this regard.
Marcus Tyler (another former student of
Prof.
Ghosh) while a student ac-
tively worked on some of the data processing and visualization methods used
herein, and I cannot thank Mr.
Tyler enough for all that work.
Several other
colleagues at the lab provided sounding boards,
advice,
and cheer,
including
in particular Andrew Lowe,
who himself
should try a little research himself
someday.
Also, several members of our research team provided valuable feed-
back as I prepared for my defense, including Juan Diego Rodriguez and Jared
Abrams.
Before leaving the domain of
work,
I must thank Dr.
Sukarno Mer-
toguno,
of
the Office of
Naval
Research.
I have worked with "Karno" and
his colleagues and ONR for many years,
and my first grant from him pro-
vided initial funding to work on the GPPF family of models described herein.
Although the bulk of my research was performed outside the context of this
grant,
Karno’s grant got it started,
and I appreciate his and ONR’s support
over the years.
Turning to more personal
influences,
my parents suppoted my educa-
tion both formal and otherwise.
I still recall teaching myself in the early ’80s to
vii
program games in BASIC and Assembly Langauage on a (Texas Instruments)
TI-99/4A with a whopping 16KB of memory and a cassette-deck "drive." This
probably cost more than a month of their income at the time.
My parents had
to go straight to work after high school, and were unable pursue various inter-
ests or goals of theirs due to their desire to make a better life for themselves
and their children.
I have always been inspired by their hard work,
determi-
nation, and ability to envision something better for themselves and to achieve
it.
My litle sister,
Dr.
Kate Teffer,
was the first in the family (extended,
I
believe) to obtain a PhD.
Well,
I am just a few years behind.
Your success
has been an inspiration for me, Sister.
I reserve my highest gratitude for my wife, Lena, who has supported me
throughout.
Since we married, I have both founded a startup and undertaken
a PhD while working, so she is clearly a tolerant person.
She helps to keep me
focused on my and our goals,
and more than that,
really makes it all
worth
while.
I love her dearly, and I am eternally grateful for her perseverance and
support.
And finally,
our two boys,
Aubrey and Everett,
with whom I now
look forward to spending more time!
. . .
viii
Distributed and Dynamic Factor Modeling of Online
Data
Publication No.
Dean Whitney Teffer, Ph.D.
The University of Texas at Austin, 2017
Supervisor:
Joydeep Ghosh
The domain of data mining and machine learning has expanded rapidly
in recent years to include both large-scale distributed and streaming compu-
tation.
Although many open-source and cloud-based frameworks are available
for these tasks,
many of
which are used in-production by industry,
this is
a rapidly-evolving technology landscape,
and the gap between the academic
role of algorithm development and discovery and code available for use with
real-world data has grown.
In addition,
although there is a rich history of
mathematical
models for streaming data on continuous vector spaces,
there
has been significantly less work on streaming discrete spaces.
However, much
if not most of the data available online is composed of high-dimensional sparse
counts, such as text corpora and interaction networks.
We attempt to help bridge this gap by extending promising Bayesian
Poisson factorization and co-factorization models that can be used,
for ex-
ix
ample,
to model not only text corpora but also related user interactions in a
social network.
We construct a dependent process prior that enables dynamic
latent factor modeling in the natural
probability space of the factors,
rather
than in the raw data.
These models are then scaled to and implemented for
distributed compute systems and streaming data.
We develop an adaptive hashing method (AdaHash) for lambda archi-
tectures that can use latent factors calculated during periodic batch mode
updates as a similarity metric for hierarchical grouping, or for finding similar
factors to reconcile parameters in a distributed compute scenario.
In addition,
we develop a novel Hidden Markov variant using particle filters to update prior
factors and probabilistically group with new factors in a dynamic inference
model (D-GaPS).
We show experimentally that the distributed model converges to simi-
lar factors as single-process inference, and the dynamic model yields superior
quality topics over batch mode alternatives.
Empirical
studies are presented
on the use of a U.S.
Senate voting and bill
summary data set that is readily
interpretable with regard to latent factors.
x
Table of Contents
Acknowledgments
v
Abstract
ix
List of Tables
xv
List of Figures
xvii
Chapter 1.
Introduction
1
1.1
Modeling Count Data:
Text Corpora and Association Networks
3
1.2
Processing Streaming Data on Distributed Compute Nodes with
Adaptive Hashing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
1.3
Modeling Dynamic Data with Dependent Process Priors and
Adaptive Hashing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
1.4
Research Contributions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
Chapter 2.
Related Work
13
2.1
Topic Models
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
2.1.1
Joint Poisson Factorization and Topic Modeling .
.
.
.
.
16
2.2
Dynamic Data Modeling
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19
2.2.1
Linear State Models
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
2.2.2
Gaussian Mixture Model Probability Hypothesis Density
Filter
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
2.2.3
Temporal Topic Models
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
23
2.3
Distributed Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
2.4
Mapreduce and Apache Spark
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
26
xi
Chapter 3.
Temporal Distributed Learning
32
3.1
Motivation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
3.1.1
Contributions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
34
3.2
Background
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
35
3.2.1
Data Imputation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
36
3.3
Temporal Distributed Learning .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
39
3.3.1
Recursive Bayesian Estimation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
39
3.3.2
Combining Heterogeneous Data .
.
.
.
.
.
.
.
.
.
.
.
.
.
40
3.3.3
Tracking Mixture Components
.
.
.
.
.
.
.
.
.
.
.
.
.
.
41
3.3.4
Implementation Notes
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
3.4
Experiments
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
3.4.1
Datasets
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
3.4.1.1
Simulated Dataset 1 – Overlapping Components,
No Missing Data,
Three Sources,
Three Initial
Clusters, Non-Stationary .
.
.
.
.
.
.
.
.
.
.
.
.
43
3.4.2
Quantitative Metrics .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
3.4.3
Temporal Distributed Learning Results
.
.
.
.
.
.
.
.
.
46
Chapter 4.
Network Discovery via Joint Nework and Topic Mod-
eling
47
4.1
Motivation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
47
4.2
Background
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
49
4.2.1
Negative Binomial Distribution .
.
.
.
.
.
.
.
.
.
.
.
.
.
49
4.2.2
Gamma Process
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50
4.3
Joint Gamma Process Poisson
Factorization (J-GPPF) .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
50
4.3.1
Inference via Gibbs Sampling .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
55
4.3.2
Gibbs Sampling for J-GPPF with Missing Entries .
.
.
.
58
4.3.3
Special cases:
Network Only GPPF (N-GPPF) and Cor-
pus Only GPPF (C-GPPF)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
58
4.3.4
Computation Complexity
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
59
4.4
Related Work
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60
4.5
Experimental Results
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
62
4.5.1
NIPS Authorship Network .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
62
xii
4.5.2
GoodReads Data .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63
4.5.3
Experimental Setup and Results
.
.
.
.
.
.
.
.
.
.
.
.
.
64
Chapter 5.
Joint Poisson Factorization Case Study:
Topic Mod-
eling of U.S. Senate Records
66
5.0.1
Related Work .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
66
5.1
Joint Model Description
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70
5.1.1
Notation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70
5.1.2
Definition .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71
5.1.3
Interpretation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
72
5.1.4
Special Cases .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
73
5.2
Data Description
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
73
5.2.1
Source .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
73
5.2.2
Preprocessing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
73
5.3
Experiments
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
75
5.3.1
The Full Corpus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
75
5.3.1.1
Overview of Network Factors
.
.
.
.
.
.
.
.
.
.
75
5.3.1.2
Ranking of Senators
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
77
5.3.1.3
Comparison with N-GPPF .
.
.
.
.
.
.
.
.
.
.
.
81
5.3.2
The Two-Year Subset
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
82
5.3.2.1
Analysis of the Third Network Factor .
.
.
.
.
.
83
5.3.2.2
Interpretation of
Document Factors and Com-
parison with C-GPPF
.
.
.
.
.
.
.
.
.
.
.
.
.
.
85
5.3.2.3
Evaluation of Topics by Group
.
.
.
.
.
.
.
.
.
85
5.4
Discussion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
90
Chapter 6.
Distributed Adaptive Hierarchical Clustering for Stream-
ing Data
92
6.1
The Proposed Adaptive Grouping Method for Streaming Data
93
6.1.1
Motivation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
93
6.1.2
Problem Setting and Online Data Characterization .
.
.
97
6.1.3
Equivalence Hashing .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
99
6.1.4
Adaptive Sequential Hashing
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
102
6.1.5
Hierarchical Grouping .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
104
xiii
6.1.6
Streaming Map-Reduce Implementation .
.
.
.
.
.
.
.
.
107
6.2
Data and Metrics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
112
6.3
Discussion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
114
Chapter 7.
Distributed and Dynamic Joint Topic-Community
Model
Inference via Factor Matching and Parame-
ter Reconciliation
119
7.1
Related Work
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
122
7.1.1
Hashing for Distributed Computing
.
.
.
.
.
.
.
.
.
.
.
123
7.2
Distributed Joint Poisson Factorization .
.
.
.
.
.
.
.
.
.
.
.
.
125
7.3
Dynamic and Distributed Joint Poisson Factorization
.
.
.
.
.
130
7.4
Experiments
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
133
7.4.1
Scalability
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
133
7.4.2
Single Session .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
135
7.4.3
Dynamic Data - Two Consecutive Sessions .
.
.
.
.
.
.
.
139
7.5
Discussion
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
141
Chapter 8.
Conclusion
143
8.1
Future Research .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
143
8.1.1
Production Implementation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
143
8.1.2
Adaptive Variance / Kalman Filter "Gate"
.
.
.
.
.
.
.
144
8.2
Dissertation Summary
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
144
Appendices
146
Appendix A.
Detailed Dynamic Topic Data
147
Appendix B.
Calculations
159
B.1 Factor Quality Using Entropy
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
159
Index
161
Bibliography
162
Vita
176
xiv
List of Tables
2.1
LDA Model Parameters
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
3.1
Data Augmentation for GMM with Prior .
.
.
.
.
.
.
.
.
.
.
.
38
3.2
Distributed Learning with Data Augmentation .
.
.
.
.
.
.
.
.
39
3.3
Modified GMM for PHD .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
3.4
Temporal Metric Comparison for Models
.
.
.
.
.
.
.
.
.
.
.
.
46
5.1
Data notation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71
5.2
Model parameter notation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71
5.3
J-GPPF Republican group rankings .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
78
5.4
J-GPPF Democrat group rankings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
79
5.5
Top members from group 16, the 108th Senate, by lift (high to
low)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
84
5.6
LDA Topics (108th Senate)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
86
5.7
Top C-GPPF Topics (108th Senate) .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
87
5.8
Top J-GPPF Topics (108th Senate)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
88
5.9
Top topics for Republicans (group 3, 108th Senate)
.
.
.
.
.
.
89
5.10 Top topics for Moderates (group 16, 108th Senate)
.
.
.
.
.
.
89
5.11 Top topics for Democrats (group 14, 108th Senate)
.
.
.
.
.
.
90
6.1
Typical online data types .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
99
6.2
Typical online data types .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
100
6.3
Identified groups by algorithm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
114
7.1
Runtime Scaling for Experiments
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
135
7.2
Topics for Single Session, 1 Node
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
136
7.3
Reduced Topics for Single Session, 2 Nodes .
.
.
.
.
.
.
.
.
.
.
137
7.4
Topics for Dual Sessions, 1 Node
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
141
A.1
Topics for Dual Sessions, Processed as Individual Years
.
.
.
.
152
xv
A.2
Topics for Dual Sessions, Processed with Dynamic Dependency
158
xvi
List of Figures
1.1
Lambda Architecture high-level view from [7]
.
.
.
.
.
.
.
.
.
6
2.1
Latent Semanting Indexing (LSI)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
2.2
Latent Dirichlet Allocation (LDA) plate model
.
.
.
.
.
.
.
.
.
16
2.3
Joint model (JGPPF) matrix inputs
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
2.4
Joint model (JGPPF) factor outputs
.
.
.
.
.
.
.
.
.
.
.
.
.
.
19
2.5
Hidden Markov Model (HMM) plate model .
.
.
.
.
.
.
.
.
.
.
21
2.6
Dyamic Latent Dirichlet Allocation (Topic) Model (dLDA) plate
model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
2.7
Mapreduce steps from [29]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
3.1
Gaussian Mixture Model (GMM) plate model
.
.
.
.
.
.
.
.
.
37
3.2
Simulated dataset (union of local data) at t=0 with correspond-
ing global parameter fit using the Distributed model .
.
.
.
.
.
44
3.3
3 Out-of-Sample Log-Likelihood for Compared Models (Central
– Blue, Distributed – Green, Distributed Temporal – Red .
.
.
45
4.1
Gibbs sampling updates in J-GPPF .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
51
4.2
Gibbs sampling for J-GPPF network components with missing
entries
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
53
4.3
Gibbs sampling for J-GPPF topic components with missing entries
55
4.4
Generative Process of N-GPPF
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
57
4.5
Generative Process of C-GPPF
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
57
4.6
AUC NIPS .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63
4.7
AUC GoodReads
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63
4.8
MAP NIPS
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63
4.9
MAP GoodReads
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
63
5.1
Voting network (B) for the 108th Senate
.
.
.
.
.
.
.
.
.
.
.
.
74
5.2
Example network factors (φ) for J-GPPF .
.
.
.
.
.
.
.
.
.
.
.
76
xvii
5.3
Example network factors (φ) for N-GPPF
.
.
.
.
.
.
.
.
.
.
.
81
5.4
Network factor strength (ρ) and factors (φ) for J-GPPF (108th
Senate) .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
83
6.1
3-Tier Database Architecture with seperate storage and processing 96
6.2
Hierarchical grouping with cuts defining a partial ordering
.
.
105
6.3
Streaming Mapreduce using state vectors .
.
.
.
.
.
.
.
.
.
.
.
111
6.4
Performance Results for Streaming Scaling with Trendline
.
.
113
6.5
Example hierarchical group formed by similarity hashing.
The
Googlebot are more similar (same UserAgent,
similar IP) and
therefore grouped together. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
115
7.1
Sparse binary hash process for latent factors (see text)
.
.
.
.
124
7.2
HMM model for dynamic factors
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
131
7.3
Dynamic model inference algorithm in sequential mapreduce
.
133
7.4
Topic merging example from experiments in Sec 7.4.3 .
.
.
.
.
138
xviii
Chapter 1
Introduction
The practices of
data mining and machine learning have undergone
both rapid adoption and expansion in recent years largely due to two congru-
ent trends:
1) the increasing instrumentation of computing systems, including
traditional systems such as servers and applications, as well as newer devices
such as the Internet of
Things (IoT);
and 2) the increasing adoption of
dis-
tributed computing, particularly cloud computing, or computing-on-demand.
One has necessitated the other in a virtuous cycle.
It is important to note that increasing data means both data volumes
and data rates.
Whereas in the past,
a new algorithm could claim relevance
by examining a dataset of hundreds of elements, now adoption in industry re-
quires scaling to millions or billions or more of elements.
In addition, whereas
traditional learning methods have focused on batch mode processing, modern
data rates often require streaming processing in which only small
data win-
dows, at best, can be retained in memory.
Therefore, algorithms that can be
used at scale must be capable of capturing state or latent factors in the data
and to associate these factors over time as new data arrives.
Fortunately,
there are a number of
classic algorithms such as Latent
1
Dirichlet Allocation (LDA) [19], and k-means that are available in streaming
form and are available in standard distributed computing frameworks.
Un-
fortunately, many of these classic methods do not accommodate concept drift
or other forms of factor non-stationarity,
nor do they allow for adapting the
algorithms in situ as new data arrives.
More fundamentally, much of the data
available online is in the form of text documents and other natural
language
or system and event logs.
This type of data is naturally expressed as sparse
count records.
In addition, much of this data is multi-relational.
For example,
text documents are often available in conjunction with authorship or access
user data, for which separate user social network or demographic data is also
available.
Few models exist to model
data with multiple relations or even as
sparse counts.
I have worked for several
years with other researchers on a Poisson
factorization model
with several
variants for modeling network associations,
document topics, and a joint model of both [10], [9], [11].
In this dissertation
I extended and adapted the joint community-topic model for use in modeling
real-world data with both text and user interaction components.
In addition,
I use this model
as a basis for exploring methods for performing distributed
inference and dynamic inference for larger data corpora and streaming corpora.
I began by conducting a deep dive into the model with regard to an interesting
and readily-interpretable data set, which I scraped from the U.S. Senate web
site.
I was required to make improvements to the joint model
and develop
tools which would be useful in the subsequent scalability efforts.
2
1.1
Modeling Count Data:
Text Corpora and Associa-
tion Networks
After conducting an extensive literature review and running multiple
LDA-based models for incorporating additional information such as authorship
networks and side information (See the brief review in Section 4.4), we decided
to work on a new method of
modeling topics,
association networks,
and a
joint model
of
both based on a Poisson factoriation model.
Early work to
develop these models was published in [10],
[9],
[11].
A driving motivation
was to ultimately be able to scale the model
to large data.
The model
had
the attractive quality of
scaling with the size of
the sparse elements in the
input matrices, rather than the overal input matrix size.
In addition, work by
Ayan Acharya led to a fast MCMC inference method.
However,
the MCMC
inference was still limited to a single compute node.
Past work on the various Poisson factorization models,
which are re-
ferred to by acronym as:
C-GPPF (corpus-gamma process Poisson factoriza-
tion), N-GPPF (network-gamma process Poisson factorization), and J-GPPF
(joint-gamma process Poisson factorization),
often employed simulated data
or relatively small
and corpora with uncertain labeling.
In order to continue
work with these models,
particularly the joint model,
J-GPPF,
I selected a
new data set with more obviously interpretable groups and topics:
U.S. Sen-
ate voting and bill summary copora over the years 2002 - 2014.
This corpora is
divided into sessions which correspond to two-year periods of operation of the
U.S. Congress.
As the J-GPPF model is not inherently dynamic, I considered
3
sessions separately in sequence.
One early realization was that the baseline J-GPPF model
was insuf-
ficient to model
this data.
The voting counts did not map easily to a binary
association model,
so I modified the network component of the model
to use
raw counts.
For the bills, I used bill summaries rather than the entire bill text.
Bills were associated with the social network via various means during testing
including authorship and voting.
The inferred factors (topics and communities, or groups) were compared
with LDA and Mixed-membership stochastic blockmodel
(MMSB) [14],
with
significant improvement using the joint model.
In order to more fully explore
the results of the joint model, I developed a metric called in-group lift
which
measures the centrality of particular user relative to a group,
conditioned on
the joint model.
The results reported in Section 5.3 show that this metric is
particularly useful in predicting which U.S. Senators are or will make a run for
U.S.
President for the political
party most closely aligned with their highest
in-group rating.
1.2
Processing Streaming Data on Distributed Compute
Nodes with Adaptive Hashing
One drawback of the J-GPPF model
is lengthy runtimes,
in part due
to the use of
MCMC inference,
which is inherently single-process.
In order
to expand the utility of this useful
model
and increase scalability,
I explored
various means for distributing computation over multiple compute nodes.
As
4
described in detail in Section 2.4, much of industry has moved to Mapreduce-
like frameworks for distributed computing, and in-memory computation using
Apache Spark [4].
Therefore,
I sought to use this framework for distributing
computation on count-based data.
As a first step, since Mapreduce proceeds in non-iterative steps, I had
to develop a method for grouping arbitrary data elements in streaming mode
based on a key assigned to those values.
During modeling, this key would be
the latent factor,
but I sought a more generic method.
I adopted the lambda
architecture which is also widely used in industry for processing streaming
data.
This architecture supports periodic batch mode Mapreduce processing in
order to parameterize streaming mode processing, which can then run until the
next batch process in (soft) real-time,
without requiring re-parameterization
or knowledge of other data in the stream (See Figure 1.1).
Our method provided a mechanism for adaptive hash function (map-
ping) definition during the batch process,
so that the hash assigned during
streaming could be used as a similarity metric during the reduce phase.
Al-
though intended to support factor similarity discovery,
it could also be used
for defining a hierarchy on the data via the grouping method, which would be
useful in many contexts.
In order to run J-GPPD on multiple compute nodes,
the document
corpus would be split over multiple nodes,
resulting in multiple inferences
for the network groups, one from each node, and some multiple inferences for
topics, some singleton topics inferences, depending on document mixing during
5
Figure 1.1:
Lambda Architecture high-level view from [7]
partitioning and the prevalence of topics across the corpus.
During reduction,
similar hashes,
or factors must be combined into a single process,
and this is
where the similarity hashing becomes useful.
Once all similar factors (topics,
groups) are collected, the parameters are reconciled into a single inference for
the factor as the final output for the distributed computation.
Since the J-GPPF inference scales linearly with the number of entries
(non-zero input elements), running the computation on distributed nodes effec-
tively scales the runtime by the number of compute nodes.
I call the adaptive
hashing method used AdaHash.
6
1.3
Modeling Dynamic Data with Dependent
Process
Priors and Adaptive Hashing
As I modeled larger corpora, I realized that topic drift rendered topics
derived from many years of
U.S.
Senate data to be not the expected topics
from a knowledge of events from the time.
Running the model
on year-long
segments yielded more granular results, but failed to capture the dependency in
those topics, such as health care reform or the Middle East wars that extended
over many years.
I leveraged the AdaHash method to develop a dynamic J-GPPF model,
using posterior inference.
In prior AdaHash experiments,
although the hash
function can change,
the groups are stationary.
For dynamic joint modeling,
I wanted to enable dynamic hash,
or factor,
evolution.
Other models have
proposed this, such as Dynamic Topic Modeling [18] using a Gaussian process
to evolve a normalized form of
model
variables.
Here,
I employ a Hidden
Markov Model (HMM) in which at each new data arrival or time step, I predict
new factors from posterior inferences from the last cycle, then update factors
for the current cycle using the factor reconciliation method.
A particle filter is used to update last-cycle factors using either a depen-
dent Gamma or dependent Dirichlet distribution,
depending on which factor
is being updated.
I call
this method D-GaPS,
for Dynamic Gamma Poisson
State modeling, and extensive details are provided in Chapter 4.
7
1.4
Research Contributions
This dissertation addresses scaling Bayesian models of streaming count
data,
in particular a joint community-topic Poisson factorization model
(J-
GPPF),
to distributed compute nodes.
Inferring latent factors over multiple
nodes requires parametric reconciliation to a single factor during the final
step of distributed inference.
I developed a general-purpose adaptive hashing
method for identifying similar latent factors to facilitate this process.
While
doing so,
I developed an adaptive and hierarchical
hashing method suitable
for streaming processing on Mapreduce frameworks, which I call AdaHash.
To process streaming data on distributed nodes with J-GPPF, the ul-
timate goal of this dissertation, I implemented a Hidden Markov model where
previous posterior factors were predicted forward to the current iteration via
a particle filter, grouped by AdaHash, and then updated to a single factor by
parametric reconciliation.
In order to verify that the J-GPPF model yields semantically meaning-
ful community and topic joint factors, I assembled a new corpus of twenty-two
years of U.S. Senate voting counts and associated bill summary text from the
U.S.
Congressional
web site at [70],
which publishes such records,
although
not in a suitable form for immediate modeling.
The model
was confirmed to
produce semantically meaningful factors, and this corpus was used to develop
and evaluate extensions of the model for distributed and streaming / dynamic
inference.
8
The following is a selected, but not comprehensive, list of contributions
of this dissertation:
• Joint Poisson Factorization Community-Topic Model
– I extend the J-GPPF model
to handle cases in which the network
associations are expressed as interaction counts, rather than binary
interactions.
This allows the model to be applied to a wider variety
of
data,
as many types of
interactions are non-binary or do not
admit to a thresholding or mapping to binary.
– I show how the J-GPPF model
can be used to infer qualitatively
more topics than non-joint models,
by incorporating the network
association side information.
– Finally,
I show a number of interesting artifacts generated by the
model for individuals, including one based on a novel metric called
in-group lift.
These indicators are absent in other baseline models
that were run on the same data.
• Adaptive Hierarchical Clustering for Online Data in Mapreduce
– I develop a hash function mechanism enabling use-defined or application-
defined similarity metric for grouping data on distributed comput-
ing clusters,
on which the hash becomes the key employed in a
Mapreduce scheme, such as Locality Sensitive Hashing (LSH).
9
– For some special
cases in streaming log data,
a hash is sometimes
provided,
or
the grouping method required in initial
processing
is straightforward,
and in these cases,
I introduce the concept of
Equivalency hashing with efficiency similar to locality sensitive hash-
ing (LSH), or linear in the amount of data and number of groups.
– In a streaming data context, as mode data is received, the number
of groups in the observed data is a priori
unknown.
However, with
accumulated data, an increasing number of groups may be observed.
In addition, structure in the data may become apparent.
I modify
our similarity hashing method to be adaptive on a periodic basis
via batch processing so that a similarity metric on the hash defines
a partial
ordering over the data.
This mechanism of
employing
periodic calculations to re-set parameters for streaming processes is
known as the lambda architecture and is widely used in industry.
– The use of an adaptive similarity hash (AdaHash) is equivalent to
each reducer in the Mapreduce streaming process collecting data
under a hierarchy of hash values, for each data cycle, or time iter-
ation.
This is a key mechanism for state-ful streaming processing.
• Distributed and Dynamic Joint Topic-Community Model Inference
– I investigated the convergence properties of the MCMC inference of
the J-GPPF model under various types of data poverty in order to
determine the best method to partition data over multiple compute
10
nodes to parallelize computation.
The result was a method for dis-
tributed inference for joint Poisson factorization of topic-community
data, using parameter reconciliation.
Parameter reconciliation is re-
quired to combine factor estimates for the same factor calculated at
multiple nodes,
such as a topic or group.
Since the index location
of a topic is undetermined, similar topics or groups must be deter-
mined by a similarity metric.
I opted to use a method similar to
the similarity hashing method developed for hierarchical grouping.
Sets of similar factors are then reconciled based on the appropriate
distribution model.
– Since I could run large corpora on distributed compute clusters,
I
did,
and I
realized that combining many years of
the U.S.
Sen-
ate data yielded poor topics.
Running years separately yielded
better results,
but there was no continuity over time.
So I
de-
veloped a fully-dynamic model
of J-GPPF that can run on distri-
bution Mapreduce nodes.
The method takes advantage of the hash-
reduction mechanism invented for adaptive streaming data.
This
hash can be matched with new data.
However, an allowance must
be made for topic or group evolution.
Thus I developed a Hidden
Markov Model
(HMM) extension in which previous posterior fac-
tors are used to predict new states, or factors, using the distributive
prior of the factor.
The new data is then used to form the update.
This is reminiscent of the Kalman filter, and in fact a particle filter
11
is used for posterior prediction and update.
– Apache PySpark software was developed for both the distributed
and dynamic methods, and based on feedback from colleagues in the
Spark community,
I would like to open source this work,
once the
main J-GPPF MCMC methods have been ported to Scala,
which
is underway.
12
Chapter 2
Related Work
Two essential attributes of online data are the distributed, or multiple-
source,
and the temporal,
or time-varying,
aspects of
the data.
There are
several
variants of
single batch topic models that attempt to accommodate
both temporally changing text and distributed corpora [1],
[2],
[3],
[4].
How-
ever, the variability of online data corpora is often large compared to curated
datasets, such as single academic conferences, while the models developed us-
ing such data are suited to a correspondingly narrow range of temporal
and
“spatial” variations.
Most if not all latent variable and topic models assume that all the ob-
servations to be used to fit model parameters will be available if not in a single
batch then at a single location.
However,
there are use cases in which this is
not the case.
One use case that has received attention from researchers is the
case in which data owners wish to preserve the privacy of data beyond simple
ID anonymization,
[5].
This work has led to investigations into distributed
learning in which model parameters rather than data elements are forwarded
to a central learner.
However, this research uses a limited definition of data pri-
vacy, an element level definition, and does not consider other forms of privacy.
13
In addition,
another use case that may require distributed learning and has
not been fully addressed is the case of data observation rates that overwhelm
data transport and / or data storage and processing capacity of
the central
learner.
Such scenarios are becoming more pressing with the large growth in
mobile platforms, dynamically-generated content, and the so-called Internet of
Things.
As an extension beyond batch data, temporal models are much more
common than distributed models,
however,
temporal
models typically allow
for relatively small changes over the “life” or run of the model.
[2], [6] This may
be satisfactory for bounded studies or curated datasets, but processing online
data requires long-run use on data that may evolve substantially from the ini-
tial
data conditions.
For example,
with topic models,
the number of
topics
may change,
the topic words and word probabilities may change,
and topics
may appear and disappear in the data,
as well
as split and merge.
Existing
temporal
topic models can accommodate changes in topic-word probabilities
and topic mixtures, but none of the other dynamics.
2.1
Topic Models
Latent Semantic Indexing, LSI, [7] and Figure 2.1, and Latent Dirichlet
Allocation,
LDA,
[8]
are two of
the most well-know and widely used topic
models.
In their original
form,
each operated on a single batch,
or corpus,
or text files and yields two outputs:
clusters of words into topics, and mixed-
membership topic assignment of the original documents.
LSI is a descriptive
matrix model,
which factors a matrix of
words by documents into a topic
14
Figure 2.1:
Latent Semanting Indexing (LSI)
matrix of
topics by words,
a document matrix of
documents by topics,
and
a singular value matrix of
topic strength.
A variety of
matrix factorization
methods can be used to solve for topics and documents given a corpus.
LDA is a generative Bayesian model
in which follows the following
process:
for all d ∈ D do
choose N
d
∼ P ossion(ζ)
choose θ ∼ Dir(α)
for all n ∈ N
d
do
choose topic z
n
∼ M ultinomial(θ)
choose word w
n
∼ M ultinomail (β
z
n
)
end for
end for
Both variational EM and Markov Chain Monte Carlo, MCMC, methods
are can be to infer model
parameters given a corpus.
The variational
EM
approach presented in the original LDA paper is as follows:
Inference Method:
Variational EM
15
Figure 2.2:
Latent Dirichlet Allocation (LDA) plate model
• E-step:
find optimizing values of variational parameters
• M-step:
maximize lower bound on α,
β
• Requires careful seeding of p (w
n
|β
z
n
) to avoid settling in local minima
Table 2.1 lists the parameters in the LDA model.
2.1.1
Joint Poisson Factorization and Topic Modeling
In [11]
a novel
model
was presented for modeling corpora of
sparse
count data expressed as an interaction matrix, an item-attribute matrix, and
a mapping matrix.
Although this may sound at first pass as an unusual con-
struction, this pattern is in fact widely encountered in online data, such as in
[11]
and [12]
in which data from conference authorship and online book re-
views and recommendation were modeled.
In this work I model data for U.S.
Senate voting and associated bill summaries.
Figure 2.3 shows the input data
model.
B is an association matrix of
N users or actors,
where the associa-
tions can be binary or count;
Y is a matrix consisting of D item rows of V
columns of sparse attribute or feature counts.
Z is a (binary) mapping from
16
Parameter
Description
Corpus, D, a priori parameters
M
number of documents
ζ
word number prior
K
number of topics
Corpus, D , latent variables
α
topic distribution prior (e.g.
symmetric, asymmetric)
β
sparse word âĂŞ topic weight matrix
Document, d, latent variables
N
number of words in document
θ
topic distribution in document
Word, w
n
, latent variables
z
n
topic index
w
n
word, observed variable
Table 2.1:
LDA Model Parameters
users to items.
In the case in which Y represents rows of word counts for doc-
uments and B represents a social network among authors, Z could represent
authorship.
Both [11]
and [12]
describe the model
and inference calculations in
detail.
Figure 2.3 provides a summary of the output factors, which are useful
for this work.
Since the model
will
be run on multiple compute nodes,
a set
of factors will be produces on each node, for each input set:
B, Y , Z.
Thus,
going forward will be referring to the objects as factor sets instead of matrices.
Each factor set is a collection of factor vectors.
Figure 2.3 shows that the association matrix B is factored into {φ}, a
set of
user groups.
The item matrix Y is factored into two pairs of
factors.
Using the vernacular of topic modeling,
one pair is the familiar topic words,
17
Figure 2.3:
Joint model (JGPPF) matrix inputs
18
Figure 2.4:
Joint model (JGPPF) factor outputs
{β} and document topic mixtures {θ}.
The other factor uses the user mapping
matrix Z to relate user groups ({φ}) to documents (Y ) via the factor {ψ},
which represents word probabilities for groups.
In the topic modeling context,
we call {ψ} the jargon factor to relate it to the {β} topic factor.
2.2
Dynamic Data Modeling
If
we make the simple assumption that at each time step,
the state
of the system is represented by x
n
and this state is conditioned only on the
19
immediately prior state, x
n−1
, such that:
p (x
n
|x
1
, ..., x
n−1
) = p (x
n
|x
n−1
)
(2.1)
Then we have a Markov model
of
temporal
evolution of
the system.
Since
we are working with latent variable models,
we assume this is the latent,
or
un-observed state,
and there is an additional
observed state,
z
n
,
at time n,
which is what we actually observe and record.
Thus,
the joint model
of the
hidden Markov model, HMM, is:
p (x
1
, ..., x
n−1
, z
1
, ..., z
n−1
) = p (x
0
)
n
Y
i=1
p (z
i
|x
i
) p (x
i
|x
i−1
)
(2.2)
Much work on temporal
models involves the case in which the con-
ditional
distribution of
the latent variables x
n
is continuous and the above
equation becomes an integral equation.
Most commonly, the distributions are
normal, yielding linear dynamical systems, which are frequently modeled with
the Kalman filter and its Bayesian variants.
2.2.1
Linear State Models
The most common approach to modeling temporal data is with a linear
state space model.
As it is often the case that there are state variables that
cannot be directly observed,
one can model
system dynamics in latent space
and in observed space.
Graphically, the HMM model is shown in Figure 2.5
The hidden state at time n,
x
n
,
is inferred via an update equation
based on the observation at time n,
z
n
,
a model
of
transitions in x from
20
Figure 2.5:
Hidden Markov Model (HMM) plate model
step to step and a model
of
observations
z
n
given state x
n
at
the same
time step:
z
n
|z
n−1
∼ p
z
n
|z
n−1
(z
n
|z
n−1
) ;
state transition equation,
which in-
cludes a noise term,
and is often only normally-distributed noise when there
is no drift or other dynamic forcing in the stochastic behavior of the system
x
n
|z
n
∼ p
x
n
|z
n
(x
n
|z
n
) ; observation equation, which includes a noise term, and
is typically only the normally-distributed noise
Given a previous sequence in which x
n−1
is observed and z
n−1
inferred,
and a new observation x
n
the current state z
n
can be inferred by
p (z
n
|x
n
) =
p (x
n
|z
n
) p (z
n
|x
n−1
)
p (x
n
|x
n−1
)
(2.3)
One method of inferring the hidden state given observations is with the
Kalman filter or one of its variants which relax assumptions for the state up-
date equation.
The update inference can also be derived in an entirely Bayesian
fashion, which yields the particle filter.
Although the Bayesian approach has a
theoretical appeal, it can be challenging to use in practice.
Nonetheless, since
real
state dynamics are rarely strictly linear,
the particle filter can often be
21
preferable.
2.2.2
Gaussian Mixture Model Probability Hypothesis Density Fil-
ter
Linear state space models are useful
for tracking single distributions
or clusters over time, but many real-world data sets contain multiple clusters.
This is compounded by complex dynamics such as clusters that appear after
the initial time step, clusters that disappear, and clusters that merge and split.
Many systems,
such as radar,
employ the Kalman filter on each cluster,
but
use separate,
often heuristic,
mechanisms to manage multiple clusters.
One
method that has been proposed to model
multiple clusters over time is the
Gaussian mixture model probability hypothesis density filter, GMM-PHD, [9],
which provides a probabilistic method to model Gaussian mixture component
birth, death, merge, and split.
The model does require a mixture component
pruning step at each time increment.
The following derivation is taken from [9], in which the usual notion is
used for latent state at the current time, x
k
, and the observed state, z
k
.
Note
the difference in notation from that employed earlier, particularly the possibly
confusing swap of
notation for latent and observed variables.
For multiple
“targets”
or clusters,
capital
letters Z
k
and X
k
are used.
The update for the
multi-cluster state given the observed states is:
p
k|k−1
(X
k
, Z
1:k−1
) =
Z
f
k|k−1
(X
k
|X) p
k−1
(X, Z
1:k−1
) µ
s
(dX)
(2.4)
22
where
p
k
(X
k
, Z
1:k
) ∝ g
k
(Z
k
|X
k
) p
k,k−1
(X
k
, Z
1:k−1
)
(2.5)
2.2.3
Temporal Topic Models
The dynamical LDA model, dLDA, [1] presents a variant of LDA that
can be run on sequential
instances of
a corpus.
In order to accommodate
a sequence of
text documents,
the Dirichlet hyperparameters α and β are
modeled to evolve via a linear state model, given by:
α
t
|α
t−1
∼ N α
t−1
, σ
2
I

(2.6)
β
t,k
|β
t−1,k
∼ N β
t−1,k
, σ
2
I

(2.7)
Note that this is equivalent to the Kalman filter state update model
in which there is only a “noise”
term.
The corresponding graphical
model
is
shown in Figure 2.6
By allowing these hyperparameters to change from time step to time
step,
the dLDA model
allows topic mixtures and word probabilities within
topics to change over time.
However,
more complex topic dynamics cannot
be modeled.
For example, a static vocabulary and topic set is assumed; thus,
dLDA has been shown to track keyword changes in high-level
topics such as
âĂĲcomputer scienceâĂİ in academic conferences but not the emergence of
23
Figure 2.6:
Dyamic Latent Dirichlet Allocation (Topic) Model
(dLDA) plate
model
entirely new topics over time.
This is satisfactory for curated corpora,
such
as single academic conferences,
and slowly evolving corpora with bounded
time frames,
as demonstrated in the literature[6].
A continuous time variant
of dLDA has been described [2]
with similar features and shortcomings,
but
which takes advantage of sparse variational inference for improved performance
despite having a greater number of parameters to infer [10][11].
2.3
Distributed Learning
The Merugu-Ghosh model,
[1]
and [2],
for distributed learning is a
constrained model
integration method for combining data from sources with
possibly non-overlapping feature spaces without requiring all data to be shared.
Due to communication restrictions, efficiency, privacy, or other constraints, a
subset of the local data can only be communicated in terms of the parameters
of a statistical model.
The set of parameters of each local distribution is used
24
to form a global GMM parametric model by minimizing the Kullback-Leibler
divergence (KL) between the global and local models.
Due to the relationship
between KL and the log-likelihood, equation,
1
m
log (p
λ
c
(χ)) = KL p
χ
||p
F
λ
c

+ H (p
χ
)
(2.8)
this can also be done for sufficiently large sampling sizes by minimizing
the log-likelihood of the global model relative to samples drawn from the model
for each local data set.
Input:
Set of models {λ
i
}
n
i=1
with weights {ν
i
}
n
i=1
summing to 1, Para-
metric family G, Global sample size e
m
c
where λ
c
is a global model, p
λ
c
(χ) is the
conditional distribution of the data give the model or the likelihood, F ⊂ F
c
is
the global feature set, χ = {x
j
}
m
j=1
, and p
F
λ
c
is the marginal distribution of p
λ
c
on F .
Output:
λ
a
c
∼
=
arg min
λ
c
∈G
n
P
i=1
ν
i
KL p
λ
i
||p
F
i
λ
c

Method:
1.
Generate e
χ
i
,
[i]
n
1
from the local
models,
λ
i
,[i]
n
1
using MCMC sampling
such that |e
χ
i
| = ν
i
e
m
c
, [i]
n
1
.
2.
Let e
χ
c
=
n
S
i=1
e
χ
i
.
Apply EM algorithm to obtain the optimal
model
λ
a
c
such that λ
a
c
∼
=
arg min
λ
c
∈G
log (p
λ
c
(χ))
25
The derivation of
this model
is presented in detail
in [1]
and [2],.
In
brief, the quality of a global model λ
c
is measured using log-likelihood
Q
DL
= (λ
c
)
n
X
i=1
log (p
λ
c
(χ))
(2.9)
The cost function is convex and can be minimized in lieu of maximizing
the above.
C
KL
(λ
c
) =
n
X
i=1
ν
i
KL p
λ
i
||p
F
i
λ
c

(2.10)
The maximum entropy principle is invoked to produce a unique global model
at the cost function minimum.
This generates the formal
Model
Integration
problem.
max
p
λ
m
H (p
λ
m
) ;
M = arg min
p
λ
m
∈P (F
c
)
C
KL
(p
λ
m
)
(2.11)
where P (F
c
) is the set of all probability distributions over the global feature
set.
In order to simplify the algorithm and to improve model
interpretation,
an additional constraint is applied that the global model belongs to a specified
parametric family G ⊆ P (F
c
), which is taken to be a Gaussian mixture model.
Note that this constrained model
may not have a convex cost function.
The
entropy of
the local
model
is independent of
the global
model
so that min-
imizing the KL divergence with respect to the local
models is equivalent to
maximizing the log-likelihoods of the sampled datasets.
2.4
Mapreduce and Apache Spark
In 2004 Google engineers described a method for the distributed pro-
cessing of
data stored in large-scale clusters of
computing nodes,
using the
26
same computing resources of the data storage nodes [30].
Mapreduce altered
the paradigm of data mining computing by moving algorithms "to the data,"
rather than the traditional
model
of
separating data storage and analytics.
However,
the effort to port algorithms to Mapreduce in-effect limited this
transition.
In 2014 work in The University of
California at Berkeley’s AMPLAb
research group led to the development of
the Resilient Distributed Dataset
(RDD), described in [86], recipient of the ACM Dissertation Award.
An RDD
is an abstraction of a data type in which the details of distribution across both
storage and compute nodes are hidden from the developer.
RDDs formed the
basis of Apache Spark [4], an open source framework now widely used for dis-
tributed computation.
Spark has been widely adopted in software engineering
for production-level systems, with established extensions for data mining and
machine learning [5] and an extensive user-base.
Sophisticated staging meth-
ods have been developed for working with online,
or streaming data,
which
is common in Internet of Things (IoT), logging, and other data-intensive uses
[85].
However, the Academic community for data mining and machine learning
has been generally slow to move away from more traditional
away-from-the-
data development environments,
inhibiting productive collaboration between
researchers with algorithmic know-how and industry with resources and data.
In distributed computing of data mining algorithms, three main frame-
works are employed.
The algorithm, typically a parametric inference method,
may be distributed over multiple compute nodes.
Such methods are often the
27
fastest, but can require the most developer effort.
For special cases such as ma-
trix multiplication, there are standard methods to make use of highly-parallel
architectures like GPUs (graphical processing units).
However, in general, this
method requires a re-implementation.
Another method can be used when a model
needs to be run multiple
times,
preferably independently,
to determine optimal
hyper-parameter set-
tings.
One example is performing a grid search for a random forest classifier
[6].
The majority of
Spark and other Mapreduce tasks employ the third
method,
which is to run a single-node inference on data that has been dis-
tributed.
This typically requires that some sort of
consensus inference be
calculated.
This step is called "reduce" in Mapreduce vernacular.
The "map"
process is then required to sort the data into appropriate bins according to
whatever model is being applied:
class, cluster, topic, etc.
The steps of
all
Mapreduce and Spark processes are as follows:
(See
Figure 2.7)
• Partition data - sort data onto nodes,
normally randomly,
but can be
controlled to optimize
• Map - map each datum (normally a row, vector, or set) to a key
• Sort - sort each datum by key
• Reduce - for each key, process data elements, often a sum or average
28
Figure 2.7:
Mapreduce steps from [29]
29
Spark and other Mapreduce are functional
programming [61]
frame-
works,
like R, and Haskell,
and unlike C and MATLAB (although MATLAB
has vector operations that are processed in a functional
way).
Although de-
velopers can use python and java bindings, the native scala encourages disuse
of loops and other crutches that are detrimental in distributed computing.
In
fact,
poorly structured algorithms submitted to a distributed scheduler may
run more slowly than on a single node.
Mapreduce was originally designed to work with data in batches.
How-
ever, many sources of data are online, or streaming.
As previously noted, the
so-called lambda architecture has emerged as a standard for processing online
data with Spark, using mini-batches (Figure 1.1).
The data is staged in mem-
ory until
it can be processed in Spark.
Depending on the application,
other
models can be run in batch mode,
say,
over the weekend,
using accumulated
data to modify hyperparameters or make other updates.
The primary library for data mining and machine learning algorithms
in Spark is MLib [5].
Although it is not supported in this library,
the well-
known method for locality sensitive hashing (LSH) [39],
has been recently
highlighted in the Spark and Mapreduce communities for use in various key-
value computation schemes.
I describe LSH briefly here, but interested readers
should consult [39] or [79] for a more thorough treatment, the latter of which
offers an extensive discussion of more recent extensions.
Hashing refers to a mapping from an input vector to an output vector
h :
M → S; h ∈ F,
the family of
hash functions,
and M = (M, d),
a metric
30
space on which a distance metric,
d,
can be defined among data elements in
M.
The output vector is referred to as the "hash." Most hashing methods
are used in applications such as cryptography to map from input space to a
divergent secondary space for obfuscation.
However,
in this case,
we wish to
hash "similar" elements to close points in S.
LSH has attractive scaling, which
is linear in the amount of data, O (nd)
31
Chapter 3
Temporal Distributed Learning
Before I attempt to model real data over time on distributed computer
nodes,
I first consider a simpler case.
Subsequent chapters note that many
sources of
streaming data provide text and other sparse count data.
This
chapter aims at modeling temporally-evolving, distributed data in a relatively
simple setting:
unsupervised learning of
Gaussian mixture models (GMMs).
It serves as a precursor to more complex distributed dynamic models in later
chapters.
This simplified model assumes a processing system with local data
collectors, data sources, or data processors, referred to as local sources or nodes
with corresponding local models.
There is also a designated central processor,
which is referred to as the central
node or global
node with a corresponding
global
model.
Synchronous data updates are also assumed.
Portions of this
chapter were previously published
1
3.1
Motivation
The analysis of single source data and accurately modeling the associ-
ated signal statistics and signal dynamics has provided innumerable scientific
1
I was the first author of [73].
32
and engineering challenges over the past decades.
The combination of
data
from multiple sources in a networked environment adds a long list of challenges
[43] from state synchronization to feature mismatch to sensor registration that
have confounded many data combination efforts in practice.
Nonetheless, the
explosion in deployed sensors in many application fields has increased the in-
terest in combining related information [47].
Additionally, whereas batch data
processing is often appropriate for static data or one-time events,
much data
is now available or can be updatable dynamically,
increasing the interest in
online learning methods.
Inconsistent or inadequate communication channels
as well as varying information sharing standards for each data source require
a distributed computation framework.
In systems with limited local
compu-
tation resources,
segmenting data and associated processing to a distributed
network has become increasing popular with frameworks such as MapReduce.
This chapter takes the distributed learning model
of [55]
which fits a
Gaussian mixture model
(GMM) for the “global”
data,
which is a union of
the local
data.
Since the local
sources may have differing feature sets,
the
global data may be non-randomly sparse in features for which there is no data
from a particular source.
Each local
data set’s information content can be
described via a parametric model that can range from a single parameterized
Gaussian component to a component for each data point.
The fewer number
of
parameters used,
the more privacy is retained,
under the model
in [55].
The local
data can have all
or a subset of
the features in the global
data.
To accommodate non-overlapping feature spaces at the global node, the Data
33
Augmentation method of [87] is used.
Features can be missing for a variety of
reasons from privacy redaction to collection differences.
For online use, data is assumed to be collected or processed at the local
sources sequentially and synchronously so that the GMM parameters learned
for the global model for each time step can be used as the observed data in a
linear dynamical system model.
Modeling the state space dynamics of
the global
data space is made
tractable for real
online data scenarios by the following approximation.
For
each GMM component, a linear motion model is assumed, enabling a variant
of the Kalman filter to be used to propagate model components in time.
The
Gaussian mixture probability hypothesis density filter [78] is a tractable form
of
the Kalman Filter for GMMs,
that also accommodates new component
formation and component death, as well as the normally challenging problem
of
component association on data updates for non-stationary data.
This is
called the multiple track problem and is called out for particular attention in
[47].
3.1.1
Contributions
This chapter pieces together the documented algorithms and methods
mentioned above and includes the appropriate glue.
More than an assemblage
or chaining of algorithms, the combination involves modification and alteration
to all
key algorithms in order to form a coherent whole.
I have developed
an online algorithm intended to learn the distribution parameters for non-
34
stationary global
data in a system involving heterogeneous feature sets from
distributed data sources with inter-node communication limitations and/or
privacy restrictions.
Noise and clutter,
or non-signal
data that can appear
to be signal,
may also corrupt local
data,
and may be properly separated by
the algorithm as these have differing temporal signatures than typical signals.
At each synchronous data update, the algorithm produces an estimate of the
parameters of a GMM fit to the global data.
The method is tested with a simulated datasets and out-of-sample log-
likelihood is used as a quality metric,
since the goal
is to best fit a global
parametric model to the union of a distributed set of source data.
3.2
Background
As described above, this model combines several areas of work.
In par-
ticular distributed learning,
dynamic data modeling,
and latent factor mod-
eling.
Related work in the area of
distributed learning under assumptions
regarding the distribution of the data is described in Section 2.3; and dynamic
data modeling is described in Section 2.2.
Related work on latent factor mod-
eling over time relevant to GMMs is described in Section 2.2.2.
As with [55],
I wish to accommodate cases in which multiple nodes that may individually
sample from not all
available data dimensions.
Section 3.2.1 discusses data
imputation for Gaussian models.
35
3.2.1
Data Imputation
Usually missing values in a model represent either the “missing” latent
variables of
GMM component assignment or actual
missing elements in the
vector of values for some data.
In the data investigated here,
there are both
missing latent variables of
GMM component assignment and missing data
elements due to missing features for data from some sources.
Each category of
missing data impacts the estimation of the other, so a simple initial pass data
imputation followed by an EM GMM parameter fit is not correct.
Reference
[87]
describes the general
method of
stochastic EM sampling when p (z|x)
cannot be directly sampled, as is the case here due to z being a concatenation
of dependent missing variable sets.
The appropriate model
to use in this case is data augmentation de-
scribed in [87].
Data augmentation prescribes an iteration of
imputation –
posterior,
IP, in which in the I step the missing data is drawn from distribu-
tions parameterized by GMM component parameters and in the P step the
GMM component parameters and the prior parameters are re-estimated given
the updated missing data.
See Table 3.1 for the details.
Note that since the data augmentation IP method prescribes a new way
to estimate the GMM component parameters,
this method replaces the EM
method in Table 3.2.
Also,
the KL minimization is maintained but the form
of the log-likelihood optimization is modified to reflect the prior, as shown in
Section 3.
The prior for a multivariate Gaussian mixture is the normal-inverted
Wishart for the component parameters, and a symmetric Dirichlet prior over
36
Figure 3.1:
Gaussian Mixture Model (GMM) plate model
the vector of weights as described in [35].
Note that atypically there is a set
of normal-inverse Wishart hyperparamters for each mixture component.
See
the plate diagram of the model in Figure 3.1
Σ
k
∼ W
−1
(m
k
, Λ
k
)
µ
k
|Λ
k
∼ N µ
0,k
, τ
−1
Σ
k

w ∼ Dir (α, ..., α)
(3.1)
37
Initialization:
Fill in missing data values with simple imputation, Seed
GMM component and prior parameters with simple clustering of imputed
data
IP-Iteration:
Gibbs sampling past burn in period (1000 samples typical)
I-Step:
Draw missing data elements from associated GMM component
(ignoring index, t, indicating iteration step)
for all x
n
, n ∈ 1, ..., N do
k : z
nk
= 1
x
n,m
∼ N µ
k,m|o
, Σ
k,mm|o

µ
k,m|o
= µ
k,m
+ Σ
T
k,om
Σ
−1
k,oo
(x
n,o
− µ
k,o
)
Σ
k,mm|o
= Σ
k,mm
− Σ
T
k,om
Σ
−1
k,oo
Σ
k,om
end for
where the missing and observed portions of the mean and covariance have
been segmented.
P-Step:
Estimate GMM component and prior
parameters
1.
Indicator variables
h
nk
=
w
k
N(x
n
|µ
k
,Σ
k
)
P
K
i=1
w
i
N(x
n
|µ
i
,Σ
i
)
, where the right-hand side parameters
are from the t-1 step z
n
∼ M (h
nl
, ..., h
nK
)
2.
Component parameters
(Note:
data element x
n
contributes to the k
th
component
indicated by z
nk
= 1.)
3.
Mixing coefficients
w
t
∼ Dir (α + N
1
, ..., α + N
K
)
Output:
GMM component, prior parameters, missing data elements
{µ, Σ} , {m, Λ, µ
0
, τ } , {x
n,m
}
Table 3.1:
Data Augmentation for GMM with Prior
38
3.3
Temporal Distributed Learning
3.3.1
Recursive Bayesian Estimation
Equation 3.2 describes the recursive Bayesian estimation problem when
updates are provided at each t.
p (x
t
|z
t
) = p (z
t
|x
t
)
Z
x
t−1
p (x
t
|x
t−1
) p (x
t−1
|z
t−1
)
(3.2)
(Note:
In this implementation, all models from parametric family G)
Input:
Set of models {λ
i
}
n
i=1
with weights {ν
i
}
n
i=1
summing to 1,
Parametric family G, Global sample size e
m
c
.
Output λ
a
c
∼
=
arg min
λ
c
∈G
n
P
i=1
ν
i
KL p
λ
i
||p
F
i
λ
c

Method:
1.
Generate e
χ
i
, [i]
n
1
from the local models, λ
i
,[i]
n
1
using MCMC
sampling such that |e
χ
i
| = ν
i
e
m
c
, [i]
n
1
, | e
χ| > 1000
2.
Let e
χ
c
=
n
S
i=1
e
χ
i
.
Apply EM algorithm to obtain the optimal model λ
a
c
such that λ
a
c
∼
=
arg min
λ
c
∈G
log (p
λ
c
(χ))
Table 3.2:
Distributed Learning with Data Augmentation
This enables the calculation of the state of system at time t given obser-
vations of a system at that time with only information from the immediately
prior time step available, fulfilling the need for an on-line method.
In practice,
the right hand side of
the equation is extremely challenging to calculate,
so
a wide variety of
approximation methods have been developed,
perhaps the
most historically popular of
which,
the Kalman filter,
was discussed in the
39
prior section.
Regardless of
the approximation method used,
in fielded ap-
plications with even single sensors,
complications arise due to myriad causes
including the aforementioned multiple target tracking,
very large numbers of
targets to track, clutter (signal-like non-targets), noise, false and spurious (sin-
gle or few update) targets, dropouts (the inverse of spurious targets), and other
algorithm –confounding data issues.
3.3.2
Combining Heterogeneous Data
A close examination of
Table 3.2 reveals that in step 2,
if
the data
is heterogeneous, there will be large segments of missing values.
For example
consider the exemplar scenario from [55] in which 3 heterogeneous data sources
with feature sets (A,B), (B,C), and (C,A) are combined into a global feature
space (A,B,C). In the Distributed Learning algorithm when samples are com-
bined, set 1 will lack feature C, and so on.
Thus if the data to be processed via
EM is vector data,
the 3-D vector data will
have one section with C-missing
values,
one with A-missing values,
and one with B-missing values.
As previ-
ously discussed, performing EM to fit the missing values of GMM parameters
simultaneously with fitting these missing data values are correlated problems.
Thus the data augmentation IP method described in Table 3.2 must be used.
Since the output of this algorithm is a GMM model
fit to the input data,
a
minor change to Table 3.2 is motivated.
This is described in Table 3.2.
For
simplicity of
analysis and evaluation,
the local
models are also assumed to
be taken from the same parametric family as the global
model,
namely the
40
Gaussian mixture model
family.
Note also that a prior term has been added
to the optimization as required by the data augmentation algorithm.
3.3.3
Tracking Mixture Components
Since as noted previously the PHD filter is a constant covariance filter
in which only mixture component means are updated,
a method is required
to provide for the update of component covariance.
I have chosen simply to
use the learned global
covariance,
or the observation model
component vari-
ance.
This coupled with the estimated component state mean and estimated
component state weight form the parameter tuple for each Gaussian mixture
component produced by the method for each data update.
As noted in Table
3.3, the state and observation components are related via a correlation variable
that is maintained in the updated form of the PHD filter and is a matrix form
of the usual binary-valued GMM component assignment variable.
41
Input:
Z
n
, set of “observation” samples for latest data update from global
model output by Table 3.2, such that missing feature data has been filled
in.
These are the means of the global GMM components in λ
a
c
, the global
model for latest data update output by Table 3.2.
Output:

w
(j)
, m
(j)
, Σ
(j)
J
j=1
weights, means, in state space and
covariances in observation space for latest data update

δ
(j)(m)
J,M
j=1,m=1
most likely correlate between observation components and
state components

H
(j)
, Q
(j)
, R
(j)
J
j=1
updated state space model parameters for each state
component
Steps 1 - 4:
PHD GMM algorithm as described in T??.
Insert prior to the second to final “end” and at the same level as the

w
(j)
, m
(j)
, P
(j)
assignments for each z :
(Note:
m is the component index of element z.)
δ
(j)(m)
= 1
Pruning:
As described in ??
Set J
max
= n (i.e.
number of state components to equal number of
observation components)
Modify m in each δ
(j)(m)
as necessary as components are grouped.
Table 3.3:
Modified GMM for PHD
3.3.4
Implementation Notes
All
algorithms were implemented in CPython with numpy and mat-
plotlib.pyplot for figure generation.
3.4
Experiments
3.4.1
Datasets
Since this is very early stage algorithmic development a data simulator
was built in order to achieve the goal
of being able to examine performance
on data with specific qualities.
The simulator also facilitates algorithm de-
42
velopment since multiple source,
multiple update data is difficult to obtain
in the research setting.
Once the method has matured, standard multisource
heterogeneous non-stationary data can be processed.
3.4.1.1
Simulated Dataset 1 – Overlapping Components, No Miss-
ing Data, Three Sources, Three Initial Clusters, Non-Stationary
The initial dataset examined covers a global 2-D space with three 2-D
data sources.
All sources have essentially no observation spherical noise,
0.1.
Separate simulation studies have demonstrated the ability of
the algorithm
to reproduce the results of [55] for single data updates by introducing widely
varying observation noise for each source.
In this case the dataset runs from t
= 0 to t = 9 or ten time steps, and there are three components to the data in
all steps.
At t =0, the means are at (0,0), (4,4), and (8,4) and at t = 9, they
are at (0,0), (1.3,1.3), and (8,4).
That is, the second component travels with a
constant velocity diagonally toward the origin.
Due to component covariance,
at t = 9,
components one and two are visibly indistinguishable.
Figure 3.2
shows the simulated dataset and distributed learning fit at t=0.
3.4.2
Quantitative Metrics
Since the veracity of alternate models is the principal question in this
work,
out-of-sample log likelihood,
OOSLL,
was
used to evaluate models.
Three models were examined:
(i) the Central
model
in which all
data was
able to be collected at a central processor at each update and processed there,
in the standard way with a EM GMM parameter fit; (ii) the model presented
43
Figure 3.2:
Simulated dataset (union of local data) at t=0 with corresponding
global parameter fit using the Distributed model
44
Figure 3.3:
3 Out-of-Sample Log-Likelihood for Compared Models (Central –
Blue, Distributed – Green, Distributed Temporal – Red
here, called the Distributed Temporal model; and (iii), a model in which dis-
tributed processing was performed at each step without a temporal
model,
called the Distributed model.
Figure 3 shows the performances obtained on
the simulated dataset.
Since in this simple example the model
performances
were similar,
I summed the difference at each data update in the OOSLL to
a reference number which I chose to be the worst or lowest score.
Thus the
larger the sum or area,
the better the overall
fit over the course of
all
data
updates.
The data is shown in Table 3.4.
This matches the visual impression
from Figure 3 which is that the Distributed model alone is marginally better
than the Central model, and that despite a couple of poor results, the overall
performance of
the Distributed Temporal
model
is superior,
though by this
metric only by less than 2%.
45
Table 3.4:
Temporal Metric Comparison for Models
Model
Out-of-sample
Log
Likeli-
hood time sum difference
Central
330,845
Distributed
366,609
Distributed Temporal
380,016
3.4.3
Temporal Distributed Learning Results
Examining the distribution parameter estimates at each data update
rather than the quality metric, it can be seen than as the second data cluster
moves progressively “into” the covariance ellipse of the first cluster, the Central
and Distributed models loose differentiation earlier than the Distributed Tem-
poral model.
This is one easy to conceptualize benefit of component “tracking.”
However,
the PHD filter appears to require data dependent tuning to deter-
mine track merge and prune.
The metric results for the Distributed Temporal
model would have been much better in this example if for the last data update
the easily noted third component had not been assigned a vanishingly low
weight by the PHD filter.
It is possible that additional improvements to this
filter or an alternate approach such as sequential
Monte Carlo may provide
more accurate parameter estimates of the global data distribution.
46
Chapter 4
Network Discovery via Joint Nework and Topic
Modeling
Portions of this chapter were previously published
1
4.1
Motivation
Social networks and other relational datasets can conveniently be repre-
sented using a binary symmetric adjacency matrix B ∈ {0, 1}
N×N
.
Often, the
nodes in such datasets are also associated with “side information”, such as doc-
uments read or written, movies rated, or messages sent by these nodes, all of
which can be represented as count-valued side information matrix Y ∈ Z
D×V
,
where Z = {0, 1, . . .}.
For example, B may represent a coauthor network and
Y may correspond to a document-by-word count matrix representing the doc-
uments written by all these authors.
In another example, B may represent a
user-by-user social
network and Y may represent a user-by-item rating ma-
trix.
In this particular setting, B can be considered as “side information" and
used for modeling Y better.
Incorporating such side information can result in
better modeling of both B and Y when either of these matrices lacks sufficient
1
I was co-author of [12] and I presented the work at the corresponding workshop.
47
information.
This chapter proposes Joint Gamma Process Poisson Factorization
(J-GPPF), to jointly factorize B and Y in a nonparametric Bayesian manner.
The paper makes the following contributions:
• We present a fast and effective model
that uses both B and Y to help
discover better network structures and cater better recommendation.
• We perform nonparametric Bayesian modeling for discovering latent struc-
tures in both B and Y and predict on missing entries in both B and Y.
• Our model scales with the number of non-zero entries S
B
in the network and
the number of non-zero entries S
Y
in the count matrix as O (S
B
K
B
+ S
Y
K
Y
),
where K
B
is the number of network groups and K
Y
is the number of latent
factors in the count matrix, preset at a high value while performing nonpara-
metric modeling.
The remainder of the chapter is organized as follows.
We present back-
ground material
in Section 4.2.
J-GPPF and its inference algorithm are ex-
plained in Section 4.3.
The related works are presented in Section 4.4.
Exper-
imental results are reported in Section 4.5.
48
4.2
Background
4.2.1
Negative Binomial Distribution
The definition of the negative binomial (NB) distribution is given in [8].
Here we introduce a few relevant lemmas which we use to derive the Gibbs
sampling updates in Section ??.
Lemma 4.2.1 ([?]). If m ∼ NB(r, p) is represented under its compound Pois-
son representation, then the conditional posterior of l given m and r has PMF:
Pr(l = j|m, r) =
Gam(r)
Gam(m + r)
|s(m, j)|r
j
,
j = 0, 1, · · · , m,
(4.1)
where |s(m, j)|
are unsigned Stirling numbers of
the first
kind.
We denote
this conditional
posterior as (l|m, r) ∼ CRT(m, r), a Chinese restaurant table
(CRT) count random variable,
which can be generated via l =
P
m
n=1
z
n
, z
n
∼
Bernoulli(r/(n − 1 + r)).
Lemma 4.2.2.
Let
X =
P
K
k=1
x
k
,
x
k
∼ Pois(ζ
k
) ∀k,
and ζ =
P
K
k=1
ζ
k
.
If
(y
1
, · · · , y
K
|X) ∼ Mult(X, ζ
1
/ζ , · · · , ζ
K
/ζ) and X ∼ Pois(ζ), then the follow-
ing holds:
P (X, x
1
, · · · , x
K
) = P (X, y
1
, · · · , y
K
).
(4.2)
Lemma 4.2.3.
If
x
i
∼ Pois(m
i
λ),
λ ∼ Gam(r, 1/c),
then x =
P
i
x
i
∼
NB(r, p), where p = (
P
i
m
i
)/(c +
P
i
m
i
).
Lemma 4.2.4.
If x
i
∼ Pois(m
i
λ),
λ ∼ Gam(r, 1/c), then
(λ|{x
i
}, r, c) ∼ Gam
r +
X
i
x
i
,
1
c +
P
i
m
i
!
.
(4.3)
49
Lemma 4.2.5.
If r
i
∼ Gam(a
i
, 1/b) ∀i,
b ∼ Gam(c, 1/d), then we have:
(b|{r
i
, a
i
}, c, d) ∼ Gam
X
i
a
i
+ c,
1
P
i
r
i
+ d
!
.
(4.4)
The proofs of Lemmas 4.2.3, 4.2.4 and 4.2.5 follow from the definitions
of Gamma, Poisson and Negative Binomial distributions.
Lemma 4.2.6.
If x
i
∼ Pois(m
i
r
2
),
r
2
∼ Gam(r
1
, 1/d),
r
1
∼ Gam(a, 1/b),
,
then (r
1
|−) ∼ Gam(a+`, 1/(b−log(1−p))) where (`|x, r
1
) ∼ CRT(
P
i
x
i
, r
1
), p =
P
i
m
i
/(d +
P
i
m
i
).
The proof and illustration can be found in Section 3.3 of
[8].
4.2.2
Gamma Process
For non-parametric modeling, we employ Gamma processes, a detailed
description of which is provided in [8, ?, 34].
4.3
Joint Gamma Process Poisson
Factorization (J-GPPF)
An overview and intuitive presentation of
the Joint Gamme Process
Poisson Factorization (J-GPPF) model
is privided in Section 2.1.1.
A math-
ematical
definition of
the model
is provided here along with a listing of
the
Gibbs sampling equations for model inference given data.
Let there be a network of N users encoded in an N × N binary matrix
B.
The users in the network participate in writing D documents summarized
in a D×V count matrix Y , where V is the size of the vocabulary.
Additionally,
50
Figure 4.1:
Gibbs sampling updates in J-GPPF
a binary matrix Z of dimension D ×N can also be maintained, where the unity
entries in each column indicate the set of documents in which the corresponding
user contributes.
In applications where B represents a user-by-user social
network and Y represents a user-by-item rating matrix, Z turns out to be an
N -dimensional identity matrix and hence can be considered as a special case
of
the document-author framework,
which we pursue to describe the model.
Also,
to make the notations more explicit,
the variables associated with the
side information have Y as a subscript (e.g.,
G
Y
) and those associated with
the network make similar use of the subscript B (e.g., G
B
).
We employ two separate Gamma Processes.
The first one models the
latent
factors
in the network.
A draw from this
Gamma Process
G
B
∼
ΓP(c
B
, H
B
) is expressed as G
B
=
P
∞
k
B
=1
ρ
k
B
δ
φ
k
B
, where φ
k
B
∈ Ω
B
is an atom
drawn from an N -dimensional base distribution as φ
k
B
∼
Q
N
n=1
Gam(a
B
, 1/σ
n
)
and ρ
k
B
= G
B
(φ
k
B
) is the associated weight.
The second Gamma Process
models
the latent
groups
of
side information.
A draw from this
gamma
51
process
G
Y
∼ ΓP(c
Y
, H
Y
)
is
expressed as
G
Y
=
P
∞
k
Y
=1
r
k
Y
δ
β
k
Y
,
where
β
k
Y
∈ Ω
Y
is
an atom drawn from a V -dimensional
base distribution as
β
k
Y
∼
Q
V
w=1
Gam(ξ
Y
, 1/ζ
w
) and r
k
Y
= G
Y
(β
k
Y
) is the associated weight.
Also,
γ
B
= H
B
(Ω
B
) is defined as the mass parameter corresponding to the
base measure H
B
and γ
Y
= H
Y
(Ω
Y
) is defined as the mass parameter cor-
responding to the base measure H
Y
.
The (n, m)
th
entry in the matrix B is
assumed to be derived from a latent count as:
b
nm
= I
{x
nm
≥1}
,
x
nm
∼ Pois (λ
nm
) , λ
nm
=
X
k
B
λ
nmk
B
,
where λ
nmk
B
= ρ
k
B
φ
nk
B
φ
mk
B
.
This is called as the Poisson-Bernoulli (PoBe)
link in [8, 88].
The distribution of
b
nm
given λ
nm
is named as the Poisson-
Bernoulli distribution, with the PMF:
f (b
nm
|λ
nm
) = e
−λ
nm
(1−b
nm
)
(1 − e
−λ
nm
)
b
nm
.
One may consider λ
nmk
B
as the strength of mutual latent community member-
ship between nodes n and m in the network for latent community k
B
, and λ
nm
as the interaction strength aggregating all possible community membership.
Using Lemma 4.2.2,
one may augment
the above representation as
x
nm
=
P
k
B
x
nmk
B
, x
nmk
B
∼ Pois (λ
nmk
B
).
Thus each interaction pattern con-
tributes a count and the total
latent count aggregates the countably infinite
interaction patters.
Unlike the usual approach that links the binary observations to latent
Gaussian random variables with a logistic or probit function,
the above ap-
proach links the binary observations to Poisson random variables,
providing
52
Figure 4.2:
Gibbs sampling for J-GPPF network components with missing
entries
several
potential
advantages.
First,
it is more interpretable because ρ
k
B
and
φ
k
B
are non-negative and the aggregation of
different interaction patterns
increases the probability of
establishing a link between two nodes.
Second,
the computational benefit is significant since the computational complexity is
approximately linear in the number of
non-zeros S
B
in the observed binary
adjacency matrix B.
To model the matrix Y , its (d, w)
th
entry y
dw
is generated as:
y
dw
∼ Pois(ζ
dw
), ζ
dw
=
X
k
Y
ζ
Y dwk
Y
+
X
k
B
ζ
Bdwk
B
!
,
where ζ
Y dwk
Y
= r
k
Y
θ
dk
Y
β
wk
Y
, Z
nd
∈ {0, 1} and Z
nd
= 1 if and only if author
n is one of
the authors of
paper d and ζ
Bdwk
B
= ρ
k
B
(
P
n
Z
nd
φ
nk
B
) ψ
wk
B
.
One can consider ζ
dw
as the affinity of document d for word w, This affinity is
influenced by two different components, one of which comes from the network
modeling.
Without the contribution from network modeling,
the joint model
reduces to a gamma process Poisson matrix factorization model, in which the
matrix Y is factorized in such a way that y
dw
∼ Pois
P
k
Y
r
k
Y
θ
dk
Y
β
wk
Y

.
53
Here,
Θ ∈ R
D×∞
+
is the factor score matrix,
β ∈ R
V ×∞
+
is the factor loading
matrix (or dictionary) and r
k
Y
signifies the weight of the k
th
Y
factor.
The num-
ber of latent factors,
possibly smaller than both D and V ,
would be inferred
from the data.
In the proposed joint model,
Y is also determined by the users par-
ticipating in writing the d
th
document.
We assume that the distribution over
word counts for a document is a function of both its topic distribution as well
as the characteristics of the users associated with it.
In the author-document
framework,
the authors employ different writing styles and have expertise in
different domains.
In the user-rating framework,
the entries in Y are also
believed to be influenced by the interaction network of
the users.
Such in-
fluence of
the authors is modeled by the interaction of
the authors in the
latent communities via the latent factors φ ∈ R
N×∞
+
and ψ ∈ R
V ×∞
+
,
which
encodes the writing style of the authors belonging to different latent commu-
nities.
Since an infinite number of network communities is maintained,
each
entry y
dw
is assumed to come from an infinite dimensional
interaction.
ρ
k
B
signifies the interaction strength corresponding to the k
th
B
network community.
The contributions of
the interaction from all
the authors participating in a
given document are accumulated to produce the total
contribution from the
networks in generating y
dw
.
Since B and Y might have different levels of
sparsity and the range of
integers in Y can be quite large,
a parameter  is
required to balance the contribution of the network communities in dictating
the structure of Y.
A low value of  forces disjoint modeling of B and Y, while
54
Figure 4.3:
Gibbs sampling for J-GPPF topic components with missing entries
a higher value implies joint modeling of B and Y where information can flow
both ways, from network discovery to topic discovery and vice-versa.
To complete the generative process, we put Gamma priors over c
B
, c
Y
,
σ
n
, ς
d
and  as:
c
B
∼ Gam(g
B
, 1/h
B
), c
Y
∼ Gam(g
Y
, 1/h
Y
),
 ∼ Gam(g
0
, 1/f
0
),
σ
n
∼ Gam(α
B
, 1/ε
B
), ς
d
∼ Gam(α
Y
, 1/ε
Y
).
4.3.1
Inference via Gibbs Sampling
Though J-GPPF supports countably infinite number of latent commu-
nities for network modeling and infinite number of
latent factors for topic
modeling, in practice it is impossible to instantiate all of them.
We consider a
finite approximation of the infinite model by truncating the number of graph
communities and the latent topics to K
B
and K
Y
respectively,
by letting
ρ
k
B
∼ Gam(γ
B
/K
B
, 1/c
B
) and r
k
Y
∼ Gam(γ
Y
/K
Y
, 1/c
Y
).
Such approxi-
mation approaches the original
infinite model
as both K
B
and K
Y
approach
55
infinity.
Sampling of (x
nmk
B
)
K
B
k
B
=1
: First, the total latent count corresponding to the
non-zero entries can be derived as:
(x
nm
|−) ∼ b
nm
Pois
+
K
B
X
k
B
=1
λ
nmk
B
!
.
(4.5)
After which, following Lemma 4.2.2 one can derive:

(x
nmk
B
)
K
B
k
B
=1
|−

∼ Mult
x
nm
,

λ
nmk
B
P
K
B
k
B
=1
λ
nmk
B

K
B
k
B
=1
!
.
(4.6)
Sampling of (y
dwk
)
k
: Again, following Lemma 4.2.2, we have:

(y
dwk
Y
)
K
Y
k
Y
=1
, (y
dnwk
B
)
K
B
k
B
=1,n∈Z
d
|−

∼
(4.7)
Mult
y
dw
,
{ζ
dwk
Y
}
k
Y
, {ζ
dnwk
B
}
n∈Z
d
,k
B
P
k
Y
ζ
dwk
Y
+
P
n∈Z
d
P
k
B
ζ
dnwk
B
!
.
Sampling of
φ
nk
B
,
ψ
wk
B
,
ρ
k
B
,
θ
dk
Y
,
β
wk
Y
,
r
k
Y
and  :
Sampling of
these
parameters follow from Lemma 4.2.4 and are given in Figure 4.1.
The sampling
of parameters φ
nk
B
and ρ
k
B
exhibits how information from the count matrix Y
influences the discovery of the latent network structure.
The latent counts from
Y impact the shape parameters for both the posterior gamma distribution of
φ
nk
B
and ρ
k
B
, while Z influences the corresponding scale parameters.
Sampling of σ
n
,
ς
d
,
,
ζ
w
,
η
w
,
c
B
and c
Y
:
Sampling of these parameters
follow from Lemma 4.2.5 and are given in Figure 4.1.
Sampling of γ
B
: Using Lemma 4.2.2, one can show that x
..k
B
∼ Pois(ρ
k
B
).
Integrating ρ
k
B
and using Lemma 4.2.4,
one can have x
..k
B
∼ NB(γ
B
, p
B
),
56
Figure 4.4:
Generative Process of N-GPPF
Figure 4.5:
Generative Process of C-GPPF
57
where p
B
= 1/(c
B
+ 1).
Similarly,
y
..k
B
∼ Pois(ρ
k
B
) and after integrating
ρ
k
B
and using Lemma 4.2.4,
we have y
..k
B
∼ NB(γ
B
, p
B
).
We now augment
l
k
B
∼ CRT(x
..k
B
+ y
..k
B
, γ
B
) and then following Lemma 4.2.6 sample:
(γ
B
|−) ∼ Gam
e
B
+
X
k
B
l
k
B
, (f
B
− q
B
)
−1
!
,
(4.8)
where q
B
=
P
k
B
log c
B
/(c
B
+
P
n
φ
nk
B
φ
−n
k
B
)

/K
B
.
Sampling of
γ
Y
:
Using Lemma 4.2.2,
one can show that
y
..(K
B
+k
Y
)
∼
Pois(r
k
Y
) and after integrating r
k
Y
and using Lemma 4.2.4, we have y
..(K
B
+k
Y
)
∼
NB(γ
Y
, p
Y
), where p
Y
= 1/(c
Y
+1).
We now augment m
k
Y
∼ CRT(y
..(K
B
+k
Y
)
, γ
Y
)
and then following Lemma 4.2.6 sample:
(γ
Y
|−) ∼ Gam
e
Y
+
X
k
Y
m
k
Y
, (f
Y
− q
Y
)
−1
!
,
(4.9)
where q
Y
=
P
k
Y
log (c
Y
/(c
Y
+ θ
.k
Y
)) /K
Y
.
4.3.2
Gibbs Sampling for J-GPPF with Missing Entries
Parameters whose update get affected in presence of missing entires are
ρ
k
B
,
φ
nk
B
,
ψ
wk
B
,
r
k
Y
,
θ
dk
Y
,
β
wk
Y
.
Sampling of these parameters follow from
Lemma 4.2.4 and are given in Figures 4.2 and 4.3.
Here M
B
and M
Y
denote
the set of missing entries in B and Y respectively.
4.3.3
Special
cases:
Network Only GPPF (N-GPPF) and Corpus
Only GPPF (C-GPPF)
A special
case of J-GPPF appears when only the binary matrix B is
modeled without the auxiliary matrix Y .
58
The update equations of
variables corresponding to N-GPPF can be
obtained with Z = 0.
Another special
case of J-GPPF appears when only the count matrix
Y is modeled without using the contribution from the network matrix B.
The
generative model
of
N-GPPF and C-GPPF are given in Figures 4.4 and 4.5
respectively.
4.3.4
Computation Complexity
The Gibbs sampling updates of J-GPPF can be calculated in O(K
B
S
B
+
(K
B
+K
Y
)S
Y
+NK
B
+DK
Y
+ V (K
B
+K
Y
)) time, where S
B
is the number
of non-zero entries in B and S
Y
is the number of non-zero entries in Y .
It
is obvious that for large matrices the computation is primarily of the order of
K
B
S
B
+ (K
B
+ K
Y
)S
Y
.
Such complexity is a huge saving when compared to
other methods like MMSB [?],
that only models B and incurs computation
cost of O(N
2
K
B
); and standard matrix factorization approaches [?]
that work
with the matrix Y and incur O(DV K
Y
) computation cost.
Interestingly, the
inference in [36]
incurs cost O(K
2
Y
D + K
Y
V + K
Y
S
Y
) with K
Y
signifying
the termination point of stick breaking construction in their model.
C-GPPF
incurs computation cost O(DK
Y
+K
Y
S
Y
+V K
Y
), an apparent improvement
over that of [36].
59
4.4
Related Work
The Infinite Relational
Model
(IRM [40]) allows for multiple types of
relations between entities in a network and an infinite number of clusters, but
restricts these entities to belong to only one cluster.
The Mixed Membership
Stochastic Blockmodel
(MMSB [?])
assumes that each node in the network
can exhibit a mixture of
communities but the computational
complexity of
the underlying inference mechanism is O(N
2
).
Such quadratic computation
complexity is also a problem with many other existing latent variable network
models,
such as the latent feature relational
model
[?]
and its max margin
version [91], and the infinite latent attribute model [62].
Some of the existing approaches handle sparsity in real-world networks
by using some auxiliary information [46, 54, 84].
Recommender system and text mining researchers,
in contrast,
tend
to take an orthogonal
approach.
In recommender systems [25, 49],
Y may
represent a user-by-item rating matrix and the objective in this setting is to
predict the missing entries in Y,
and the social
network matrix B plays a
secondary role in providing auxiliary information to facilitate this task [49].
Similarly, in the text mining community, many existing models [16, 52, 60, 80]
use the network information or other forms of side information to improve the
discovery of
“topics" from the document-by-word matrix Y.
The matrix B
can represent, for example, the interaction network of authors participating in
writing the documents.
60
The Relational
Topic Model
[26]
discovers links between documents
based on their topic distributions, obtained through unsupervised exploration.
The Author-Topic framework (AT [?]) and the Author-Recipient-Topic model
(ART [52]) jointly model documents along with the authors of the documents.
Block-LDA [16], on the other hand, provides a generative model for the links
between authors and recipients in addition to documents.
J-GPPF differs from these existing approaches in mathematical formu-
lation, including more effective modeling of both sparsity and the dependence
between network interactions and side information.
A large number of discrete latent variable models for count matrix fac-
torization can be united under Poisson factor analysis (PFA) [?], which factor-
izes a count matrix Y ∈ Z
D×V
under the Poisson likelihood as Y ∼ Pois(ΦΘ),
where Φ ∈ R
D×K
+
is the factor loading matrix or dictionary, Θ ∈ R
K×V
+
is the
factor score matrix.
A wide variety of algorithms, such as non-negative matrix
factorization [?, ?],
gamma-Poisson model
[21, 77],
LDA [?],
and gamma-NB
processes [?, ?],
although constructed with different motivations and for dis-
tinct problems,
can all
be viewed as PFA with different prior distributions
imposed on Φ and Θ.
J-GPPF models both Y and B using Poisson factorization.
As dis-
cussed in [8], Poisson factorization has several practical advantages over other
factorization methods that use Gaussian assumptions (e.g.
in [49]).
First,
zero-valued observations could be efficiently processed during inference, so the
model
can readily accommodate large,
sparse datasets.
Second,
Poisson fac-
61
torization is a natural representation of count data.
The collaborative topic Poisson factorization (CTPF) framework pro-
posed in [37]
solves a different problem where the objective is to recommend
articles to users of similar interest.
CTPF is a parametric model and variational approximation is adopted
to solve the inference.
Although both models make use of Poisson factoriza-
tion to infer low-rank matrices in order to recommend items to users, J-GPPF
is a fundamentally different model.
As recently summarized in [?],
there are
many useful approaches for employing social side information to improve rec-
ommendation.
For example,
in both [?, ?],
the authors define a dependence
for item recommendations based on each node’s neighbors or community.
J-
GPPF is a joint model
of both the social
network and the item ratings,
and
solves for the latent space factorization of each, and missing elements of each,
simultaneously.
In addition to this innovation, model inference scales with the
number of observed elements in each matrix and the number of latent groups,
not the full dimension of each matrix.
4.5
Experimental Results
4.5.1
NIPS Authorship Network
This dataset contains a list of all papers and authors from NIPS 1988
to 2003.
We took the 234 authors who had published with the most other
people and looked at their co-authorship information.
After standard pre-
processing and removing words that appear less than 50 times in the over-all
62
Figure 4.6:
AUC NIPS
Figure 4.7:
AUC GoodReads
corpus corresponding to these users,
the number of
users in the graph who
write at least one document,
is 225 and the total
number of unique words is
1354.
The total number of documents is 1165.
Figure 4.8:
MAP NIPS
Figure 4.9:
MAP GoodReads
4.5.2
GoodReads Data
Using the Goodreads API,
we collect a base set of
users with recent
activity on the website.
For each user in the base set,
the user’s friends as
well
as friends of
friends on the site are collected (two hops in the graph).
This process is repeated over a 24−hour time period,
with a new base set
constructed each time (i.e.
friends are not polled recursively).
By running
63
for a full
day,
multiple time zones are covered and the reviews are collected
for all identified users, with a maximum of 200 reviews per user.
Each review
consists of a book ID and a rating from 0 to 5.
Similar dataset has also been
used in [25].
After standard pre-processing and removing words that appear
less than 10 times in the over-all corpus,
the number of users in the graph is
84 and the total number of unique words is 189.
4.5.3
Experimental Setup and Results
In all
the experiments,
we initialize  to 2 and let the sampler decide
what value works best for joint modeling.
We use K
B
= K
Y
= 50 and
initialize all the hyper-parameters to 1.
In the first set of experiments, for each
dataset,
we hold out data from B only and ran 20 different experiments and
display the mean AUC and one standard error.
In this setup, we consider N-
GPPF, the infinite relational model (IRM) of [40] and the Mixed Membership
Stochastic Block Model (MMSB) [?]
as the baseline algorithms.
Fig.
4.6 and
4.7 demonstrate the performances of
the models in predicting the held-out
data.
J-GPPF clearly has advantage over other network-only models when the
network is sparse enough and the auxiliary information is sufficiently strong.
However, all methods fail when the sparsity increases beyond a certain point.
The performance of J-GPPF also drops below the performances of network-
only models in highly sparse networks, as the sampler faces additional difficulty
in extracting information from both B and Y.
In the second set of experiments, we hold out data from Y only and run
64
20 different experiments and display the precision@top-20 for J-GPPF.
This
evaluation is structured along the lines of the work in [36].
We calculate the
intersection of the top 20 predicted set of words (arranged in the decreasing
order of counts) and the top 20 words in a document and divide the number
by 20 to get the precision for each document.
We then calculate mean average
precision (MAP) by taking the average of the precision over all the documents.
C-GPPF and the hierarchical
Poisson matrix factorization (HPMF) [36]
are
considered as the baselines,
both of
which model
only Y.
Fig.
4.8 and 4.9
show that B helps in boosting the predictive performance in J-GPPF over a
wide range of fractions of the data that is held out from Y.
65
Chapter 5
Joint Poisson Factorization Case Study:
Topic
Modeling of U.S. Senate Records
The joint community-topic Poisson factorization model
(J-GPPF) de-
scribed in Chapter 4 has been well-motivated and defined mathematically.
Prior to proposing the model for expanded use, however, we wish to not only
extended it for distributed inference and dynamic data (Chapter 7) but also to
see if the model yields semantically meaningful results.
In this chapter, we per-
form an in-depth analysis of J-GPPF model results using a novel corpus that
contains both community and topical
structure as well
as joint community-
topical
structure.
In addition,
the corpus contains some temporally-evolving
topics and is readily interpretable to most data mining researchers in the
United States (and likely to a global
audience),
even though the corpus was
taken from the political science domain.
5.0.1
Related Work
There is a strong precedent for using topic modeling in political science.
As might be expected, LDA is one of the most popular models [38].
DiMaggio
et al.
use LDA to track news coverage of public art funding [31], and quite a
number of researchers use LDA as the starting point for more complex models.
66
Yano et al.
extend LDA to model both blog posts and comments, with com-
menters as additional latent factors [82], and in another work Yano and Smith
create a Topic-Poisson model
that builds a mixture of Poisson distributions
on top of LDA,
in an effort to model
the number of comments a given blog
post will receive [83].
Roberts et al.
define the Structural Topic Model
(STM)
that encodes covariates into an LDA framework, allowing known information
such as political
affiliation to influence the topics in a given document [66].
Among other things,
STM can be used to track topic strengths for a given
covariate, or even a combination of covariates, allowing for richer interpretive
possibilities.
Some researchers use a combination of
methods for different types of
data; for instance, [33] analyzes the history of climate change polarization with
a network of contrarian organizations and their corresponding literature.
The
author uses STM for the documents,
encoding attributes such as the publi-
cation year and funding levels,
and investigates the network separately using
the Fruchterman-Reingold algorithm–which is not a model,
but a visualiza-
tion technique,
leaving open the possibility of
network factorization.
Given
that the organizations are associated with specific time periods,
the author
processes the network at different years,
watching it grow over discrete time
steps, which is similar to how I will handle time in this chapter.
While [33] is the most similar to our work on a structural level, [63] is
the most similar in document content.
In [63],
the authors look at speeches
in the U.S.
Senate,
from the 105th to the 108th Congresses.
They use a
67
model like LDA that allows only one topic per document (where each speech
is a document), and also includes a temporal element–specifically, a Dynamic
Linear Model (DLM) [22].
They are thus able to model topic trends over time.
They manually compare these topic trends with historic events,
notable roll
calls, etc., and use a weighted distance measure to determine how distinctive
each word is to a given topic.
In this work, I follow a similar course, and look at data from the U.S.
Senate,
which comprises networks and documents associated with different
years.
Through Poisson factorization,
I find topics and groups,
and identify
group members that are most distinctive to the groups.
My main contribution
is in the use of
a joint topic model,
which simultaneously factorizes both a
network and document corpus,
allowing information from each to interact
with the other.
I show how the inclusion of
documents as side information
affects network factors in a tangible way.
I implement a lift metric that seeks
to find the most distinctive senator(s) in each group,
similar to the weighted
distance measure in [63].
Finally, I show how a “static” model such as J-GPPF
(which does not include time as a variable) can be used with time-based data
to produce meaningful topical insights.
In the United States Senate,
a considerable amount of information is
recorded over the course of a congressional session, from roll calls and speeches
to bill
content.
This information is structured in a variety of
ways,
such as
count summaries (for voting) and text documents (for bills and transcribed
speeches), and contains hidden topical content indicating the political interests
68
of the senators.
Traditionally, researchers analyzed this information by hand,
through human coders [63], leading to datasets such as the Congressional Bills
Project [13]
and the Comparative Manifesto Project [20].
However,
human
coding is prone to error [56],
and can take a prohibitive amount of
time for
large corpora such as the Congressional
Record,
making automated methods
desirable.
One way to address this problem is through topic modeling,
which
defines topics as latent variables in a statistical
framework.
For instance,
one might consider a document to be a “bag of words”
with count values for
each word in the vocabulary.
One could then encode a set of
documents,
the corpus,
as a nonnegative integer matrix of
dimensions D × V (number
of documents versus the size of the vocabulary).
Then the problem becomes
a matrix factorization problem:
given that K is the number of latent topics,
which two matrices (D × K and K × V ) can be multiplied to best approximate
the original corpus?
In this framework, a topic becomes a distribution of words, and a doc-
ument becomes a distribution of topics.
The most standard solution is latent
Dirichlet allocation (LDA) [19], a Bayesian model that assigns these distribu-
tions Dirichlet priors, and assigns multinomial distributions for the likelihood
of words and topics (per the respective conjugate prior).
It is essentially an un-
supervised machine learning algorithm that finds clusters of words that tend to
appear together in documents.
Some topic models take into account side infor-
mation, like document authorship [68].
There is a related class of models that
69
identifies groups in social networks, such as the Mixed Membership Stochastic
Blockmodel
(MMSB) [14], which treats the latent groups as network factors.
In this paper, I show how Senate roll calls can be interpreted as a social
network,
making them amenable to topic modeling,
with bill
summaries as
side information.
Such combined datasets are often called “multi-relational”
data.
It is less common for topic models to factorize both documents and
networks simultaneously, but Joint Gamma Process Poisson Factorization (J-
GPPF) [10]
is up to the task,
and produces two sets of factors,
for both the
latent topics and groups.
I show how to implement J-GPPF in a practical
setting, comparing it with simpler methods such as LDA, and I show how the
resulting factors can be used to identify topical
interests,
political
posturing
for campaigns, and the relative partisanship of senators.
5.1
Joint Model Description
Joint Gamma Process Poisson Factorization (J-GPPF) is a Bayesian
model that simultaneously identifies latent factors in document count matrices
and network adjacency matrices [10].
I will refer to the factors as topics and
groups, respectively.
5.1.1
Notation
I first describe our notation.
The data matrices are shown in Table
5.1, for D documents, V words in the dictionary, and N authors.
The model
parameters are shown in Table 5.2, for K
Y
and K
B
factors.
70
Table 5.1:
Data notation
Symbol
Description
Y
D × V document count matrix
B
N × N network adjacency matrix
Z
N × D authorship binary matrix
Table 5.2:
Model parameter notation
Symbol
Description
θ
D × K
Y
document factors (topics)
β
V × K
Y
document factors (topics)
φ
N × K
B
network factors (groups)
ψ
V × K
B
network factors (groups)
r
K
Y
vector of document factor strengths
ρ
K
B
vector of network factor strengths
5.1.2
Definition
The likelihood functions are shown in Eq.
5.1 and 5.2,
with b
nm
as
the elements of
B,
and y
dw
as the elements of
Y.
The priors are mostly
gamma distributions, with θ and φ drawn from gamma processes, and β and
ψ drawn from Dirichlet distributions (see [10] for details).
Thus K
Y
and K
B
are truncations of
infinite numbers of
factors.
The posterior is inferred via
Gibbs sampling.
b
nm
∼ Pois
X
k
B
ρ
k
B
φ
nk
B
φ
mk
B
!
(5.1)
71
y
dw
∼ Pois
X
k
Y
r
k
Y
θ
dk
Y
β
wk
Y
+ 
X
k
B
ρ
k
B
X
n
Z
nd
φ
nk
B
!
ψ
wk
B
!
(5.2)
5.1.3
Interpretation
The topics and groups represent clusters of words and people that tend
to associate together.
Thus J-GPPF can be used to find who is communicating
with whom,
and what they are communicating about.
In the context of this
work,
we are interested in which senators vote together,
and what types of
bills they support.
For the network B,
we are essentially factorizing a matrix of Poisson
parameters into φ and ρ,
which represent the latent groups.
The document
matrix Y is factorized in a similar way,
except that information flows from
both the topics and the groups,
and mixes according to .
Thus  represents
the degree that the documents and network are allowed to interact.
Note that
the Z authorship matrix is considered to be a constant in the definition of the
model.
Also note that the model in [10] restricts B to binary edges, using the
Poisson-Bernoulli link, which essentially truncates positive samples in Eq.
5.1.
In this model,
we allow all
positive integer edges,
giving us a more nuanced
look into the network structure.
72
5.1.4
Special Cases
We obtain special
cases of the Poisson factorization model
by setting
 = 0 and modeling B and Y separately.
The former case is referred to as
Network-only Gamma Process Poisson Factorization (N-GPPF) and the latter
case is Corpus-only Gamma Process Poisson Factorization (C-GPPF).
They
are useful
for comparison with the joint model,
and evaluation of the effects
of joint factorization.
5.2
Data Description
5.2.1
Source
Our dataset was compiled from the voting records of the United States
Senate for the years 1992-2014.
The full
corpus was broken into 11 two-year
subsets,
with each corresponding to two congressional
sessions.
The corpus
subsets contain network relations and documents derived from the roll
calls
(votes) and bill summaries of both sessions, which were downloaded from the
Senate website [70].
Both H.R. bills (if introduced to the Senate) and S. bills
were accessed.
Only bills that were voted on in the Senate were considered.
5.2.2
Preprocessing
Each two-year subset was preprocessed independently.
The network
relations were encoded as a count matrix B ∈ Z
N×N
indicating the number of
times that each pair of senators agreed on the vote of a bill–i.e.
a senator pair’s
count incremented when they both voted for a bill
or when they both voted
73
Figure 5.1:
Voting network (B) for the 108th Senate
against a bill.
For example,
the 108th Senate has dimensions of
N = 100,
the size of the senate body.
For the sake of normalization, only senators who
served throughout the entirety of both sessions were included, so some subsets
have N slightly less than 100.
Additionally, the counts were normalized such
that max B = 20, which aids in model fitting.
An example B matrix from the
108th Senate is shown in Figure 5.1.
The documents were encoded as a count matrix Y ∈ Z
D×V
indicating
the number of times that a word appeared in a bill summary (e.g.
for the 108th
Senate there were D = 433 bills and V = 5098 words in the dictionary).
The
bill summaries were preprocessed by removing html tags, stopwords, punctua-
tion, and numbers.
Word stems were obtained by applying standard stemming
and lemmatization techniques.
The dictionary was filtered by removing the
most common words (appearing in > 50% of documents) and the least common
74
words (appearing in < 5 documents).
The word stems were finally mapped
back to proper words by selecting the most common word in the original corpus
with the respective stem.
Additionally,
an authorship matrix Z ∈ {0, 1}
N×D
was constructed
which indicates the senators that voted for each bill.
5.3
Experiments
In evaluating J-GPPF I ran two experiments:
a breadth study using
every two-year subset in the corpus, and a depth study using a single subset.
Each are described in the following sections.
5.3.1
The Full Corpus
In this section we take a look at the entire corpus,
from 1992-2014.
For this part I processed each two-year subset with J-GPPF separately,
and
focused on the groups,
or network factors,
corresponding to a factorized B
matrix.
5.3.1.1
Overview of Network Factors
Example groups for the Clinton and Obama presidencies are shown
in Figure 5.2,
and for the Bush presidency in Figure 5.4.
The senators are
arranged from Democrats to Republicans, then alphabetically by state.
Each run produced three groups, corresponding to a Democrat column,
a Republican column,
and a combined column.
The combined column spans
75
Figure 5.2:
Example network factors (φ) for J-GPPF
76
both parties,
and represents a moderate group,
showing the willingness of
senators to vote with other parties,
and will
be discussed in greater detail
in
section 5.3.2.
There is some variation in group strengths throughout the φ graphs, but
the ρ values compensate,
as shown in Figure 5.4.
To compare across groups,
the φ values must be squared and normalized by ρ (per Eq.
5.1).
Another
complicating feature is that some senators abstained from voting, resulting in
overall
lower strengths within their groups–this is especially apparent in the
striations seen in the 104th senate, Figure 5.2.
Thus overall voting tendencies
must also be accounted for when evaluating a senator’s strength in a particular
group.
It is also apparent in the graphs that many Senates have a few members
who defect from their parties,
showing up as isolated dots in the respective
group columns.
5.3.1.2
Ranking of Senators
In many cases it is useful
to rank the senators from within each of
the three inferred groups, but as previously mentioned, the φ matrix must be
normalized across multiple dimensions.
One way to compensate is to use a lift
metric.
For a group j, I rank the senators within j according to Eq.
5.3.
Lift
n,j
=
ρ
j
φ
2
n,j
P
k
B
ρ
k
B
φ
2
n,k
B
(5.3)
77
Table 5.3:
J-GPPF Republican group rankings
Senate
Years
McCain
Smith
Miller
103
1992-1994
11
4
n/a
104
1994-1996
12
7
n/a
105
1996-1998
44
7
n/a
106
1998-2000
2
1
n/a
107
2000-2002
47
8
49
108
2002-2004
48
n/a
1
109
2004-2006
42
n/a
n/a
110
2006-2008
1
n/a
n/a
111
2008-2010
12
n/a
n/a
112
2010-2012
20
n/a
n/a
113
2012-2014
37
n/a
n/a
In this manner I ranked the Republicans and the Democrats,
which
revealed trends in voting behavior.
One notable trend I discovered is that
presidential candidates change significantly in their respective party rankings
in the two years leading up to elections.
For example,
look at Senator John
McCain, in the Republican group rankings shown in Table 5.3.
In our ordering,
lower values correspond to a stronger group membership.
Note how in the
years 2000-2006 Senator McCain was one of the most moderate members of
the party,
and jumped to being the most partisan in 2006-2008,
which lines
up with his presidential nomination and campaign.
This trend is not confined to the Republican Party.
For instance, take
Senator John Kerry, shown in the Democrat group rankings in Table 5.4.
In the
years leading up to his 2004 campaign, he jumped to the top rank.
Similarly,
Senator John Edwards jumped from a rank of 31 in 2000-2002 to a rank of 2
78
Table 5.4:
J-GPPF Democrat group rankings
Senate
Years
Kerry
Obama
Biden
Clinton
103
1992-1994
20
n/a
18
n/a
104
1994-1996
19
n/a
30
n/a
105
1996-1998
8
n/a
19
n/a
106
1998-2000
14
n/a
24
n/a
107
2000-2002
16
n/a
19
17
108
2002-2004
1
n/a
21
16
109
2004-2006
13
19
16
20
110
2006-2008
12
1
3
2
111
2008-2010
8
n/a
n/a
n/a
112
2010-2012
35
n/a
n/a
n/a
113
2012-2014
n/a
n/a
n/a
n/a
in 2002-2004.
This is likely because the candidates are changing their voting behav-
iors to compete in the primaries.
For instance,
we see the same trend with
Senator McCain in 1998-2000,
when he campaigned and lost the Republican
nomination to George W. Bush.
Senator McCain was surpassed in ranking by
Senator Bob Smith, who was also seeking the nomination.
We also see this in the Democrat rankings for Senator Hillary Clinton.
In the years 2000-2006, she had rankings of 17, 16, and 20, respectively, before
jumping to a rank of 2 in 2006-2008, when she lost the Democratic nomination
to Senator Barack Obama.
She was surpassed in ranking only by Obama, and
was trailed by Senator Joe Biden in third place, each of whom were vying for
the nomination.
It is worth noting that there is a strong correlation between the number
79
of votes cast and campaigns–specifically,
the fewer votes a senator casts,
the
more likely he or she is campaigning for a party nomination.
This could be
either politically strategic or purely practical,
as the campaigners are more
likely to be on the road.
However,
in our analysis we normalize for voting
levels by the lift ranking, and thus our results suggest that the candidates are
indeed moving to the party centers, and are voting along party lines to secure
a nomination.
A similar jump can also be observed in the history of Senator Zell Miller,
although he did not run for president.
He served a relatively short senate term,
from 2000-2005, but during that time he jumped from a ranking of 49 in the
Republican group to a ranking of 1 in the years 2002-2004.
Note that Senator
Miller was officially a Democrat.
This anomalous ranking is explained by the
fact that, in spite of his party, he endorsed President Bush at the Republican
National Convention in 2004.
There are a whole host of
other analyses that can be done with the
senator rankings.
For instance, our rankings show that senators affiliated with
the Tea Party movement took 9 of the 15 most partisan spots in the Republican
group from 2010 through 2014;
in contrast,
only one Tea Party senator was
in the 15 least partisan spots.
We can also look at the third group rankings,
which I will discuss in section 5.3.2.
80
Figure 5.3:
Example network factors (φ) for N-GPPF
5.3.1.3
Comparison with N-GPPF
For comparison, I modeled each Senate subset with N-GPPF, which had
some notable differences.
The most significant is that only the main parties
were identified in the Network-only model.
The third group disappeared in
every run,
as shown in Figure 5.3,
and though some runs had partial
third
groups (as in the 104th Senate),
there were no groups that spanned both
parties.
Another obvious difference is that the values of
φ are smaller for the
N-GPPF runs, but this is inconsequential, as the group strengths ρ increased
to compensate.
One of
the more subtle and interesting differences is in the senator
81
rankings.
The presidential
campaign patterns discussed in subsection 5.3.1.2
essentially disappeared with N-GPPF; the only senator to display this pattern
was Clinton,
who jumped from a Democrat rank of
18 in 2004-2006 to a
rank of 3 in 2006-2008.
Senator Obama went from 17 to 7 during that time,
and Biden’s rankings were 22 for both time periods.
Senator Kerry actually
dropped to the most moderate position during his 2004 campaign.
On the Republican side, McCain showed a similar trend, and dropped
in rankings during his presidential
campaigns.
Although these drops were
notable,
and suggest that some outside factors were at play,
there were not
consistent enough patterns across all the presidential candidates to draw any
conclusions.
It should also be said that Senator Miller was grouped with the
Democrats in the 2002-2004 rankings, essentially hiding his political defection.
All in all, the N-GPPF network factors were not nearly as informative
as the J-GPPF network factors,
which suggests that the inclusion of side in-
formation in the form of
bill
summaries is important for effective modeling.
Similarly,
it suggests that bill contents are uniquely important to a senator’s
political maneuvering.
In many cases, they can reveal who is running for pres-
ident, who is collaborating with the other party, and who has defected to the
other party altogether.
5.3.2
The Two-Year Subset
In this section we look at a two-year subset in isolation for a more in-
depth analysis of
what the latent factors mean for both the documents (Y)
82
Figure 5.4:
Network factor strength (ρ) and factors (φ) for J-GPPF (108th
Senate)
and the network (B).
The subset I
selected was the 108th Senate,
which
corresponds to the years of 2003 and 2004, the start of the Iraq War.
5.3.2.1
Analysis of the Third Network Factor
The J-GPPF network was factorized into three groups,
as shown in
Figure 5.4, in the 108th Senate.
The two major political parties (groups 3 and
14) are clearly visible, as well as a third group (16) which is a moderate group,
spanning both of the major parties.
We can show that group 16 is moderate by ranking the senators with
Eq.
5.3.
Results are shown in Table 5.5.
The top members are composed
entirely of senators from swing states or senators whose party affiliations differ
83
Table 5.5:
Top members from group 16,
the 108th Senate,
by lift (high to
low)
Rank
Senator
1
Breaux (D-LA)
2
Nelson (D-NE)
3
Chafee (R-RI)
4
Landrieu (D-LA)
5
Specter (R-PA)
6
Collins (R-ME)
7
Snowe (R-ME)
8
Baucus (D-MT)
9
Lincoln (D-AR)
10
Bayh (D-IN)
from the primary affiliations of their states.
Many have reputations for taking
moderate stances,
perhaps due to the political
pressures of their electorates.
This trend was observed in the other years,
as well;
overall,
an average of
57.3% of
the senators in the top ten of
the moderate groups had parties at
odds with their states,
as opposed to 10% of the senators in the bottom ten
of the moderate groups.
It is also worth noting that the only Independent, Jeffords (I-VT), was
grouped with the Democrats (14), and that Miller (D-GA) was grouped with
the Republicans (3),
making him the only senator to completely cross party
lines.
84
5.3.2.2
Interpretation of Document Factors and Comparison with
C-GPPF
In our work the topics, or document factors, correspond to a factorized
Y matrix.
In this section we explore the J-GPPF topics from the 108th Senate,
and compare them to the topics identified with LDA [19] and C-GPPF.
For the Poisson factorization models, the top topics were selected from
the factor strengths r,
and the top words per topic were selected from the
highest valued words in β.
Unlike with the network rankings,
there was no
need to normalize β.
LDA topics were obtained with the Gensim Python
package [65],
with hyperparameters α and η set to a symmetric 1/K,
and
K = 10 topics.
LDA results are listed in Table 5.6.
Overall,
there was little differen-
tiation between the topics,
which suggests that the LDA model did not have
enough documents for a strong inference.
C-GPPF topics are listed in Table
5.7,
and are more coherent than in the LDA run,
with clear themes such as
defense,
Iraq,
Medicare,
and finance.
J-GPPF topics are listed in Table 5.8.
The J-GPPF model produced topics that were close to C-GPPF, though with
a more clearly defined Energy topic (7).
5.3.2.3
Evaluation of Topics by Group
We can also evaluate the top topics per group, as listed in Tables 5.9,
5.10, and 5.11.
The topics are derived from ψ
T
β (the group × topic distribu-
tion).
Topic numbers are based on the r ordering used in Table 5.8.
85
Table 5.6:
LDA Topics (108th Senate)
Topic # Top Words
1
pm subtitle rep intelligence dod fuel
mr
energy medicare
homeland army prescription
2
medicare beneficiaries prescription coverage energy subtitle
pm enrolled iraq months fuel dod
3
pm rep energy mr dod medicare army iraq subtitle intelligence
whole minutes
4
pm intelligence medicare homeland coverage energy mr sub-
title prescription beneficiaries iraq enrolled
5
pm medicare mr coverage rep prescription beneficiaries home-
land iraq whole dod conferees
6
pm dod energy iraq subtitle army mr navy acquisition aircraft
nuclear medicare
7
pm mr dod subtitle army energy iraq fuel
intelligence rep
homeland whole
8
medicare prescription energy iraq coverage pm beneficiaries
subtitle rep enrolled premium mr
9
pm medicare iraq intelligence energy coverage prescription
subtitle mr beneficiaries homeland reconstruction
10
medicare prescription beneficiaries coverage pm rep energy
subtitle enrolled mr covered months
86
Table 5.7:
Top C-GPPF Topics (108th Senate)
Topic # Top Words
1
dod army subtitle navy acquisition hearings energy nuclear
closure war civilian iraq
2
medicare prescription beneficiaries coverage enrolled premium
months covered pdp card price xviii
3
pm homeland mr immigration preparedness whole rep rogers
cochran ky minutes postponed
4
pm mr army navy whole dod wide burns postponed minutes
rep interior
5
iraq reconstruction iraqi freedom afghanistan deployment war
leaving debt saddam army damages
6
pm mr conferees obey hhs minutes disease whole students nih
dewine disability
7
hiv pm firearms lugar frist liability global
victims definition
crime tn abortions
8
dividends
grassley baucus
exclusion renewal
medicare
de-
ductible jobs fair mary reconciliation santorum
9
fuel deductible taxable taxpayers stock company grassley sub-
title transactions quot gain shelters
10
rep pm forest fuel conferees rept judiciary hazardous miller ga
environmental forestry
87
Table 5.8:
Top J-GPPF Topics (108th Senate)
Topic # Top Words
1
pm homeland mr preparedness whole immigration rep rogers
cochran minutes ky disaster
2
iraq reconstruction iraqi
freedom afghanistan deployment
leaving war debt hussein army damages
3
pm mr conferees obey hhs whole students minutes disease
dewine nih postponed
4
dod subtitle army navy acquisition hearings energy iraq clo-
sure nuclear civilian does
5
fuel
deductible taxable taxpayers company subtitle grassley
stock transactions quot gain recommit
6
rep pm forest mr burns fuel interior minutes sought postponed
taylor udall
7
energy fuel electric gas subtitle efficient power hydrogen lease
nuclear renewal oil
8
dividends
grassley baucus
exclusion renewal
medicare
de-
ductible jobs accelerate mary cuts fair
9
internet firearms disability definition lugar tn frist victims li-
ability crime parent peace
10
pm army navy dod wide aircraft mr iraq whole intelligence
lewis marine
88
Table 5.9:
Top topics for Republicans (group 3, 108th Senate)
Topic # Top Words
10
pm army navy dod wide aircraft mr iraq whole intelligence
lewis marine
6
rep pm forest mr burns fuel interior minutes sought postponed
taylor udall
20
airport aviation faa aircraft passenger carriers flight certifica-
tion traffic noise capacity environmental
3
pm mr conferees obey hhs whole students minutes disease
dewine nih postponed
16
intelligence terrorist homeland subtitle terrorism germane dhs
fbi visa travel communications liberty
Table 5.10:
Top topics for Moderates (group 16, 108th Senate)
Topic # Top Words
5
fuel
deductible taxable taxpayers company subtitle grassley
stock transactions quot gain recommit
16
intelligence terrorist homeland subtitle terrorism germane dhs
fbi visa travel communications liberty
8
dividends
grassley baucus
exclusion renewal
medicare
de-
ductible jobs accelerate mary cuts fair
19
highway motor
fuel
carriers
hazardous
registration driver
roads surface excise traffic bridge
10
pm army navy dod wide aircraft mr iraq whole intelligence
lewis marine
89
Table 5.11:
Top topics for Democrats (group 14, 108th Senate)
Topic # Top Words
8
dividends
grassley baucus
exclusion renewal
medicare
de-
ductible jobs accelerate mary cuts fair
1
pm homeland mr preparedness whole immigration rep rogers
cochran minutes ky disaster
11
prescription coverage beneficiaries enrolled premium months
grassley pdp choice baucus card medicare
9
internet firearms disability definition lugar tn frist victims li-
ability crime parent peace
14
pm conferees mr consumer rep postponed moved announced
put demanded conclusion speaker
Overall the topics tend to follow the traditional priorities of the parties.
In particular the Republicans have a number of top words related to defense
and homeland security, as well as words relating to the FAA and aviation.
This
type of language is notably absent in the top words for the Democrats, and is
replaced with more domestic concerns such as medical care, immigration, and
crime.
The Moderate top words take a little of each,
combining the defense
topics with infrastructure, medicare, and finance.
5.4
Discussion
The breadth and variety of data in the Congressional
Record allowed
us to explore the J-GPPF model
in a practical
setting,
and to describe the
utility of
Poisson factorization in topic modeling.
It allowed us to develop
tools for interpretation of the resulting factors, such as the lift metric and the
group-topic distributions, and it illustrated a case where Poisson factorization
90
improved on baseline models like LDA. Through comparisons with the related
network-only and corpus-only models,
I was able to demonstrate the added
value of
side information in the form of
“bag of
words”
documents.
While
some of
the insights the model
provided were expected,
such as the overall
polarization of
the U.S.
Senate,
there were a few surprises as well,
such as
how closely the senator rankings correlate with presidential
campaigns.
In a
similar manner, one could look at the topic trends over time, and see how they
correlate with world events.
All
of this bodes well
for the applicability of J-
GPPF in other political science questions, and any field with multi-relational
data like the Senate corpus.
91
Chapter 6
Distributed Adaptive Hierarchical Clustering for
Streaming Data
Although functional distributed computation frameworks have seen in-
creased adoption in both research and software development communities,
standard libraries such as Apache Spark MLib [5]
currently offer only para-
metric clustering methods.
However, the so-called "lambda architecture" [51]
data architecture has been widely adopted in industry to mitigate throughput
and latency issues arising from online data,
which is the predominant form
of
data in production data systems.
These issues have become increasingly
pressing due to instrumentation and new technologies, such as the Internet of
Things (IoT).
We developed an adaptive hashing method to aggregate data elements
in a lambda architecture framework for online data.
A key feature of
the
method is that during each iterative mini-batch, a representative data element
is defined for each data aggregate,
or cluster.
Subsequent mini-batches only
need to access the representative element, saving both memory and processing
time.
The size of the hash defines a maximum number of clusters or groups,
92
unless a hash expansion scheme is adopted,
such as the widely-used b-tree
described originally in [32].
Although there has been some recent work on
distributed b-tree indices, (e.g.
[48]) we propose a different approach.
The main contributions of this work are as follows:
• Adaptive grouping (clustering) of data elements in sequential Mapreduce
frameworks
• Hash function mechanism enabling use-defined or
application-defined
similarity metric for grouping data
• Efficiency similar to locality sensitive hashing (LSH),
or linear in the
amount of data and number of groups
• Reduction step in each mini-batch replaces each group with a single
data element,
thus keeping data overall
data growth in the clustering
step lower
• An adaptive similarity hash method enabling hierarchical
clustering in
distributed and streaming computational framework
6.1
The Proposed Adaptive Grouping Method for Stream-
ing Data
6.1.1
Motivation
Modern research and business information systems facilitate bringing
together all
data together into a common storage system,
regardless of
pre-
93
defined schemas and structures.
Previous generations of
IT and database
technology and related algorithm execution and data processing engines, such
as the well-know three-level
database architecture shown in Figure 6.1,
re-
quired pre-defined data schema and separate compute nodes for data storage
and data processing and analytics.
Importantly,
strict rules regarding data
schemas tended to prevent the intermingling of related data, forcing algorithms
to query from multiple systems,
as well
as often limiting the ability to store
the results of analysis with the source data.
In addition, the query - download
- process - analyze - repeat
cycle favored batch-mode algorithms,
which until
recently dominated the focus of both researchers and software utilities.
However,
much data nowadays arrives in online or steaming fashion.
For many research and industry purposes,
the trend has been clearly in the
direction of more streaming data,
and this trend has been accelerating with
increased instrumentation and related event logging such as the Internet of
Things (IoT) and other forms of "smart" technology.
In addition,
there has
been a progression of technologies to address growing data storage and pro-
cessing needs, from the data warehouse to the data lake to various cloud-based
or -enabled technologies.
This progression of data management technology has
led to the adoption of paradigm of data locality,
or storing data on the same
compute nodes used for processing and analytics.
This approach was pioneered
by Google and the open source Hadoop project, and had been adopted widely
by the business community.
Unfortunately,
some elements of
the research
community have been slower to make the transition, possibly because the data
94
locality paradigm is predicated on having very large amounts of data and oper-
ating in an environment in which data is arriving regularly if not continuously.
Many research environments use data gifted by institutions or industrial part-
ners, so the data is not connected as in a production environment, and having
multiple compute nodes can be a luxury.
Nonetheless, there is a clear need for models that can operate on stream-
ing data in a distributed compute environment, as the existing set of methods
(as of the time of this writing) is thin.
The key characteristics required are:
• Not limited in the number of groups, elements, classes, etc., since unless
a particular application limits this,
new data will
typically inject new
items over time
• Functional implementation, versus imperative, with immutable data types.
Distributed computation requires functional coding style with no looping
and no mutable data types.
See below for a discussion.
• State-ful handling of model parameters.
Model factors, or state, should
be tracked between processing iterations so that new data can be pro-
cessed without requiring access to prior data
A complete discussion of functional and state-ful streaming algorithms
is beyond the scope of this dissertation.
Many algorithms can be "ported" to a
functional style with some effort.
Others, such as MCMC inference cannot, as
there is an inherent sequence to the method.
Nonetheless, algorithms employ-
ing MCMC inference, and others with strict sequential dependence, can often
95
Figure 6.1:
3-Tier Database Architecture with seperate storage and processing
[76]
still
be run on multiple compute nodes.
As mentioned previously,
data can
be partitioned on the compute nodes,
and the sequential
method can be run
independently on each node.
The way in which the separate model results are
reduced to a single model inference for the entire set will be model-dependent.
Often in large data systems,
logged events need to be bin-ed or orga-
nized into clear groups as a first-order organization and reduction of the data,
with additional hierarchical structuring or analysis following the initial group-
ing.
In such cases,
the initial
groups can be hard as they form the basis for
additional
organization and can be based on clear structure in the data.
For
example, in arriving telemetry data from IoT devices, a device ID can be used
to group log events from each device into a device record.
Or log events from
96
a medical
device can be grouped according to a course time bin such as day
or hour (depending on the logging rate.)
6.1.2
Problem Setting and Online Data Characterization
In this chapter we describe a method for adaptive clustering of online
data in Spark.
Although the method is presented in Spark,
it is generic to
Mapreduce frameworks.
We make use of a variant we call equivalence hashing
(EH) as this was appropriate for our data, but as we will explain, LSH could
be used instead.
EH differs from LSH in that M is a pseudo-metric space, in
which we define d to be a degenerate distance metric.
In the proposed hash-based Mapreduce clustering method,
the map
process defines the clustering operation via a similarity metric, and the reduce
operation defines a representative vector or state vector.
This allows the clus-
tering to form new groups in an open-ended manner, and to add new elements
to the cluster as new data arrives.
Thus, this method can be used in batch or
mini-batch lambda settings.
A few common types of online data types are listed in Table 6.1.
We
will
consider web server access logs (request logs) as an example and in the
experiments (Section 6.2) in part because many people are familiar with web
browsing, which is one way to generate these log entries, so the discussion can
be substantive.
Others types of
activity that can trigger request log entries
include activity by Internet search engines and other web crawlers.
Depending
on server configuration, the request log format can contain a variety of fields.
97
The Apache Common Log format is often used [2].
The minimal attribute set
in a request log is listed in Table 6.2
Consider a request log composed of the following six fields or attributes:
time stamp, IP address, User agent, session tag, request url, return code.
First-
order grouping can be performed to collect log entries for request activity
from the same user or activity for a compact set of events, such as one would
commonly associate with a web browsing session.
For automated activities like
web crawling,
this definition may be less applicable,
although many crawlers
group requests into compact sets of activity similar to a user session.
In fact,
this sort of activity grouping is so foundational to web activity processing and
analysis that is has a name:
sessionization.
Sessionization is facilitated for some types of web site activity where the
web server can assign the browser a session tag,
as indicated in the example
in Table 6.2.
However, the log will contain potential a large number of entries
that must be re-ordered and organized by session tags for subsequent analysis
and processing.
For example, a typical web request will yield many subsequent
requests for images, style sheets, and other data required to display the page.
So even a modest server with 100,000 average daily users with an average views
of 10 per day can have over 10 million requests in the request log, which must
be sessionized into 100,000 sessions.
In the case in which sessionization can use a field in the log attributes,
the session tag can be used.
However,
the number of
groups is not known
in advance and in fact increases over time.
Also,
it may be possible for data
98
Data Source
Target
Format
Rsyslog
various computer sys-
tem status messages
plain text rows in sys-
log format
IRC
chat communications
plain
text
IRC for-
mat (decrypted if nec-
essary)
Web server log
Web site user requests
plain
text
Common
Log Format
Table 6.1:
Typical online data types
to be received at a later time that should be grouped into a session that was
initially processed at an earlier time.
In addition,
the grouping process may
need to accommodate scenarios where the session tag is unavailable or multiple
attributes must be used for grouping.
Finally, the sessionization process should
be able to process log entries on streaming data and on a cluster of processing
nodes so that the method can scale to large log sizes an numbers of sessions.
6.1.3
Equivalence Hashing
Consider the scenario in which one wishes to sessionize entries in a re-
quest log of the example format specified in the above section, using distributed
and streaming processing.
We define an attribute subset:
A
0
⊆ A
(6.1)
where in the ongoing example
A = {time, IP address, U ser agent, session tag,
request U RL, return code}
99
Request
Log
At-
tribute
Description / Example
Time stamp
2017-03-13 10:04:08.410
Request source identi-
fier
identifier for requester
IP address
172.16.254.1
User agent
Mozilla/5.0 (iPad;
U;
CPU OS
3_2_1 like Mac OS X; en-us) Ap-
pleWebKit/531.21.10
(KHTML,
like Gecko) Mobile/7B405
Session tag
SID:ANON:www.w3.org:j6oAOxC
WZh/CD723LGeXlf-01:34
Request URL (Univer-
sal Request Locator)
GET /apache_pb.gif HTTP/1.0
Return code
200 (success)
Table 6.2:
Typical online data types
A
0
= {session tag}
(6.2)
One way to sessionize logs is to loop through all entries in the log, and
for each entry, loop through the remaining entries to find a match.
However,
not only would this be an O(n
2
) process, where n is the number of log entries.
Not only does this scale poorly as the log size grows, but such nested looping
cannot be spread across multiple compute nodes.
In this case, the session tag is already in the form of a hash.
If it were
not, or if multiple fields were being employed, a mapping could be used of the
form:
h

A
0
0

− h

A
0
1

=

0 if f A
0
0
, A
0
1
element − wise match
1 otherwise
(6.3)
100
Once the attribute subset fields are hashed (if needed),
we propose a
degenerate quasi-distance metric as follows:
d(x
(i)
, x
(j)
) =
(
∀a ∈ A
0
| x
(i)
a
= x
(j)
a
: 0
∃a ∈ A
0
| x
(i)
a
6= x
(j)
a
: 1
(6.4)
where the distance between two entries in a log is either 0 or 1 as a function
of
the identified attribute subset of
each.
If
all
attributes in the subset are
equal, then the distance is 0, otherwise it is 1.
This quasi-distance defines an
equivalence relation.
Previous work [39] [39] [79] has described the LSH method for defining a
hash function on data elements for nearest-neighbor clustering.
We adapt this
method by replacing the h() permutation hash with the equivalence relation
defined above to define an Equivalence Hash,
(EH).
Each data element that
arrives to the compute nodes in a streaming context is assigned a hash com-
puted on the attribute subset - which could be a concatenation of the attribute
subset.
Using the ongoing example, using session tag only, the hash would be
just session tag.
Using session tag and date (for course time binning),
the
hash would be session tag & date.
LSH has been shown to have a time complexity of O(nd) where n is the
number of
data elements and d is the dimensionality of
the attribute space.
EH has the same time complexity.
101
6.1.4
Adaptive Sequential Hashing
As useful
as EH is for being able to keep up with streaming data,
it
has limited use, in that only one level of grouping is possible and the grouping
method mediated through the equivalency metric is static.
We would prefer an
adaptive grouping.
In addition,
although the number of groups can increase
as new data is observed,
since the hash size is fixed,
in fact,
the groups are
being filled in as new data is observed, rather than new groups being created.
As noted earlier, extensive work has been done on extensible hashing, but the
adaptation of this to distributed computation is a new and unfinished area of
work ([48]).
Although the EH method has the advantage of
accuracy and speed
with respect to the selected feature subset A
0
, we propose an extension using
a larger feature subset, such as:
A
00
= {time, IP address, U ser agent}
(6.5)
As in (6.3) we construct a hash on the set of attributes.
This can use a
concatenation of the attributes, for example.
Unfortunately, clustering based
on a concatenation of these fields would not be adaptive or flexible in any way.
One alternative is to use the data mining workhorse k-means.
In particular,
the optimal
seeding method k -means||,
[15],
is available within the Apache
SparkML library for use in Mapreduce.
This method provides an O(1) seeding
over nodes.
The drawback to this method is that it is not congruent with a
streaming processing model.
102
One option is implicit in the lambda architecture,
which is to periodi-
cally batch process some window of accumulated data, then to use the results
of the batch process to configure, or re-configure streaming processing.
To be
clear,
streaming processing cannot involve any iterative steps over the com-
pute nodes.
It might be possible with creative configuration to perform 2 or
3 Mapreduce steps over a sufficiently large compute cluster,
but inherently
iterative algorithms such as k -means||
cannot be performed on streaming
data.
The solution we advocate is to use a batch-mode execution of such an
algorithm to set a hashing function for subsequent streaming operation, until
the next batch is run.
This allows the hash function to chance periodically,
while still
enabling the hash-based pseudo real-time operation of
streaming
processing.
This is,
in fact,
the purpose of
the lambda design:
fast stream-
ing processing with periodic or occasional
batch processing that may update
parameters of the streaming processing.
What we need is a function that will map new multi-variate values to
the correct group (hash) in streaming mode, given the results of a batch pro-
cess, k -means|| or some other distributed and multivariate clustering method.
Note that we will later recommend an alternate to k -means||, for a number of
reasons, most notably that the published work on fast-seeding for distributed
k-means favors methods with pre-specified clusters, by necessity, which is not
a luxury for most realistic streaming data sets.
Given a clustering from a batch process,
we want to express this as a
103
mapping from the attribute subset to group identifiers:
∀a ∈ A
00
: M

A
00

∈ {G}
(6.6)
where G is the set of group identifiers, which we would like to be hash values,
to use as Mapreduce keys.
This is simple enough for a values from the set
of data used in the batch processing.
However,
during subsequent streaming
processing, other values are likely to be observed.
These should be mapped in
a reasonable way, to close groups, and not a null
group, for instance.
This is
the new data problem
6.1.5
Hierarchical Grouping
The solution to the new data problem in adaptive hashing is to take
advantage of hierarchies in the data organization.
Consider:
if the mapping
in (6.6) mapped all attribute sets to a single group,
this would be equivalent
to mapping to the root node.
At the other end of
the spectrum,
using a
given labeling for a degenerate non-adaptive grouping,
as in the case of
the
session tag example given earlier,
is equivalent to mapping to the leaf nodes.
Many data have some structure in between,
which defines a hierarchy.
Such
a hierarchy is often application or use dependent.
For example,
a web site
operator may wish to group the activity of
web site visitors or users
over
multiple visits or sessions.
Users can be grouped in a variety of ways, such as
by area of activity on the web site.
If the web site involves staged behaviors
(e.g.
purchase transactions), users may be grouped based on transaction stage.
104
Figure 6.2:
Hierarchical grouping with cuts defining a partial ordering
Alternately, users could be grouped into family sets, demographic sets, or some
other market-segmentation.
These can build up to the single root node.
Figure 6.2 illustrates a hierarchical grouping in which non-overlapping
cuts define a partial
ordering over the set of groups.
Our goal
is to define a
mapping (Eq.
6.6) using batch-mode processing from an attribute subset for
each data element arriving in streaming mode to a hash key,
or vector,
that
can be compared using Hamming distance to other hash vectors,
to form a
hierarchy of groups, as in Figure 6.2.
It is clear that k -means|| and related algorithms would not provide an
appropriate mapping function, under the assumption that during subsequent
streaming data processing there may be data that does not exactly match
the training data.
We propose a model-based grouping or clustering batch
process for two reasons:
1) the model can be constructed to yield a functional
grouping, which can be used for mapping, and 2) if the model is Bayesian, then
this could be extensible to non-stationary clusterings.
We do not explore the
105
second scenario here, but there are examples from recurrent Bayesian filtering.
Consider a simple model of the above A
00
fields in which each attribute
is stationary and equal.
Thus:
IP
i
→ IP
j
;
U seragent
i
→ Useragent
j
(6.7)
Also, consider a simple model of structure in these two attributes.
For IP, there
are 4 octets.
Typically, an organization will be granted a a range of addresses,
a ( netblock), identified by the starting number and a bit-length.
For example,
192.168.0.0/16 is a block of un-routed addresses reserved for use on internal
networks,
rather than on the Internet.
This means the address range from
192.168.0.0 to 192.168.255.255.
It may be possible to derive an empirical dis-
tribution for IP addresses reflecting allocation and use, but an a priori
distri-
bution reflecting the structure of the address space is (2
−8
,
2
−16
,
2
−24
,
2
−32
),
dividing the IP attribute into constituent 4 octets.
A similar approach can be take for the User agent field, which describes
the operating system, browser, and browser add-ons for the user system.
This
attribute can also be parsed into 3 components, with an a priori
weight being
given to each component.
In this case,
for the probability for the various
operating systems, browsers and add-ons.
If
we compose a feature vector by splitting the attribute vector A
00
into constituent parts as described above,
we can compute a straightforward
similarity between two vectors using the Jaccard similarity:
J
s
=
x
1
∩ x
2
x
1
∪ x
2
(6.8)
106
Other candicate metrics include cosine similarity hx
1
, x
2
i and Hamming dis-
tance HammingDistance (x
1
, x
2
).
Hamming distance is defined by:
D
s
= HammingDistance (x
1
, x
2
) =
X
XOR (x
1
, x
2
)
(6.9)
Hamming distance is often the best choice for comparing factor vectors.
However, based on the above discussion, we also include the weighting for the
various attribute components.
In that case, we want to define a weighted norm
as follows:
D
s
= hx
1
, x
2
i
W
(6.10)
where W is a Hermitian semi-definite matrix,
and is constructed by one of
the methods outlined above.
In the simple case of a priori
structure, W can
be expressed as a diagonal
matrix of
weights.
When constructed based on
empirical data distributions, W may be more complex.
In the proposed model, w can be calculated via empirical distribution,
set a priori,
or some other adaptive method.
We report results below using
the a priori
method so as to compare with the provided session tag.
6.1.6
Streaming Map-Reduce Implementation
Although hashing mechanisms, such as LSH, were originally created for
clustering, the Mapreduce community has adopted these to generate keys for
use in the key-value structure fundamental for data sorting in the Mapreduce
framework.
A full discussion of Mapreduce is beyond the scope of this report,
but many other references can be reviewed for a more thorough discussion,
107
including [30].
Briefly, Figure 2.7 shows the key elements of distributed com-
puting in Mapreduce.
Data is partitioned across many nodes and is processed
on the node where it is stored.
One catch-phrase of this method of distributed
processing is:
"bring the algorithm to the data." This is in contrast to ear-
lier frameworks in which data was downloaded or queried from a database or
other storage system to a separate compute system,
distributed or not.
The
first processing step is a map process which defines a key to each data element.
It is important to understand that data is processed element-wise in Mapre-
duce.
There is no opportunity to accumulate data or loop over elements at the
mapper.
This promotes a functional
implementation style which is sometimes
called vectorization and is in contrast to imperative programming types that
favor looping over individual data elements.
The next step is sorting in which
data is sorted by key to reducers, which aggregate data by key.
Reducers can
process data by value in streaming mode as they receive elements or aggregate
multiple elements,
although it is preferable for performance to process single
elements.
However,
not all computations can be performed entirely in online
mode.
As noted, LSH has been adopted by the Mapreduce community as one
method to yield key values in map processes in clustering applications.
Thus,
the data collected by reducers will have the same hash value and will be likely
to be close in attribute space.
In our proposed method, hashes are calculated
based on the EH method which ensures that data elements collected at each
reducer will
be for data that are equivalent in attribute subspace as defined
108
by the equivalence metric.
This is particularly useful
for sessionization,
in which one wishes to
group log entries that match one or more log fields exactly or in part (e.g.
time binned).
However,
as noted previously,
sessionization not only must be
performed on distributed compute nodes for scalability,
which this method
facilitates, but must also operate on streaming data.
We also propose a method for using hashing methods generally and
EH in particular for processing streaming data for clustering.
One detail that
complicates algorithm development in mapreduce that may not be obvious to
non-functional developers is that all data passed through a map-reduce cycle
must be of the same type.
Related to this, since each data element is processed
individually by the mapper process and compute nodes are managed by a
cluster manager, the algorithm designer must specify a generic process for each
data element.
One effect of this is that the simplest way to process streaming
clustering is to simply concatenate newly-arrived data with previously-received
data and process the entire data set each time.
However this has the worst-
possible scaling,
even under O(nd).
Consider a new "mini
batch" of data of
size n
0
.
The processing time would be O((n + n
0
)d).
We propose a method for capturing a set of state vectors at each pro-
cessing cycle, similar to a radar tracking system or other system in which data
is received that may update one or more previously-observed systems and/or
provide observations for new systems (states).
In the case described here, we
assume stationarity,
which means that states do not change with respect to
109
the attribute subset used for association.
It may be the case that other at-
tributes change on a per-element basis, but handling this information storage
and retrieval task is not within the scope of this work.
The content of the state vector is dependent upon how data elements
are combined in the reducer process.
For instance, if a running count of data
elements of
some other attribute is performed,
then the state vector would
require a field or attribute for this sum so that subsequent processing could
continue to update this count.
Minimally,
the state vector must include the
attribute subset used to calculate the hash.
In the basic scenario, since each mapper process processes all data items
equivalently, each time a new input data batch, the state vectors can also be
read in as input and injected into the data stream.
Since the state vector con-
tains the attribute subset used for hash value generation, the map process will
generate an appropriate hash for the injected state vectors.
Thus,
the state
vectors will
be aggregated along with any observed new data matching the
state hashes at the relevant reducers, enabling new data to be properly asso-
ciated with the proper previously-associated state, and any other information
associated with that state.
See Figure 6.3 for a diagram streaming Mapreduce
using state vectors, compared with Figure 2.7 for batch-mode, non-state aware
Mapreduce.
Note that this mechanism may be extensible to non-stationary
states; this will be discussed further as future work.
Employing state vectors for streaming data processing has an attractive
scalability profile for time complexity:
O((n
0
+s)d) where n
0
is the size of a new
110
Figure 6.3:
Streaming Mapreduce using state vectors
data mini-batch, s is the size of the state vector set, and d is the dimensionality
of
the aub-attribute set used for hashing,
as mentioned previously.
In the
worst-case scenario in which there is no state re-use across time iteration,
assuming the number of states, or groups in the data is equal in expectation,
the method would scale as O((n
0
+ st)d) where t is the number of time stamps
processed so far.
Thus, the growth in processing time is O(std) in the worst-
case,
where s << n
0
,
which yields an attractive scaling curve slightly above
linear in n
0
, an essential attribute for algorithms for streaming data.
Figure 6.4
lists the streaming state-ful equivalence hashing algorithm for non-parametric
attribute sub-space association.
111
6.2
Data and Metrics
We continue to use the running example of website request log by pro-
cessing a set of
anonymized weblog data from a service provider.
For the
purpose of
this study,
the request logs contain a superset of
attributes that
include the attributes cited above in Section 6.1.
The goal
is to sessionize
the log entries using the session tag attribute.
Approximately 146, 385 log
entries covering 117, 255 tagged sessions were fed to a map-reduce system im-
plemented in Spark as 4 map processes and an unspecified number of reducer
processes.
The data was split into 3 temporal mini-batches,
thus initiating 2
state handling cycles.
Note that the initial
cycle in this state-ful
method is
identical to a stand-alone, non-stateful batch.
In order to obtain performance results for a large amount of data.
The
relatively small
amount of
provided data was duplicated 700 times to yield
approximately one hundred million (102, 469, 500) log entries.
The session
tags in the duplicated entries were modified on each duplication to increase
the number of
sessions by an equivalent amount.
Data was processing in a
streaming fashion as 24 mini-batches.
Detailed results are shown in Figure
6.4.
Note the trend in the performance curve which is consistent with a worst-
case O((n
0
+ st)d) time complexity.
We also implemented the hierarchical method in which a attribute vec-
tor is composed of IP and Useragent and a weighted norm is composed using
a priori
feature information,
as described in Section 6.1.5.
Possibly due to
the relatively short sample size used,
there was a complete match with the
112
Figure 6.4:
Performance Results for Streaming Scaling with Trendline
113
Grouping Method
Number Groups
Label (Session tag)
117, 255
IP+Useragent Hash
119, 374
Table 6.3:
Identified groups by algorithm
session tag data.
Some of
the data did not contain a session tag,
and for
these instances,
there was no way to verify correct grouping.
However,
we
have confidence the method properly grouped this data,
as a few examples
demonstrate that the groupings that matches the session tags,
in this short-
time frame data) tended to align with IP and Useragent matches,
as did the
groupings the hierarchical
algorithm for non-session tag data.
See Table 6.3
for these results.
Figure 6.5 shows one example of two leaf nodes that can be joined at
a higher level
in the tree partitioning using the similarity hashing method
proposed here.
Note that the two sessions are from the Google Bot, which per-
petually scans web sites for indexing purposes.
There may be more members
of this "user", but these were the closest two for this data set.
Higher levels of
grouping are not possible in streaming mode for current distributed methods
such as k-means.
6.3
Discussion
We describe a method for non-parametric log file aggregation in func-
tional
distributed Mapreduce frameworks using an attribute-subset to define
a degenerate similarity metric as an Equivalency Hash.
This hash function is
114
Figure 6.5:
Example hierarchical
group formed by similarity hashing.
The
Googlebot
are
more
similar
(same
UserAgent,
similar
IP)
and therefore
grouped together.
115
used in a manner similar to LSH to map data elements to unique keys that
enable scaling linear in the amount of data O(dn).
We extend this method for
streaming data using mini-batches for Lambda architectures in Spark where
the reducer process defines a stationary state vector containing minimally the
attribute subset required to produce the hash key.
Subsequent iterations inject
the state vectors into the data stream.
For s state vectors and t time steps,
this scales worst case as O(n
0
+ st)d) where n
0
is the expected number of data
elements in a new time iteration.
Experiments were performed using web server request logs in which a
single attribute,
session tag was employed for equivalency hashing.
This ef-
fected a distributed and streaming sessionization of the request log data, which
is a common initial
processing step for such data.
The observed processing
times were consistent with the predicted worst-case time complexity.
Also,
we showed that the similarity hash can match results using the
provided session tag,
at least for a small
data sample.
Also,
using similarity
in the hash, higher-order groups can be discovered, such as multi-IP users.
The work described here was implemented in PySpark on the Apache
Spark extension to Mapreduce.
The lambda architecture is one method to
achieve streaming processing in the batch-processing oriented Spark frame-
work.
However,
as more users in industry and academics adopt streaming
data processing,
more mechanisms have become available for streaming pro-
cessing.
For instance,
instead of using the Hadoop filesystem or a database,
data can be provisioned in a memory queue tied directly to tasks by the Apache
116
Kafka component.
In addition, a newer component, Apache Flink, is designed
specifically for streaming and state-ful
data processing,
facilitating many as-
pects of keeping track of data that arrives over time, including saving states so
that the system can recover state if (and when) there is a node or other failure.
We would like to port our method from Spark to Flink to take advantage of
these state management mechanisms and to aid in transitioning the method to
production clusters to test on much larger data streams over longer run times.
A natural
extension to this work would be to handle non-stationary
states.
There is already a rich literature in this area where each group is com-
posed as a hidden Markov model
(HMM),
and each state is transitioned via
a specified state model.
Typical implementations for state transitions assume
Gaussian noise (Kalman filter) or, more generically, a recurrent Bayesian model
(particle filter).
There are many excellent implementations of these models for
continuous data.
However, for categorical and count data, such as the attribute
types considered in this work, there is significantly less prior work.
In addition,
being able to accommodate state transitions in a functional
distributed pro-
cessing context (such as Mapreduce) has been even less explored.
Nonetheless,
this would be required to accommodate non-stationary state transitions in the
context presented here.
We are exploring methods to do this, and in fact have
preliminary results for two broad types of count data:
association data,
such
as social networks;
and entity-attribute data, such as topic models and other
multi-group clustering data.
In addition,
that model
appears to also handle
the joint case where there is some association between the association data
117
and the topic data, such as is common in authorship and consumer data.
This
work explicitly builds on the EH streaming hashing method presented here,
and takes advantage of the advantageous scaling factor therein.
118
Chapter 7
Distributed and Dynamic Joint
Topic-Community Model Inference via Factor
Matching and Parameter Reconciliation
In recent years, as the majority of data infrastructures have migrated to
public cloud or private cloud-like systems such as Amazon AWS and Microsoft
Azure, the favored data processing paradigm has also migrated, from moving
data to the algorithm to moving the algorithm to the data.
Thus, distributed
processing frameworks such as Map-reduce and Apache Spark have increased
significantly in sophistication and popularity among software and systems de-
velopers.
However, the range of algorithm options available within the Apache
Spark MLib library for machine learning and data mining is limited compared
to other systems.
One reason for the strong appeal of Apache Spark in production systems
is the ease in implementing and deploying the so-called lambda architecture
[51], which facilitates processing the online data common to many real-world
scenarios, such as log and telemetry data.
Unfortunately, many current unsu-
pervised algorithms in Spark MLib, including the clustering methods k-means
and LDA,
offer non-parametric models,
whereas online data processing typ-
ically cannot specify in advance the number of
clusters or groups in a data
119
set.
I introduce a non-parametric matrix factorization method for cluster-
ing count data via Poisson factorization in distributed map-reduce frameworks.
This method substantially builds on the Gamma Process Poisson Factoriza-
tion (JGPPF) model
for association networks [11],
document corpora,
and a
joint model of both associations of users and a related document corpus [11],
all
which were inferred via Monte Carlo sampling on a single processor.
In
this work, I significantly expand the scope of the models by developing a dis-
tributed inference method for map-reduce frameworks, with an Apache Spark
implementation.
Since the J-GPPF model is non-parametric, each processing node iden-
tifies latent space parameters separate,
notionally,
from other nodes.
When
combining parameter inferences from multiple nodes that reference the same
latent factors, such as a document topic, some mechanism is required to deter-
mine latent factor similarity, allowing for possible small differences among the
inferred factor values from each node.
A novel
method for binning arbitrary
binary feature vectors in map-reduce is developed to perform this function.
This is not only fast and accurate, but can be scaled to extremely large latent
spaces as needed using locality sensitive hashing (LSH).
Once similar factors
across the distributed compute nodes are collected,
a single output is calcu-
lated, which I call parametric reconciliation.
There is also a lack of
reported work for
clustering or
grouping of
streaming data with non-stationary cluster features on distributed compute
120
nodes.
I propose a recurrent Bayesian filter model which adapts the aforemen-
tioned joint model to an HMM in which factor observations are governed by a
Poisson process and factors transitions occur via a dependent process.
In the
case of mixtures of topics and association groups, I use a dependent Dirichlet
process.
For topic keywords and group keywords,
I use a dependent gamma
process.
Hence the name of the model:
Dependent Gamma Poisson Fa Model
(D-GaP). The formulation described here is a distributed algorithm,
and the
stationary or non-dynamic version is equivalent to the initial
time increment
of the dynamic case.
The main contributions of this work are as follows:
• Distributed inference for joint Poisson factorization of topic-community
data, using parameter reconciliation
• Apache Spark implementation of the model
• Hashing method for identifying parameter vectors that should be com-
bined from multiple nodes - this provides an extensible mechanism for
extension in lambda architectures for online data where state updates to
parameters are desirable, without requiring retaining the original data
• HMM for
count
and categorical
data with Poisson observations
and
gamma or Dirichlet prior, with a particle filter posterior inference
• Apache Spark implementation for streaming inference of non-stationary
factors of the joint model, in particular topics and user groups
121
7.1
Related Work
This research draws on multiple threads of
prior work.
The primary
ones,
for which there are separate subsections below are:
the J-GPPF model
for joint inference of
factors for corpora with both interaction and attribute
count data and a mapping between these (Section 2.1.1,
and Section ??);
the Mapreduce framework for distributed computing,
which has become a
standard in industry but not yet in academia (Section 2.4);
and the use of
hashing functions for distributed computing (Section 7.1.1).
In distributed computing of data mining algorithms, three main frame-
works are employed.
The algorithm, typically a parametric inference method,
may be distributed over multiple compute nodes.
Such methods are often the
fastest, but can require the most developer effort.
For special cases such as ma-
trix multiplication, there are standard methods to make use of highly-parallel
architectures like GPUs (graphical processing units).
However, in general, this
method requires a re-implementation.
Another method can be used when a model
needs to be run multiple
times,
preferably independently,
to determine optimal
hyper-parameter set-
tings.
One example is performing a grid search for a random forest classifier
[6].
The majority of Mapreduce tasks employ the third method, which is to
run a single-node inference on data that has been distributed.
This typically
requires that some sort of consensus inference be calculated.
This step is called
122
"reduce" in Mapreduce vernacular.
The "map" process is then required to sort
the data into appropriate bins according to whatever model is being applied:
class, cluster, topic, etc.
7.1.1
Hashing for Distributed Computing
Since it is unlikely that the same latent vectors will
be inferred on
separate nodes in a distributed compute scenario,
the equivalency hashing
method described in [74] cannot be used exactly as described to group similar
latent vectors.
Our proposed method for identifying similar latent vectors is
based on making a sparse binary version of each.
We select the top c elements
by weight from each vector such that the cumulative weights of the c elements
are greater than or equal to some pre-specified threshold .
Figure 7.1 illustrates this method.
Factors with a Dirichlet prior sum
to 1.0.
Factors with a gamma prior sum to greater than one, so these must be
re-normalized to 1.0 prior to this process.
I call
this SparseBinHash() or Sparse Binary Hashing.
Given an in-
dividual
factor vector,
x
i
,
SparseBinHash() returns h
i
.
As noted above,
the
range of the elements of x
i
will depend on the prior.
∀x ∈ x
i

Dirichlet prior : x ∈ (0, 1)
Gamma prior : x ∈ (0, ∞)
(7.1)
The values of the elements of the corresponding h
i
are either 0 or 1:
∀h ∈ h
i
, h ∈ {0, 1}
(7.2)
Each factor is converted into a hash vector by the following algorithm:
123
Figure 7.1:
Sparse binary hash process for latent factors (see text)
• ∀x
i
∈ X:
•
Sort elements x ∈ x
i
, largest to smallest
•
if X of type φ or θ, Normalize elements x
•
Select
k elements x in sorted order such that
k
P
x <= ,
where  is
predefined sparsity level.
•
Set
k same elements of new vector h
i
= 1 leaving all
other elements
= 0
This algorithm can be run as a map process for each latent factor.
124
7.2
Distributed Joint Poisson Factorization
Implementing inference for the J-GPPF model
on multiple compute
nodes in the Mapreduce framework (see Figure 2.7) requires two sequential
Mapreduce processes for a batch or corpus of
data.
The formulation is as
follows:
• 1st Step
•
Partition:
Split data arbitrarily and assign to execution nodes
•
Map:
Execute J-GPPF for each data split,
as on a single-node;
four
output matrix values are generated, each with a key equal to the matrix
type
•
Reduce:
For each matrix type, form a file or RDD of all latent factors,
concatenated.
• 2nd Step (4 parallel processes, 1 for each matrix type)
•
Partition:
Split latent factor rows arbitrarily
•
Map:
Generate a sparse binary hash for each factor value
•
Map:
Generate minhash key for each sparse binary hash value
•
Reduce:
For each minhash key, calculate the parametric mean of all
vector value elements
• Output:
β
c
, ψ
c
, φ
c
, θ
c
, single central inferences for each matrix type
125
Since the output matrices are generated on separate compute nodes, on
different segments of the input corpus, the latent factors may or may not match
over multiple nodes.
In addition, even if the same input data is processed on
multiple nodes,
the ordering of
the latent factors may not be the same over
multiple nodes or even on the same node over multiple runs of
the model.
This is because the J-GPPF model is non-parametric in the number of latent
factors and more than the anticipated number of matrix rows are configured,
so that at least as many as are required will be generated by the model.
During
MCMC inference, it is indeterminate a priori in which order latent factors will
settle in the converged matrices.
Thus,
the distributed processing method must identify similar latent
factors through a matching process.
Since the latent factors are in the form
of
sparse vectors,
I adopted the methods outlined in my prior work in [74]
related to grouping data in distributed computing using feature subsets.
How-
ever, since in this case the feature vectors may not match exactly, the method
must be modified.
There are two options.
As mentioned in that paper,
one
is to use Locality Sensitive Hashing (LSH), in which a hash such as minhash
can be calculated for each vector.
Unlike other hash functions that are used
for applications like security, minhash and others in this class essentially are a
bin-ing using a similarity metric such that similar vectors are grouped together
under the same hash or with hash-es that are close in value.
Using minhash
would be required for processing extremely large input corpora, which would
yield a correspondingly large number of latent factors, thus necessitating dis-
126
tribute processing of the 2nd step of latent factor.
However,
even for relatively large corpora,
the data reduction by the
J-GPPF model is significant, such that the size of the latent space factors may
not require that the 2nd step actually be performed in a distributed manner.
Consider:
A corpus consisting of 1, 000 users with a 10% interaction rate and
100, 000 associated documents with an average of 1, 000 words per document
will have 100, 000, 000 document inputs and 1, 000, 000 network inputs, clearly
requiring parallel execution of J-GPPF inference.
However, if the model can be
configured with 100 groups for associations and topics, then the latent factors
will all be on the order of 100, 000 elements at worst.
These can be processed
on a single node.
This is advantageous when considering very large streaming
data sets,
as in Section 7.3,
as the methods described herein can be scaled
additionally by running the 2nd step on Mapreduce as described.
For non-massive data, the 2nd step can be run on a single node, and in
that case,
another method can be used to directly compare latent factors for
similarity.
As described in Chapter 6, Hamming distance is normally used for
latent factors in sparse binary form:
D
s
= HammingDistance (x
1
, x
2
) =
X
XOR (x
1
, x
2
)
(7.3)
Hamming distance can be directly calculated for the sparse binary for-
mat keys from Step 1.
It is equal
to the number of
’1’s in an XOR of
two
vectors being compared.
Although a pairwise comparison of latent vectors has
a poor time complexity of O (n(n − 1)), as noted earlier, for even moderately-
127
sized input corpora, the latent space is sufficiently small that this is acceptable
for the trade-off of computational control, versus the approximation inherent
in using minhash and other LSH methods.
Regardless of
the method used to find similar factors,
each 2nd-step
reducer obtains one or more equivalent vectors from one or more compute
nodes.
The task of
the reducer is to generate a single factor vector.
Since
the data across the multiple nodes is assumed to be sampled from the same
probability space,
each reducer input is like a sample for which we desire a
single parametric estimate.
As noted previously, each factor vector is composed
of individual elements sampled from a Poisson distribution, with a Gamma or
Dirichlet prior,
depending on the factor.
Fortunately,
we are only interested
in reducing to vector elements in expectation, otherwise the described setup of
one or a few samples would be insufficient for an accurate inference.
We consider the four matrix factors in two groups,
depending on the
factor prior construction:
Gamma-prior, for φ, θ and Dirichlet-prior, for ψ, β
E [x ∼ Dirichlet (α
i
)] =
α
i
P
n
α
n
(7.4)
E [x ∼ Gamma (α, β)] =
α
β
(7.5)
In the 2nd reduce step, the parametric mean for each latent factor set
collected by each reducer will be a set of vectors from one of the 4 matrix types
produced by the J-GPPF model.
In order to yield a single "central" inference
128
for the factor, each vector element is averaged.
The general case was explored
in detail
in [59]
and [72],
which describe a method to accommodate skewed
data partitions and other complex scenarios.
In addition,
the authors in [72]
are able to construct a provably robust algorithm for convergent distributed
inference.
However, in this case, under typical distributed computational sce-
narios,
the data is well-mixed over multiple compute nodes.
This is the case
explored in [69], which advocates for a weighted mean under such assumption.
[69]
also notes that using a uniform weighting yields acceptable approximate
convergence under typical distributed computation conditions, and as an ini-
tial
trial
that is the approach used here:
weighted mean with equal
weights
for nodes in which the data is present.
To confirm the appropriateness of the
mean,
during development,
I tried both random data partitioning and time-
wise sequential
data partitioning.
Since there is a temporal
dependence to
the topical
factoring,
which will
be discussed in greater detail
in Section 7.3,
sensitivity to the mean should have been made apparent by this partitioning
different, but none was detected.
Thus, k latent factors, v, or values are reduces to a single central value,
x
c
by the following function, for each of the i elements:
s
c
i
= MLE

x
1
i
, ..., x
k
i
(7.6)
Note that for the φ and θ matrices,
which have a Gamma prior on
elements, the MLE (maximum-likelihood estimator) is equal to the geometric
average.
For the β and ψ matrices, which have a Dirichlet prior on elements,
129
there is no closed-form MLE, but the geometric mean approximates.
7.3
Dynamic and Distributed Joint Poisson Factoriza-
tion
Figure 7.2 shows the factor model
used for the dynamic joint model.
This is a typical
hidden Markov model
(HMM).
In this case,
the factor evo-
lution f
t
is not based on the usual
Gaussian distribution,
but rather either
the Gamma distribution or Dirichlet distribution,
depending on which factor
is being evolved.
Figure 7.3 shows how the dynamic model
is implemented algorithmi-
cally in Mapreduce.
Note that this shows the 2nd Mapreduce iteration in
each calculation step which is where the latent factors are handled.
(The 1st
Mapreduce iteration is where the raw input data are input into the J-GPPF
model
to yield latent factors in the map and reduced to stacked matrices for
the 2nd step.
See Section 7.2 for more details.)
At the beginning of the 2nd
step in the dynamic model, factors must be predicted for the factors inferred
in the prior step.
We employ a particle filter to accommodate the dependent
gamma or dependent Dirichlet distribution process as needed for each factor.
Note that this is similar to other dynamic Bayesian models,
although many
of these employ a Kalman filter by representing the temporal dependencies as
a Gaussian process or by performing a change of variables in order to use a
Gaussian.
See for example [18].
The generated factors are merged with the newly observed factors to
130
Figure 7.2:
HMM model for dynamic factors
sequence into a similar map process as used in the distributed computation
method described above.
During sparse binary hashing, a hash value is calcu-
lated for each factor, using as many elements as required based on an entropy
metric.
Continuing parallels to the distributed processing method, the reducer
groups by hash and performs a parametric mean to yield a single latent factor,
which is the new state for the factor for the current time increment.
In order to properly implement the recursive filter outlined above, dur-
ing reduction, if a new factor is grouped with a factor predicted from a prior
state, a weighting is applied to the predicted factor, based on the predict equa-
tion.
For this purpose, I modified the code from the distributed processing for
dynamic processing to add a flag to the factor data schema to keep track of
whether a factor is derived from newly-observed data or predicted from prior
factor data.
131
Recurrent Bayesian Filter (RBF) Factor Evolution Process:
x
t
∼ f
t
(x
t−1
) = p (x
t
|x
t−1
)
(7.7)
Observation Process:
z
t
∼ h
t
(x
t
) = p (z
t
|x
t
)
(7.8)
The key cyclical
process of the RBF is to use the previous update to
make a prediction, then to use that prediction to make a new update, and to
repeat.
P redict : p (x
t−1
|z
1:t−1
) → p (x
t
|z
1:t−1
)
.
(7.9)
U pdate : p (x
t
|z
1:t−1
) → p (x
t
|z
1:t
)
Predict factor forward and deform with noise:
p (x
t
|z
1:t−1
) =
X
x
t
p (x
t
|x
t−1
) p (x
t−1
|z
1:t−1
)
(7.10)
where the factor evolution process is a dependent Gamma or Dirichlet process,
depending on the factor type:
p (x
t
|x
t−1
) ∼ Gamma

x
t−1
,
1
c
t

(7.11)
p (x
t
|x
t−1
) ∼ Dirichlet (x
t−1
)
(7.12)
132
Figure 7.3:
Dynamic model inference algorithm in sequential mapreduce
(See [9] for a discussion of the dependent gamma process prior in the context
of
the underlying mathematical
Poisson process model.)
Update prediction
with observed data:
p (x
t
|z
1:t
) =
p (z
t
|x
t
) p (x
t
|z
1:t−1
)
P
x
t
p (z
t
|x
t
) p (x
t
|z
1:t−1
)
(7.13)
where the observation process is unspecified by the JGPPF model, so we take
it to be a Gaussian noise process:
p (z
t
|x
t
) = N ormal (µ, σ) .
(7.14)
7.4
Experiments
The data use is the U.S.
Senate voting and associated bill
summaries
described in Chapter 5
7.4.1
Scalability
Table 7.1 shows mean execution time in hours for the distributed exper-
iments run for this work.
The distributed experiments were run for two data
133
sizes, a single session, or two year data set, and a dual consecutive session, or
four year set.
The single session set was run on a single node and on two nodes.
The dual
session set was run on a single node and on four nodes.
The run
times are reported as the average of three separate runs in hours.
Note that
the multiple-node runs were configured slightly differently:
the MCMC model
was set for 25% fewer total iterations.
This was done because as described, by
splitting the data over multiple nodes, the model required fewer iterations to
achieve equilibrium.
The data shows that the model,
as expected,
has a time complexity
linear in the number of
(sparse) data rows and in the number of
iterations
run in the MCMC inference.
Thus, the dual session data takes approximately
twice the time to run as the single session data on a single node.
Also, splitting
the data over multiple nodes and reducing the MCMC iterations by 25% yields
a proportionate decrease in runtime.
Prior work with this model
[75]
described how the model
is able to
converge on interpretable and stable network groups and document topics
with less data than other models.
Distributing data over multiple nodes has
the effect of reducing the size of the document corpus input to the model.
I did
not run extensive experiments on corpus smaller than one year per node, but
this may approach a lower bound on a sufficient data size.
For this data, this
is approximately 150 documents and 5000 words with 5 - 10 topics,
roughly
speaking.
(See below for a detailed review of the data.) Note that the model
has successfully been run in other, unreported work on document corpora with
134
Data
Setup
Mean
Execution
Time (hrs)
Senate 108
1 x 1
2.69
Senate 108
1 x 2
0.76
Senate 107 + 108
1 x 1
4.86
Senate 107 + 108
1 x 4
0.73
Table 7.1:
Runtime Scaling for Experiments
as few as approximately 50 documents and 5 topics, but as with any statistical
model, one’s mileage may vary.
7.4.2
Single Session
Table 7.2 shows most probable keywords in order of
probability for
topics inferred by the J-GPPF model run on the Senate-108 data on a single
compute node.
Table 7.3 shows the same information where the inference is run
on two compute nodes.
The topics in Table 7.3 are the final results, in which
any topics inferred on multiple nodes are reduces to a single common topic,
as described above,
via parametric averaging.
The topics in the two tables
are aligned to show matching topics from the two computational methods.
In
most cases the most probable words match closely or exaclty.
One notable
case is Topic 12 from the single-node case for which there is an apparently
poor match in Topic 1 in the multiple-node case.
Considering a larger set
of
probably keywords,
Topic 12 appears to be about firearms,
crime,
and
abortion, possibly issues on the Judiciary Committee; and Topic 1 is appears
to be about abortion and the Judiciary Committee.
For the other principal
topics, the agreement in keywords is much more clear, if not equivalent.
Note
135
Topic
Keywords - Most Probable Words
Topic 0
homeland mr
preparedness
whole
immigration rep rogers
cochran minutes ky postponed
Topic 3
mr conferees obey hhs whole minutes students disease dewine
nih moved postponed
Topic 4
reconstruction iraqi
freedom afghanistan deployment leaving
war debt hussein army damages coalition
Topic 6
pm mr forest postponed conferees whole minutes sought fuel
announced interior burns put
Topic 11
deductible taxable taxpayers grassley company subtitle stock
transactions quot loss recommit repeal
Topic 19
fuel electric gas subtitle efficient power hydrogen lease nuclear
renewal oil alaska prescribe
Topic 12
pm liability firearms definition victims tn crime frist injury
abortions judiciary message
Topic 1
army navy dod wide aircraft mr whole iraq intelligence marine
lewis aside
Topic 16
hiv mr whole global
rep lugar minutes kolbe malaria peace
debt tuberculosis
Topic 15
prescription beneficiaries coverage enrolled premium months
Topic 2
terrorist
homeland subtitle terrorism germane dhs fbi
visa
communications travel liberty border
Table 7.2:
Topics for Single Session, 1 Node
that the size of the corpus used for inference on the individual
nodes in the
multiple-node case is 1/10 the size of
the minimum corpus size required to
achieve qualitatively-meaningful topic convergence for this data using Latent
Dirichlet Allocation, LDA, the widely-used and well-regarded topic model, as
reported in our prior work [75].
Similar results, qualitatively were obtained for the dual-session dataset,
which combines Senate sessions 107 and 108, which span 4 consecutive years.
136
Topic
Keywords - Most Probable Words
Topic 7
pm homeland mr rep preparedness whole immigration rogers
cochran ky minutes disaster
Topic 25
pm mr conferees obey hhs whole students minutes disease
dewine nih postponed
Topic 2
iraq reconstruction iraqi
freedom afghanistan deployment
leaving debt hussein war army damages
Topic 26
pm mr burns forest interior postponed whole water taylor rep
sought park
Topic 9
fuel
deductible taxable taxpayers company subtitle grassley
stock transactions quot recommit energy
Topic 34
energy fuel electric subtitle gas efficient power hydrogen lease
nuclear prescribe renewal
Topic 1
quot energy justice conservation university earmarks export
kaptur tobacco rural disability abortions
Topic 29
pm army navy iraq dod wide marine aircraft intelligence mr
whole lewis
Topic 27
hiv pm lugar global rep peace malaria tuberculosis whole debt
un minutes
Topic 32
prescription beneficiaries coverage enrolled premium months
covered pdp card price xviii medicaid
Topic 3
intelligence terrorist homeland subtitle terrorism germane dhs
fbi visa travel liberty communications
Table 7.3:
Reduced Topics for Single Session, 2 Nodes
137
Figure 7.4:
Topic merging example from experiments in Sec 7.4.3
In this case,
the multi-node inference was split over 4 nodes,
and the final
reduced topics yielded a similar match to the single node topics,
although
the computational
speed ratio was approximately twice as fast as the single-
session / 2-node scenario, as noted in Table 7.1.
However, we noted that the
final topics did not provide as useful a summary of the copora of work of the
Senate over the modeled four years.
In fact,
the final
topics lacked some of
the topics found in the single-session model.
I posited that this was due to grouping all four years of documents into
a single corpus,
whereas the work of the Senate over that time,
year-to-year,
may have consisted of more subtle topical content when modeled on a smaller
time scale.
I show results for modeling four individual
corporal
of votes and
documents in the next section,
Section 7.4.3.
However,
a set of
individual
annual models is of limited value to the researcher.
Thus, as described above,
in Section 7.3, I developed a dynamic version of J-GPPF for use in distributed
compute environments so that stateful updates can be made to latent vectors
138
in each matrix:
β, φ, ψ, θ
7.4.3
Dynamic Data - Two Consecutive Sessions
Table 7.4 shows topics inferred for 2 consecutive sessions, 107 and 108,
processed as a single corpus.
Although not obviously a poor result,
these
are not semantically the topics one would expect given an understanding of
the work of the 107th ad 108th congress.
For comparison,
see the results of
inferring topics for each year independently, listed in the Appendix, Table A.1.
Table A.2 lists the topics inferred by the dynamic model.
Figure 7.4
shows a detail
for one topic in the dynamic model
that was seen to evolve
from an earlier topic and merge with new corpus data in the current time
increment.
The dynamic model
as run yields an annual
set of
topics that
provides the granular detail of the independently-run batches, while providing
temporal context and dependence.
Topic
Keywords - Most Probable Words
Topic 6
estate teachers deduction elementary secondary bracket sub-
title esea reauthorization jeffords phase marginal
gifts par-
ent trust beginning higher wellstone taxpayers pension death
kerry professional taxable leas
Topic 5
whole homeland rep minutes postponed sought sustained lee
announced rogers jackson immigration ky preparedness con-
stitution put conclusion prevailed disaster
Topic 2
electric gas renewable efficiency power vehicles nuclear oil sub-
title hydrogen alaska transmission consumer murkowski craig
generation laboratory
continued on next page
139
continued from previous page
Topic
Keywords - Most Probable Words
Topic 17
army mr supplemental military inouye procurement navy dod
whole aside rescinds guard aircraft chapter wide obey lewis
minutes court young attacks corps intelligence columbia ma-
rine
Topic 9
deduction taxable taxpayers dividends exclusion stock sub-
title companies baucus jobs transactions paid vehicles quot
loss partnership excise renewable shelters gain holding chuck
recommit santorum refundable
Topic 4
rural
justice conservation export lugar alaska guarantee stu-
dents earmarks commodity bennett defense food mcconnell
regional
whole veterans quot director water democracy mili-
tary peace bank commerce
Topic 15
prescription beneficiaries coverage enrolled hospital premiums
month adjustments card pdp price
Topic 14
issuer patients criminal frist audits attorney damages tn fraud
professional
victims
firearm brought
claims
court
genetic
gramm allard group companies crime devices desk william
brownback
Topic 7
jr forest phil
gramm agriculture ga jim nc rept w fuel
ok al
tn recommit ky michael
railroad ar miller nh unemployment
mike minutes judiciary
Topic 19
candidate voters campaign political ballot communications in-
ternet parties feca disbursements registration uniform
Topic 8
labor conferees obey whole hhs students minutes director dis-
ease nih dewine abuse postponed secondary regula phsa ele-
mentary workforce sought rep global
Topic 12
negotiated customs duties tariff labor free taa adjustments ar-
ticles rep firms agriculture ernest certification principal com-
petitive beneficiaries jobs andean
Topic 10
reconstruction
military
iraqi
defense
freedom
armed
afghanistan deployment leaving duties war debt
continued on next page
140
continued from previous page
Topic
Keywords - Most Probable Words
Topic 1
bankruptcy debt chapter court claims consumer trustee cred-
itors estate
Table 7.4:
Topics for Dual Sessions, 1 Node
7.5
Discussion
This work described a novel
method for inferring Bayesian models of
count data on distributed compute frameworks by reconciling similar factors
from multiple nodes into a single central
factor inference.
This mechanism was
extended to serially-processed or sequential
data by making use of a Hidden
Markov Model abstraction in which the factor matching mechanism developed
for distributed nodes is used to match factors over time.
Each previously-
computed factor must be advanced forward via a predict
step based on the
appropriate distribution model.
In the case studied here, the joint community-
topic model (J-GPPF), factors are predicted forward using either a dependent
Dirichlet or dependent Gamma distribution.
Results comparing single-batch runs on single nodes and multiple nodes
compare closely.
Results comparing extended time periods run as a single
batch to those run as a sequence of smaller batches show significant differences.
The dynamic inference model
yields results similar in quality to the single-
batch case, but with topic continuity and "blending" over multiple time periods
for topics that span multiple batches.
An interesting extension to the model
would be to include temporal
141
dependence in the Gamma process prior in the scale parameter.
This would
enable the recurrent Bayesian filter to be fully fleshed out to include an ex-
panding and contracting "gate" similar to the variance gate in the Gaussian
Kamlan filter.
As many researchers and developers continue to contribute to open-
source projects, the quality and sophistication of distributed computing com-
ponents continues to expand.
In particular for this work, two frameworks for
streaming processing of data with factor objects have become more robust and
accessible to new algorithm researchers.
These are the Apache Fink [3]
and
the Amazon Kinesis streams [1]
components.
I would like to implement the
distributed and dynamic inference methods in one or both frameworks to take
advantage of factor handling features offered,
including fault-tolerance.
It is
unclear if either of these systems can currently accommodate non-stationary
factor keys.
If not, this could be another area for future work, as the dynamic
model
offers a potentially robust way to handle key-evolution for count and
categorical data.
Some of
our colleagues have suggested that the joint model
may be
useful
in modeling various industry data,
such as consumer-item data and
other behavioral
data sets.
One block to experiments in the past has been
large scale of
such data and the streaming nature of
such data.
The work
reported here solves both these issues, so additional future work could finally
consider some of
these interesting data that have yet to be modeled with a
fully-integrated joint factor model such as ours.
142
Chapter 8
Conclusion
8.1
Future Research
The potential next steps for this work are broadly in the areas of pro-
duction implementation and in modeling,
specifically in improving modeling
through adaptive variance in the predict step.
8.1.1
Production Implementation
As production frameworks improve for streaming and distributed data
processing, new components have been developed or extended for state-ful pro-
cessing.
In particular the Apache Fink [3]
and the Amazon Kinesis streams
[1] components are currently popular for the distributed processing of stream-
ing data using state.
One could implement the various lambda architecture
methods for tracking latent factors over sequential
data sets in one of
these
frameworks as a possibly more comprehensive approach to running the Ada-
Hash and D-GaPS algorithms.
It is unclear whether Fink can natively accom-
modate changing state / factor keys, or hashes, so this may require extending
the core framework, but these components both offer attractive features such
as fault tolerance.
143
8.1.2
Adaptive Variance / Kalman Filter "Gate"
The dynamic posterior inference method described here is like a partial
Kalman filter implementation for Gamma and Dirichlet driven state processes.
A missing feature is the ability to modulate variance, especially independently
for the Gamma process factors.
This would be an especially useful mechanism
to handle factors that are changing rapidly or are vanishing altogether.
8.2
Dissertation Summary
In summary,
this
dissertation addresses
scaling Bayesian models
of
count data, in particular the J-GPPF, or joint community-topic factorization
model,
to multiple distributed compute nodes.
In the process of
developing
a general-purpose adaptive hashing mechanism for identifying similar latent
factors for parametric reconciliation to a single factor inference,
I developed
an adaptive and hierarchical
hashing method suitable for streaming process-
ing on Mapreduce systems I call
AdaHash.
I employed AdaHash again to
process dynamic data with J-GPPF to implement a Hidden Markov model
where previous posterior factors were predicted forward to the current itera-
tion via particle filter, grouped via AdaHash, and updated to a single factor via
parametric reconciliation.
The particle filter predict method varied based on
whether the factors were based on a dependent Gamma or dependent Dirichlet
prior in the underlying model.
Results based on the U.S.
Senate voting records and bill
summaries
over the period from 2002 - 2014 demonstrated the validity of the distributed
144
processing approach and the added value of
the dynamic processing model,
over simple single-compute node batch processing.
145
Appendices
146
Appendix A
Detailed Dynamic Topic Data
The following tables provide a complete list of topics and keywords for
the four years in the dual-session data for both the single-batch inference (7.4)
and dynamic inference (A.2) models.
These results are discussed in detail
in
Section 7.4.3.
A few comments about how the data was selected.
The same
metrics were used for both models.
For each topic,
words were ordered from
highest to lowest probability and the top words,
or keywords were displayed.
We used  <= 0.2 here and have found in general that the words representing
20% of
the probability vector of
a topic usuall
provide a reliable qualititive
representation of
a topic.
In addition,
the number of
words in this set can
be used as a measure of
topic,
or group,
quality.
We describe this metric
in greater detail
in the next appendix,
and only topics with high quality are
listed in the following tables.
Since the model is non-parametric, in each run
of
the model,
there will
be several
topics with poor quality that will
not be
used.
Also, although a rigorous factor death mechanism has not been defined
for the present model,
a simple policy of
holding factors (topics or groups)
for only one time iteration without new data.
Factors are deleted from the
factor queue on the second time ieration if
there are no new data elements
that match within the threshold .
In the below data,
7 topics are sunset in
147
the final time iteration (year 4).
Topic
Keywords - Most Probable Words
Session 107 - 2001
Topic 0
patrick paul
hatch exchange election spousal
savings al
un-
derage wellstone identical ut
Topic 1
defense duties warner procurement navy nuclear weapons de-
pendents installation matters war guard
Topic 2
military dod subtitle army defense duties naval
armed con-
veyance conferees closure civilian
Topic 3
rep leas students mr teachers reauthorization achieve whole
esea language academic postponed
Topic 4
pipeline gas hazardous liquidation integrated inspection trans-
mission accidents dot corzine supplies owners
Topic 5
rep mr whole justice minutes lee jackson sought judiciary com-
merce wolf court
Topic 6
mr water whole river nuclear minutes tancredo sought post-
poned rep traficant waste
Topic 7
mr
whole highway veterans
minutes
sought
sustained rep
young airport priority lee
Topic 8
estate deduction bracket subtitle marginal phase trust begin-
ning gifts taxpayers taxable death
Topic 9
election candidate campaign political
feca communications
parties disbursements television disclosure solicitation dona-
tions
Topic 10
debtor bankruptcy debt chapter claims court trustee creditors
consumer estate petition guidelines
Topic 11
supplemental
defense chapter export agriculture rescinds in-
dian jesse nc mr forest interior
Session 107 - 2002
Topic 0
students mr labor istook torricelli
whole minutes hhs sought
disease ok mental
continued on next page
148
continued from previous page
Topic
Keywords - Most Probable Words
Topic 1
audits fraud criminal issuer companies firms joseph bank chief
investor director carl
Topic 2
unemployment fell ar irc recovery language weeks nh stricken
lincoln medicaid al
Topic 3
fuel renewable electric efficiency gas vehicles power murkowski
craig blended nuclear oil
Topic 4
defense inouye mr army dod aside procurement navy military
attacks pe terrorist
Topic 5
supplemental
mr defense whole obey rescinds court chapter
agriculture minutes postponed columbia
Topic 6
rep phil gramm jr railroad ok jim nc tn ga ky w
Topic 7
agriculture mr food subtitle conferees farm conservation rural
rep commodity farmer guarantee
Topic 8
defense military dod army navy nuclear procurement subtitle
offset duties warner rdt
Topic 9
election
voters
ballot
uniform registration
absentee
cast
polling register counting eac mcconnell
Topic 10
agriculture
conservation nutrition cloture
commodity live-
stock lugar rural invoked crop forestry feed
Topic 11
terrorism terrorist intelligence attorney money communica-
tions electronic candidate criminal laundering aliens parties
Topic 12
airport aviation aircraft carrier passenger screening leas flight
baggage faa weapons teachers
Topic 13
prescription medicare affordable patents generic cloture phar-
maceuticals outpatient greater wy pension nickles
Topic 14
workers negotiated customs duties labor tariff taa free adjust-
ments rep articles firms
Session 108 - 2003
Topic 0
mr forest indian interior park rep conservation wildlife whole
arts postponed bureau
continued on next page
149
continued from previous page
Topic
Keywords - Most Probable Words
Topic 1
rep mr conferees whole terrorism postponed recommit put
election loss demanded rept
Topic 2
phsa medicaid uses medicare substance global automatic con-
stitution higher abuse manzullo less
Topic 3
homeland rep dhs
functions
bureau terrorism director
dis-
charge immigration minutes subtitle explosive
Topic 4
conferees mr postponed climate minutes speaker whole divi-
sion prevailed conclusion hunter demanded
Topic 5
mr
labor
conferees
obey whole
hhs
director
nih students
dewine disease minutes
Topic 6
homeland mr whole sustained rogers preparedness immigra-
tion threat ky constitution rep obey
Topic 7
justice chief director armed terrorism positions council
goals
basic terrorist crime exchange
Topic 8
prescription beneficiaries medicare coverage premiums begin-
ning biological medicaid month furnished xviii discount
Topic 9
medicare enrolled hospital pdp card adjustments ssa price res-
idence choice january suppliers
Topic 10
defense military dod armed subtitle procurement army warner
nuclear duties matters weapons
Topic 11
dividends
baucus
exclusion renewable
deduction medicare
jobs mt mary accelerate low cuts
Topic 12
airport faa aircraft aviation passenger court traffic judiciary
conferees minority capacity carrier
Topic 13
guard coast vessel maritime inouye aside port chemical tierney
biological marine warfare
Topic 14
fuel
gas electric subtitle efficiency vehicles hydrogen power
lease renewable nuclear oil
Session 108 - 2004
Topic 0
mr kolbe mcconnell
whole minutes rep hiv jackson military
earmarks export sustained
continued on next page
150
continued from previous page
Topic
Keywords - Most Probable Words
Topic 1
duties armed subtitle guard civilian functions does conveyance
veterans director selection positions
Topic 2
airport aviation gas faa aircraft passenger fuel mr carrier flight
efficiency subtitle
Topic 3
fuel internet fell vehicles cloture frist invoked motor chuck jim
commerce taxation
Topic 4
mr rep postponed shelby sustained istook airport whole high-
way travel announced minutes
Topic 5
water river nuclear flood mr army supplies lake whole county
aside basin
Topic 6
iraq reconstruction military iraqi
freedom defense
armed
afghanistan deployment leaving duties debt
Topic 7
parent leas tanf students idea behavioral hearings complaints
intervention sea due parties
Topic 8
homeland mr immigration whole rep preparedness cochran
disaster postponed minutes customs rogers
Topic 9
mr labor conferees obey hhs whole minutes disease dewine nih
postponed director
Topic 10
defense military dod warner army navy hearings procurement
nuclear contractors war closure
Topic 11
intelligence director terrorist homeland subtitle terrorism ger-
mane dhs visa travel fbi liberty
Topic 12
medicare consumer hospital
conferees prescription physician
furnished coverage beneficiaries adjustments price mr
Topic 13
defense military dod nuclear vessel army procurement matters
conferees mr navy subtitle
Topic 14
defense dod army military navy procurement wide aside lewis
aircraft whole mr
Topic 15
taxable deduction fuel
taxpayers
stock companies
subtitle
transactions partnership quot paid shelters
Topic 16
highway vehicles motor fuel carrier hazardous registration ex-
cise road surface traffic driver’s
continued on next page
151
continued from previous page
Topic
Keywords - Most Probable Words
Table A.1:
Topics for Dual Sessions, Processed as Individual Years
Topic
Keywords - Most Probable Words
Session 107 - 2001
Topic 0
patrick paul
hatch exchange election spousal
savings al
un-
derage wellstone identical ut
Topic 1
defense duties warner procurement navy nuclear weapons de-
pendents installation matters war guard spence veterans com-
mand does
Topic 2
military dod subtitle army defense duties naval
armed con-
veyance conferees closure civilian
Topic 3
rep leas students mr teachers reauthorization achieve whole
esea language academic postponed
Topic 4
pipeline gas hazardous liquidation integrated inspection trans-
mission accidents dot corzine supplies owners subsequently
oversight regulatory interstate england comply fines release
incident reasonable storage
Topic 5
rep mr whole justice minutes lee jackson sought judiciary com-
merce wolf court
Topic 6
mr water whole river nuclear minutes tancredo sought post-
poned rep traficant waste drilling lake gas
Topic 7
mr
whole highway veterans
minutes
sought
sustained rep
young airport priority lee motor rogers jackson postponed
Topic 8
estate deduction bracket subtitle marginal phase trust begin-
ning gifts taxpayers taxable death elementary minimum mar-
riage skips secondary higher
Topic 9
election candidate campaign political
feca communications
parties disbursements television disclosure solicitation dona-
tions
continued on next page
152
continued from previous page
Topic
Keywords - Most Probable Words
Topic 10
debtor bankruptcy debt chapter claims court trustee creditors
consumer estate petition guidelines
Topic 11
supplemental
defense chapter export agriculture rescinds in-
dian jesse nc mr forest interior conservation helms guarantee
park lease postponed minutes military
Session 107 - 2002
Topic 0
students mr labor istook torricelli
whole minutes hhs sought
disease ok mental weldon regula inspector postponed restruc-
turing director elementary rep coverage abuse
Topic 1
audits fraud criminal issuer companies firms joseph bank chief
investor director carl
Topic 2
unemployment fell ar irc recovery language weeks nh stricken
lincoln medicaid al depreciation blanche
Topic 3
fuel renewable electric efficiency gas vehicles power murkowski
craig blended nuclear oil
Topic 4
defense inouye mr army dod aside procurement navy military
attacks pe terrorist young aircraft sustained lewis
Topic 5
supplemental
mr defense whole obey rescinds court chapter
agriculture minutes postponed columbia food military young
justice terrorist restore rises fish carrier attacks terrorism
Topic 6
rep phil gramm jr railroad ok jim nc tn ga ky w
Topic 7
agriculture mr food subtitle conferees farm conservation rural
rep commodity farmer guarantee crop water animal
minutes
plant regional postponed
Topic 8
defense military dod subtitle naval army conveyance navy du-
ties armed jurisdiction milestone
Topic 9
election debtor voters campaign political feca candidate ballot
prescribe creditors chapter court
Topic 10
agriculture
conservation nutrition cloture
commodity live-
stock lugar rural invoked crop forestry feed
continued on next page
153
continued from previous page
Topic
Keywords - Most Probable Words
Topic 11
terrorism terrorist intelligence attorney money communica-
tions electronic candidate criminal
laundering aliens parties
crime visa political offenses
Topic 12
airport aviation aircraft carrier passenger screening leas flight
baggage faa weapons teachers
Topic 13
prescription medicare affordable patents generic cloture phar-
maceuticals outpatient greater wy pension nickles
Topic 14
workers negotiated customs duties labor tariff taa free adjust-
ments rep articles firms agriculture ernest certification princi-
pal competitive
Topic 15
mr whole columbia minutes sought rep postponed announced
food kohl rept unfinished conferees conclusion prevailed
Topic 16
paul
wellstone
hatch spousal
relevant
undisputed election
wyden reissuance patrick exchange paperwork
Topic 17
defense head duties warner classified o navy attacks does pro-
curement polling spence
Topic 18
rep teachers leas esea students whole minutes academic inno-
vation subpart reauthorization demanded
Topic 19
pipeline gas accidents liquidation integrated reasonable in-
spection supplies
oversight
prescribe
hazardous
submeters
courtroom
Topic 20
mr rep conclusion conversion minutes detention whole judi-
ciary justice wolf lee lucas
Topic 21
mr sought water tancredo california river gas kucinich engi-
neering dam ramapo hearings
Topic 22
mr road lee whole sought veterans deficiency interstate de-
manded minutes highway young
Topic 23
deduction trust
marginal
categories kay chuck rollover
self
heirs estate ar marriage
Session 108 - 2003
Topic 0
mr forest indian interior park rep conservation wildlife whole
arts postponed bureau lease california cloture decrease sought
continued on next page
154
continued from previous page
Topic
Keywords - Most Probable Words
Topic 1
rep mr whole minutes conversion conclusion columbia wash-
ington conferees recommit abduction judicial
Topic 2
phsa medicaid uses medicare substance global automatic con-
stitution higher
abuse manzullo less
transmission railroad
david
Topic 3
homeland rep dhs
functions
bureau terrorism director
dis-
charge
immigration minutes
subtitle
explosive
intelligence
lieberman threat mr
Topic 4
conferees mr postponed climate minutes speaker whole divi-
sion prevailed conclusion hunter demanded global greenhouse
installation sought announced no davis arctic strategic
Topic 5
mr
labor
conferees
obey whole
hhs
director
nih students
dewine disease minutes postponed
Topic 6
homeland mr whole sustained rogers preparedness immigra-
tion threat ky constitution rep obey
Topic 7
justice chief director armed terrorism positions council
goals
basic terrorist crime exchange month freedom violence mission
lugar firearm corps
Topic 8
medicare enrolled hospital pdp card adjustments ssa price res-
idence choice january suppliers
Topic 9
defense military dod armed subtitle procurement army warner
nuclear duties matters weapons navy
Topic 10
dividends
baucus
exclusion renewable
deduction medicare
jobs mt mary accelerate low cuts refundable
Topic 11
airport faa aircraft aviation passenger court traffic judiciary
conferees minority capacity carrier partial abortions birth cer-
tification flight sexual noise attorney criminal sentencing emis-
sions police capitol
Topic 12
guard coast vessel maritime inouye aside port chemical tierney
biological marine warfare kucinich command navy
Topic 13
renewable
fuel
efficiency
gas
electric
vehicles
consumer
murkowski alaska subtitle power remote
continued on next page
155
continued from previous page
Topic
Keywords - Most Probable Words
Topic 14
defense army procurement military navy iraq inouye dod wide
guard aircraft marine corps
Topic 15
whole students death gramm hhs mr medicare multifamily
restructuring minutes announced nickles
Topic 16
unemployment weeks irc workers net fell
language city dis-
placed kerry lincoln pool
Topic 17
defense inouye aircraft guard mr division procurement mili-
tary dod navy waiver manzullo armed peacekeeping
Topic 18
mr
fish highway plant
supplemental
defense
immigration
whole hiv court justice chapter young
Topic 19
rep gramm jr ky mink pete jim howard phil estate al dennis
Topic 20
agriculture mr rural crop bioterrorism subtitle sought farmer
conferees storage interstate regional rice retaliating
Topic 21
attorney
communications
terrorist
intelligence
electronic
criminal in terrorism data aliens illegal sort
Topic 22
airport
aviation cockpit
aircraft
carrier
screening achieve
flight leas passenger aside subsequently
Topic 23
free workers duties women customs negotiated wool
wto be-
ginning entry articles xli
Session 108 - 2004
Topic 0
mr kolbe mcconnell
whole minutes rep hiv jackson military
earmarks
export
sustained constitution weiner
hefley chal-
lenge millennium sought bank humanitarian democracy
Topic 1
duties armed subtitle guard civilian functions does conveyance
veterans director selection positions combat
Topic 2
airport aviation gas faa aircraft passenger fuel mr carrier flight
efficiency subtitle climate alaska
Topic 3
fuel
internet fell
vehicles cloture frist invoked motor chuck
jim commerce taxation conservation clean electronic ut paid
efficiency tn workers procurement firearm adjustments
continued on next page
156
continued from previous page
Topic
Keywords - Most Probable Words
Topic 4
mr rep postponed shelby sustained istook airport whole high-
way travel
announced minutes conclusion demanded sought
prevailed put content
Topic 5
water river nuclear flood mr army supplies lake whole county
aside basin feasibility nevada defense waste san reclamation
harbor
Topic 6
iraq reconstruction military iraqi
freedom defense
armed
afghanistan deployment leaving duties debt
Topic 7
parent leas tanf students idea behavioral hearings complaints
intervention sea due parties academic teams subpart ssa func-
tions language
Topic 8
homeland mr preparedness sustained rogers constitution im-
migration disaster cochran minutes brady ky
Topic 9
conferees labor mr dewine students obey hhs elementary sec-
ondary whole rep data
Topic 10
defense military dod warner matters army guard command
ready weapons missile procurement
Topic 11
intelligence director terrorist homeland subtitle terrorism ger-
mane dhs visa travel fbi liberty communications border aliens
Topic 12
medicare consumer hospital
conferees prescription physician
furnished coverage beneficiaries adjustments price mr
Topic 13
defense military dod nuclear vessel army procurement matters
conferees mr navy subtitle
Topic 14
defense army navy military procurement dod wide inouye mr
arrangements armed reconstruction
Topic 15
taxable deduction fuel
taxpayers
stock companies
subtitle
transactions partnership quot paid shelters gain recommit ex-
cise loss
Topic 16
highway vehicles motor fuel carrier hazardous registration ex-
cise road surface traffic driver’s bridge interstate trust com-
merce htf
continued on next page
157
continued from previous page
Topic
Keywords - Most Probable Words
Topic 17
rep forest mr agriculture fuel burns interior nc minutes indian
whole postponed
Topic 18
mr wildland indian lease forest interior bureau salary cobell
conclusion hayworth paul
Topic 19
rep mr whole washington subcontracting minutes conversion
traficant illicit conclusion voters enterprise
Topic 20
automatic coverage global cease higher less rata david medi-
caid africa innovation manzullo
Topic 21
homeland rep disaster whole border functions bureau jury dhs
selection minutes misconduct regional
Topic 22
conferees postponed mr division demanded global mode lake
minutes speaker hardship adaptation
Topic 23
goals director justice cantwell
illegal
defense travel
building
armed presence political license terrorism
Topic 24
medicare enrolled physician price choice pdp month hospital
january enrollees low geographic
Topic 25
exclusion dividends
renewable cuts
beneficiaries
accelerate
janitors deduction reconciliation armed sunset professional
Topic 26
airport passenger sentencing judiciary delivered death attor-
ney emissions birth must unable staffing
Topic 27
coast vessel
guard maritime aside biological
command depot
reusable wellstone enterprise radio
Topic 28
renewable electric subtitle fuel power vehicles royalty procure-
ment net fleet commerce efficiency
Topic 32
mr
groundfish
supplemental
immigration
obey
wildland
deficit trafficking guard ocean cultural
Table A.2:
Topics for Dual Sessions, Processed with Dynamic Dependency
158
Appendix B
Calculations
B.1
Factor Quality Using Entropy
As designed, the model suggests the use of r
K
, the factor weight vector,
to select topics or groups.
As documented in our prior work for the GPPF
models, the r
K
vector will contain a limited set of non-zero values correspond-
ing to groups or topics that are inferred by the model from the data.
However,
for distrbuted and dynamic computation,
we wish to have a mechanism to
evaluate individual factors, independent of the weight vector.
Taking inspiration from the longstanding practice in the topic model-
ing community of listing a short list of most-probably words,
or keywords to
represent a topic,
we developed a metric for factor quality,
both topics and
groups, based on the number of elements in the factor required to compose a
minimal probability threshold, refered to elsewhere in this work as .
We noted
that qualitatively well-formed topics were typified by < 25 keywords out of a
vocabulary size of approximately 5000, whereas low-quality or poorly-formed
topics were typified by > 200 keywords,
using the keyword concept defined
earlier of the most-probably words for the topic where the cumulative sum of
probability is <= , and we have normally used  = 0.2.
For the joint model,
159
the latent factors representing topics are in the β matrix.
This method also
works well
for factors in the ψ and φ matrices,
for user topics (jargon) and
groups, respectively.
We can generalize this to use Shannon’s information entropy:
H (v
i
) = −
X
v∈v
i
vlog (v)
(B.1)
where we simply calculate Entropy over the entire factor, noting that the factor
elements already represent probability values in the case of the .
In the case
of the β and ψ matrices.
For the φ and θ matrices, since these have elements
with a Gamma prior, we must first normalize the elements with the sum over
each factor, as we do in preparing the hash values.
(See Section 7.2.)
Lemma
H (v
i
) < H (v
j
) if f v
i
is less uniform than v
j
.
Thus we can use an Entropy-minimization metric or latent factor qual-
ity in the form of
element asymmetry.
The preferential
manner for imple-
menting this is in the course of forming hash values, where the factor elements
are already sorted by value.
A threshold can be set,
similar to  in counting
keywords for forming the sparse hash.
Factors that fail
to reach a specified
threshold level
of
cumulative probability in a specified fraction of
elements
can be discarded as poorly-formed.
Alternately,
the Entropy can be used as
a proxy for the ratio of probability to number of factors, and used as a factor
rank.
In this case, the decision to save or remove factors can be deferred, based
on available resources or some other mechanism.
160
Index
Abstract, ix
Acknowledgments, v
Appendices, 146
Appendix
Calculations, 159
Detailed Dynamic Topic Data,
147
Bibliography, 175
Conclusion, 143
Dedication, iv
Distributed Adaptive Hierarchical Clus-
tering for Streaming Data,
92
Distributed and Dynamic Joint Topic-
Community Model Inference
via Factor Matching and Pa-
rameter Reconciliation, 119
Introduction, 1
J-GPPF, 70
Joint Gamma Process Poisson Fac-
torization, 70
Joint Poisson Factorization Case Study:
Topic Modeling of U.S. Sen-
ate Records, 66
Network Discovery via Joint Nework
and Topic Modeling, 47
Related Work, 13
Temporal
Distributed Learning, 32
161
Bibliography
[1]
Amazon kinesis.
https://aws.amazon.com/kinesis/.
Accessed:
2017-
03-13.
[2]
Apache common log format.
https://httpd.apache.org/docs/1.3/
logs.html#common.
Accessed:
2017-03-13.
[3]
Apache fink.
https://flink.apache.org/.
Accessed:
2017-03-13.
[4]
Apache spark.
http://spark.apache.org/.
Accessed:
2017-01-20.
[5]
Apache spark mlib.
http://spark.apache.org/docs/latest/ml-guide.
html.
Accessed:
2017-01-15.
[6]
Databricks spark scikit-learn.
https://databricks.com/blog/2016/
02/08/auto-scaling-scikit-learn-with-apache-spark.html.
Ac-
cessed:
2017-01-22.
[7]
The lambda architecture.
https://www.bigdatareviews.org/?page_
id=783.
Accessed:
2017-03-14.
[8]
A.
Acharya,
J.
Ghosh,
and M.
Zhou.
Nonparametric Bayesian Factor
Analysis for Dynamic Count Matrices.
In Proc.
of AISTATS. 2015.
162
[9]
Ayan Acharya,
Joydeep Ghosh,
and Mingyuan Zhou.
Nonparametric
bayesian factor analysis for dynamic count matrices.
In Lebanon and
Vishwanathan [44].
[10]
Ayan Acharya,
Dean Teffer,
Jette Henderson,
Marcus Tyler,
Mingyuan
Zhou, and Joydeep Ghosh.
Gamma process poisson factorization for joint
modeling of network and documents.
In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases, pages 283–299.
Springer, 2015.
[11]
Ayan Acharya,
Dean Teffer,
Jette Henderson,
Marcus Tyler,
Mingyuan
Zhou,
and Joydeep Ghosh.
Gamma process poisson factorization for
joint modeling of
network and documents.
In Machine Learning and
Knowledge Discovery in Databases - European Conference, ECML PKDD
2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I, pages
283–299, 2015.
[12]
Ayan Acharya,
Dean Teffer,
Mingyuan Zhou,
and Joydeep Ghosh.
Net-
work discovery and recommendation via joint network and topic modeling.
SIGKDD Social
and Recommender Systems Workshop, Aug 2015.
[13]
E Scott Adler and John Wilkerson.
Congressional
bills project.
NSF,
880066:00880061, 2006.
[14]
Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing.
Mixed membership stochastic blockmodels.
Journal of Machine Learning
Research, 9(Sep):1981–2014, 2008.
163
[15]
Bahman Bahmani,
Benjamin Moseley,
Andrea Vattani,
Ravi
Kumar,
and Sergei
Vassilvitskii.
Scalable k-means++.
Proc.
VLDB Endow.,
5(7):622–633, March 2012.
[16]
R.
Balasubramanyan and W.
W.
Cohen.
Block-LDA:
Jointly modeling
entity-annotated text and entity-entity links.
In Proc.
of
SDM,
pages
450–461, 2011.
[17]
Christopher M. Bishop.
Pattern recognition and machine learning.
Springer,
2013.
[18]
David M. Blei and John D. Lafferty.
Dynamic topic models.
In Proceed-
ings of
the 23rd International
Conference on Machine Learning,
ICML
’06, pages 113–120, New York, NY, USA, 2006.
ACM.
[19]
David M Blei, Andrew Y Ng, and Michael I Jordan.
Latent dirichlet al-
location.
Journal
of Machine Learning Research, 3(Jan):993–1022, 2003.
[20]
Ian Budge.
Mapping policy preferences:
estimates for parties,
electors,
and governments,
1945-1998,
volume 1.
Oxford University Press on
Demand, 2001.
[21]
J.
Canny.
Gap:
a factor model
for discrete data.
In Proc.
of
SIGIR,
2004.
[22]
Claudia Cargnoni, Peter Müller, and Mike West.
Bayesian forecasting of
multinomial time series through conditionally gaussian dynamic models.
Journal
of the American Statistical
Association, 92(438):640–647, 1997.
164
[23]
François Caron,
Manuel
Davy,
Arnaud Doucet,
Emmanuel
Duflos,
and
Philippe Vanheeghe.
Bayesian inference for linear dynamic models with
dirichlet process mixtures.
IEEE Transactions on Signal
Processing,
56(1):71–84, 2008.
[24]
A.
T.
Cemgil.
Bayesian inference for nonnegative matrix factorisation
models.
Intell.
Neuroscience, 2009.
[25]
A.J.B.
Chaney,
P.
Gopalan,
and D.M.
Blei.
Poisson trust factorization
for incorporating social networks into personalized item recommendation.
In NIPS Workshop:
What Difference Does Personalization Make?, 2013.
[26]
J.
Chang and D.
Blei.
Relational
topic models for document networks.
In Proc.
of AISTATS, 2009.
[27]
S. C. Choi and R. Wette.
Maximum likelihood estimation of the parame-
ters of the gamma distribution and their bias.
Technometrics, 11(4):683–
690, 1969.
[28]
Joselíto J. Chua and Peter E. Tischer.
Hierarchical Ordering for Approx-
imate Similarity Ranking,
pages 496–500.
Springer Berlin Heidelberg,
Berlin, Heidelberg, 2003.
[29]
Jeffrey Dean and Sanjay Ghemawat.
Mapreduce:
Simplified data pro-
cessing on large clusters.
In Proceedings of the 6th Conference on Sympo-
sium on Opearting Systems Design & Implementation - Volume 6, OSDI’04,
pages 10–10, Berkeley, CA, USA, 2004.
USENIX Association.
165
[30]
Jeffrey Dean and Sanjay Ghemawat.
Mapreduce:
Simplified data pro-
cessing on large clusters.
In 6th Symposium on Operating System Design
and Implementation (OSDI 2004), San Francisco, California, USA, De-
cember 6-8, 2004, pages 137–150, 2004.
[31]
Paul
DiMaggio,
Manish Nag,
and David Blei.
Exploiting affinities be-
tween topic modeling and the sociological perspective on culture:
Appli-
cation to newspaper coverage of
us government arts funding.
Poetics,
41(6):570–606, 2013.
[32]
Ronald Fagin,
Jurg Nievergelt,
Nicholas Pippenger,
and H.
Raymond
Strong.
Extendible hashing&mdash;a fast access method for dynamic
files.
ACM Trans.
Database Syst., 4(3):315–344, September 1979.
[33]
Justin Farrell.
Corporate funding and ideological polarization about cli-
mate change.
Proceedings of the National Academy of Sciences, 113(1):92–
97, 2016.
[34]
T.
S.
Ferguson.
A Bayesian analysis of
some nonparametric problems.
Ann.
Statist., 1973.
[35]
Chris Fraley and Adrian E. Raftery.
Bayesian regularization for normal
mixture estimation and model-based clustering.
Journal of Classification,
24(2):155âĂŞ181, 2007.
[36]
P. Gopalan, F. Ruiz, R. Ranganath, and D. Blei.
Bayesian nonparametric
poisson factorization for recommendation systems.
In Proc.
of AISTATS,
166
2014.
[37]
P.K. Gopalan, L. Charlin, and D. Blei.
Content-based recommendations
with poisson factorization.
In Proc.
of NIPS, pages 3176–3184.
2014.
[38]
Justin Grimmer and Brandon M Stewart.
Text as data:
The promise and
pitfalls of automatic content analysis methods for political texts.
Political
analysis, pages 267–297, 2013.
[39]
Piotr Indyk and Rajeev Motwani.
Approximate nearest neighbors:
To-
wards removing the curse of dimensionality.
In Proceedings of the Thirti-
eth Annual ACM Symposium on the Theory of Computing, Dallas, Texas,
USA, May 23-26, 1998, pages 604–613, 1998.
[40]
C.
Kemp,
J.B.
Tenenbaum,
T.L.
Griffiths,
T.
Yamada,
and N.
Ueda.
Learning systems of concepts with an infinite relational model.
In Proc.
of AAAI, pages 381–388, 2006.
[41]
Hisashi Koga, Tetsuo Ishibashi, and Toshinori Watanabe.
Fast Hierarchi-
cal Clustering Algorithm Using Locality-Sensitive Hashing, pages 114–128.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2004.
[42]
A.
V.
Senthil
Kumar.
Web usage mining techniques and applications
across industries.
IGI Global, 2017.
[43]
P R Kumar.
Challenges in cyberphysical systems, Oct 2010.
167
[44]
Guy Lebanon and S.
V.
N.
Vishwanathan,
editors.
Proceedings of
the
Eighteenth International
Conference on Artificial
Intelligence and Statis-
tics, AISTATS 2015, San Diego, California, USA, May 9-12, 2015,
vol-
ume 38 of JMLR Workshop and Conference Proceedings.
JMLR.org, 2015.
[45]
D.
D.
Lee and H.
S.
Seung.
Algorithms for non-negative matrix factor-
ization.
In NIPS, 2001.
[46]
J.
Leskovec and J.M.
Julian.
Learning to discover social
circles in ego
networks.
In Proc.
of NIPS, pages 539–547.
2012.
[47]
J. Liu, M. Chu, and J.e.
Reich.
Multitarget tracking in distributed sensor
networks.
IEEE Signal
Processing Magazine, 24(3):36âĂŞ46, 2007.
[48]
Peng Lu,
Gang Chen,
Beng Chin Ooi,
Hoang Tam Vo,
and Sai
Wu.
Scalagist:
Scalable generalized search trees for mapreduce systems [inno-
vative systems paper].
Proc.
VLDB Endow.,
7(14):1797–1808,
October
2014.
[49]
H. Ma, H. Yang, M.R. Lyu, and I. King.
Sorec:
Social recommendation
using probabilistic matrix factorization.
In Proc.
of CIKM,
pages 931–
940, 2008.
[50]
Zdravko Markov and Daniel T. Larose.
Data mining the Web:
uncovering
patterns in Web content, structure, and usage.
Wiley, 2007.
168
[51]
Nathan Marz and James Warren.
Big Data:
Principles and Best Prac-
tices of
Scalable Realtime Data Systems.
Manning Publications Co.,
Greenwich, CT, USA, 1st edition, 2015.
[52]
A.
McCallum,
X.
Wang,
and A.
Corrada-Emmanuel.
Topic and role
discovery in social
networks with experiments on enron and academic
email.
J. Artif.
Int.
Res., 30(1):249–272, October 2007.
[53]
Xiangrui
Meng,
Joseph Bradley,
Burak Yavuz,
Evan Sparks,
Shivaram
Venkataraman,
Davies Liu,
Jeremy Freeman,
DB Tsai,
Manish Amde,
Sean Owen,
Doris Xin,
Reynold Xin,
Michael
J.
Franklin,
Reza Zadeh,
Matei Zaharia, and Ameet Talwalkar.
Mllib:
Machine learning in apache
spark.
J. Mach.
Learn.
Res., 17(1):1235–1241, January 2016.
[54]
A.
Menon and C.
Elkan.
Link prediction via matrix factorization.
In
Machine Learning and Knowledge Discovery in Databases,
volume 6912
of Lecture Notes in Computer Science, pages 437–452.
Springer Berlin /
Heidelberg, 2011.
[55]
Srujana Merugu and Joydeep Ghosh.
A distributed learning framework
for heterogeneous data sources.
Proceeding of the eleventh ACM SIGKDD
international
conference on Knowledge discovery in data mining - KDD
’05, 2005.
[56]
Slava Mikhaylov, Michael Laver, and Kenneth R Benoit.
Coder reliability
and misclassification in the human coding of party manifestos.
Political
Analysis, 20(1):78–91, 2012.
169
[57]
K. Miller, M. I. Jordan, and T. L. Griffiths.
Nonparametric latent feature
models for link prediction.
In Proc.
of NIPS, pages 1276–1284.
2009.
[58]
Stanislav Minsker,
Sanvesh Srivastava,
Lizhen Lin,
and David Dunson.
Scalable and robust bayesian inference via the median posterior.
In
Proceedings of The 31st International
Conference on Machine Learning,
pages 1656–1664, 2014.
[59]
Stanislav Minsker,
Sanvesh Srivastava,
Lizhen Lin,
and David B.
Dun-
son.
Scalable and robust bayesian inference via the median posterior.
In
Proceedings of the 31st International Conference on International Confer-
ence on Machine Learning - Volume 32, ICML’14, pages II–1656–II–1664.
JMLR.org, 2014.
[60]
R.M.
Nallapati,
A.
Ahmed,
E.P.
Xing,
and W.W.
Cohen.
Joint latent
topic models for text and citations.
In Proc.
of
KDD,
pages 542–550,
2008.
[61]
Chris Okasaki.
Purely functional
data structures.
Cambridge University
Press, 1999.
[62]
K. Palla, Z. Ghahramani, and D. A. Knowles.
An infinite latent attribute
model for network data.
In Proc.
of ICML, pages 1607–1614, 2012.
[63]
Kevin M Quinn,
Burt L Monroe,
Michael
Colaresi,
Michael
H Crespin,
and Dragomir R Radev.
How to analyze political attention with minimal
170
assumptions and costs.
American Journal of Political Science, 54(1):209–
228, 2010.
[64]
Anand Rajaraman and Jeffrey D Ullman.
Mining of
Massive Datasets,
volume 67.
2011.
[65]
Radim Řehůřek and Petr Sojka.
Software Framework for Topic Modelling
with Large Corpora.
In Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.
ELRA.
http://is.muni.cz/publication/884893/en.
[66]
Margaret E Roberts,
Brandon M Stewart,
Dustin Tingley,
Christopher
Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson,
and David G Rand.
Structural
topic models for open-ended survey re-
sponses.
American Journal
of Political
Science, 58(4):1064–1082, 2014.
[67]
Alan Rodriguez and Kelvin Chu.
Locality Sensitive Hashing By Spark.
Apache Software Foundation, Jun 2016.
[68]
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth.
The author-topic model for authors and documents.
In Proceedings of the
20th conference on Uncertainty in artificial
intelligence,
pages 487–494.
AUAI Press, 2004.
[69]
Steven L.
Scott,
Alexander W.
Blocker,
Fernando V.
Bonassi,
Hugh A.
Chipman,
Edward I.
George,
and Robert E.
McCulloch.
Bayes and big
171
data:
The consensus monte carlo algorithm.
International
Journal
of
Management Science and Engineering Management, 11:78–88, 2016.
[70]
Senate.
Legislation.
https://www.congress.gov/legislation,
2016.
Accessed:
2016-04-15.
[71]
Myra Spiliopoulou,
Haixun Wang,
Diane J.
Cook,
Jian Pei,
Wei
Wang,
Osmar R.
Zaïane,
and Xindong Wu,
editors.
Data Mining Workshops
(ICDMW), 2011 IEEE 11th International Conference on, Vancouver, BC,
Canada, December 11, 2011.
IEEE Computer Society, 2011.
[72]
S. Srivastava, C. Li, and D. B. Dunson.
Scalable Bayes via Barycenter in
Wasserstein Space.
ArXiv e-prints, August 2015.
[73]
Dean Teffer, Amanda Hutton, and Joydeep Ghosh.
Temporal distributed
learning with heterogeneous data using gaussian mixtures.
In Spiliopoulou
et al.
[71], pages 196–203.
[74]
Dean Teffer,
Ravi
Srinivasan,
Brent
Schneeman,
and Joydeep Ghosh.
Non-parametric clustering for online data via equivalence hashing in map-
reduce frameworks.
2017.
in preparation.
[75]
Dean Teffer,
Marcus Tyler,
and Joydeep Ghosh.
Topic modeling via
joint poisson factorization using network side information.
2017.
in
preparation.
172
[76]
Warren Thornthwaite.
Dimensional
modeling basics get flexible real-
world dimensions with analysis services 2005.
SQL Server Pro,
Mar
2006.
[77]
M.
K.
Titsias.
The infinite gamma-Poisson feature model.
In Proc.
of
NIPS, 2008.
[78]
B.-N.
Vo and W.-K.
Ma.
The gaussian mixture probability hypothesis
density filter.
IEEE Transactions on Signal Processing, 54(11):4091âĂŞ4104,
2006.
[79]
Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji.
Hashing
for similarity search:
A survey.
CoRR, abs/1408.2927, 2014.
[80]
Z. Wen and C. Lin.
Towards finding valuable topics.
In Proc.
of SDM,
pages 720–731, 2010.
[81]
R.
L.
Wolpert,
M.
A.
Clyde,
and C.
Tu.
Stochastic expansions using
continuous dictionaries:
Lévy Adaptive Regression Kernels.
Annals of
Statistics, 2011.
[82]
Tae Yano,
William W Cohen,
and Noah A Smith.
Predicting response
to political blog posts with topic models.
In Proceedings of Human Lan-
guage Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 477–485.
Association for Computational Linguistics, 2009.
173
[83]
Tae Yano and Noah A Smith.
What’s worthy of comment? content and
comment volume in political blogs.
In ICWSM, 2010.
[84]
T.
Yoshida.
Toward finding hidden communities based on user profile.
J. Intell.
Inf.
Syst., 40(2):189–209, April 2013.
[85]
Joseph Zadeh,
George Apostolopoulos,
Christos Tryfonas,
and Muddu
Sudhakar.
Defense at scale:
Building a central
nervous system for the
soc.
2017.
in preparation.
[86]
Matei Zaharia.
An Architecture for Fast and General Data Processing on
Large Clusters.
PhD thesis, EECS Department, University of California,
Berkeley, Feb 2014.
[87]
Jufen Zhang and R. Everson.
Bayesian estimation and classification with
incomplete data using mixture models.
2004 International
Conference
on Machine Learning and Applications, 2004.
Proceedings., 2004.
[88]
M.
Zhou.
Infinite edge partition models for overlapping community de-
tection and link prediction.
In Proc.
of AISTATS (to appear).
2015.
[89]
M.
Zhou and L.
Carin.
Augment-and-conquer negative binomial
pro-
cesses.
In Proc.
of NIPS, 2012.
[90]
M.
Zhou and L.
Carin.
Negative binomial
process count and mixture
modeling.
To appear in IEEE Trans.
Pattern Anal.
Mach.
Intelligence,
2014.
174
[91]
J. Zhu.
Max-margin nonparametric latent feature models for link predic-
tion.
In Proc.
of ICML, 2012.
175
Vita
Dean Whitney Teffer was born in Rantoul, Illinois on 11 July 1970, the
son of USAF MAJ (SEL) Garry E.
Teffer and Helen M.
Teffer.
He received
the Bachelor of
Science degree in Physics from Tulane University in 1992.
He moved to Austin,
TX in 1992 to pursue graduate studies and revceived a
Master of Arts in Physics from the University of Texas at Austin in 1996 while
employed at the University of Texas at Austin Applied Research Laboratories,
at which point he transitioned to a full-time role at the "Lab." He left the
Lab in 1999 to explore the burgeoning Austin tech scene,
where he worked
as a consultant project manager and started his own company in 2001.
After
many trials and travails, he sold the company’s itellectual property to Siemens
USA in 2005, where he worked as a senior program mananger for a year and a
half.
Looking for another challenge, his former supervisor at the Lab enticed
him back,
where he remains.
However,
he realized that in order to provide
effective research leadership for the university, he should really complete that
long-abandoned PhD.
So in 2009,
after taking a few preliminary classes,
he
formerly returned to UT as a graduate student, while working as a project and
then program manager at the Lab.
Sometimes these duties are congruent, but
usually not.
However, his work is generally in the area of processing streaming
data on distributed systems using machine learning algorithms,
often with
human input or human evaluation via novel information or interactive displays.
176
Permanent address:
10710 Spicewood Club Drive
Austin, Texas 78750
This dissertation was typeset with L
A
T
E
X
†
by the author.
†
L
A
T
E
X is a document preparation system developed by Leslie Lamport as a special
version of Donald Knuth’s T
E
X Program.
177
