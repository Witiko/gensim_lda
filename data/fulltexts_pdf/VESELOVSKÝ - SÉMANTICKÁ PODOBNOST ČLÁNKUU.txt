VYSOKÉ UČENÍ TECHNICKÉ V BRNĚ
BRNO UNIVERSITY OF TECHNOLOGY
FAKULTA INFORMAČNÍCH TECHNOLOGIÍ
ÚSTAV POČÍTAČOVÉ GRAFIKY A MULTIMÉDIÍ
FACULTY OF INFORMATION TECHNOLOGY
DEPARTMENT OF COMPUTER GRAPHICS AND MULTIMEDIA
SÉMANTICKÁ PODOBNOST ČLÁNKŮ
BAKALÁŘSKÁ PRÁCE
BACHELOR’S THESIS
AUTOR PRÁCE
MARTIN VESELOVSKÝ
AUTHOR
BRNO 2014
VYSOKÉ UČENÍ TECHNICKÉ V BRNĚ
BRNO UNIVERSITY OF TECHNOLOGY
FAKULTA INFORMAČNÍCH TECHNOLOGIÍ
ÚSTAV POČÍTAČOVÉ GRAFIKY A MULTIMÉDIÍ
FACULTY OF INFORMATION TECHNOLOGY
DEPARTMENT OF COMPUTER GRAPHICS AND MULTIMEDIA
SÉMANTICKÁ PODOBNOST ČLÁNKŮ
SEMANTIC SIMILARITY OF ARTICLES
BAKALÁŘSKÁ PRÁCE
BACHELOR’S THESIS
AUTOR PRÁCE
MARTIN VESELOVSKÝ
AUTHOR
VEDOUCÍ PRÁCE
Ing. JAN KOUŘIL
SUPERVISOR
BRNO 2014
Abstrakt
Tato práce se zabývá modelováním struktury sémantických vztahů mezi
články v anglic-
kém jazyce.
Představuje existující
metody pro reprezentaci
a výpočet podobnosti
článků.
Základnou metodou je vektorový model,
který reprezentuje dokument jako vektor slov.
Jednotlivým slovům jsou v rámci
modelu určené váhy důležitosti
metodou TF-IDF.
Dále
jsou zde popsány pokročilé metody modelování a to Latentní sémantická analýza (LSA) a
Latentní
Dirichletova alokace (LDA).
Práce se také zabývá články,
které jsou sémanticky
anotované,
přičemž váhy anotačních slov jsou vypočítány na základe metody SGD.
Vy-
hodnocení
výsledků probíhá na připraveném testovacím korpusu dokumentů,
ke kterému
existuje referenční hodnocení podobnosti.
Abstract
This bachelor’s thesis deals with modelling of structure of semantic relationships among ar-
ticles in English language. There are introduced existing methods of articles representation
and computation of
similarity.
The base method is vector space model,
which represents
document as vector of words. There are given weights of importance to these words using
TF-IDF method. Next, there are described advanced methods of modelling, Latent semantic
analysis (LSA) and Latent Dirichlet allocation (LDA). This thesis also deals with articles,
which are semantically annotated, while weights of annotation words are computed by Sto-
chastic Gradient Descent method.
Evaluation of
results takes place on the prepared test
corpus of documents to which there is reference similarity evaluation.
Klíčová slova
sémantická podobnost,
sémantické anotace,
vektorový model,
TF-IDF,
SGD,
LSA,
LDA,
Python, Gensim, Elasticsearch, zpracování přirozeného jazyka, křížová validace
Keywords
semantic similarity, semantic annotations, vector space model, TF-IDF, SGD, LSA, LDA,
Python, Gensim, Elasticsearch, natural language processing, cross validation
Citace
Martin Veselovský: Sémantická podobnost článků, bakalářská práce, Brno, FIT VUT v Brně,
2014
Sémantická podobnost článků
Prohlášení
Prohlašuji,
že jsem tuto bakalářskou práci
vypracoval
samostatně pod vedením Ing.
Jana
Kouřila. Uvedl jsem všechny literární prameny a publikace, ze kterých jsem čerpal.
. . . . . . . . . . . . . . . . . . . . . . .
Martin Veselovský
20. mája 2014
Poděkování
Děkuji Ing. Janu Kouřilovi za hodnotné rady a spolehlivé vedení během mé práce.
c
Martin Veselovský, 2014.
Tato práce vznikla jako školní dílo na Vysokém učení technickém v Brně, Fakultě informa-
čních technologií. Práce je chráněna autorským zákonem a její užití bez udělení oprávnění
autorem je nezákonné, s výjimkou zákonem definovaných případů.
Obsah
1
Úvod
2
2
Analýza problému
3
2.1
Vektorový model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
2.2
Váhovanie hodnôt termov
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
2.3
Kosínusová podobnosť
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
2.4
Latentná sémantická analýza
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
2.5
Latentná Dirichletova alokácia
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
2.6
Výskyt anotačných termov
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
2.7
Váhovanie anotačných termov .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
3
Použité technológie
11
3.1
Elasticsearch
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
3.2
Knižnica gensim .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
3.3
Sémantické anotácie
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
3.4
Webové rozhranie .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
4
Návrh a príprava prostredia
14
4.1
Kolekcia dokumentov .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
4.2
Extrakcia termov a váhovanie .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
4.3
Reprezentácia a podobnosť
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15
4.4
Ovládanie aplikácie .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
4.5
Iteratívne aplikovanie metódy LSA .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
17
5
Implementácia
18
5.1
Štruktúra aplikácie .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
5.2
Metóda SGD .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
5.3
Výpočet podobnosti
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
6
Testovanie
23
6.1
Dokumenty bez anotácií
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
6.2
Dokumenty s anotáciami .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
25
6.3
Porovnanie
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
27
7
Záver
29
A Ovládanie aplikácie
32
B Testovací korpus
35
1
Kapitola 1
Úvod
Určovanie sémantickej podobnosti dokumentov je predmetom výskumu spracovania priro-
dzeného jazyka (Natural language processing - NLP), ktorý je jedným z hlavných aplikácií
umelej inteligencie. Cieľom je naučiť počítač chápať významové vzťahy medzi jednotlivými
dokumentmi, pričom tieto dokumenty nemusia zdieľať žiadne spoločné slová. Výsledkom ta-
kéhoto postupu môže byť inteligentné odpovedanie na užívateľské otázky, resp. ponúkanie
relevantných informácií.
V nasledujúcich kapitolách sú vysvetlené známe metódy výpočtu sémantickej
podob-
nosti dokumentov, ktoré sú ďalej použité pri návrhu a implementácii konkrétnej aplikácie.
Výskum je zameraný predovšetkým na spracovanie kratších článkov o umení
v anglickom
jazyku.
Týmto článkom je naviac možné špecializovaným nástrojom priradiť sémantické
anotácie, tzn. rozšíriť konkrétny článok o ďalšie slová, pričom vyhodnotenie vplyvu týchto
anotácií na celkový podobnostný model je súčasťou tejto práce.
Testovanie matematických metód pre výpočet podobnosti prebehlo na pripravenej ko-
lekcii dokumentov, v rámci ktorej bola vzájomne pre všetky dokumenty určená referenčná
podobnosť človekom. Príprava podobnostného modelu býva v praxi časovo náročná. Vytvo-
renie modelu vychádza z kolekcie tzv. trénovacích dokumentov a užívateľské dotazy ako aj
ďalšie dokumenty sú do modelu len premietnuté za účelom výpočtu podobnosti a sami ho
viac neovplyvňujú. Pre zaistenie vierohodnosti výsledkov testov na relatívne malom korpuse
dokumentov je použitá metóda overenia validity modelu (4.1).
Výsledkom výskumu je porovnanie známych metód výpočtu podobností a nájdenie najv-
hodnejšieho kandidáta pre konkrétnu situáciu.
Práca sa okrem toho zaoberá zlepšovaním
výsledkov týchto metód použitím vhodnejšieho ohodnotenia slov, pričom uvažuje nad odli-
šným ohodnotením anotačných slov a pôvodných slov dokumentu (2.7).
2
Kapitola 2
Analýza problému
Sémantickú analýzu je možné chápať ako identifikáciu významu jednotlivých slov v kolekcii
dokumentov, resp. identifikáciu významu dokumentov. Súčasťou analýzy je potom reprezen-
tácia tejto podobnosti pomocou zvoleného modelu. Pokročilejšie metódy obsiahnuté v tejto
kapitole riešia otázky podobnosti klasifikovaním slov do tzv. tém a dokumenty porovnávajú
na základe váhovanej príslušnosti k týmto témam.
2.1
Vektorový model
Rozšíreným modelom pre reprezentáciu dokumentov v oblasti získavania informácií je vek-
torový model (vector space model - VSM [6, str. 11-15]), v ktorom sú dokumenty a dotazy
reprezentované ako body v n-rozmernom priestore. Každá dimenzia tohto priestoru prináleží
jednému konkrétnemu slovu, tzv. termu, z usporiadanej kolekcie vybraných slov zo všetkých
slov použitých v dokumentoch.
Obr. 2.1: Vektorový model
Dokument je potom možné vyjadriť ako vektor,
kde jeho jednotlivé zložky majú ne-
nulovú hodnotu pokiaľ dokument dané slovo obsahuje. Nedostatkom tejto reprezentácie je
fakt, že väčšina hodnôt vektoru bude nulová a informácia o poradí slov v dokumente bude
stratená.
Nespornou výhodou VSM je však jeho jednoduchosť a tiež schopnosť reprezen-
tovať aj dokumenty, ktoré neboli v počiatočnej trénovacej množine dokumentov. V tomto
3
prípade však budú nové termy ignorované,
pretože by to predstavovalo zvýšenie dimenzi-
onality celého modelu.
Na obrázku 2.1 je znázornený príklad vektorovej reprezentácie troch dokumentov v troj-
rozmernom priestore, kde každá dimenzia reprezentuje práve jedno slovo (term) z množiny
troch slov.
Potom platí,
že dokument d
1
obsahuje jedenkrát term t
1
a trikrát term t
2
.
Dokument d
2
obsahuje všetky tri
termy,
pričom t
3
trikrát a t
2
a t
1
po dvakrát.
Posledný
dokument d
3
pozostáva len z jedného termu t
3
.
O tom,
prečo je v priestore uvažovaný aj
počet výskytov konkrétneho slova v dokumente, pojednáva nasledujúca podkapitola.
2.2
Váhovanie hodnôt termov
Základným spôsobom vektorového modelu je nastavenie hodnoty príslušného termu vo vek-
tore konkrétneho dokumentu na nenulovú hodnotu.
Z tohto postupu vyplýva,
že jednot-
livým termom je možné nastaviť rôzne váhy,
napríklad na základe počtu opakovaní
slova
v dokumente metódou T F (Term Frequency) [8].
Definujme f (t, d) ako frekvenciu opako-
vania (počet výskytov) slova t v dokumente d (použité na predchádzajúcom obrázku 2.1).
Túto frekvenciu je potom vhodné normalizovať napríklad vydelením frekvenciou opako-
vania najviac vyskytovaného termu v dokumente d,
t.j.
max
t
f (t, d),
potom pre výpočet
frekvencie termov platí vzťah:
T F (t, d) = 0.5 +
0.5 × f (t, d)
max
t
f (t, d)
(2.I)
Nepriaznivým dôsledkom tejto metódy je,
že vyššiu váhu budú mať aj
termy,
ktoré
v skutočnosti
nie sú veľmi
dôležité.
Niektoré slová v súvislosti
s konkrétnou kolekciou do-
kumentov navyše môžu byť príliš obecné na to,
aby boli
užitočné.
Napr.
slovo painting
v korpuse o umení
môže byť takmer v každom článku,
takže pre vyhľadávanie dokumen-
tov nad touto kolekciou je v podstate nepoužiteľné, pretože dokumenty od seba navzájom
nerozlišuje.
Podobným spôsobom ako metódu T F (t, d) je možné definovať DF (t) ako počet doku-
mentov, v ktorých sa term t vyskytuje (Document Frequency). Pre nastavenie váh termov
sa potom používa metóda IDF (Inverse Document Frequency),
ktorá je definovaná ako
logaritmus podielu počtu všetkých dokumentov N k počtu dokumentov obsahujúcich dané
slovo, teda DF (t) (2.II). Výsledkom aplikácie metódy IDF je stav, kedy slová vyskytujúce
sa vo všetkých dokumentoch majú váhu nulovú,
zatiaľ
čo slová,
ktoré boli
unikátne majú
váhu najvyššiu.
Toto je žiadané,
pretože zriedkavo vyskytujúce sa slová majú najväčšiu
výpovednú informáciu vzhľadom k dokumentu.
IDF (t) = log(
N
DF (t)
)
(2.II)
Najpopulárnejším spôsobom je nastavovanie váh matematickou metódou T F –IDF (Term
Frequency - Inverse Document Frequency)[8], ktorá kombinuje metódy frekvencie výskytu
termov v dokumente a inverznej frekvencie dokumentov obsahujúcich daný term. Váhovanie
T F –IDF priraďuje váhu termu t v dokumente d na základe súčinu T F s IDF (2.III).
T F –IDF (t, d) = T F (t, d) × IDF (t)
(2.III)
4
Inými slovami, T F –IDF priradí váhu termu t v dokumente d tak, že
1.
váha termu t bude najvyššia,
keď sa t vyskytuje veľakrát v rámci
malého množstva
dokumentov (term je s týmito dokumentmi
úzko spojený,
a preto je pre ne silne
charakteristický).
2.
váha termu t bude nižšia, ak sa t v dokumente d vyskytuje málo alebo ak sa vyskytuje
v mnohých dokumentoch (term ponúka slabšiu rozlišovaciu schopnosť pre dokumenty)
3.
váha termu t bude najnižšia, ak sa vyskytuje skoro vo všetkých dokumentoch
4.
váha termu t bude nulová, pokiaľ sa t v dokumente d nevyskytuje
Metóda T F –IDF pre váhovanie termov predstavuje prvý krok značného vylepšenia
vlastností vektorového modelu. Neskoršie podkapitoly problematiku váhovania ďalej rozší-
ria.
Pozn.
Uvažovanú množinu vektorov je možné reprezentovať aj
ako maticu X,
ktorej
riadky predstavujú termy a stĺpce dokumenty.
Obsah matice sú váhy vypočítané kon-
krétnou metódou váhovania, typicky T F –IDF . Táto matica sa potom tiež nazýva matica
dokumenty x termy.
2.3
Kosínusová podobnosť
Pomocou vektorového modelu popísaného v kapitole 2.1 je možné univerzálne reprezento-
vať dokumenty aj užívateľské dotazy, pretože tie sú v skutočnosti tiež dokumentom, resp.
multimnožinou slov. Vzhľadom k tejto reprezentácií môžeme intuitívne poznamenať, že čím
bližšie si budú dva vektory v priestore, tým väčšia bude ich podobnosť. Vzdialenosť medzi
vektormi je možné zmerať viacerými spôsobmi, napr. z hľadiska euklidovskej vzdialenosti.
V tomto prípade však môže dôjsť k nežiaducemu javu,
a to v prípade,
že všetky slová
v jednom vektore zduplikujeme. Dôsledkom by bol fakt, že vektor by sa v priestore predĺžil
a euklidova vzdialenosť k ostatným vektorom by sa zmenila, zatiaľ čo význam dokumentu
by aj napriek zdvojeniu slov bol
stále rovnaký.
Inými
slovami,
euklidovská vzdialenosť je
veľká pre vektory rôznych dĺžok.
Pre odstránenie tohto javu je výhodnejšie merať veľkosti uhlov medzi vektormi. Pre me-
ranie podobnosti dvoch dokumentov sa preto najčastejšie používa metrika kosínusovej po-
dobnosti [6, str. 18-19]. Čím je zmeraný uhol menší, tým sú si dva vektory bližšie, resp. tým
sú si dva dokumenty podobnejšie a naopak, čím je uhol medzi dvomi vektormi väčší, tým
sú tieto vektory od seba vzdialenejšie a teda dokumenty,
ktoré vektory reprezentujú sú si
menej podobné.
Kosínusová podobnosť medzi
vektormi
dvoch dokumentov u a v je daná
vzťahom:
similarity = cos(θ) =
u · v
ku k ∗ kv k
=
P
n
i=1
u
i
× v
i
p
P
n
i=1
(u
i
)
2
×
p
P
n
i=1
(v
i
)
2
(2.IV)
Výsledná podobnosť náleží do intervalu h−1, 1i, pričom −1 značí úplný opak a 1 zhodu.
Predchádzajúca kapitola sa venovala nastavovaniu váh jednotlivým dimenziám vektorov
práve preto, aby sme dosiahli čo najlepšie rozloženie vektorov v n-dimenzionálnom priestore.
5
2.4
Latentná sémantická analýza
Jedným z najväčších problémov reprezentácie článkov pomocou vektorov, kde každý vektor
má práve toľko dimenzií
koľko je v danom modele slov,
je príliš vysoká dimenzionalita.
S nárastom počtu dimenzií
exponenciálne rastie veľkosť priestoru,
v ktorom sa vektory
nachádzajú a väčšina z nich sa začína hromadiť na samých okrajoch.
Dôsledkom toho sú
pri
veľkom počte dimenzií
všetky dokumenty od seba vzdialené približne rovnako.
K zní-
ženiu počtu dimenzií
existuje viacero techník,
v ktorých dochádza k filtrovaniu dimenzií
alebo k nahradeniu pôvodných príznakov novými.
V tomto prípade sa v podstate jedná
o zhlukovanie slov,
pričom sa často využíva stemming alebo lemmatizácia[11].
Stemming
je výpočetná procedúra,
ktorá upravuje slová s rovnakým koreňom do základného tvaru
(tzv. stem), a to obvykle oddelením prípon, ktoré vznikli pri skloňovaní alebo stupňovaní.
Nájdenie morfologického koreňa slova však nie je zaručené a v takom prípade sa uvažuje
o najdlhšej spoločnej časti. Tomuto nedostatku predchádza lemmatizácia, ktorá proces vy-
lepšuje a vždy určuje správny základ slova (tzv.
lemma),
pričom pracuje s informáciami
o slovnom druhu, čísle, páde, vzore, atď.
2.4.1
Teoretický základ LSA
Táto práca sa však sústredí
na iné techniky,
pričom prvou z nich je Latentná sémantická
analýza (skr. LSA; metóda je známa aj pod názvom Latentná sémantická indexácia - LSI)
[4].
Princípom tejto metódy je zníženie dimenzionality vytvorením tzv.
konceptov alebo
tém, kde témou v článkoch o umení môže byť napríklad obdobie renesancie alebo portréty.
Jednotlivé slová nemusia patriť len do jednej
témy,
ale typicky patria do viacerých tém
s rôznymi
váhami.
Pri
automatických metódach identifikácie tém,
ako je LSA,
je dôležité
určiť počet týchto tém hneď na začiatku,
no pre zvolenie zodpovedajúceho počtu tém nie
je odporúčané pravidlo, a preto je vždy nutné vyskúšať, ktorá hodnota poskytuje najlepšie
výsledky. Vo všeobecnosti však platí, že vyšší počet tém nezaručuje lepšiu výkonnosť, pre-
tože témy sa môžu stať príliš špecifickými.
Naopak pri
malom počte dimenzií
budú témy
príliš obecné.
LSA pracuje s informáciou o spoločnom výskyte dvoch alebo viacerých slov v doku-
mente.
Prepokladá,
že ak sa dva slová vyskytujú vo väčšine dokumentov spolu,
tak sú si
istým spôsobom blízke.
Takýto spoločný výskyt môže byť považovaný za spoluvýskyt pr-
vej úrovne. Vďaka tomuto princípu potom LSA dokáže rozoznať sémantickú podobnosť aj
medzi slovami, ktoré sa spolu nemusia vyskytovať nikdy alebo len zriedka. Napr. slová ako
painting a sculpture sa spolu v jednom článku príliš často vyskytovať nemusia, ale obidve
slová sa môžu často vyskytovať so slovom art. Toto sa nazýva spoluvýskyt druhej úrovne,
pričom úrovní môže byť viac, ale miera sémantickej podobnosti sa so stúpajúcou úrovňou
znižuje. Miera spoločných výskytov potom určuje tému, resp. tieto slová sú zobrazované do
tej istej dimenzie. Teraz je zjavné, že názvy týchto dimenzií/tém je niekedy obtiažne slovne
interpretovať,
no pokiaľ
je cieľom výpočtu len sémantická podobnosť dokumentov a slov,
názvy tém nie sú potrebné.
2.4.2
Singular Value Decomposition
Pre nájdenie tém,
príslušnosti
slov k témam a miery príslušnosti
dokumentov k týmto
témam sa používa matematická metóda nazývaná Singular Value Decomposition (SVD)[5].
V podkapitole 2.2 sme si predstavili maticu X, ktorá je pre SVD východzým bodom. Táto
obdĺžniková matica X je dekompovaná do troch iných matíc,
ktoré pozostávajú z ľavých
6
singulárnych vektorov (T ), pravých singulárnych vektorov (D) a posledná zo singulárnych
hodnôt (S):
X = TSD
0
(2.V)
Tieto špeciálne matice ukazujú rozpis pôvodných dát do lineárne nezávislých kompo-
nentov alebo faktorov. Obecne mnoho z týchto faktorov je veľmi malých, čo znamená, že je
možné ich ignorovať. Toto vedie k aproximácii modelu, ktorý má omnoho menej faktorov.
Redukcia prebieha tak, že v zoradenej matici singulárnych hodnôt S je ponechaných len k
hodnôt a ostatné sú nastavené na nuly.
Produktom výsledných matíc je matica
ˆ
X,
ktorá
je približne zhodná s maticou X a má hodnosť k. Keďže S teraz obsahuje nuly, reprezentá-
cia môže byť zjednodušená odstránením nulových riadkov a stĺpcov pre získanie matice S
k
a tiež odstránením zodpodvedajúcich sĺpcov T a riadkov D pre získanie T
k
a D
k
. Výsledkom
je redukovaný model:
X ≈
ˆ
X = T
k
S
k
D
0
k
(2.VI)
Tento model má hodnosť k a je možné dokázať, že v zmysle metódy najmenších štvorcov
je k pôvodnej matici X najbližšie. Na obrázku 2.2 je znázornená schéma dekompozície pre
maticu dokumenty x termy (X),
kde t
je počet riadkov X,
d počet stĺpcov X a k je
hodnosť výslednej matice.
To znamená,
že zvolením čísla k ovplyvňujeme počet dimenzií
modelovaného priestoru, pričom platí k ≤ min(t, d). V prípade k = min(t, d) je výslednou
maticou pôvodná matica X, naopak ak bude k < min(t, d), tak výslednou maticou je matica
ˆ
X hodnosti k.
Obr. 2.2: Singular Value Decomposition
Podobnosť z SVD modelu [4, str. 398-399] je možné vypočítať pre:
Dva termy Riadky T
k
S
k
môžu byť považované za súradnice pre termy. Potom skalárnym
súčinom týchto bodov je porovnanie (podobnosť) dvoch termov.
Dva dokumenty Riadky D
k
S
k
môžu byť považované za súradnice pre dokumenty.
Ska-
lárnym súčinom týchto bodov je porovnanie (podobnosť) dvoch dokumentov.
Term a dokument Podobnosť termu a dokumentu je hodnota príslušnej
bunky matice
ˆ
X.
7
Nemenej
dôležitým výpočtom je podobnosť pre objekty,
ktoré sa nenachádzajú v pô-
vodnej analýze.
Jedná sa o užívateľské dotazy,
tzv.
pseudo-dokumenty.
Prvým krokom je
reprezentácia dotazu ako vektoru termov q s použitím konkrétnej metódy váhovania (2.2).
Poloha dotazu v redukovanom k -dimenzionálnom priestore je potom vypočítaná vzťahom:
q = q
0
T
k
S
−1
k
(2.VII)
Po získaní
polohy v danom priestore je možné vypočítať kosínus vzhľadom k polohe
iných termov alebo dokumentov (2.3).
2.5
Latentná Dirichletova alokácia
Latentná Dirichletova alokácia (skr. LDA) [1] je Bayesovský pravdepodobnostný model vy-
tvorený na korpuse dokumentov. Základnou myšlienkou je, že dokumenty sú reprezentované
príslušnosťou k niekoľkým témam podobne ako v LSA. Rozdiel oproti LSA je v tom, že prí-
slušnosť je udávaná ako pravdepodobnosť výskytu témy v danom dokumente. Každá téma
je definovaná ako multinomiálna distribúcia pravdepodobnosti
nad slovníkom slov,
ktorá
je založená na Dirichletovom rozložení,
β
k
∼ Dirichlet(η).
Po zvolení
k tém nasleduje
generatívny proces
1.
Pre každý dokument d z kolekcie všetkých dokumentov urči
distribúciu pravdepo-
dobnosti nad všetkými z k tém na základe multinomického rozdelenia s parametrom
θ
d
z Dirichletovho rozdelenia s parametrom α. Obe rozdelenia majú k dimenzií, teda
toľko,
koľko tém chceme identifikovať.
α je vektor k reálnych čísel
menších než 1,
ktoré sú spoločné pre všetky dokumenty a jedná sa o tzv. hyperparameter LDA mo-
delu. Vďaka voľbe parametrov Dirichletovho rozdelenia menších než 1 bude zaistené,
že s najväčšou pravdepodobnosťou bude mať len niekoľko málo tém nezanedbateľnú
pravdepodobnosť voči
dokumentu,
nevieme však ktoré.
Toto chovanie pritom odpo-
vedá skutočnosti, pretože dokument väčšinou pojednáva len o niekoľkých témach.
2.
Pre každé slovo t dokumentu d
(a) Zvoľ
tému z
td
∈ {1, ..., k} na základe váh tém θ
d
- multinomického rozdelenia
s parametrom θ
d
(b) Zvoľ
slovo w
td
z p(w
td
|z
td
, β) - z multinomického rozdelenia pravdepodobnosti
podmieneného témou z
td
. Podobne ako θ aj toto rozdelenie má parametry gene-
rované z Dirichletovho rozdelenia, tentokrát však s parametrom β.
Metódou LDA je korpus analyzovaný skúmaním posteriórnej
distribúcie slov nad té-
mami β, distribúcie tém nad dokumentmi θ a priradení tém z slovám podmienených doku-
mentmi. To naznačuje latentnú štruktúru v kolekcii, ktorá môže byť použitá pre predikciu
alebo prieskum dát. Úplná spoločná pravdepodobnosť tohto modelu je
p(w, z, θ, β|α, η) =
k
Y
i=1
p(β
i
|η)
n
Y
d=1
p(θ
d
|α)
len(d)
Y
t=1
p(z
td
|θ
d
)p(w
td
|β
z
td
)
(2.VIII)
kde len(d) je dĺžka dokumentu v slovách, α (resp. η) sú priórne funkcie na množinu tém
pre dokument (resp. distribúcia slov nad témami) [9, str. 30].
8
2.5.1
Odhad parametrov LDA
Pre nájdenie parametrov LDA, a teda aj požadovaných vektorov tém pre dokumenty, je po-
trebné maximalizovať pravdepodobnosť modelu pre poskytnuté trénovacie dáta. Kľúčovým
problémom je výpočet distribúcie pravdepodobnosti,
ktorú však nie je možné vypočítať
priamo, a preto sa pre túto úlohu používajú aproximačné algoritmy variačného odvodzova-
nia, Markovov reťazec Monte Carlo alebo Gibbsonovo vzorkovanie [7].
Empirický Bayesovský odhad pre LDA model
môžeme nájsť prostredníctvom varia-
čného EM algoritmu (Expectation-Maximization),
ktorý odhadne skryté premenné (prav-
depodobnosť príslušnosti
slova k téme) na základe posteriórnej
distribúcie tém a potom
maximalizuje parametry modelu tak,
aby bola dolná hranica pravdepodobnosti
čo najvä-
čšia s ohľadom na modelové parametre α a β. Algoritmus pracuje iteratívne v dvoch krokoch
(E-krok a M-krok) pokiaľ
spodná hranica pravdepodobnosti
koverguje.
V tomto projekte
bola použitá implementácia priameho (online) variačného odvodzovania EM algoritmom,
ktorý aproximuje posteriórnu distribúciu a dokáže analyzovať veľké kolekcie dokumentov.
Bližšie informácie o princípe tejto metódy je možné nájsť v [7, str. 3-5] a [1, str. 1003-1006].
2.6
Výskyt anotačných termov
Ako bolo spomenuté už v úvode,
články o umení
je možné špecializovanými
nástrojmi
sémanticky anotovať,
tzn.
okomentovať.
Princíp anotácie je znázornený na nasledujúcom
príklade.
Pôvodný dokument
Picasso demonstrated extraordinary artistic talent in his early years.
Anotovaný dokument
Picasso[person;Spanish_nationality;Francisco_Goya_influencedby] demonstra-
ted extraordinary artistic talent in his early years.
Z príkladu môžeme vidieť,
že slovu
”
Picasso“ bola priradená skupina slov ohraničená
hranatými zátvorkami. V praxi to potom znamená, že z dokumentu môžeme vyextrahovať
viac termov. Pre tento dokument by to mohli byť napríklad slová:
Picasso, [person], [Spanish_nationality], [Fransisco_Goya_influencedby],
demonstrated, extraordinary, artistic, talent, early, years
Použitie hranatých zátvoriek na uzavretie každého anotačného termu slúži práve na ro-
zoznanie pôvodných termov od anotačných. Ako bolo spomenuté v predchádzajúcich pod-
kapitolách,
každé slovo v základnom vektorovom priestore reprezentuje jednu dimenziu
a všetky nenulové hodnoty vektoru dokumentu sa prepočítajú pomocou metódy T F –IDF .
Toto počiatočné nastavenie, nad ktorým možno aplikovať pokročilé metódy, je považované
za efektívne pre pôvodné termy dokumentu.
Načŕta sa však otázka,
či
je možné zlepšiť
vlastnosti modelu prepočítaním váh anotačných termov iným spôsobom.
9
2.7
Váhovanie anotačných termov
Uvažujme teraz o vektoroch pozostávajúcich len z dimenzií
anotačných termov,
pôvodné
termy zanedbajme.
Predpokladajme tiež,
že disponujeme kolekciou dokumentov,
ktorých
sémantická podobnosť bola vzájomne ohodnotená človekom. Každá vzorka z je pár (x, y),
kde x je vektor dokumentu a y je referenčné hodnotenie.
Majme funkciu l(ˆ
y, y) merajúcu
stratu predpokladanej podobnosti
ˆ
y voči
skutočnej referenčnej podobnosti
y dokumentov
a zvoľme skupinu F funkcií f
w
(x) parametrizovateľných vektorom váh w. Hľadáme funkciu
f ∈ F ,
ktorá minimalizuje stratu Q(z, w) = l(f
w
(x), y) naprieč vzorkami.
Tento postup
aplikuje optimalizačná metóda Stochastic gradient descent (skr. SGD) [2], ktorá pracuje vo
viacerých krokoch, pričom v každom kroku odhadne gradient trénovacej kolekcie pomocou
stratového vektoru náhodnej vzorky z
t
a na základe tohto výpočtu aktualizuje váhy podľa
vzťahu:
w
t+1
= w
t
− γ 5 Q(z
t
, w
t
)
(2.IX)
kde γ je adekvátne zvolené tempo učenia. Výsledkom použitia metódy SGD je výpočet
nových váh pre hodnoty vektorov tak, aby po vynásobení vektoru dokumentu vektorom váh
w bola podobnosť určená funkciou l(ˆ
y, y) čo najlepšia. Pomocou nových váh pre anotačné
termy by sme mali
dosiahnuť vylepšenie vlastností
podobnostného modelu dokumentov,
pričom váhy pôvodných termov dokumentu zostali zachované.
Inými
slovami,
vzájomnú podobnosť vektorov dokumentov vypočítanú pomocou me-
tódy kosínusovej podobnosti porovnávame s referenčnou podobnosťou. Potom v prípade, že
existuje rozdiel medzi týmito podobnosťami, snažíme sa nájsť také váhy pre termy (anota-
čné termy), aby sa táto chyba minimalizovala. Preto sa metóda SGD popisuje ako metóda
pre hľadanie lokálneho minima.
10
Kapitola 3
Použité technológie
Projekt je implementovaný v jazyku Python 2.7.5+
1
a k svojej
práci
využíva niekoľko
knižníc a nástrojov, ktoré budú popísané v tejto kapitole.
3.1
Elasticsearch
Pre fyzické uloženie a prístup k dokumentom je použitý nástroj Elasticsearch (skr.
ES)
2
,
ktorý je postavený nad Apache Lucene a využíva jej funkcie a vlastnosti.
Elasticsearch je
flexibilný a distribuovaný open-source fulltextový vyhľadávač a nástroj pre analýzu textu.
Pre reprezentáciu dokumentov používa formát JSON. Vzhľadom k uloženiu dát sa v prípade
ES jedná o bezschémovú databázu,
pretože Elasticsearch je schopný vytvoriť štruktúru
databáze na základe dát.
Vďaka odozve v reálnom čase,
vysokej
spoľahlivosti
a ďalším
vlastnostiam ako jednoduchému ovládaniu cez RESTful API a možnosti pridania vlastnej
funkcionality vytvorením pluginov,
je výhodné uchovávať spracovávané dokumenty práve
týmto nástrojom.
3.1.1
Apache Lucene
Lucene
3
je veľmi
výkonná a flexibilná open-source knižnica pre vyhľadávanie informácií.
Lucene je jedným z fulltextových vyhľadávačov, ktoré ukladajú dáta do invertovaných in-
dexov.
Aby Lucene mohla vytvoriť invertovaný index,
potrebuje rozdeliť vstupný text na
jednotlivé slová a prípadne tieto slová upraviť (napríklad previesť na základný tvar). Tento
proces sa nazýva analýza a skladá sa z niekoľkých častí.
Filter znakov môže meniť,
pridávať alebo odoberať jednotlivé znaky vstupného textu
(typickým použitím je prevod veľkých písmen na malé)
Tokenizér je zodpovedný za rozdelenie vstupného textu na jednotlivé slová (tzv.
token
alebo term)
Token filtre každý vstupný token je podrobený nastaveným filtrom.
Tieto filtre môžu
token spracovať alebo modifikovať,
takže výstupom filtru môže byť žiadny,
jeden
alebo viac tokenov
1
https://www.python.org/
2
http://www.elasticsearch.org/
3
http://lucene.apache.org/
11
Po získaní tokenov je vytvorený invertovaný index. Stručne povedané sa jedná o dvojroz-
merné pole, v ktorom riadky predstavujú tokeny a stĺpce dokumenty (podobne ako matica
X v 2.2). Hodnota na priesečníku určuje počet výskytov slova v dokumente. Prednosť tejto
reprezentácie sa prejaví pri zložitejších dotazoch, v ktorých sa vyskytujú logické operátory
AND,
OR a NOT.
Vyhľadávanie užívateľského dotazu potom vráti
zoznam dokumentov
s prideleným skóre, ktoré je normalizované do intervalu h0, 1i.
Pre modelovanie sémantickej
štruktúry je vhodné filtrovať niektoré slová,
napríklad
predložky alebo iné veľmi
bežné slová.
Toto filtrovanie je zabezpečené práve nastavením
tokenového filtra v knižnici Lucene, resp. Elasticsearch.
3.1.2
Knižnica pyes
Napriek kvalitnému ovládaniu ES pomocou jeho API, existuje pre prístup a ďalšie zjedno-
dušenie práce s ES množstvo knižníc pre viaceré programovacie jazyky. Jednou z možností
pre jazyk python je knižnica pyes
4
, ktorá ponúka solídnu dokumentáciu a jednoduché ovlá-
danie, a preto som sa ju rozhodol v tomto projekte použiť.
3.1.3
Rozšírenia ES - pluginy
Pre účely sémantickej anotácie je použitý plugin Elasticsearch payload
5
,
ktorý vkladá
anotácie presne na miesta termov,
ku ktorým sa vzťahujú ako synonymá.
Po indexácií
dokumentov s anotáciami je potom možné na základe týchto anotácií vyhľadávať. Špecifickú
konfiguráciu indexu je možné zaviesť pomocou webového API ES.
V projekte je použité
nastavenie indexu podľa manuálu pluginu.
Napriek tomu,
že ES interne uvažuje o tokenoch,
základným návratovým údajom vy-
hľadávania je vždy celý dokument. Pre získanie dokumentu ako kolekcie jeho termov (ktoré
boli získané a analyzované práve analyzátorom ES) je nutné použiť plugin. Pre tento účel
bol využitý plugin tf-idf.
Veľmi užitočným nástrojom pre prácu s ES je aj nástroj Elasticsearch-head
6
, ktorý
umožňuje prehľadávanie dokumentov a ovládanie ďalších funkcií
ES cez grafické webové
rozhranie. Pre chod programu popisovaného v tomto dokumente však nie je nevyhnutný.
3.2
Knižnica gensim
Gensim
7
je populárna knižnica strojového učenia pre spracovanie textov [10]. Je zameraná
na automatickú tématickú analýzu rozsiahlych kolekcií
neštrukturovaných (prirodzených)
textov. Hlavným mottom tejto knižnice je ”Topic Modelling for Humans”, voľne preložené
ako ”Tématické modelovanie pre ľudí”.
Gensim sprístupňuje niektoré nízkoúrovňové matematické knižnice napísané v jazykoch
Fortran a C do jazyka python a používa ich ako stavebné bloky pre vyššie úrovňové al-
goritmy analýzy textu.
Vďaka užitiu týchto knižníc je potom gensim dostatočne rýchly.
Dôležitým aspektom tejto knižnice je tiež možnosť distribuovaného výpočtu a rozumná
správa zdrojov, tzn. aby nedochádzalo k zahlteniu operačnej pamäte pri veľkých kolekciách
textov.
4
http://pythonhosted.org/pyes/index.html
5
http://knot.fit.vutbr.cz/wiki/index.php/Elasticsearch_payload
6
http://mobz.github.io/elasticsearch-head/
7
http://radimrehurek.com/gensim/
12
Knižnica gensim ponúka implementáciu niekoľkých algoritmov pre modelovanie séman-
tickej
štruktúry.
Prvým krokom modelovania je vždy spracovanie zoznamu termov do-
kumentu,
tzn.
gensim na vstupe očakáva dokument ako zoznam jeho slov (tzv.
”Bag of
words”model dokumentu [9, str. 7-8], tzn. multimnožina slov), z ktorých následne vytvorí
slovník slov (priradí
slovám identifikačné čísla) a pokračuje vytvorením základného vek-
torového modelu,
resp.
vytvorením korpusu dokumentov,
v ktorom sú pre reprezentáciu
termov aj dokumentov použité ich identifikačné čísla (matica dokumenty x termy (2.2)).
Gensim ďalej sprístupňuje algoritmy tfidf, LSA (LSI), LDA a ďalšie, pričom v tomto
projekte boli použité práve tieto tri. Výhodou je možnosť perzistentného uloženia natréno-
vaného modelu pre neskoršie použitie a tiež jednoduché znovunačítanie uloženého modelu.
Toto sa využíva hlavne pri spracovaní užívateľských dotazov.
Pre porovnanie blízkosti
(podobnosti) dokumentov v rámci
vypočítaného modelu je
pripravený tzv.
index dokumentov,
ktorý obsahuje vzájomné podobnosti
všetkých doku-
mentov daného modelu vypočítané štandartnou metrikou kosínusovej podobnosti. Nakoľko
môže byť index veľmi veľký, je možné rozložiť ho do niekoľkých častí, o čo sa dokáže interne
postarať knižnica.
Gensim teda ponúka také mechanizmy práce s indexom,
aby nedošlo
k zahlteniu operačnej pamäte.
Numpy, Scipy
Knižnica gensim interne využíva knižnice NumPy a SciPy
8
, ktoré ponúkajú rýchly výpočet
náročných matematických výpočtov, nakoľko jadro týchto knižníc pracuje v jazyku C. Pre
použitie gensim-u je preto nutná inštalácia týchto dvoch knižníc.
Funkcie knižníc NumPy
a SciPy sú naviac využité aj v iných oblastiach tohto projektu.
3.3
Sémantické anotácie
Výpočet sémantických anotácií je náročná operácia na zdroje i čas. Preto získávanie týchto
anotácií
bolo uskutočňované cez volania na vzdialený server,
kde je anotačný program
spustený ako služba. Pre sémantické anotovanie dokumentov o umení slúži nástroj ner. Je
implementovaný v jazyku python,
ale pre náročné výpočty,
spojené s hľadaním možných
anotácií pre dokument, využíva jazyk C.
Praktický význam týchto anotácií je znázornený
v podkapitole 2.6.
3.4
Webové rozhranie
Pre tvorbu webového rozhrania bol
použitý framework Flask
9
,
ktorý poskytuje dyna-
mické generovanie webstránky. Pre vytvorenie interaktívnych prvkov bol ďalej využitý jazyk
javascript spolu s knižnicou jquery
10
.
8
http://www.scipy.org/
9
http://flask.pocoo.org/
10
http://jquery.com/
13
Kapitola 4
Návrh a príprava prostredia
4.1
Kolekcia dokumentov
Ako bolo spomenuté už v úvode, táto práca ja zameraná na spracovanie článkov o umení.
Samotná príprava kolekcie takýchto dokumentov je jednoduchá a rýchla.
Použil
som pre-
dovšetkým výťažky z článkov z anglickej wikipédie, pričom jeden takýto výťažok pozostáva
z 1-3 viet o priemerne 15 slovách. Pre použitie metódy SGD, a predovšetkým pre testovanie,
je však potrebné, aby vzájomná podobnosť týchto článkov bola ohodnotená človekom, resp.
aby existovali
referenčné hodnoty podobností
pre porovnávanie použitých metód.
Vlast-
ným úsilím som pre tento účel zostrojil testovací korpus s referenčným hodnotením pre 50
dokumentov (
50×50
2
= 1250 porovnaní, hodnotenie podobnosti v intervale h0, 1i).
Krížová validácia
Pretože vytvorený korpus je pomerne malý,
je pre overenie validity
modelu použitá krížová validácia (cross-validation) [3]. Princíp tejto metódy spočíva v zvo-
lení k dokumentov z korpusu, ktoré označíme ako testovacie dokumenty. Použitím zvyšných
dokumentov je potom vytvorený požadovaný model a testovacím dokumentom je následne
vypočítaná poloha v priestore tohto modelu.
Pomocou kosínusovej
podobnosti
je potom
určená vzájomná podobnosť testovacích dokumentov,
ktorá je následne porovnaná s refe-
renčnou podobnosťou. Po vypočítaní a uložení chyby metóda pokračuje zvolením nasledu-
júcich k dokumentov ako testovacích. Princíp metódy je znázornený na obrázku 4.1.
Obr. 4.1: Krížová validácia na korpuse 50-ich článkov pri k = 10
Po získaní
chyby pre testovacie dokumenty v každom kroku algoritmu je vypočítaný
14
aritmetický priemer týchto chýb, tzn. celková chyba modelu. Krížová validácia teda zisťuje,
ako veľmi bude model statickej analýzy ovplyvňovať nezávislé vzorky dát. Tento postup je
významný pre predikciu správania neznámych dokumentov (dotazov) po predchádzajúcej
analýze známych dokumentov.
4.2
Extrakcia termov a váhovanie
Ako bolo naznačené v kapitole 3, dokumenty sú uchované nástrojom ElasticSearch, pri-
čom za použitia pluginu tfidf je možné priamo obdržať termy vybraného dokumentu
s informáciami o frekvenciách termov. Tieto frekvencie je možné hneď využiť, pretože uni-
kátne termy, ktoré sa vyskytujú len v rámci jedného dokumentu naprieč korpusom, sú pre
modelovanie sémantickej štruktúry bezvýznamné, a preto sú zanedbateľné. Ostatné termy
sú potom pripravené na použitie, resp. v tomto kroku reprezentujú dokument. Pre matema-
tické výpočty je však potrebná číselná reprezentácia a teda bezprostredne ďalším krokom je
zavedenie slovníka, ktorý termom priradí identifikačné čísla id. Dokument je teraz reprezen-
tovaný vektorom čísel, čím vzniká matica dokumenty x termy, kde počiatočné nastavenie
váh predpokladá hodnoty TF namiesto TF-IDF (ako by mohlo byť predpokladané podľa
názvu pluginu) a to kvôli konzistencii s ďalšími výpočtami realizovanými knižnicou gensim.
Nezávisle na kofigurácii modelu je prvým krokom aplikovanie metódy tfidf (z knižnice
gensim) pre nastavenie váh všetkých termov.
Metódu tfidf je okrem iného možné uložiť
ako natrénovaný statický model pre neskoršiu predikciu váh termov užívateľských dotazov.
Práve preto je výhodnejšie použiť implementáciu tejto metódy knižnicou gensim
1
.
Pri použití sémantických anotácií sú ďalej váhy týchto anotácií prenastavené metódou
SGD,
pričom výsledkom tejto metódy je vektor váh o dĺžke rovnej počtu anotačných ter-
mov
2
.
Tento vektor je takisto vhodné uchovať a v prípade,
že budeme chcieť do priestoru
zobraziť ďalšie anotované dokumenty (bez prepočtu modelu), tak tento vektor môžeme vy-
užiť na prenásobenie váh nových dokumentov (pokiaľ
obsahujú anotačné termy také,
aké
sa už v modele vyskytli).
4.3
Reprezentácia a podobnosť
Naváhované vektory je teraz možné porovnať napr. kosínusovou podobnosťou. Pokiaľ však
stále uvažujeme len o základnom vektorovom modele,
problémom naďalej zostáva vysoký
počet dimenzií.
Aby sa ušetrili
pamäťové nároky a zlepšila prehľadnosť,
tak je výhodné
použiť vhodnú štruktúru zobrazenia vektorov dokumentov.
Tieto vektory majú totiž vä-
čšinu svojich hodnôt rovných nule, pretože z celkového počtu slov korpusu (počet dimenzií)
obsahujú len niekoľko slov.
Zrejme najvhodnejším spôsobom je použitie slovníka.
To zna-
mená,
že vektory budú interne reprezentované nie ako vektor číslic,
ale ako vektor dvojíc
kľúč-hodnota. Potom viac nebude záležať na polohe hodnoty v rámci vektoru, pretože iden-
tifikátor termu bude kľúčom. Výsledok vyzerá nasledovne:
v = [0.2, 0.0, 0.0, 0.3, 0.5, 0.0, 0.9] −→ v
0
= [(0, 0.2), (3, 0.3), (4, 0.5), (6, 0.9)]
1
Neskoršiu predikciu váh termov užívateľského dotazu je možné vypočítať aj
pomocou pluginu,
avšak
implementácia cez knižnicu gensim je niekoľkonásobne rýchlejšia.
2
Každá zložka vektoru reprezentuje jeden anotačný term. Prenastavanie prebieha tak, že váhy anotačných
termov v dokumentoch sú vynásobené príslušnou hodnotou vektoru z SGD.
15
4.4
Ovládanie aplikácie
Podstatou projektu je porovnanie rôznych metód a postupov výpočtu podobnosti
doku-
mentov,
a preto je dôležitá jednoduchosť nastavenia týchto kombinácií.
Priamočiarejším,
avšak možno menej prehľadným spôsobom, je vytvorenie a používanie aplikácie v prostredí
terminálu s použitím na to pripravených prepínačov.
Pre názornejšiu a užívateľsky príve-
tivejšiu obsluhu je vhodné ovládanie pomocou webového rozhrania aplikácie.
Kombinácie
možností zvolenia parametrov modelu sú vyznačené v tabuľke 4.1.
dokumenty
váhovanie
model
počet tém krížová validácia
s anotáciami
TF-IDF + SGD
VSM
-
áno
LSI / LDA
*
áno
bez anotácií
TF-IDF
VSM
-
áno
LSI / LDA
*
áno
bez referenčnej
TF-IDF
VSM
-
nie
podobnosti
3
LSI / LDA
*
nie
Tabuľka 4.1: Kombinácie možností pri vytváraní modelu
Metódu SGD (2.7) môžeme pochopiteľne použiť len pokiaľ
pracujeme s anotovanými
dokumentmi (keďže v tomto prípade hľadáme váhy pre anotačné termy) a máme k dispo-
zícií
referenčné hodnoty vzájomných podobností
dokumentov.
Jediným ďalším rozdielom
je potom možnosť použitia krížovej validácie,
ktorá stráca opodstatnenie,
pokiaľ
nemáme
hodnoty,
oproti
ktorým by bolo možné výsledky porovnať.
Nezávisle na použitom modele
sa podobnosť vždy počíta metódou kosínusovej podobnosti.
Postup modelovania
Vytváraním zvoleného modelu na základe požadovaných parametrov z tab. 4.1 sa rozumie
nasledovný postup:
1.
Vytvorenie vektorovej reprezentácie dokumentov (korpus)
2.
Aplikovanie váhovania termov metódou TF-IDF
3.
Aplikovanie metódy SGD na anotačné termy (ak je toto v danom modele požadované
a dokumenty obsahujú anotácie)
4.
Aplikovanie metódy LSA alebo LDA so zvoleným počtom tém na korpus (ak je toto
v danom modele požadované)
5.
Vytvorenie indexu polôh vektorov (dokumentov) v danom priestore
Pozn. Pri použití krížovej validácie je tento postup aplikovaný v každej jej iterácii.
3
V prípade použitia rozsiahlejšej kolekcie dokumentov nie je možné obdržať referenčné hodnotenie po-
dobností človekom. Nejedná sa o pripravený testovací korpus pre tento projekt.
16
4.5
Iteratívne aplikovanie metódy LSA
Podľa princípu metódy LSA popísaného v kapitole 2.4,
je možné faktorizovať maticu X
na tri
matice a následne súčinom týchto troch matíc opäť získať maticu X,
resp.
maticu
ˆ
X hodnosti
k,
ktorá je aproximáciou matice X.
Súčasťou tejto práce je experiment,
pri
ktorom bude metóda LSA aplikovaná opakovane na ten istý korpus. Nasledovný algoritmus
predstavuje priebeh modelovania sémantickej štruktúry v tomto projekte pomocou metódy
LSA v niekoľkých iteráciách.
1.
Vytvorenie vektorovej reprezentácie dokumentov (korpus)
2.
Aplikovanie váhovania termov metódou TF-IDF
3.
Vo zvolenom počte iterácií
(a) Aplikuj metódu SGD na anotačné termy v matici X (pokiaľ je tento parameter
v danom modele žiadaný a dokumenty obsahujú anotácie)
(b) Aplikuj metódu LSA so zvoleným počtom tém na maticu X - korpus
(c) Vytvor index polôh vektorov (dokumentov) v LSA priestore
(d) Zisti
aktuálnu chybu vzájomnej podobnosti
testovacích dokumentov (ak nie je
použitá krížová validácia uvažuj trénovacie dokumenty) voči
referenčným hod-
notám a ukonči cyklus, ak je táto chyba väčšia ako v predchádzajúcej iterácii
(e) Nahraď aktuálnu maticu X výsledkom skalárneho súčinu matíc T
k
S
k
D
0
k
z LSA
modelu (nahradenie X jej aproximáciou
ˆ
X)
Pozn. Pri použití krížovej validácie je tento postup aplikovaný v každej jej iterácii.
17
Kapitola 5
Implementácia
Program pre modelovanie sémantickej štruktúry medzi
dokumentmi,
o ktorom pojednáva
táto práca, je možné používať prostredníctvom linuxového terminálu pomocou pripravených
prepínačov alebo prostredníctvom grafického webového rozhrania.
5.1
Štruktúra aplikácie
Pre implementáciu jadra aplikácie bol použitý objektovo orientovaný návrh, zatiaľ čo doda-
točné funkcie sú implementované imperatívne. Základné nastavenia sú uložené vo formáte
JSON v súbore settings.json.
Okrem nastavení
ciest k súborom,
ktoré aplikácia vytvorí
alebo použije, sa v tomto súbore nastavuje konfigurácia pripojenia k serveru Elasticsearch.
Preto pokiaľ je program používaný prostredníctvom terminálu, je tento súbor pred prvým
použitím potrebné správne nakonfigurovať pre pripojenie k žiadanému serveru. Celá súbo-
rová štuktúra aplikácie je znázornená na obrázku 5.1.
5.1.1
Nastavenie ES
Kód,
ktorý reálne ovplyvňuje stav serveru Elasticsearch je obsiahnutý v súboroch confi-
gure.py, create index and mapping.sh a feedes.py. Jedná sa o pripravenie prostredia
pre modelovanie sémantickej štruktúry, pretože toto modelovanie predpokladá kolekciu do-
kumentov pripravených v špecifikovanom indexe na serveri ES. Súbor configure.py tvorí
akýsi
obal
pre ďalšie dva súbory,
pretože využíva ich kód.
Inými
slovami,
pomocou tohto
súboru je možné a odporúčané vytvoriť nový index s potrebným mapovaním a následne
do neho zaindexovať kolekciu dokumentov. Je tiež možné špecifikovať, či nové dokumenty
majú byť sémanticky anotované a až potom zaindexované.
Pre lepšiu prehľadnosť,
ale aj
korektné fungovanie aplikácie, je nutné dodržiavať konvenciu názvov indexov nasledovne:
názov indexuPOČET DOKUMENTOVanotovaný
pričom najdôležitejším prvkom v reťazci
je údaj o sémantických anotáciách a v praxi
sa značí malým písmenom a. Príklady indexov:
• wiki50a - index s názvom wiki s päťdesiatimi anotovanými dokumentmi
• ksc513 - index s názvom ksc s päťstotrinástimi dokumentmi
18
Pre použitie nainštalovaného ES pluginu tfidf je k dispozícii súbor tfidf es.py posky-
tujúci rozhranie pre tento plugin. Pre získanie sémantických anotácií nástrojom ner, ktorý
je spustený ako služba na vzdialenom serveri, slúži súbor get entities.py.
Pretože pri použití krížovej validácie sa vždy postupuje po zoradenej kolekcii dokumen-
tov, súčasťou aplikácie je aj súbor docs shuffle.py obsahujúci implementáciu spoločného
zamiešania dokumentov a referenčných hodnotení
týchto dokumentov v oboch súboroch.
Keďže dokumenty boli
do kolekcie vyberané ručne,
tento postup zaistí
rozprestrenie prí-
buzných dokumentov v kolekcii,
čo je pre testovanie validity modelu metódou krížovej
validácie vhodné.
semantic similarity
models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . súbory jednotlivých modelov
static . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . css a javascript
templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . html
stránky
results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . informačné výpisy posledného modelu
base.html
connection.html
docs.html
model.html
search.html
txt docs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . dokumenty a referenčná podobnosť
wiki50 with title
wiki50 ref sim
configure.py . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vytvorenie a naplnenie indexu v ES
create index and mapping.sh . . . . . . . . . . . . tvorba indexu a jeho nastavení v ES
docs shuffle.py . . . . . . . . . . zmena poradia dokumentov a ref. hodnotení v súboroch
escorpus.py . . . . . . . . . . . . . . . . . . . . . . . . vytvorenie slovníka termov a korpusu z ES
feedes.py . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . indexácia kolekcie dokumentov do ES
get entities.py . . . . . . . . . . . . . . . . . . . získanie sémantických anotácií pre dokument
latentmodel.py . . . . . . . . . . . . . . . . . . . . . . aplikácia zvolených metód tvorby modelu
modelling.py . . . . . . . . . . . . . . . . . . . . . . . . . . . . . hlavný skript pre vytvorenie modelu
query.py . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . spracovanie užívateľského dotazu
settings.json . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . súbor s nastaveniami
sgd.py . . . . . . . . . . . . . . . . . . . . . . . implementácia metódy stochastic gradient descent
tfidf es.py . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . prístup k termom dokumentu z ES
view.py . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . server webového rozhrania
Obr. 5.1: Súborová štruktúra aplikácie
5.1.2
Vytváranie modelu
Vstupným bodom výpočtu a správcom celého procesu modelovania je trieda Modelling
v súbore modelling.py. V prípade použitia aplikácie cez terminál je tento súbor interpre-
tovaný jazykom python a s požadovanými parametrami modelu predaných cez argumenty
skriptu je vytvorený objekt danej triedy. Medzi hlavné činnosti tohto objektu patrí vyhod-
notenie krížovej validácie, tzn. vytváranie konkrétneho modelu je uskutočňované iteratívne
vždy na určenej podmnožine kolekcie dokumentov a výsledkom je aritmetický priemer chýb
19
jednotlivých modelov.
Objekt tiež zabezpečuje iteratívne aplikovanie metódy LSA na ten
istý model.
Dôležitým faktom je, že súbory každého výsledného modelu sú ukladané do samostat-
ného adresára.
Toto je potom možné využiť pre porovnanie výsledkov vyhľadávania nad
rôznymi
modelmi.
Objekt triedy Modelling preto zabezpečuje prepisovanie ciest súbo-
rov v súbore s nastaveniami na základe čísla vytváraného modelu. Pridávanie stále ďalších
a ďalších modelov je samozrejme možné resetovať,
a to práve pripravenou funkciou v sú-
bore modelling.py,
ktorá maže súbory a priečinky jednotlivých modelov a reinicializuje
cesty v súbore s nastaveniami.
Objekt triedy Modelling vytvára dva súbory vo formáte
HTML.
Prvý obsahuje stručné zhodnotenie vytvoreného modelu s farebným zvýraznením
najdôležitejších výsledkov. Druhý je vytvorený pri prvom spustení aplikácie, resp. pri resete
počtu modelov. Slúži pre porovnanie jednotlivých modelov, tzn. tie najdôležitejšie informá-
cie o poslednom modele sú do tohto súboru pridávané. Informácie pre porovnanie sú v tvare:
(poradové číslo) metóda modelovania počet tém* počet iterácií** annotated
info o poslednom SGD - posledná iterácia a chyba
krok krížovej validácie
výsledky krížovej validácie v jednotlivých krokoch
priemer chyby modelu vypočítaný pomocou krížovej validácie
Počet tém sa uvádza len pri metódach LSA a LDA. Počet iterácií len pri metóde LSA.
Značenie annotated sa pri neanotovaných dokumentoch vynecháva. Informácie o SGD a krí-
žovej validácií
sú samozrejme zobrazené,
len pokiaľ
boli
tieto metódy použité.
V prípade
nepoužitia krížovej
validácie sú posledné tri
riadky nahradené jedným,
ktorý zobrazuje
chybu podobností trénovacích dokumentov
1
(dokumenty použité pre vytvorenie modelu).
Vytvorenie korpusu
Prvým krokom vytvorenia modelu je získanie termov zaindexo-
vaných dokumentov.
Tento proces zabezpečuje trieda EsCorpus v súbore escorpus.py.
Vďaka pluginu tfidf máme hneď na začiatku prehľad o výskyte slov.
Dôležitým je hlavne
údaj o počte výskytov slova v rôznych dokumentoch. Slovo nachádzajúce sa len v jednom
dokumente totiž pre model nemá príliš význam, pretože by sa zbytočne zvýšila dimenziona-
lita, zatiaľ čo kladná hodnota na patričnej zložke vektoru by bola prítomná len vo vektore
jedného dokumentu. Okrem toho je na získané termy pripravený filter pre vynechanie pred-
ložiek atp. Po získaní termov sú vytvorené dva súbory, a to slovník mapovania termov na
identifikačné čísla a korpus dokumentov v tvare popísanom v podkapitole 4.3.
Sémantická štruktúra
Nasledujúcim procesom je aplikovanie požadovaných metód a mo-
delov na vytvorený korpus. Táto činnosť je pokrytá triedou LsiSpace v súbore lsispace.py.
V celej aplikácii je dôležité rozlíšiť stav, kedy je použitá metóda krížovej validácie. V tomto
prípade totiž dochádza k špeciálnym prípravám referenčných hodnôt,
tzn.
”vystrihnu-
tím”tých hodnôt, ktoré sú pre aktuálny model potrebné. Toto je nutné hlavne pre výpočet
váh anotačných termov metódou SGD implementovanou v súbore sgd.py, ale aj pre vzá-
jomné porovnanie testovacích dokumentov a vyhodnotenie chyby.
Tieto vystrihnuté hod-
noty sú uchované v novovytvorených súboroch s príponou sliced. Trieda LsiSpace ukladá
aplikované metódy ako súbory do priečinku daného modelu. Tieto priečinky sú pomenované
jednoducho poradovým číslom aktuálneho modelu.
1
V prípade,
že nie je k dispozícií
referenčné hodnotenie vzájomných podobností
dokumentov,
tak je
výsledkom nula.
20
5.1.3
Webové rozhranie
Ako je možné vidieť na obrázku 5.1,
pre účely webového rozhrania slúžia adresáre sta-
tic a templates, pričom tento ďalej obsahuje adresár results, do ktorého sú umiestňované
všetky súbory s výsledkami, napr. obidva súbory vytvorené v triede Modelling. Implemen-
tácia webového serveru s použitím frameworku Flask (3.4) je uložená v súbore view.py.
Ovládanie aplikácie cez webové rozhranie umožňuje názornejšie zrovnanie jednotlivých mo-
delov a odstraňuje nutnosť ručného upravovania konfiguračného súboru pre zmenu nastavení
pripojenia k serveru ES.
5.2
Metóda SGD
Pripravenie vstupu pre túto metódu sa odohráva v metódach triedy LsiSpace,
ktoré sú
volané objektom triedy Modelling. V prvom rade sa jedná o extrakciu anotačných termov
zo slovníka (mapovania identifikačných čísel na termy) vytvorenom spolu s korpusom doku-
mentov v triede EsCorpus. Identifikačné čísla anotačných termov sú uložené do zoznamu,
ktorý je následne vzostupne zoradený.
Prechádzaním cez kolekciu dokumentov a cez termy jednotlivých dokumentov je potom
vytvorená matica dokumenty x anotačné termy,
ktorá má na zodpovedajúcich miestach
tfidf hodnoty termov, zatiaľ čo ostatné pozície v matici sú nulové. Dokumenty bez akejko-
ľvek anotácie sú vynechané a toto je potom vo výpočtoch zohľadnené. Pri prechádzaní cez
kolekciu je vytvorená aj nová matica dokumenty x termy obsahujúca len pôvodné termy
a matica indexov anotačných termov,
t.j.
dvojrozmerné pole,
kde na riadkoch sú uložené
indexy použitých anotačných termov v dokumente.
Indexom sa myslí
pozícia v rámci
zo-
radeného zoznamu identifikačných čísel
anotačných termov.
Toto sa potom využíva pre
rekonštrukciu celého korpusu s tým,
že anotačné termy už majú priradené nové váhy na
základe výpočtu SGD.
def
update ( s e l f
,
alpha ,
n u m i t e r ) :
# vo
zvolenom p o c t e
i t e r a c i i
f o r
i
in range ( n u m i t e r ) :
c o s t
= s e l f . l o s s ( )
# c e l k o v a
chyba
modelu
i f
abs ( c o s t )
> abs ( s e l f .
l a s t
c o s t ) :
break
# p r i
z h o r s e n i
v y s l e d k o v
s k o n c i
e l s e :
s e l f .
l a s t
c o s t
= c o s t
# pre
kazdy
dokument
f o r
z
in range ( len ( s e l f . x ) ) :
s e l f . o n e
l o s s ( z )
# chyba
a k t u a l n e h o
dokumentu
# v e k t o r
vah = v e k t o r
vah − tempo
u c e n i a
∗
g r a d i e n t
p o d l a
chyby
a k t u a l n e h o
dokumentu
s e l f . w = s e l f . w − a l p h a
∗
np . d o t ( np . t r a n s p o s e ( s e l f . x ) ,
s e l f . o n e
l o s s
m a t r i x )
Výpis 5.1: SGD: Algoritmus metódy učenia v rámci triedy Sgd v súbore sgd.py
Vstupnými
zdrojmi
pre metódu stochastic gradient descent je matica anotačných ter-
mov (self.x) a matica referenčných hodnôt,
ktorá je pred samotným výpočtom upravená
21
tak, aby reflektovala len dokumenty obsahujúce anotačné termy. Výstupným údajom je vek-
tor váh pre anotačné termy, získaný po určitom počte iterácií metódy SGD. Váhy v tomto
vektore sú modifikované na základe vypočítaného gradientu pre každý dokument,
pričom
tento jav sa nazýva učenie a je ďalej ovplyvnený tempom učenia tzv. learning rate alebo α.
Typický algoritmus výpočtu vektoru váh w je zobrazený vo výpise 5.1. Implementácia me-
tódy učenia môže naviac obsahovať náhodné zamiešanie dokumentov pred začiatkom cyklu
for z in range (len(self.x)).
Toto je v triede pripravené,
avšak zamiešanie nemalo
skoro žiadny vplyv na výsledky a kvôli zvýšeniu času výpočtu bolo preto vynechané.
5.3
Výpočet podobnosti
Po aplikovaní
zvolených metód na korpus dokumentov je posledným krokom vytvorenie
podobnostnej matice x veľkosti počet_dokumentov x počet_dokumentov, ktorá uchováva
vzájomnú podobnosť medzi
jednotlivými
dokumentmi.
Vďaka použitej knižnici
gensim je
možné vytvorenú maticu jednoducho rozdeliť na niekoľko častí, čím je možné predísť stavu,
kedy sa všetky informácie načítajú do operačnej pamäte naraz.
Pri
veľkých korpusoch by
totiž mohol nastať problém s nedostatkom operačnej pamäte.
5.3.1
Porovnanie voči referenčným hodnotám
Pre testovanie úspešnosti vytvoreného modelu je vytvorená matica podobností x porovná-
vaná s maticou referenčných hodnôt y, ktorá je rovnakej veľkosti
2
. Pre výpočet rozdielnosti
týchto dvoch matíc je použitá stredná kvadratická odchýlka.
Jednotlivé rozdiely všetkých
dvojíc sú umocnené na druhú a sčítané. Výsledkom je suma vydelená počtom dvojíc.
loss =
1
m
2
m
X
i=1
m
X
j=1
(y
ij
− x
ij
)
2
(5.I)
5.3.2
Porovnanie voči užívateľskému dotazu
Užívateľský dotaz je chápaný ako jedna alebo viac viet s niekoľkými
slovami
oddelenými
bielymi
znakmi.
Aby bolo možné porovnať takýto dotaz voči
dokumentom v korpuse,
je
v prvom rade potrebné dotaz previesť do bag-of-words modelu (3.2), tzn. vytvoriť multim-
nožinu slov z vety. Tieto slová sú následne reprezentované pomocou identifikačných čísel zo
slovníka, ktorý bol vytvorený spolu z modelom. Je nutné spomenúť obmedzenie, že pokiaľ
užívateľský dotaz obsahuje slová,
ktoré sa v slovníku nenachádzajú,
tak tieto slová budú
ďalej ignorované.
Ďalším krokom spracovania dotazu je implicitné priradenie váh na korpuse natrénova-
nou metódou TF-IDF, ktorej stav sme si pre tento účel uložili. Pripomeňme si, že užívate-
ľský dotaz teraz vyzerá podobne ako v podkapitole (4.3).
Pre porovnanie vektoru dotazu
s vytvoreným modelom je ďalej nutné aplikovať na tento vektor všetky metódy, ktoré boli
aplikované na korpus dokumentov.
Napr.
pri
modele vytvorenom metódou LSA,
potrebu-
jeme získať reprezentáciu dotazu v priestore LSA.
Potom už je možné porovnať vektor
dotazu s vektormi
dokumentov v modele.
Na základe kosínusovej
podobnosti
sú potom
vrátené dokumenty, zoradené zostupne podľa veľkosti tejto podobnosti.
2
Matica referenčných hodnôt vzájomných podobností dokumentov testovacieho korpusu. Podkapitola 4.1
22
Kapitola 6
Testovanie
Testovanie za účelom porovnania jednotlivých modelov bolo uskutočnené tak, že bola me-
raná chyba vzájomnej
podobnosti
testovacích dokumentov pri
použití
krížovej
validácie
s krokom 10 voči referenčnej podobnosti týchto dokumentov. To znamená, že model obsa-
huje práve 40 dokumentov, zatiaľčo zvyšných 10 predstavuje testovacie dokumenty. Celková
chyba vzájomných podobností dokumentov voči referenčným podobnostiam však neposta-
čuje na kompletné porovnanie.
Týka sa to hlavne základného vektorového modelu,
ktorý
celkovú chybu nemusí
mať nutne najhoršiu,
avšak pri
spracovaní
užívateľských dotazov
vždy vráti
len dokumenty,
ktoré obsahujú slová z dotazu.
Práve preto sa v tejto kapitole
nachádza aj
porovnanie výsledkov,
ktoré ponúkajú jednotlivé modely pre vymyslený uží-
vateľský dotaz.
Testovacím korpusom,
ktorý bude v tejto kapitole viackrát spomenutý sa
rozumie korpus z podkapitoly 4.1.
Modely budú testované porovnaním výsledkov voči
očakávaným výsledkom pre užíva-
teľský dotaz who painted the most known woman. Sedem najrelevantnejších článkov
v korpuse vyzerá nasledovne
1
.
id
úryvok dokumentu
0
The Mona Lisa is a half-length portrait of a woman by the Italian artist. . .
7
Italian master Titian. . . young woman, identified with the goddess Venus. . .
12
Ginevra de’ Benci. . . the subject of a portrait painting by Leonardo da Vinci.
14
Madonna and Child with Flowers. . . one of two Madonnas Leonardo da Vinci. . .
29
Portrait of Do˜
na Isabel de Porcel. . made by the Spanish painter Francisco Goya.
36
Raphael. . . portrait of a young woman like Leonardo’s completed Mona Lisa.
41
Two of Goya’s best known paintings are The Nude Maja (La maja desnuda). . .
Pozn. Význam skratiek použitých v nasledujúcich podkapitolách:
• VSM - Vektorový model
• LSA - Latentná sémantická analýza
• LSA 2i / LSA 3iter - LSA aplikovaná v dvoch (v troch) iteráciách
• LDA - Latentná Dirichletova alokácia
• SGD - Metóda stochastic gradient descent aplikovaná na anotačné termy
1
Články sú skrátené. Celé znenie každého dokumentu testovacieho korpusu je k dispozícií v prílohe.
23
6.1
Dokumenty bez anotácií
Pri
metódach LSA a LDA je v prvom rade potrebné určiť počet tém.
Pretože neexistuje
žiadny overený vzťah, rozhodol som sa empiricky zistiť optimálny počet tém pre testovací
korpus.
Ako je vidieť na grafe 6.1 pri
metóde LDA aj pri
metóde LSA sa so zvyšovaním
počtu tém zlepšuje presnosť. Pri pohľade na krivku metódy LSA si však môžeme všimnúť,
že najmenšia chyba sa nachádza približne pri
počte tém 35 a nie pri
maximálnych 40.
To
je dôkazom, že metóda funguje, pretože zmenšuje dimenzionalitu a zároveň celkovú chybu
modelu.
V reálnych korpusoch sa vyskytujú státisíce dokumentov, a pretože také veľké korpusy
nemajú vzájomné referenčné hodnotenie od ľudí,
zvolenie najvhodnejšieho počtu tém je
skúšané na základe výsledkov užívateľských dotazov a väčšinou sa pohybuje v ráde stoviek
[4,
str.
402].
Čím je počet tém menší,
tým je zabezpečená vyššia rýchlosť pri
spracovaní
užívateľského dotazu.
Aj preto sa pri
veľkých korpusoch používa nižší
počet tém.
Z grafu
môžeme okrem iného vidieť fakt, že v intervale 20-40 tém sa už chyba metódy LSA oproti
referenčným hodnotám výrazne nezmenšuje.
Práve preto som sa rozhodol
pri
testovaní
výsledkov užívateľského dotazu použiť LSA model
o 20 témach.
V prípade LDA budeme
uvažovať podobne a pre porovnanie zvolíme tiež práve 20 tém.
0
5
10
15
20
25
30
35
40
Počet tém
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
Chyba testovacích dokumentov
(20, 0.195) lsa
(20, 0.215) lda
(20, 0.193) vsm
(20, 0.188) lsa 3iter
LSA
LDA
VSM
LSA 3iter
Obr. 6.1: Porovnanie VSM, LSA, LSA v troch iteráciách a LDA pri počte tém 1 až 40. Graf
zobrazuje priemernú chybu vzájomnej podobnosti testovacích dokumentov v rámci krížovej
validácie s krokom 10 oproti ref. podobnostiam.
24
Keďže test je uskutočňovaný v rámci krížovej validácie, tak v každom jej kroku obsahuje
iný počet vybraných slov. V priemere je to však 174 vybraných slov na model pre dokumenty
bez sémantických anotácií.
Súčasťou testovania je aj porovnanie s experimentom, kedy je metóda LSA aplikovaná
na ten istý model viackrát. Pri teste s rôznym počtom iterácií som zistil, že počet iterácií
väčší ako 3 už model ovplyvňuje veľmi málo a výsledky sa viac nezlepšujú. Preto som vzal
do úvahy len počet iterácií
2 a 3,
pričom s počtom iterácií
3 je výsledná chyba o trochu
menšia.
Preto je v grafe 6.1 zobrazená metóda LSA aplikovaná na ten istý model
práve
v troch iteráciách.
Porovnanie voči
dotazu
Ako vidieť z predchádzajúceho grafu,
základná metóda po-
skytuje oproti
ostatým metódam veľmi
malú chybu.
Neznamená to však,
že je najlepšia,
pretože vyhľadáva len na základe výskytu slov. Porovnanie výsledkov jednotlivých modelov
nad testovacím dotazom približuje nasledujúca tabuľka.
model
id prvých 10 dokumentov
id rel. dok.
počet rel. dok.
VSM
0, 33, 41, 31, 5, 28, 27, 10, 22, 30
0, 41
2
LSA
0, 36, 33, 25, 10, 30, 13, 41, 22, 29
0, 29, 36, 41
4
LDA
41, 5, 44, 29, 33, 3, 0, 11, 8, 14
0, 14, 29, 41
4
LSA 3iter
0, 10, 26, 43, 22, 37, 36, 25, 32, 30
0, 36
2
Tabuľka 6.1:
Zoradené výsledky jednotlivých modelov voči
testovaciemu dotazu.
Skratka
rel. dok. znamená relevantné dokumenty. Modely sú vytvorené na testovacom korpuse o 50
dokumentoch, pričom metódy LSA (LSA 3iter) a LDA modelujú 20 tém.
Najrelevantnejšie výsledky pre testovací
dotaz pri
použití
modelu s práve 20 témami
poskytuje metóda LSA a metóda LDA. Ako vidieť z grafu, pri trinástich témach sa zdá byť
LSA aplikovaná v troch iteráciách najlepšia.
Pokúsil
som sa preto porovnať výsledky pre
testovací
dotaz aj
v tomto mieste.
Najlepšie obstála metóda LSA s piatimi
relevantnými
výsledkami. LSA 3iter ponúkla len tri.
6.2
Dokumenty s anotáciami
V prípade dokumentov so sémantickými anotáciami je situácia obdobná. Pribúda však mo-
žnosť osobitného váhovania sémantických termov.
Toto je znázornené na grafe 6.2,
kde je
vidieť, že vlastnosti modelu LDA sa s použitím metódy SGD viditeľne zlepšili
2
. Pri ostat-
ných modeloch k výraznej zmene nedošlo.
Test je takisto ako v predchádzajúcom prípade
uskutočňovaný v rámci
krížovej
validácie a modely v jednotlivých jej
krokoch priemerne
obsahujú 465 slov. Tento počet je výrazne vyšší, čo naznačuje potenciálne zlepšenie výsled-
kov. Na druhú stranu však zdôrazňuje fakt, že vhodnými metódami ako LSA alebo LDA je
nutné zmenšovať dimenzionalitu a to hlavne pri veľkých kolekciách dát.
Experiment s viacnásobným opakovaním LSA na ten istý model
zlepšil
celkovú chybu
testovacích dokumentov len pri
počte iterácií
2.
Pri
väčšom počte viac nedochádza k vý-
razným zmenám. Toto platí aj pri opakovanom aplikovaní LSA pri použití SGD váhovania
anotačných termov, ktoré je oproti variante bez použitia SGD veľmi podobné, ale o trochu
lepšie. Kvôli prehľadnosti je v grafe 6.2 zobrazené len toto.
2
Tempo učenia, learning rate - α, pre metódu SGD je 0.01
25
0
5
10
15
20
25
30
35
40
Počet tém
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
Chyba testovacích dokumentov
(20, 0.194) vsm
(20, 0.198) vsm sgd
(20, 0.213) lsa
(20, 0.210) lsa sgd
(20, 0.227) lda
(20, 0.222) lda sgd
(20, 0.221) lsi2 sgd
LSA
LDA
VSM
LSA SGD
LDA SGD
VSM SGD
LSA2 SGD
Obr. 6.2: Sémanticky anotované dokumenty. Porovnanie VSM, LSA a LDA pri počte tém
1 až 40 bez použitia SGD a s použitím SGD pre váhovanie sémantických anotácií.
Graf
znázorňuje priemernú chybu vzájomnej podobnosti testovacích dokumentov v rámci krížovej
validácie s krokom 10 oproti ref. podobnostiam.
Porovnanie voči dotazu
Najrelevantnejšie odpovede pre testovací dotaz ponúkla podľa
tabuľky 6.2 metóda LSA,
a to bez použitia SGD.
Test pre jeden užívateľský dotaz však
nestačí pre dokázanie najlepšej metódy, avšak z grafu 6.2 je vidieť, že metóda LSA ponúka
veľmi
malú chybu modelu pre nezávislé dokumenty v rámci
krížovej
validácie.
Použitie
metódy SGD výsledky mierne zhoršilo,
ale za úspech považujem aspoň lepšie situovanie
relevantných dokumentov, tj. že sa nachádzajú na popredných miestach.
Dôležitým výsledkom testu je zlepšenie výsledkov modelu LDA pri použití metódy SGD,
čo dokazuje ako graf,
tak aj výsledky pre testovací
dotaz.
Zaujímavým výsledkom je tiež
to, že LDA ako jediná dokázala nájsť relevantný dokument s číslom 14.
Z tabuľky je tiež vidieť zhoršenie pozícií relevantných dokumentov metódy LSA apliko-
vanej v dvoch iteráciách oproti normálnej metóde LSA. V prípade použitia SGD však došlo
k výraznejšiemu zhoršeniu, čo znamená, že toto riešenie nezlepšuje vlastnosti LSA modelu
ani v jednej situácii.
26
model
id prvých 10 dokumentov
id rel. dok.
počet rel. dok.
VSM
0, 33, 31, 28, 41, 22, 27, 7, 10, 29
0, 7, 29, 41
4
LSA
36, 0, 10, 25, 43, 31, 41, 29, 7, 28
0, 7, 29, 36, 41
5
LDA
7, 22, 31, 33, 48, 29, 45, 0, 46, 47
0, 7, 29
3
LSA 2i
10, 36, 0, 43, 31, 25, 22, 7, 41, 29
0, 7, 29, 36, 41
5
VSM SGD
0, 22, 28, 27, 41, 31, 33, 29, 10, 38
0, 29, 41
3
LSA SGD
0, 43, 36, 29, 10, 22, 25, 15, 28, 31
0, 29, 36
3
LDA SGD
14, 46, 32, 33, 10, 42, 0, 7, 36, 35
0, 7, 14, 36
4
LSA 2i SGD 36, 22, 43, 0, 20, 25, 10, 31, 17, 2
0, 36
2
Tabuľka 6.2:
Zoradené výsledky jednotlivých modelov voči
testovaciemu dotazu.
Skratka
rel. dok. znamená relevantné dokumenty. Modely sú vytvorené na testovacom korpuse o 50
anotovaných dokumentoch, pričom metódy LSA (LSA 2i) a LDA modelujú 20 tém.
6.3
Porovnanie
Graf 6.3 porovnáva jednotlivé modely,
predstavené v predošlých dvoch podkapitolách,
na
základe aritmetického priemeru ich celkovej chyby pre testovacie dokumenty pri počte tém 1
až 40. Modely sú podľa tejto chyby zoradené. Mohla by vzniknúť námietka, že by bolo vhod-
nejšie porovnať najmenšie dosiahnuté chyby jednotlivých modelov, avšak treba brať ohľad
na fakt, že pri reálnych korpusoch nevieme stopercentne určiť najlepší počet tém. Metódy
VSM,
LSA,
LSA aplikovaná viacnásobne a LDA sú vyobrazené vždy v troch variantách,
pričom v prvom prípade sa jedná o základné použitie metód na obyčajné dokumenty,
po-
tom na dokumenty, ktoré sú anotované (a.d.) a nakoniec na anotované dokumenty, ktorých
anotačné termy boli preváhované metódou SGD.
Je vidieť,
že napriek predpokladu zlepšenia sémantickej
štruktúry pri
použití
séman-
tických anotácií,
to nemusí
vždy platiť.
Konkrétnym príkladom sú metódy LSA a LDA.
Avšak pri použití metódy SGD pre osobitné váhovanie týchto anotačných termov je možné
dosiahnuť zlepšenie celkovej chyby modelov. Keďže vieme, že vektorový model dokáže vrátiť
len dokumenty,
ktoré obsahujú niektoré slovo z dotazu,
nebudeme ho považovať za najle-
pší. Dokazuje to aj fakt, že podľa tabuliek 6.1 a 6.2 tento model, na rozdiel od ostatných,
nikdy nevrátil dokumenty s číslom 14, 36 a 43, pretože tieto obsahujú len po jednom slove
z dotazu.
Experiment viacnásobného aplikovania metódy LSA na ten istý model
sa ukázal
ako
nespoľahlivý,
a to napriek relatívne malej
chybe testovacích dokumentov.
Najdôležitejšie
porovnanie tohto postupu je s normálnym (1-násobným) aplikovaním metódy LSA,
voči
ktorému sa výsledky mierne zhoršili. Dôležitým faktom je tiež reakcia na váhovanie metódou
SGD, ktorá, na rozdiel od jednorázového aplikovania LSA, vlastnosti modelu nezlepšila.
Najlepší
model
Najviac relevantných dokumentov pre testovací
dotaz ponúkli
modely
LSA a.d., LSA, LDA SGD a LDA. Najlepšie výsledky v rámci krížovej validácie, po zaned-
baní vyššie spomenutých nevyhovujúcich postupov, priniesli metódy LSA, LSA SGD a LDA
SGD. Vzhľadom na výsledky za najlepší model pre dokumenty bez anotácií považujem LSA,
zatiaľ čo pre dokumenty s anotáciami model LSA SGD. Keďže použitie sémantických ano-
tácií
má vysoký potenciál
ďalšieho zlepšovania,
za najvhodnejší
model
celkovo považujem
LSA SGD.
27
0.00
0.05
0.10
0.15
0.20
0.25
0.30
an LDA
LDA
an LSA
an sgd LSA2
an sgd LDA
an sgd LSA
LSA
an LSA2
LSA3
an sgd VSM
an VSM
VSM
Obr. 6.3: Porovnanie rôznych postupov modelovania sémantickej štruktúry na základe pred-
chádzajúcich výsledkov. V grafe sú znázornené aritmetické priemery chýb jednotlivých mo-
delov pri počtoch tém 1-40 podľa grafov 6.1 a 6.2. Skratka än”značí anotované dokumenty.
Najdôležitejšími faktami sú :
• Počet relevantných dokumentov pre testovací dotaz pri modele LSA SGD je síce nižší
než pri modele LSA a.d., avšak relevantné dokumenty sú sústredené viac na začiatku,
čo považujem za úspešný výsledok metódy SGD.
• Váhovanie anotačných termov metódou SGD výrazne ovplynilo priemernú chybu mo-
delu pri metóde LDA. Taktiež sa zlepšili výsledky pre testovací dotaz. Zdá sa, že pri
modelovaní sémantickej štruktúry pomocou metódy LDA je výhodné použiť osobitné
váhovanie anotačných termov metódou SGD.
• Ak základný vektorový model
dokáže pri
anotovaných dokumentoch ponúknuť viac
relevantných výsledkov než pri neanotovaných, tak je to dobrý predpoklad na to, že
pokročilejšie metódy takisto ponúknu lepšie výsledky pri anotovaných dokumentoch.
• Viacnásobné aplikovanie LSA metódy na ten istý model nemusí byť úplne spoľahlivé,
napriek nízkej priemernej chybe v rámci krížovej validácie.
28
Kapitola 7
Záver
Cieľom tejto práce bolo porovnať existujúce metódy pre modelovanie sémantickej štruktúry
pre články o umení,
ktorú je potom možné použiť pre získávanie relevantných výsledkov
pre užívateľské otázky.
Projekt sa tiež výrazne venoval
spracovaniu článkov,
ktoré boli
sémanticky anotované špecializovaným nástrojom (2.6).
Základným predpokladom vytvá-
rania modelu sémantickej štruktúry bola reprezentácia článku ako multimnožiny jeho slov,
tzv.
termov,
kde každý term má pridelené svoje ohodnotenie (váhu) v rámci
modelu.
Pre
výpočet tohto ohodnotenia boli
použité dve metódy,
pričom jedna z nich sa špecializuje
práve na anotácie, resp. anotačné termy (2.7).
Testovanie prebiehalo na testovacom korpuse dokumentov, ktorý bol pre tento účel zho-
tovený (4.1). Pre overenie vlastností jednotlivých modelov je vhodné pozorovať ich výsledky
pre nezávislé dokumenty, tj. dokumenty, ktoré neboli súčasťou modelovania. Pre tento účel
bola pri testovaní použitá krížová validácia, kde pre vytváranie modelu slúžila vždy len časť
korpusu,
zatiaľčo zvyšok dokumentov slúžil pre testovanie (4.1). Pre porovnanie modelov,
tzn.
skupiny metód použitých pri
modelovaní
sémantickej
štruktúry,
potom slúžilo refe-
renčné hodnotenie podobností (vzájomne všetkých) dokumentov v korpuse (5.3.1). Okrem
testovania pomocou krížovej validácie bol testovaný aj počet relevantných výsledkov jednot-
livých modelov pre testovací dotaz. Popis a výsledok testovania je obsiahnutý v samostatnej
kapitole.
Práca so sémanticky anotovanými
dokumentmi
ponúka zaujímavé možnosti.
Použitie
špeciálneho ohodnotenia anotačných termov dovoľuje meniť vlastnosti
modelu bez zásahu
do ohodnotenia pôvodných slov. Metóda SGD sa pre tento účel ukázala ako efektívna, avšak
jej
nedostatkom je potreba referenčného hodnotenia vzájomných podobností
pre všetky
dokumety.
Výsledný vektor váh je však potom možné použiť aj
pre dokumenty,
ktoré sa
výpočtu neúčastnili.
Vhodným rozšírením tejto práce by bolo spracovanie jednotlivých termov do základného
slovného tvaru pred samotným vytvorením sémantickej štruktúry.
Metódy,
ktoré sa tejto
problematike venujú, boli spomenuté v podkapitole 2.4.
29
Literatúra
[1]
Blei, D. M.; Ng, A. Y.; Jordan, M. I.: Latent Dirichlet Allocation. Journal
of
Machine Learning Research, ročník 3, Leden 2003: s. 993 – 1022, ISSN 1532-4435.
URL http://jmlr.org/papers/volume3/blei03a/blei03a.pdf
[2]
Bottou, L.: Large-Scale Machine Learning with Stochastic Gradient Descent. In
Proceedings of the 19th International
Conference on Computational
Statistics
(COMPSTAT’2010), editace Y. Lechevallier; G. Saporta, Paris, France: Springer,
August 2010, s. 177–187.
URL http://leon.bottou.org/papers/bottou-2010
[3]
Bro, R.; Kjeldahl, K.; Smilde, A. K.; aj.: Cross-validation of component models: A
critical look at current methods. Analytical
and Bioanalytical
Chemistry, ročník 390,
č. 5, 2008: s. 1241 – 1251, ISSN 16182642.
[4]
Deerwester, S.: Indexing by Latent Semantic Analysis. Journal
of the American
Society for Information Science, ročník 41, č. 6, 1990: s. 391–407, ISSN 00028231.
URL http://www.cs.bham.ac.uk/~pxt/IDA/lsa_ind.pdf
[5]
Forsythe, G. E.; Malcolm, M. A.; Moler, C. B.: Computer Methods for Mathematical
Computations, kapitola 9: Least squares and the singular value decomposition.
Englewood Cliffs, N.J.: Prentice Hall Professional Technical Reference, 1977, ISBN
0131653326.
[6]
Grossman David, A.; Ophir, F.: Information retrieval
algorithms and heuristics.
Springer, 2004, ISBN 402030045.
[7]
Hoffman, M. D.; Blei, D. M.; Bach, F.: Online learning for latent Dirichlet allocation.
In Neural
Information Processing Systems, 2010.
URL http://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf
[8]
Manning, C. D.; Raghavan, P.; Schütze, H.: Introduction to Information Retrieval,
kapitola 6. Scoring, term weighting and the vector space model. New York, NY, USA:
Cambridge University Press, 2008, ISBN 0521865719, 9780521865715, s. 109–133.
URL http://nlp.stanford.edu/IR-book/pdf/06vect.pdf
[9]
Řehůřek, R.: Scalability of Semantic Analysis in Natural
Language Processing.
Dizertační práce, Masaryk University, Faculty of Informatics, 2011.
URL http://radimrehurek.com/phd_rehurek.pdf
[10]
Řehůřek, R.; Sojka, P.: Software Framework for Topic Modelling with Large Corpora.
In Proceedings of the LREC 2010 Workshop on New Challenges for NLP
30
Frameworks, Valletta, Malta: ELRA, Květen 2010, s. 45–50.
URL http://is.muni.cz/publication/884893/en
[11]
Šanda, P.: Určení základního tvaru slova. Diplomová práce, Vysoké učení technické v
Brně, Fakulta elektrotechniky a komunikačních technologií, Brno, 2010.
31
Dodatok A
Ovládanie aplikácie
Cez terminál
Pri prvom spustení aplikácie je v prvom rade potrebné nastaviť požadované pripojenie na
server Elasticsearch. Je tiež dôležité nastaviť vstupný súbor s dokumentmi a súbor s refere-
nčnými podobnosťami týchto dokumentov tak
1
, aby odpovedali indexu, s ktorým budeme
pracovať. Toto sa uskutočňuje prepísaním patričných riadkov v súbore settings.json.
{
” conn ” :
{
” index name ” :
” w i k i 5 0 a ” ,
” i n d e x t y p e ” :
” a r t w o r k ” ,
” s e r v e r ” :
” l o c a l h o s t : 9 2 0 0 ”
} ,
” f i l e ” :
{
” d o c s ” :
” t x t
d o c s / w i k i 5 0
w i t h t i t l e ” ,
” d o c s
r e f s i m ” :
” t x t
d o c s / w i k i 5 0
r e f
s i m ”
}
}
Výpis A.1: Nastavenia, ktoré je potrebné pri používaní aplikácie cez linuxový terminál meniť
ručne.
Vytvorenie indexu
Po nastavení pripojenia na spustený server Elasticsearch je možné
vytvoriť a naplniť konkrétny index príkazom
python
c o n f i g u r e . py −f
t x t
d o c s / w i k i 5 0
w i t h t i t l e
−a
pričom použitím parametru -f môžeme zvoliť cestu k súboru, ktorý sa bude indexovať.
Parameter je nepovinný a implicitne sa používajú dokumenty v súbore špecifikovanom v
settings.json. Parametrom -a môžeme určiť, či majú byť tieto dokumenty pred indexo-
vaním sémanticky anotované.
Vytvorenie modelu
Vstupným bodom výpočtu je súbor modelling.py, ktorý príjíma
niekoľko parametrov, podľa ktorých vytvára požadovaný model. Informácie o jednotlivých
parametroch je možné nájsť vypísaním nápovedy (parameter -h).
1
Ak neexistuje referenčné hodnotenie, je treba nastaviť prázdny reťazec.
32
Spracovanie dotazu
Pre spracovanie užívateľského dotazu slúži
súbor query.py,
kto-
rého jediným argumentom je veta, voči ktorej následne ponúkne 15 najrelevantnejších do-
kumentov z indexu.
Cez webové rozhranie
Server webového rozhrania je spúšťaný príkazom
python
view . py
pričom nepovinným argumentom je ip adresa,
na ktorej bude služba spustená.
Implicitne
je to 127.0.0.1 na porte 5000. Webové rozhranie pozostáva zo štyroch stránok: model, docs,
search a connection.
connection Táto stránka slúži
pre nastavenie pripojenia k ES serveru,
zmenu indexu a
tiež k vytvoreniu nového indexu.
Okrem toho zobrazuje aktuálne používaný súbor s
dokumentmi
a súbor s referenčným hodnotením.
Tieto sa potom menia na základe
názvu indexu.
search Stránka search slúži
pre vyhľadávanie relevantných dokumentov voči
užívateľs-
kej
otázke.
Ponúka možnosť prepínania medzi
vytvorenými
modelmi,
čo poskytuje
možnosť pohldlného porovnávania modelov.
docs Stránka docs ponúka prehľad dokumentov použitých v poslednom vytvorenom mo-
dele. Ukazuje ako pôvodný dokument, tak aj zoznam termov, ktoré z neho boli vyex-
trahované.
Špeciálne potom tieto termy v dokumente aj zvýrazní.
Každý dokument
je teda na tejto stránke zobrazený v troch reprezentáciách.
Okrem toho sa v pravej
časti nachádza zoznam všetkých použitých termov (slovník mapovania id na termy).
model
Poslednou a najdôležitejšou stránkou je model,
kde je pomocou prepínačov vy-
tvorený požadovaný model.
Výsledky vytvoreného modelu sú v tejto stránke potom
zobrazené v troch prevedeniach. V spodnej časti je možnosť prepínať karty results a
log, pričom log obsahuje všetky výpisy aplikácie poslané na štandartný výstup (toto
je výsledkom použitia aplikácie cez terminál).
Karta results obsahuje farebný výpis
priebehu modelovania a dôležitých výsledkov vytvoreného modelu.
Tie najdôležitejšie výsledky sú napokon pridané na pravú stranu stránky, ktorá slúži
na porovnanie výsledkov jednotlivých modelov. Štruktúra týchto najdôležitejších vý-
sledkov je opísaná v podkapitole 5.1.2. Obsah tohto porovnania (a teda aj priečinok
každého vytvoreného modelu) je možné zmazať tlačítkom Reset comparison.
Pre
testovacie účely je celý korpus možno náhodne zamiešať a preindexovať tlačítkom
Shuffle and reindex.
33
Obr. A.2: Príklad webového rozhrania. Stránka pre vytváranie modelov.
34
Dodatok B
Testovací korpus
0.
The Mona Lisa is a half-length portrait of a woman by the Italian artist Leonardo da Vinci,
which has been acclaimed as ”the best known, the most visited, the most written about, the
most sung about, the most parodied work of art in the world.
1.
David is a masterpiece of Renaissance sculpture created between 1501 and 1504, by the Italian
artist Michelangelo. It is a 5.17-metre marble statue of a standing male nude.
2.
The Last Supper is a late 15th-century mural painting by Leonardo da Vinci in the refectory
of the Convent of Santa Maria delle Grazie, Milan.
3.
The Last Judgment,
or The Last Judgement,
is a fresco by the Italian Renaissance master
Michelangelo executed on the altar wall of the Sistine Chapel in Vatican City.
4.
The Pieta is a masterpiece of Renaissance sculpture by Michelangelo Buonarroti,
housed in
St.
Peter’s Basilica,
Vatican City.
It is the first of a number of works of the same theme by
the artist.
5.
The Creation of Adam is a fresco painting by Michelangelo, forming part of the Sistine Chapel
ceiling, painted circa 1511–1512.
6.
The Virgin of the Rocks is the name used for two paintings by Leonardo da Vinci, of the same
subject, and of a composition which is identical except for several significant details.
7.
The Venus of
Urbino is a 1538 oil
painting by the Italian master Titian.
It depicts a nude
young woman, identified with the goddess Venus, reclining on a couch or bed in the sumptuous
surroundings of a Renaissance palace.
8.
The Doni
Tondo or Doni
Madonna,
sometimes called The Holy Family,
is the only finished
panel painting by the mature Michelangelo to survive.
9.
The Moses is a sculpture by the Italian High Renaissance artist Michelangelo Buonarroti,
housed in the church of San Pietro in Vincoli in Rome.
10.
Sacred and Profane Love is an oil
painting by Titian,
painted circa 1514.
The painting is
presumed to have been commissioned by Niccol`o Aurelio, a secretary to the Venetian Council
of Ten to celebrate his marriage to a young widow, Laura Bagarotto.
11.
Mars and Venus is a c. 1483 painting by the Italian Renaissance master Sandro Botticelli. It
shows the Roman gods Venus and Mars in an allegory of beauty and valour.
12.
Ginevra de’ Benci was an aristocrat from fifteenth-century Florence, admired for her intelli-
gence by Florentine contemporaries. She is the subject of a portrait painting by Leonardo da
Vinci.
35
13.
The Madonna of
the Stairs is a relief
sculpture by Michelangelo in the Casa Buonarroti,
Florence. It was sculpted in around 1491, when Michelangelo was about seventeen. This and
the Battle of the Centaurs were Michelangelo’s first two sculptures.
14.
Madonna and Child with Flowers, otherwise known as the Benois Madonna, could be one of
two Madonnas Leonardo da Vinci
had commented on having started in October 1478.
The
other one could be Madonna of the Carnation from Munich.
15.
The Raft of the Medusa is an oil painting of 1818–1819 by the French Romantic painter and
lithographer Théodore Géricault. Completed when the artist was 27, the work has become an
icon of French Romanticism.
16.
Death of
Sardanapalus is an oil
painting on canvas,
dated 1827 by Eug`ene Delacroix.
Its
dimensions are 392 x 496 cm or 12’–1”x 16’–3”. It currently hangs in the Musée du Louvre,
Paris.
17.
Wanderer above the Sea of Fog is an oil painting composed in 1818 by the German Romantic
artist Caspar David Friedrich.
It currently resides in the Kunsthalle Hamburg in Hamburg,
Germany.
18.
Liberty Leading the People is a painting by Eug`ene Delacroix commemorating the July Re-
volution of 1830, which toppled King Charles X of France.
19.
The Third of
May 1808 is a painting completed in 1814 by the Spanish painter Francisco
Goya, now in the Museo del Prado, Madrid.
20.
The Lady of Shalott is an 1888 oil-on-canvas painting by the English Pre-Raphaelite painter
John William Waterhouse in Tate Britain in London, where it is usually on display, in room
1840 in 2013.
21.
The Barque of Dante, sometimes known as Dante and Virgil in Hell, is the first major painting
by the French artist Eug`ene Delacroix, and one of the works signalling a shift in the character
of narrative painting from Neo-Classicism towards the Romantic movement. It was completed
in time for the opening of
the Salon of
1822 and currently hangs in the Musée du Louvre,
Paris.
22.
Rain, Steam and Speed – The Great Western Railway is an oil painting by the 19th century
British painter J.
M.
W.
Turner.
The painting was first exhibited at the Royal
Academy in
1844, though it may have been painted earlier.
23.
The Second of May 1808,
also known as The Charge of the Mamelukes is a painting by the
Spanish painter Francisco Goya. It is a companion to the painting The Third of May 1808 and
is set in the Calle de Alcalá near Puerta del Sol, Madrid, during the Dos de Mayo Uprising.
24.
The Women of Algiers is an 1834 oil
on canvas painting by Eug`ene Delacroix.
It is located
in the Louvre,
Paris,
France.
The painting was first displayed at the Salon,
where it was
universally admired.
25.
The Nightmare is a 1781 oil painting by Anglo-Swiss artist Henry Fuseli (1741–1825). Since
its creation,
it has remained Fuseli’s best-known work.
With its first exhibition in 1782 at
the Royal
Academy of
London,
the image became famous;
an engraved version was widely
distributed and the painting was parodied in political satire.
26.
The Orphan Girl
at the cemetery is a painting by the French artist Eug`ene Delacroix.
The
girl’s body language and clothing evoke tragedy and vulnerability. For Delacroix, colors were
the most important ingredients for his paintings.
36
27.
The Entry of the Crusaders in Constantinople or ”The Crusaders Entering Constantinople¨ıs
a large painting by Eug`ene Delacroix.
It was commissioned by Louis-Philippe in 1838,
and
completed in 1840. Painted in oil on canvas, it is in the collection of the Musée du Louvre in
Paris.
28.
The Charging Chasseur, or An Officer of the Imperial Horse Guards Charging is an oil painting
on canvas of
about 1812 by the French painter Théodore Géricault,
portraying a mounted
Napoleonic cavalry officer who is ready to attack.The painting represents French romanticism
29.
Portrait of Do˜
na Isabel
de Porcel
is an oil-on-canvas painting made by the Spanish painter
Francisco Goya around 1805.
The portrait depicts Isabel
Lobo Velasco de Porcel,
who was
born at Ronda around 1780 and was the second wife of Antonio Porcel.
30.
Michelangelo di Lodovico Buonarroti Simoni, commonly known as Michelangelo, was an Ita-
lian sculptor,
painter,
architect,
poet,
and engineer of
the High Renaissance.
Two of
his
best-known works, the Pietà and David, were sculpted before he turned thirty.
31.
In Rome,
Michelangelo lived near the church of Santa Maria di
Loreto.
It was at this time
that he met the poet,
Vittoria Colonna,
woman of
Pescara,
who was to become one of
his
closest friends until her death in 1547.
32.
Michelangelo’s David is the most famous male nude of all time and destined to be reproduced
in order to grace cities around the world. The twisting forms and tensions of the Victory, the
Bruges Madonna and the Medici Madonna make them the heralds of the Mannerist art.
33.
Artists who were directly influenced by Michelangelo include Raphael,
who imitated Miche-
langelo’s prophets in two of his works. Other artists such as Pontormo drew on the writhing
forms of the Last Judgement and the frescos of the Capella Paolina.
34.
Michelangelo’s foyer of
the Laurentian Library was one of
the earliest buildings to utilise
Classical
forms in a plastic and expressive manner.
The dome of St Peter’s was to influence
the building of churches for many centuries, including Sant’Andrea della Valle in Rome and
St Paul’s Cathedral, London.
35.
Raphael
was an Italian painter and architect of
the High Renaissance.
Together with Mi-
chelangelo and Leonardo da Vinci,
he forms the traditional
trinity of great masters of that
period. The Marriage of the Virgin, also known as Lo Sposalizio, is an oil painting by him.
36.
Raphael, painter of Urbino, led a ”nomadic”life, working in various centres in Northern Italy,
but spent a good deal of time in Florence. Another drawing is a portrait of a young woman
like Leonardo’s completed Mona Lisa.
37.
Delacroix took for his inspiration the art of Rubens and painters of the Venetian Renaissance.
Friend and spiritual heir to Théodore Géricault, Delacroix was also inspired by Lord Byron,
with whom he shared a strong identification with the ”forces of
the sublime”,
of
nature in
often violent action.
38.
Delacroix’s most influential work came in 1830 with the painting Liberty Leading the People,
which for choice of
subject and technique highlights the differences between the romantic
approach and the neoclassical
style.
Less obviously,
it also differs from the Romanticism of
Géricault and the Raft of the Medusa.
39.
Following the Revolution of 1848, Liberty Leading the People, was finally put on display by
the newly elected President, Louis Napoleon. Eug`ene Delacroix died in Paris, France, and was
buried there in P`ere Lachaise Cemetery.
37
40.
Goya was born in Fuendetodos, Aragón, Spain. He was Spanish romantic painter and print-
maker. He moved to Madrid where he studied with Anton Raphael Mengs, then he relocated
to Rome, where in 1771 he won second prize in a painting competition organized by the City
of Parma.
41.
Two of Goya’s best known paintings are The Nude Maja (La maja desnuda) and The Clothed
Maja (La maja vestida). They depict the same woman in the same pose, naked and clothed,
respectively.
42.
In 1799 Goya published a series of 80 prints titled Caprichos.
The dark visions depicted in
these prints are partly explained by his caption,
”The sleep of
reason produces monsters”.
In 1774, he was asked by the German artist Anton Raphael Mengs, on behalf of the Spanish
crown, to undertake the series.
43.
Le Louvre,
Paris,
France,
was a medieval
fortress and the palace of
the kings of
France
before becoming a museum two centuries ago.
The museum’s collections,
which range from
antiquity to the first half of the 19th century,
are among the most important in the world.
Main attractions are Venus de Milo, Winged Victory of Samothrace, and Leonardo da Vinci’s
Mona Lisa.
44.
State Hermitage in Russia may be isolated from the artistic centers of
Paris,
Rome,
and
London, but the Hermitage has managed to acquire a spectacular collection of world art from
the Stone Age to the early 20th century.
Main attractions are The Treasure Gallery’s Gold
Rooms showcase golden masterpieces from Eurasia,
the Black Sea Littoral
in antiquity,
and
the Orient.
45.
The British Museum, London, England, looks after the national collection of archaeology and
ethnography—more than eight million objects ranging from prehistoric bones to chunks of
Athens’ Parthenon, from whole Assyrian palace rooms to exquisite gold jewels.
46.
The Prado, Madrid, Spain. The Spanish royal family is responsible for the Prado’s bounty of
classical masterpieces. Over centuries, kings and queens collected and commissioned art with
passion and good taste.
In addition to stars of
Spanish painting such as Velázquez,
Goya,
Ribera, and Zurbarán, the Prado has big collections of Italian (including Titian and Raphael)
and Flemish artists.
47.
The Vatican Museums,
Vatican City,
Italy.
The renowned Sistine Chapel
and the Raphael
Rooms are not to be missed and the Pinacoteca (picture gallery), which contains the cream
of the Vatican’s collection of medieval and Renaissance paintings.
48.
Michelangelo mixes his colors with both black and white to maximize the contrast range for
all
the colors he uses.
Michelangelo was the undisputed master of
drawing in 16th century
Italy. The cleaning of the Sistine Chapel ceiling and the Doni Tondo have revealed him to be
a colorist of great originality.
49.
Just a few years after Leonardo da Vinci
(1452-1519) achieved tonal
unity,
Michelangelo
Buonarroti
(1475-1564) tried a different approach.
His colors are brilliant and contrasted,
whereas da Vinci’s are subdued and unified.
38
