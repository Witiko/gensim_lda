arXiv:1801.05617v1 [cs.CL] 17 Jan 2018
Automatic Detection of Cyberbullying in Social Media Text
Cynthia Van Hee
1
, Gilles Jacobs
1
, Chris Emmery
2
, Bart Desmet
1
, Els Lefever
1
, Ben
Verhoeven
2
, Guy De Pauw
2
, Walter Daelemans
2
, and V´eronique Hoste
1
1
LT3, Ghent University
2
CLiPS, University of Antwerp
1
{cynthia.vanhee, gillesm.jacobs, bart.desmet, els.lefever, veronique.hoste}@ugent.be
2
{ben.verhoeven,
guy.depauw, walter.daelemans}@uantwerpen.be
2
c.d.emmery@uvt.nl
Abstract
While social media offer great communication opportunities, they also increase the vulnera-
bility of young people to threatening situations online.
Recent studies report that cyberbullying
constitutes a growing problem among youngsters.
Successful
prevention depends on the ad-
equate detection of
potentially harmful
messages and the information overload on the Web
requires intelligent systems to identify potential
risks automatically.
The focus of
this paper
is on automatic cyberbullying detection in social
media text by modelling posts written by
bullies,
victims,
and bystanders of online bullying.
We describe the collection and fine-grained
annotation of a training corpus for English and Dutch and perform a series of binary classifi-
cation experiments to determine the feasibility of automatic cyberbullying detection.
We make
use of
linear support vector machines exploiting a rich feature set and investigate which in-
formation sources contribute the most for this particular task.
Experiments on a holdout test
set reveal
promising results for the detection of
cyberbullying-related posts.
After optimisa-
tion of the hyperparameters,
the classifier yields an F
1
-score of 64% and 61% for English and
Dutch respectively, and considerably outperforms baseline systems based on keywords and word
unigrams.
1
Introduction
Web 2.0 has had a substantial impact on communication and relationships in today’s society.
Chil-
dren and teenagers go online more frequently, at younger ages, and in more diverse ways (e.g. smart-
phones,
laptops and tablets).
Although most of
teenagers’
Internet use is harmless and the ben-
efits of
digital
communication are evident,
the freedom and anonymity experienced online makes
young people vulnerable,
with cyberbullying being one of
the major threats (Livingstone et al.,
2010; Tokunaga, 2010; Livingstone et al., 2013).
Bullying is not a new phenomenon,
and cyberbullying has manifested itself
as soon as digital
technologies have become primary communication tools.
On the positive side, social media like blogs,
social
networking sites (e.g.
Facebook) and instant messaging platforms (e.g.
WhatsApp) make it
possible to communicate with anyone and at any time.
Moreover,
they are a place where people
engage in social interaction, offering the possibility to establish new relationships and maintain ex-
isting friendships (Gross et al., 2002; Mckenna & Bargh, 1999).
On the negative side however, social
media increase the risk of children being confronted with threatening situations including grooming
1
or sexually transgressive behaviour, signals of depression and suicidal thoughts, and cyberbullying.
Users are reachable 24/7 and are often able to remain anonymous if desired:
this makes social media
a convenient way for bullies to target their victims outside the school yard.
With regard to cyberbullying,
a number
of
national
and international
initiatives have been
launched over the past few years to increase children’s online safety.
Examples include
KiVa
1
,
a
Finnish cyberbullying prevention programme, the ‘
Non au harc`element
’ campaign in France, Belgian
governmental
initiatives and helplines (e.g.
clicksafe.be
,
veiligonline.be
,
mediawijs.be
) that provide
information about online safety, and so on.
In spite of these efforts, a lot of undesirable and hurtful content remains online. Tokunaga (2010)
analysed a body of
quantitative research on cyberbullying and observed cybervictimisation rates
among teenagers between 20% and 40%.
Juvonen & Gross (2008) focused on 12 to 17 year olds
living in the United States and found that no less than 72% of them had encountered cyberbullying
at least once within the year preceding the questionnaire. Hinduja & Patchin (2006) surveyed 9 to
26 year olds in the United States, Canada, the United Kingdom and Australia, and found that 29%
of the respondents had ever been victimised online.
A study among 2,000 Flemish secondary school
students (age 12 to 18) revealed that 11% of them had been bullied online at least once in the six
months preceding the survey (Van Cleemput et al.,
2013).
Finally,
the 2014 large-scale EU Kids
Online Report (Online,
2014) published that 20% of 11 to 16 year olds had been exposed to hate
messages online.
In addition,
youngsters were 12% more likely to be exposed to cyberbullying as
compared to 2010, clearly demonstrating that cyberbullying is a growing problem.
The prevalence of
cybervictimisation depends on the conceptualisation used in describing cy-
berbullying,
but also on research variables such as location and the number and age span of
its
participants.
Nevertheless,
the above-mentioned studies demonstrate that online platforms are in-
creasingly used for bullying,
which is a cause for concern given its impact.
As shown by Cowie
(2013);
Fekkes et al.
(2006);
O’Moore & Kirkham (2001),
cyberbullying can have a negative im-
pact on the victim’s self-esteem, academic achievement and emotional well-being. Price & Dalgleish
(2010) found that self-reported effects of
cyberbullying include negative effects on school
grades,
feelings like sadness, anger, fear, and depression and in extreme cases, cyberbullying could even lead
to self-harm and suicidal thoughts.
The above studies demonstrate that cyberbullying is a serious problem the consequences of
which can be dramatic.
Successful
early detection of
cyberbullying attempts is therefore of
key
importance to youngsters’
mental
well-being.
However,
the amount of
information on the Web
makes it practically unfeasible for moderators to monitor all
user-generated content manually.
To
tackle this problem,
intelligent systems are required that process this information in a fast way
and automatically signal potential threats.
This way, moderators can respond quickly and prevent
threatening situations from escalating.
According to recent research,
teenagers are generally in
favour of
such automatic monitoring,
provided that effective follow-up strategies are formulated,
and that privacy and autonomy are guaranteed (Van Royen et al., 2014).
Parental control tools (e.g.
NetNanny
2
) already block unsuited or undesirable content and some
social networks make use of keyword-based moderation tools (i.e., using lists of profane and insult-
ing words to flag harmful
content).
However,
such approaches typically fail
to detect implicit or
subtle forms of cyberbullying in which no explicit vocabulary is used.
There is therefore a need for
intelligent and self-learning systems that can go beyond keyword spotting and hence improve recall
of cyberbullying detection.
The ultimate goal
of
this sort of
research is to develop models which could improve manual
monitoring for cyberbullying on social
networks.
We explore the automatic detection of
textual
1
http://www.kivaprogram.net/
2
https://www.netnanny.com/
2
signals of cyberbullying,
in which it is approached as a complex phenomenon that can be realised
in various ways (see Section 3.5 for a detailed overview).
While a lot of the related research focuses
on the detection of
cyberbullying ‘attacks’,
the present study takes into account a broader range
of
textual
signals of
cyberbullying,
including posts written by bullies,
as well
as by victims and
bystanders.
We propose a machine learning method to cyberbullying detection by making use of
a linear
SVM classifier (Chang & Lin, 2011; Cortes & Vapnik, 1995) exploiting a varied set of features.
To
the best of our knowledge, this is the first approach to the annotation of fine-grained text categories
related to cyberbullying and the detection of
signals of cyberbullying events
.
It is also the first
elaborate research on automatic cyberbullying detection on
Dutch
social
media.
For the present
experiments,
we focus on an English and Dutch ASKfm
3
corpus,
but the methodology adopted is
language and genre independent, provided there is annotated data available.
The remainder of this paper is structured as follows:
the next section presents a theoretic overview
and gives an overview of the state of the art in cyberbullying detection, whereas Section 3 describes
the corpus.
Next, we present the experimental setup and discuss our experimental results.
Finally,
Section 6 concludes this paper and provides perspectives for further research.
2
Related Research
Cyberbullying is a widely covered topic in the realm of social sciences and psychology.
A fair amount
of research has been done on the definition and prevalence of the phenomenon (Hinduja & Patchin,
2012; Livingstone et al., 2010; Slonje & Smith, 2008),
the identification of different forms of cyber-
bullying (O’Sullivan & Flanagin, 2003; Vandebosch & Van Cleemput, 2009; Willard, 2007), and its
consequences (Cowie,
2013;
Price & Dalgleish,
2010;
Smith et al.,
2008).
In contrast to the efforts
made in defining and measuring cyberbullying,
the number of studies that focus on its annotation
and automatic detection,
is limited (Nadali et al.,
2013).
Nevertheless,
some important advances
have been made in the domain over the past few years.
2.1
A Definition of Cyberbullying
Many social
and psychological studies have worked towards a definition of cyberbullying.
A com-
mon starting point for conceptualising cyberbullying are definitions of traditional (or
offline
) bully-
ing.
Seminal work has been published by (Olweus, 1993; Nansel et al., 2001; Salmivalli et al., 1999;
Wolak et al., 2007), who describe bullying based on three main criteria, including i)
intention
(i.e.,
a bully intends to inflict harm on the victim), ii)
repetition
(i.e., bullying acts take place repeatedly
over time) and iii) a
power imbalance
between the bully and the victim (i.e., a more powerful bully
attacks a less powerful
victim).
With respect to cyberbullying,
a number of definitions are based
on the above-mentioned criteria.
A popular definition is that of Smith et al.
(2008,
p.
376) which
describes cyberbullying as “an aggressive, intentional act carried out by a group or individual, using
electronic forms of contact, repeatedly and over time, against a victim who cannot easily defend him
or herself”.
Nevertheless,
some studies have underlined the differences between offline and online bullying,
and have therefore questioned the relevance of the three criteria to the latter.
Besides theoretical
objections,
a number of
practical
limitations have been observed.
Firstly,
while Olweus (1993)
claims
intention to be inherent
to traditional
bullying,
this
is
much harder to ascertain in an
3
https://ask.fm/
3
online environment.
Online conversations lack the signals of
a face-to-face interaction like into-
nation,
facial
expressions and gestures,
which makes them more ambiguous than real-life conver-
sations.
The receiver may therefore get the wrong impression that they are being offended or
ridiculed (Vandebosch & Van Cleemput,
2009).
Another criterion for bullying that might not hold
in online situations, is the power imbalance between bully and victim.
Although this can be evident
in real
life (e.g.
the bully is larger,
stronger,
older than the victim),
it is hard to conceptualise or
measure in an online environment.
It may be related to technological
skills,
anonymity or the in-
ability of the victim to get away from the bullying Dooley & Cross (2009); Slonje & Smith (2008);
Vandebosch & Van Cleemput (2008).
Empowering for the bully are also inherent characteristics of
the Web:
once defamatory or confidential
information about a person is made public through the
Internet, it is hard, if not impossible, to remove.
Finally, while arguing that repetition is a criterion to distinguish cyberbullying from single acts
of aggression,
Olweus (1993) himself states that such a single aggressive action can be considered
bullying under certain circumstances,
although it is not entirely clear what these circumstances
involve.
Accordingly,
Dooley & Cross (2009) claim that repetition in cyberbullying is problematic
to operationalise, as it is unclear what the consequences are of a single derogatory message on a public
page.
A single act of aggression or humiliation may result in continued distress and humiliation for
the victim if it is shared or liked by multiple perpetrators or read by a large audience. Slonje et al.
(2013,
p.
26) compare this with a ‘snowball
effect’:
one post may be repeated or distributed by
other people so that it becomes out of the control of the initial bully and has larger effects than was
originally intended.
Given these arguments, a number of less ‘strict’ definitions of cyberbullying were postulated by
among others (Hinduja & Patchin,
2006;
Juvonen & Gross,
2008;
Tokunaga,
2010),
where a power
imbalance and repetition are not deemed necessary conditions for cyberbullying.
The above paragraphs demonstrate that defining cyberbullying is far from trivial,
and varying
prevalence rates (cf.
Section 1) confirm that a univocal definition of the phenomenon is still lacking
in the literature (Tokunaga,
2010).
Based on existing conceptualisations,
we define cyberbullying
as
content that is published online by an individual
and that is aggressive or hurtful
against a vic-
tim
.
Based on this definition, an annotation scheme was developed (see Van Hee, Verhoeven, et al.
(2015)) to signal
textual
characteristics of
cyberbullying,
including posts from bullies,
as well
as
reactions by victims and bystanders.
2.2
Detecting and Preventing Cyberbullying
As mentioned earlier,
although research on cyberbullying detection is more limited than social
studies on the phenomenon,
some important advances have been made in recent years.
In what
follows, we present a brief overview of the most important natural language processing approaches
to cyberbullying detection.
Although some studies have investigated the effectiveness of rule-based modelling (Reynolds et al.,
2011), the dominant approach to cyberbullying detection involves machine learning.
Most machine
learning approaches are based on supervised (Dadvar,
2014;
Dinakar et al.,
2011;
Yin et al.,
2009)
or semi-supervised learning (Nahar et al., 2014).
The former involves the construction of a classifier
based on labeled training data, whereas semi-supervised approaches rely on classifiers that are built
from a training corpus containing a small
set of labeled and a large set of unlabelled instances (a
method that is often used to handle data sparsity).
As cyberbullying detection essentially involves
the distinction between bullying and non-bullying posts,
the problem is generally approached as a
binary classification task where the positive class is represented by instances containing (textual)
cyberbullying, while the negative class includes instances containing non-cyberbullying or ‘innocent’
4
text.
A key challenge in cyberbullying research is the availability of suitable data, which is necessary
to develop models that characterise cyberbullying.
In recent years, only a few datasets have become
publicly available for this particular task,
such as the training sets provided in the context of the
CAW 2.0 workshop
4
and more recently, the Twitter Bullying Traces dataset (Sui, 2015).
As a result,
several studies have worked with the former or have constructed their own corpus from social media
websites that are prone to bullying content, such as YouTube (Dadvar, 2014; Dinakar et al., 2011),
Formspring
5
(Dinakar et al.,
2011),
and ASKfm (Van Hee, Lefever, et al.,
2015b) (the latter two
are social networking sites where users can send each other questions or respond to them).
Despite
the bottleneck of
data availability,
existing approaches to cyberbullying detection have shown its
potential,
and the relevance of automatic text analysis techniques to ensure child safety online has
been recognised (Desmet, 2014; Royen et al., 2016).
Among the first studies on cyberbullying detection are Yin et al. (2009); Reynolds et al. (2011);
Dinakar et al. (2011), who explored the predictive power of
n
-grams (with and without tf-idf weight-
ing), part-of-speech information (e.g. first and second pronouns), and sentiment information based on
profanity lexicons for this task.
Similar features were also exploited for the detection of cyberbullying
events and fine-grained text categories related to cyberbullying (Van Hee, Lefever, et al., 2015b,a).
More recent studies have demonstrated the added value of combining such content-based features
with user-based information, such as including users’ activities on a social network (i.e., the number
of
posts),
their age,
gender,
location,
number of
friends and followers,
and so on (Dadvar,
2014;
Nahar et al.,
2014;
Al-garadi et al.,
2016).
Moreover, semantic features have been explored to fur-
ther improve classification performance of the task.
To this end, topic model information (Xu et al.,
2012), as well as semantic relations between
n
-grams (according to a
Word2Vec
model (Zhao et al.,
2016)) have been integrated.
As mentioned earlier, data collection remains a bottleneck in cyberbullying research.
Although
cyberbullying has been recognised as a serious problem (cf.
Section 1),
real-world examples are
often hard to find in public platforms.
Naturally,
the vast majority of
communications do not
contain traces of
verbal
aggression or transgressive behaviour.
When constructing a corpus for
machine learning purposes,
this results in imbalanced datasets,
meaning that one class (e.g.
cy-
berbullying posts) is much less represented in the corpus than the other (e.g.
non-cyberbullying
posts).
To tackle this problem,
several
studies have adopted resampling techniques (Nahar et al.,
2014; Al-garadi et al., 2016; Reynolds et al., 2011) that create synthetic minority class examples or
reduce the number of negative class examples (i.e.,
minority class oversampling and majority class
undersampling (Chawla et al., 2002)).
Table 1 presents a number of recent studies on cyberbullying detection,
providing insight into
the state of
the art in cyberbullying research and the contribution of
the current research to the
domain.
The studies discussed in this section have demonstrated the feasibility of automatic cyberbullying
detection in social media data by making use of a varied set of features.
Most of them have, however,
focussed on cyberbullying ‘attacks’, or posts written by a bully.
Moreover, it is not entirely clear if
different forms of cyberbullying have been taken into account (e.g. sexual intimidation or harassment,
or psychological threats), in addition to derogatory language or insults.
In the research described in this paper, cyberbullying is considered a complex phenomenon con-
sisting of
different forms of
harmful
behaviour online,
which are described in more detail
in our
annotation scheme (Van Hee, Verhoeven, et al.,
2015).
Purposing to facilitate manual
monitoring
efforts on social
networks,
we develop a system that automatically detects signals of
cyberbully-
4
http://caw2.barcelonamedia.org/
5
https://spring.me/
5
Data
source
Balanced? Classifier
Features
Result
(Reynolds et al.,
2011)
Formspring
✓
J48
lexical
Recall=62%,
Acc=82%
(Dadvar, 2014)
YouTube
✗
SVM (linear)
lexical + seman-
tic + user-based
F
1
=64%
(Dinakar et al.,
2012)
YouTube
✓
SVM (poly-
nomial)
lexical
F
1
=77%
(Yin et al., 2009)
Kongregate,
Slashdot,
MySpace
✗
SVM (linear)
lexical
F
1
=48%
(Nahar et al., 2014)
Kongregate,
Slashdot,
MySpace
✗
semi-
supervised
fuzzy SVM
lexical
+ user-
based
F
1
=47%
(Van Hee, Lefever, et al.,
2015b)
ASKfm
✗
SVM (linear)
lexical
F
1
=55%
(Al-garadi et al.,
2016)
Twitter
✗
Random
Forest
+
SMOTE
lexical
+
se-
mantic
+
user-based
+
Twitter-based
F
1(micro)
=94%
AUC=94%
(Xu et al., 2012)
Twitter
✓
SVM (linear)
lexical
F
1
=77%
(Zhao et al., 2016)
Twitter
✗
SVM (linear)
lexical + seman-
tic
F
1
=78%
Table 1.
State-of-the-art approaches to cyberbullying detection.
ing,
including attacks from bullies,
as well
as victim and bystander reactions.
Similarly,
Xu et al.
(2012) investigated bullying traces posted by different author roles (accuser, bully, reporter, victim).
However,
they collected tweets by using specific keywords (i.e.,
bully,
bullied
and
bullying
).
As
a result,
their corpus contains many reports or testimonials of
a cyberbullying incident (example
1),
instead of
actual
signals that cyberbullying is going on.
Moreover,
their method implies that
cyberbullying-related content devoid of such keywords will not be part of the training corpus.
1.
‘Some tweens got violent on the n train, the one boy got off after blows 2 the chest...
Saw him
cryin as he walkd away :( bullying not cool’ (Xu et al., 2012, p.
658)
For this research,
English and Dutch social
media data were annotated for different forms of
cyberbullying,
based on the actors involved in a cyberbullying incident.
After preliminary experi-
ments for Dutch (Van Hee, Lefever, et al.,
2015b,a),
we currently explore the viability of detecting
cyberbullying-related posts in Dutch and English social
media.
To this end,
binary classification
experiments are performed exploiting a rich feature set and optimised hyperparameters.
3
Data Collection and Annotation
To be able to build representative models for cyberbullying,
a suitable dataset is required.
This
section describes the construction of two corpora, English and Dutch, containing social media posts
that are manually annotated for cyberbullying according to our fine-grained annotation scheme.
6
This allows us to develop a detection system covering different forms and participants (or
roles
)
involved in a cyberbullying event.
3.1
Data Collection
Two corpora were constructed by collecting data from the social
networking site ASKfm,
where
users can create profiles and ask or answer questions,
with the option of
doing so anonymously.
ASKfm data typically consists of
question-answer pairs published on a user’s profile.
The data
were retrieved by crawling a number of seed profiles using the GNU Wget software
6
in April
and
October,
2013.
After language filtering (i.e.,
non-English or non-Dutch content was removed),
the
experimental corpora comprised 113,698 and 78,387 posts for English and Dutch, respectively.
3.2
Data Annotation
Cyberbullying has been a widely covered research topic recently and studies have shed light on direct
and indirect types of cyberbullying, implicit and explicit forms, verbal and non-verbal cyberbullying,
and so on.
This is important from a sociolinguistic point of view, but knowing what cyberbullying
involves is also crucial
to build models for automatic cyberbullying detection.
In the following
paragraphs, we present our data annotation guidelines(Van Hee, Verhoeven, et al., 2015) and focus
on different types and roles related to the phenomenon.
3.3
Types of Cyberbullying
Cyberbullying research is mainly centered around the conceptualisation,
occurrence and preven-
tion of the phenomenon (Hinduja & Patchin, 2012; Livingstone et al., 2010; Slonje & Smith, 2008).
Additionally,
different forms of
cyberbullying have been identified (O’Sullivan & Flanagin,
2003;
Price & Dalgleish,
2010;
Willard,
2007) and compared with forms of
traditional
or offline bully-
ing (Vandebosch & Van Cleemput,
2009).
Like traditional
bullying,
direct and indirect forms of
cyberbullying have been identified.
Direct cyberbullying refers to actions in which the victim is di-
rectly involved (e.g. sending a virus-infected file, excluding someone from an online group, insulting
and threatening),
whereas indirect cyberbullying can take place without awareness of
the victim
(e.g.
outing
or publishing confidential information, spreading gossip, creating a hate page on social
networking sites) (Vandebosch & Van Cleemput, 2009).
The present annotation scheme describes some specific textual categories related to cyberbully-
ing, including threats, insults, defensive statements from a victim, encouragements to the harasser,
etc.
(see Section 3.5 for a complete overview).
All
of
these forms were inspired by social
stud-
ies on cyberbullying(Van Cleemput et al.,
2013;
Vandebosch & Van Cleemput,
2009) and manual
inspection of cyberbullying examples.
3.4
Roles in Cyberbullying
Similarly to traditional
bullying,
cyberbullying involves a number of participants that adopt well-
defined roles.
Researchers have identified several roles in (cyber)bullying interactions.
Although tra-
ditional studies on bullying have mainly concentrated on bullies and victims (Salmivalli et al., 1996),
the importance of bystanders in a bullying episode has been acknowledged (Bastiaensens et al., 2014;
Salmivalli, 2010).
Bystanders can support the victim and mitigate the negative effects caused by the
bullying (Salmivalli, 2010),
especially on social networking sites,
where they hold higher intentions
6
https://www.gnu.org/software/wget/
7
to help the victim than in real life conversations (Bastiaensens et al., 2015).
While Salmivalli et al.
(1996) distinguish four different bystanders, Vandebosch et al. (2006) distinguish three main types:
i) bystanders who participate in the bullying, ii) who help or support the victim and iii) those who
ignore the bullying.
Given that passive bystanders are hard to recognise in online text,
only the
former two are included in our annotation scheme.
3.5
Annotation Guidelines
To operationalise the task of
automatic cyberbullying detection,
we developed and tested a fine-
grained annotation scheme and applied it to our corpora.
While a detailed overview of the guide-
lines is presented in our technical report (Van Hee, Verhoeven, et al.,
2015),
we briefly present the
categories and main annotation steps below.
-
Threat/Blackmail:
expressions containing physical or psychological threats or indications of
blackmail.
-
Insult:
expressions meant to hurt or offend the victim.
*
General insult:
general expressions containing abusive, degrading or offensive language
that are meant to insult the addressee.
*
Attacking relatives:
insulting expressions towards relatives or friends of the victim.
*
Discrimination:
expressions of unjust or prejudicial treatment of the victim.
Two types
of discrimination are distinguished (i.e., sexism and racism).
Other forms of discrimina-
tion should be categorised as general insults.
-
Curse/Exclusion:
expressions of a wish that some form of adversity or misfortune will befall
the victim and expressions that exclude the victim from a conversation or a social group.
-
Defamation:
expressions that reveal
confident or defamatory information about the victim
to a large public.
-
Sexual
Talk:
expressions with a sexual
meaning or connotation.
A distinction is made
between innocent sexual talk and sexual harassment.
-
Defense:
expressions in support of the victim,
expressed by the victim himself or by a by-
stander.
*
Bystander defense:
expressions by which a bystander shows support for the victim or
discourages the harasser from continuing his actions.
*
Victim defense:
assertive or powerless reactions from the victim.
-
Encouragement to the harasser:
expressions in support of the harasser.
-
Other:
expressions that contain any other form of cyberbullying-related behaviour than the
ones described here.
Based on the literature on role-allocation in cyberbullying episodes
(Salmivalli et al.,
2011;
Vandebosch et al.,
2006),
four roles are distinguished,
including victim,
bully,
and two types of
bystanders.
1.
Harasser or Bully:
person who initiates the bullying.
8
2.
Victim:
person who is harassed.
3.
Bystander-defender:
person who helps the victim and discourages the harasser from con-
tinuing his actions.
4.
Bystander-assistant:
person who does not initiate, but helps or encourages the harasser.
Essentially, the annotation scheme describes two levels of annotation.
Firstly, the annotators were
asked to indicate, at the post level, whether the post under investigation was related to cyberbullying.
If the post was considered a signal of cyberbullying, annotators identified the author’s role.
Secondly,
at the subsentence level,
the annotators were tasked with the identification of
a number of
fine-
grained text categories related to cyberbullying.
More concretely,
they identified all
text spans
corresponding to one of the categories described in the annotation scheme.
To provide the annotators
with some context,
all
posts were presented within their original
conversation when possible.
All
annotations were done using the Brat rapid annotation tool (Stenetorp et al., 2012), some examples
of which are presented in Table 2.
Annotation
Annotation example
category
Threat/blackmail
[I am going to find out who you are & I swear you are going to regret
it.]
threat
Insult
[Kill
yourself]
curs
[you fucking mc slut !!!!]
gen. insult
[NO ONE
LIKES YOU !!!!!]
gen. insult
[You are an ugly useless little whore !!!!]
gen. insult
Curse/Exclusion
[Fuck
you.]
gen. insult
[Now
shush
I
don’t
wanna
hear
anything.]
curse or exclusion
Defamation
[She slept with her ex behind his girlfiends back and she and him had
broken up.]
defamation
Sexual Talk
[Naked pic of you now.]
sexual harassment
Defense
[I
would
appreciate
if
you
dindn’t
talk
shit
about
my
bestfriend.]
gen. victim defense
He has enough to deal with already.
Encour.
to har.
[She
is
a
massive
slut]
gen. insult
[i
agree
with
you
@user
she
is!]
encour. harasser
[LOL
AT HER mate,
im on
your
side]
encour. harasser
Table 2.
Definitions and brat annotation examples of more fine-grained text
categories related to cyberbullying.
3.6
Annotation Statistics
The English and Dutch corpora were independently annotated for cyberbullying by trained linguists.
All were Dutch native speakers and English second-language speakers.
To demonstrate the validity
of
our guidelines,
inter-annotator agreement scores were calculated using Kappa on a subset of
9
each corpus.
Inter-rater agreement for Dutch (2 raters) is calculated using Cohen’s Kappa (Cohen,
1960).
Fleiss’ Kappa (Fleiss, 1971) is used for the English corpus (
>
2 raters).
Kappa scores for the
identification of cyberbullying are
κ
= 0.69 (Dutch) and
κ
= 0.59 (English).
As shown in Table 3,
inter-annotator agreement for the identification of the more fine-grained
categories for English varies from fair to substantial (McHugh, 2012), except for
defamation
, which
appears to be more difficult to recognise.
No encouragements to the harasser were present in this
subset of
the corpus.
For Dutch,
the inter-annotator agreement is fair to substantial,
except for
curse
and
defamation
.
Analysis revealed that one of both annotators often annotated the latter as
an insult, and in some cases even did not consider it as cyberbullying-related.
Threat
Insult
Defense
Sexual
Curse/
Defamation
Encouragements
Talk
Exclusion
to the harasser
English
0.65
0.63
0.45
0.38
0.58
0.15
N/A
Dutch
0.52
0.66
0.63
0.53
0.19
0.00
0.21
Table 3.
Inter-annotator agreement on the fine-grained categories related to cyberbullying.
In short, the inter-rater reliability study shows that the annotation of cyberbullying is not trivial
and that more fine-grained categories like
defamation
,
curse
and
encouragements
are sometimes
hard to recognise.
It appears that defamations were sometimes hard to distinguish from insults,
whereas curses and exclusions were sometimes considered insults or threats.
The analysis further
reveals that encouragements to the harasser are subject to interpretation.
Some are straightforward
(e.g.
‘I agree we should send her hate’),
whereas others are subject to the annotator’s judgement
and interpretation (e.g. ‘hahaha’, ‘LOL’).
4
Experimental Setup
In this paper,
we explore the feasibility of
automatically recognising signals of
cyberbullying.
A
crucial difference with state-of-the-art approaches to cyberbullying detection is that we aim to model
bullying attacks,
as well
as reactions from victims and bystanders (i.e.,
all
under one binary label
‘signals of cyberbullying’),
since these could likewise indicate that cyberbullying is going on.
The
experiments described in this paper focus on the detection of
such posts,
which are signals of
a
potential cyberbullying
event
to be further investigated by human moderators.
The English and Dutch corpus contain 113,698 and 78,387 posts,
respectively.
As shown in
Table 4,
the experimental
corpus features a heavily
imbalanced
class distribution with the large
majority of posts not being part of cyberbullying.
In classification, this class imbalance can lead to
decreased performance.
We apply cost-sensitive SVM as a possible hyperparameter in optimisation
to counter this.
The cost-sensitive SVM reweighs the penalty parameter
C
of the error term by the in-
verse class-ratio.
This means that misclassifications of the minority positive class are penalised more
than classification errors on the majority negative class.
Other pre-processing methods to handle
data imbalance in classification include feature filtering metrics and data resampling (He & Garcia,
2009).
These methods were omitted as they were found to be too computationally expensive given
our high-dimensional dataset.
For the automatic detection of cyberbullying, we performed
binary classification experiments
using a linear kernel support vector machine (SVM) implemented in LIBLINEAR (Fan et al., 2008)
by making use of Scikit-learn (Pedregosa et al., 2011),
a machine learning library for Python.
The
motivation behind this is twofold:
i) support vector machines (SVMs) have proven to work well
for tasks similar to the ones under investigation (Desmet,
2014) and ii) LIBLINEAR allows fast
10
Corpus size
Number
(ratio)
of
bullying posts
English
113,698
5,375
(4.73%)
Dutch
78,387
5,106
(6.97%)
Table 4.
Statistics of the English and Dutch cyberbullying corpus.
Hyperparameter
Values
Penalty of error term
C
1
e
{−3,−2,...,2,3}
Loss function
Hinge, squared hinge
Penalty:
norm used in penalisation
‘l1’
(‘least absolute deviations’) or ‘l2’
(‘least squares’)
Class weight (sets penalty
C
of class
i
to weight*
C
)
None
or
‘balanced’,
i.e.,
weight
in-
versely proportional to class frequencies
Table 5.
Hyperparameters in grid-search model selection.
training of large-scale data which allow for a linear mapping (which was confirmed after a series of
preliminary experiments using LIBSVM with linear, RBF and polynomial kernels).
The classifier was optimised for feature type (cf.
Section 4.1) and hyperparameter combinations
(cf.
Table 5).
Model selection was done using 10-fold cross validation in grid search over all possible
feature types (i.e.,
groups of similar features, like different orders of
n
-gram bag-of-words features)
and hyperparameter configurations.
The best performing hyperparameters are selected by F
1
-score
on the positive class.
The winning model
is then retrained on all
held-in data and subsequently
tested on a hold-out test set to assess whether the classifier is over- or under-fitting.
The holdout
represents a random sample (10%) of all
data.
The folds were randomly stratified splits over the
hold-in class distribution.
Testing all
feature type combinations is a rudimentary form of
feature
selection and provides insight into which types of features work best for this particular task.
Feature selection over all
individual
features was not performed because of
the large feature
space (NL:
795,072 and EN:
871,296 individual
features).
Hoste (2005),
among other researchers,
demonstrated the importance of
joint optimisation,
where feature selection and hyperparameter
optimisation are performed simultaneously, since the techniques mutually influence each other.
The optimised models are evaluated against
two baseline systems
:
i) an unoptimised linear-
kernel
SVM (configured with default parameter settings) based on word
n
-grams only and,
ii) a
keyword-based system that marks posts as positive for cyberbullying if they contain a word from
existing vocabulary lists composed by aggressive language and profanity terms.
4.1
Pre-processing and Feature Engineering
As pre-processing,
we applied tokenisation,
PoS-tagging and lemmatisation to the data using the
LeTs Preprocess Toolkit (van de Kauter et al.,
2013).
In supervised learning,
a machine learning
algorithm takes a set of training instances (of which the label is known) and seeks to build a model
that generates a desired prediction for an unseen instance.
To enable the model
construction,
all
instances are represented as a vector of
features (i.e.,
inherent characteristics of
the data) that
contain information that is potentially useful
to distinguish cyberbullying from non-cyberbullying
content.
We experimentally tested whether cyberbullying events can be recognised automatically by lexical
11
markers in a post.
To this end,
all
posts were represented by a number of information sources (or
features
) including lexical
features like bags-of-words,
sentiment lexicon features and topic model
features, which are described in more detail below.
Prior to feature extraction, some data cleaning
steps were executed,
such as the replacement of
hyperlinks and @-replies,
removal
of
superfluous
white spaces, and the replacement of abbreviations by their full form (based on an existing mapping
dictionary
7
).
Additionally, tokenisation was applied before
n
-gram extraction and sentiment lexicon
matching, and stemming was applied prior to extracting topic model features.
After pre-processing of the corpus, the following feature types were extracted:
•
Word
n
-gram bag-of-words:
binary features indicating the presence of
word unigrams,
bigrams and trigrams.
•
Character
n
-gram bag-of-words:
binary features indicating the presence of character bi-
grams, trigrams and fourgrams (without crossing word boundaries).
Character
n
-grams pro-
vide some abstraction from the word level and provide robustness to the spelling variation that
characterises social media data.
•
Term lists:
one binary feature derived for each one out of six lists,
indicating the presence
of an item from the list in a post:
proper names, ‘allness’ indicators (e.g.
always
,
everybody
),
diminishers (e.g.
slightly
,
relatively
),
intensifiers (e.g.
absolutely,
amazingly
),
negation words
and aggressive language and profanity words.
Person alternation is a binary feature indicating
whether the combination of
a first and second person pronoun occurs in order to capture
interpersonal intent.
•
Subjectivity lexicon features:
positive and negative opinion word ratios,
as well
as the
overall
post polarity were calculated using existing sentiment lexicons.
For Dutch,
we made
use of the Duoman (Jijkoun & Hofmann,
2009) and Pattern (De Smedt & Daelemans,
2012)
lexicons.
For English,
we included the Hu and Liu opinion lexicon (Hu & Liu,
2004),
the
MPQA lexicon (Wilson et al., 2005), General Inquirer Sentiment Lexicon (Stone et al., 1966),
AFINN (Nielsen,
2011),
and MSOL (Mohammad et al.,
2009).
For both languages,
we in-
cluded the relative frequency of all
68 psychometric categories in the Linguistic Inquiry and
Word Count (LIWC) dictionary for English(Pennebaker et al., 2001) and Dutch (Zijlstra et al.,
2004).
•
Topic model features:
by making use of the Gensim topic modelling library (Rehurek & Sojka,
2010),
several
LDA (Blei et al.,
2003) and LSI
(Deerwester et al.,
1990) topic models with
varying granularity (
k
= 20, 50, 100 and 200) were trained on data corresponding to each fine-
grained category of a cyberbullying event (e.g.
threats,
defamations,
insults,
defenses).
The
topic models were based on a background corpus (EN:
±
1
,
200
,
000 tokens, NL:
±
1
,
400
,
000
tokens) scraped with the BootCAT (Baroni & Bernardini, 2004) web-corpus toolkit.
BootCaT
collects ASKfm user profiles using lists of manually determined seed words that are character-
istic of the cyberbullying categories.
When applied to the training data,
this resulted in 871
,
296 and 795
,
072 features for English and
Dutch, respectively.
7
http://www.chatslang.com/terms/abbreviations/
12
5
Results
In this section, we present the results of our experiments on the automatic detection of cyberbullying-
related posts in an English (EN) and Dutch (NL) corpus of ASKfm posts.
Ten-fold cross-validation
was performed in exhaustive grid-search over different feature type and hyperparameter combinations
(see Section 4).
The
unoptimised word
n
-gram-based
classifier and
keyword-matching
system serve
as baselines for comparison.
Precision,
Recall
and F
1
performance metrics were calculated on the
positive class (i.e., ‘binary averaging’).
We also report Area Under the ROC curve (AUC) scores, a
performance metric that is more robust to data imbalance than precision, recall and micro-averaged
F-score (Fawcett, 2006).
Feature combination
Cross-validation scores
Holdout scores
F
1
P
R
Acc
AUC
F
1
P
R
Acc
AUC
EN
B + C + D + E
64.26
73.32
57.19
96.97
78.07
63.69
74.13
55.82
97.21
77.47
A + B + C
64.24
73.22
57.23
96.96
78.09
64.32
74.08
56.83
97.24
77.96
A + C + E
63.84
73.21
56.59
96.94
77.78
62.94
72.82
55.42
97.14
77.24
word n-gram baseline
58.17
67.55
51.07
96.54
74.93
59.63
69.57
52.17
96.57
75.50
profanity baseline
17.17
9.61
80.14
63.73
71.53
17.61
9.90
78.51
63.79
71.34
NL
A + B + C + E
61.20
56.76
66.40
94.47
81.42
58.13
54.03
62.90
94.58
79.75
A + B + C + D + E
61.03
71.55
53.20
95.53
75.86
58.72
67.40
52.03
95.62
75.21
A + C + E
60.82
71.66
52.84
95.53
75.68
58.15
67.71
50.96
95.61
74.71
word n-gram baseline
50.39
67.80
40.09
94.81
69.38
49.54
64.29
40.30
95.09
69.44
profanity baseline
28.46
19.24
54.66
81.99
69.28
25.13
16.73
50.53
81.99
67.26
Table 6.
Cross-validated and holdout scores (%) according to different
metrics (F
1
, precision, recall, accuracy and area under the curve)
for the English and Dutch top 3 combined feature type systems.
A
word n-grams
B
subjectivity lexicons
C
character n-grams
D
term lists
E
topic models
Table 7.
Feature group mapping (Table 6)
Table 6 gives us an indication of which feature type combinations score best and hence contribute
most to this task.
A total
of 31 feature type combinations,
each with 28 different hyperparameter
sets have been tested.
Table 6 shows the results for the three best scoring systems by included
feature types with optimised hyperparameters.
The maximum attained F
1
-score in cross-validation
is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety
of
feature types.
The results on the holdout test set show that the trained systems generalise
well
on unseen data,
indicating little under- or overfitting.
The simple keyword-matching baseline
system has the lowest performance for both languages even though it obtains high recall for English,
suggesting that profane language characterises many cyberbullying-related posts.
Feature group and
hyperparameter optimisation provides a considerable performance increase over the unoptimised
word
n
-gram baseline system.
The top-scoring systems for each language do not differ a lot in
performance, except the best system for Dutch, which trades recall for precision when compared to
the runner-ups.
13
Table 8 presents the scores of the (hyperparameter-optimised) single feature type systems, to gain
insight into the performance of these feature types when used individually.
Analysis of the combined
and single feature type sets reveals that
word
n
-grams,
character
n
-grams
,
and
subjectivity
lexicons
prove to be strong features for this task.
In effect,
adding character
n
-grams always
improved classification performance for both languages.
They likely provide robustness to lexical
variation in social
media text,
as compared to word
n
-grams.
While subjectivity lexicons appear
to be discriminative features,
term lists perform badly on their own as well
as in combinations for
both languages.
This shows once again (cf.
profanity baseline) that cyberbullying detection requires
more sophisticated information sources than profanity lists.
Topic models seem to do badly for
both languages on their own, but in combination, they improve Dutch performance consistently.
A
possible explanation for their varying performance in both languages would be that the topic models
trained on the Dutch background corpus are of better quality than the English ones.
In effect,
a
random selection of background corpus texts reveals that the English scrape contains more noisy
data (i.e., low word-count posts and non-English posts) than the Dutch data.
Feature type
Cross-validation scores
holdout scores
F
1
P
R
Acc
AUC
F
1
P
R
Acc
AUC
EN
word
n
-grams
60.09
60.49
59.69
96.22
78.87
58.35
57.12
59.64
96.27
78.79
subjectivity lexicons
56.82
73.32
46.38
96.64
72.77
56.16
72.61
45.78
96.87
72.50
character
n
-grams
52.69
58.70
47.80
95.91
73.06
53.33
62.37
46.59
96.43
72.65
term lists
40.48
38.98
42.12
94.10
69.41
39.56
39.56
39.56
94.71
68.39
topic models
17.35
9.73
79.91
63.72
71.41
15.70
8.72
78.51
63.07
70.44
NL
word
n
-grams
55.53
72.64
44.94
95.27
71.88
54.99
70.20
45.20
95.57
71.99
subjectivity lexicons
54.34
54.12
54.56
93.97
75.65
51.82
50.61
53.09
94.09
74.90
character
n
-grams
51.70
67.58
41.86
94.86
70.22
50.46
65.20
41.15
95.17
69.88
term lists
28.65
19.36
55.10
81.97
69.48
25.13
16.73
50.53
81.99
67.26
topic models
24.74
21.24
29.61
88.16
60.94
17.99
23.15
14.71
91.98
55.80
Table 8.
Cross-validated and holdout scores (%) according to different
metrics (F
1
, precision, recall, accuracy and area under the ROC curve)
for English and Dutch single feature type systems.
A shallow
qualitative analysis
of the classification output provided insight into some of the
classification mistakes.
Table 9 gives an overview of the error rates per cyberbullying category of the best performing
and baseline systems.
This could give an indication of which types of bullying the current system
has trouble classifying.
All categories are always considered positive for cyberbullying (i.e., the error
rate equals the false negative rate), except for
Sexual
and
Insult
which can also be negative (in case
of harmless sexual talk and ‘socially acceptable’ insulting language like ‘hi bitches, in for a movie?’
the corresponding category was indicated,
but the post itself was not annotated as cyberbullying)
and
Not
cyberbullying
,
which is always negative.
Error rates often being lowest for the profanity
baseline confirms that it performs particularly well
in terms of recall
(at the expense of precision,
see Table 8) When looking at the best system for both languages, we see that
Defense
is the hardest
category to correctly classify.
This should not be a surprise as the category comprises defensive posts
from bystanders and victims, which contain less aggressive language than cyberbullying attacks and
are often shorter in length than the latter.
Assertive defensive posts (i.e., a subcategory of
Defense
)
that attack the bully) are, however, more often correctly classified.
There are not enough instances of
Encouragement for either language in the holdout to be representative.
In both languages, threats,
14
curses and incidences of sexual
harassment are most easily recognisable,
showing (far) lower error
rates than the categories
Defamation
,
Defense
,
Encouragements to the harasser
, and
Insult
.
Qualitative error analysis of the English and Dutch predictions reveals that false positives often
contain aggressive language directed at a second person, often denoting personal flaws or containing
sexual
and profanity words.
We see that misclassifications are often short posts containing just a
few words and that false negatives often lack explicit verbal
signs of
cyberbullying (e.g.
insulting
or profane words) or are ironic (examples 2 and 3).
Additionally,
we see that cyberbullying posts
containing misspellings or grammatical
errors and incomplete words are also hard to recognise as
such (examples 4 and 5).
The Dutch and English data are overall similar with respect to qualitative
properties of classification errors.
2.
You might want to do some sports ahah x
3.
Look who is there...
my thousandth anonymous hater, congratulations!
4.
ivegot 1 word foryou...
yknow whatit is?
→
slut
5.
One word for you:
G - A - ...
Category
Nr.
occurrences
Profanity
Word n-gram
Best system
in holdout
baseline
baseline
EN
Curse
n=109
14.68
30.28
24.77
Defamation
n=21
23.81
47.62
38.10
Defense
n=165
22.42
52.12
43.64
Encouragement
n=1
0.00
100.00
100.00
Insult
n=345
26.67
41.74
35.94
Sexual
n=165
63.80
21.47
21,47
Threat
n=12
8.33
41.67
25.00
Not cyberbullying
n=10,714
36.94
1.10
0.76
NL
Curse
n=96
39.58
50.00
22.92
Defamation
n=6
100.00
66.67
33.33
Defense
n=200
52.50
63.50
46.00
Encouragement
n=5
40.00
60.00
40.00
Insult
n=355
43.38
47.89
28.17
Sexual
n=37
37.84
21.62
27.03
Threat
n=15
33.33
46.67
20.00
Not cyberbullying
n=7,295
15.63
1.23
3.07
Table 9.
Error rates (%) per cyberbullying category on holdout for
English and Dutch systems.
In short, the experiments show that our classifier clearly outperforms both a keyword-based and
word
n
-gram baseline.
However,
analysis of the classifier output reveals that false negatives often
lack explicit clues that cyberbullying is going on, indicating that our system might benefit from irony
recognition and integrating world knowledge to capture such implicit realisations of cyberbullying.
Given that we present the first elaborate research on detecting signals of cyberbullying regardless
of the author role instead of bully posts alone, crude comparison with the state of the art would be
irrelevant.
We observe, however, that our classifier obtains competitive results compared to Dadvar
(2014); Dinakar et al. (2011); Nahar et al. (2014); Yin et al. (2009); Van Hee, Lefever, et al. (2015b).
15
6
Conclusions and Future Research
The goal of the current research was to investigate the automatic detection of cyberbullying-related
posts on social
media.
Given the information overload on the web,
manual
monitoring for cyber-
bullying has become unfeasible.
Automatic detection of
signals of
cyberbullying would enhance
moderation and allow to respond quickly when necessary.
Cyberbullying research has often focused on detecting cyberbullying ‘attacks’, hence overlooking
posts written by victims and bystanders.
However,
these posts could just as well
indicate that
cyberbullying is going on.
The main contribution of
this paper is that it presents a system for
detecting
signals of
cyberbullying
on social
media,
including posts from bullies,
victims and
bystanders.
A manually annotated cyberbullying dataset was created for two languages, which will
be made available for public scientific use.
Moreover, while a fair amount of research has been done
on cyberbullying detection for English, we believe this is one of the first papers that focus on Dutch
as well.
A set of binary classification experiments were conducted to explore the feasibility of automatic
cyberbullying detection on social
media.
In addition,
we sought to determine which information
sources contribute to this task.
Two classifiers were trained on English and Dutch ASKfm data and
evaluated on a holdout test of the same genre.
Our experiments reveal that the current approach is
a promising strategy for detecting signals of cyberbullying in social media data automatically.
After
feature selection and hyperparameter optimisation,
the classifiers achieved an F
1
-score of
64.32%
and 58.72% for English and Dutch,
respectively.
The systems hereby significantly outperformed a
keyword and an (unoptimised)
n
-gram baseline.
Analysis of the results revealed that false positives
often include implicit cyberbullying or offenses through irony, the challenge of which will constitute
an important area for future work.
Another interesting direction for future work would be the detection of fine-grained cyberbullying-
related categories such as threats,
curses and expressions of racism and hate.
When applied in a
cascaded model,
the system could find severe cases of
cyberbullying with high precision.
This
would be particularly interesting for monitoring purposes, since it would allow to prioritise signals
of bullying that are in urgent need for manual inspection and follow-up.
Finally,
future work will
focus on the detection of participants (or
roles
) typically involved in
cyberbullying.
This would allow to analyse the context of a cyberbullying incident and hence evaluate
its severity.
When applied as moderation support on online platforms,
such a system would allow
to provide feedback in function of the recipient (i.e., a bully, victim, or bystander).
7
Acknowledgment
The work presented in this paper was carried out in the framework of the AMiCA IWT SBO-project
120007 project, funded by the government Flanders Innovation & Entrepreneurship (VLAIO) agency.
References
Al-garadi, M. A., Varathan, K. D., & Ravana, S. D.
(2016).
Cybercrime detection in online commu-
nications:
The experimental
case of cyberbullying detection in the Twitter network.
Computers
in Human Behavior
,
63
, 433–443.
doi:
http://dx.doi.org/10.1016/j.chb.2016.05.051
Baroni, M., & Bernardini, S. (2004). BootCaT: Bootstrapping Corpora and Terms from the Web. In
Proceedings of
the Fourth International
Conference on Language Resources and Evaluation
(pp.
1313–1316).
16
Bastiaensens, S., Vandebosch, H., Poels, K., Van Cleemput, K., DeSmet, A., & De Bourdeaudhuij, I.
(2014). Cyberbullying on social network sites. An experimental study into bystanders’ behavioural
intentions to help the victim or reinforce the bully.
Computers in Human Behavior
,
31
, 259–271.
Bastiaensens, S., Vandebosch, H., Poels, K., Van Cleemput, K., DeSmet, A., & De Bourdeaudhuij,
I.
(2015).
‘Can I afford to help?’
How affordances of communication modalities guide bystanders’
helping intentions towards harassment on social
network sites.
Behaviour & Information Tech-
nology
,
34
(4), 425–435.
doi:
10.1080/0144929X.2014.983979
Blei,
D.
M.,
Ng,
A.
Y.,
& Jordan,
M.
I.
(2003).
Latent dirichlet allocation.
Journal
of
Machine
Learning Research
,
3
, 993–1022.
Chang,
C.-C.,
& Lin,
C.-J.
(2011).
LIBSVM:
A Library for Support Vector Machines.
ACM
Transactions on Intelligent
Systems and Technology (TIST)
,
2
(3),
27:1–27:27.
doi:
10.1145/
1961189.1961199
Chawla,
N.
V.,
Bowyer,
K.
W.,
Hall,
L.
O.,
& Kegelmeyer,
P.
W.
(2002).
SMOTE:
Synthetic
Minority Over-sampling TEchnique.
Journal
of Artificial
Intelligence Research (JAIR)
,
16
, 321–
357.
Cohen,
J.
(1960).
A coefficient of
agreement for nominal
scales.
Educational
and Psychological
Measurement
,
20
(1), 37–46.
Cortes,
C.,
& Vapnik,
V.
(1995).
Support-Vector Networks.
Machine Learning
,
20
(3),
273–297.
doi:
10.1023/A:1022627411411
Cowie, H.
(2013).
Cyberbullying and its impact on young people’s emotional health and well-being.
The Psychiatrist
,
37
(5), 167–170.
doi:
10.1192/pb.bp.112.040840
Dadvar, M.
(2014).
Experts and machines united against cyberbullying
(PhD thesis).
University of
Twente.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by
latent semantic analysis.
Journal
of the American Society for Information Science
,
41
, 391–407.
De Smedt,
T.,
& Daelemans, W.
(2012).
“Vreselijk mooi!” (“Terribly Beautiful!”):
A Subjectivity
Lexicon for Dutch Adjectives.
In
Proceedings of the Eight International
Conference on Language
Resources and Evaluation
(p. 3568-3572).
Istanbul, Turkey.
Desmet, B.
(2014).
Finding the online cry for help:
automatic text classification for suicide preven-
tion
(PhD thesis).
Ghent University.
Dinakar, K., Jones, B., Havasi, C., Lieberman, H., & Picard, R.
(2012).
Common Sense Reasoning
for Detection,
Prevention,
and Mitigation of
Cyberbullying.
ACM Transactions on Interactive
Intelligent Systems
,
2
(3), 18:1–18:30.
Dinakar,
K.,
Reichart,
R.,
& Lieberman,
H.
(2011).
Modeling the Detection of Textual
Cyberbul-
lying.
In
The Social
Mobile Web
(Vol. WS-11-02, pp. 11–17).
AAAI.
Dooley,
J.
J.,
& Cross,
D.
(2009).
Cyberbullying versus face-to-face bullying:
A review of
the
similarities and differences.
Journal
of Psychology
,
217
, 182–188.
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J.
(2008).
LIBLINEAR: A Library
for Large Linear Classification.
Journal
of Machine Learning Research
,
9
, 1871–1874.
17
Fawcett, T.
(2006).
An introduction to roc analysis.
Pattern recognition letters
,
27
(8), 861–874.
Fekkes,
M.,
Pijpers,
F.
I.,
Fredriks,
A.
M.,
Vogels,
T.,
& Verloove-Vanhorick,
S.
P.
(2006).
Do
Bullied Children Get Ill,
or Do Ill
Children Get Bullied?
A Prospective Cohort Study on the
Relationship Between Bullying and Health-Related Symptoms.
Pediatrics
,
117
(5),
1568–1574.
doi:
10.1542/peds.2005-0187
Fleiss, J. L.
(1971). Measuring nominal scale agreement among many raters.
Psychological
Bulletin
,
76
(5), 378–382.
Gross,
E.
F.,
Juvonen,
J.,
& Gable,
S.
L.
(2002).
Internet Use and Well-Being in Adolescence.
Journal
of Social
Issues
,
58
(1), 75–90.
He, H., & Garcia, E. A.
(2009).
Learning from imbalanced data.
IEEE Transactions on knowledge
and data engineering
,
21
(9), 1263–1284.
Hinduja, S., & Patchin, J. W.
(2006).
Bullies Move Beyond the Schoolyard:
A Preliminary Look at
Cyberbullying.
Youth Violence And Juvenile Justice
,
4
(2), 148–169.
Hinduja,
S.,
& Patchin, J. W.
(2012).
Cyberbullying:
Neither an epidemic nor a rarity.
European
Journal
of Developmental
Psychology
,
9
(5), 539–543.
Hoste, V.
(2005).
Optimization Issues in Machine Learning of Coreference Resolution
(PhD thesis).
Antwerp University.
Hu,
M.,
& Liu,
B.
(2004).
Mining and summarizing customer reviews.
In
Proceedings of
the 10th
ACM SIGKDD international
conference on Knowledge discovery and data mining
(pp. 168–177).
ACM.
Jijkoun,
V.,
& Hofmann,
K.
(2009).
Generating a Non-English Subjectivity Lexicon:
Relations
That Matter.
In
Proceedings of the 12th Conference of the European Chapter of the Association
for Computational
Linguistics
(p. 398-405).
Stroudsburg, PA, USA.
Juvonen,
J.,
& Gross,
E.
F.
(2008).
Extending the school
grounds?
– Bullying experiences in
cyberspace.
Journal
of School
Health
,
78
(9), 496–505.
Livingstone,
S.,
Haddon,
L.,
G¨orzig,
A.,
&
´
Olafsson,
K.
(2010).
Risks and safety on the internet:
The perspective of European children. Initial
Findings.
London:
EU Kids Online.
Livingstone,
S.,
Kirwil,
L.,
Ponte,
C.,
& Staksrud,
E.
(2013).
In their own words:
what
bothers
children online?
London:
EU Kids Online.
McHugh, M. L. (2012). Interrater reliability:
the kappa statistic.
Biochemia Medica
,
22
(3), 276–282.
Mckenna, K. Y., & Bargh, J. A.
(1999).
Plan 9 From Cyberspace:
The Implications of the Internet
for Personality and Social Psychology.
Personality & Social
Psychology Review
,
4
(1), 57–75.
Mohammad,
S.,
Dunne,
C.,
& Dorr,
B.
(2009).
Generating High-coverage Semantic Orientation
Lexicons from Overtly Marked Words and a Thesaurus.
In
Proceedings of the 2009 Conference on
Empirical
Methods in Natural
Language Processing:
Volume 2
(pp.
599–608).
Stroudsburg, PA,
USA: Association for Computational Linguistics.
18
Nadali,
S.,
Azmi
Murad,
M.
A.,
Sharef,
N.
M.,
Mustapha,
A.,
& Shojaee,
S.
(2013).
A review
of cyberbullying detection:
An overview.
In
13th International
Conference on Intellient Systems
Design and Applications
(pp. 325–330).
Salangor, Malaysia,.
doi:
10.1109/ISDA.2013.6920758
Nahar, V.,
Al-Maskari, S., Li, X.,
& Pang, C.
(2014).
Semi-supervised Learning for Cyberbullying
Detection in Social Networks.
In
ADC.Databases Theory and Applications
(pp. 160–171).
Nansel,
T.
R.,
Overpeck,
M.,
Pilla,
R.
S.,
Ruan,
J.
W.,
Morton,
B.
S.,
& Scheidt,
P.
(2001).
Bullying behaviors among US youth:
prevalence and association with psychosocial
adjustment.
JAMA
,
285
(16), 2094–2100.
Nielsen,
F.
˚
A.
(2011).
A New ANEW:
Evaluation of
a Word List for Sentiment Analysis in Mi-
croblogs.
In M.
Rowe,
M.
Stankovic,
A.-S.
Dadzie,
& M.
Hardey (Eds.),
Proceedings of
the
ESWC2011 Workshop on ‘Making Sense of
Microposts’:
Big things
come in small
packages
(Vol. 718, pp. 93–98).
CEUR-WS.org.
Olweus, D.
(1993).
Bullying at School:
What We Know and What We Can Do
(2nd ed.).
Wiley.
O’Moore,
M.,
& Kirkham,
C.
(2001).
Self-esteem and its relationship to bullying behaviour.
Ag-
gressive Behavior
,
27
(4), 269–283.
Online,
E.
K.
(2014).
EU Kids Online:
findings,
methods,
recommendations.
EU Kids Online,
LSE,
London,
UK.
http://eprints.lse.ac.uk/60512/.
London:
EU Kids Online.
Retrieved from
http://eprints.lse.ac.uk/60512/
O’Sullivan,
P.
B.,
& Flanagin,
A.
J.
(2003).
Reconceptualizing ‘flaming’
and other problematic
messages.
New Media & Society
,
5
(1), 69-94.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . .
Duchesnay, E.
(2011).
Scikit-learn:
Machine Learning in Python.
Journal
of
Machine Learning Research
,
12
,
2825–2830.
Pennebaker,
J.
W.,
Francis,
M.
E.,
& Booth,
R.
J.
(2001).
Linguistic Inquiry and Word Count:
LIWC 2001
.
Mahwah, NJ: Lawrence Erlbaum Associates.
Price,
M.,
& Dalgleish,
J.
(2010).
Cyberbullying:
Experiences,
Impacts and Coping Strategies as
Described by Australian Young People.
Youth Studies Australia
,
29
(2), 51–59.
Rehurek,
R.,
& Sojka,
P.
(2010).
Software framework for topic modelling with large corpora.
In
The LREC 2010 Workshop on new Challenges for NLP Frameworks
(pp. 45–50).
Reynolds,
K.,
Kontostathis,
A.,
& Edwards,
L.
(2011).
Using Machine Learning to Detect Cy-
berbullying.
In
Proceedings of the 2011 10th International
Conference on Machine Learning and
Applications and Workshops
(pp. 241–244).
Washington, DC, USA: IEEE Computer Society.
Royen,
K.
V.,
Poels,
K.,
& Vandebosch,
H.
(2016).
Harmonizing freedom and protection:
Ado-
lescents’ voices on automatic monitoring of social networking sites.
Children and Youth Services
Review
,
64
, 35 - 41.
doi:
http://dx.doi.org/10.1016/j.childyouth.2016.02.024
Salmivalli,
C.
(2010).
Bullying and the peer group:
A review.
Aggression and Violent Behavior
,
15
(2), 112–120.
19
Salmivalli,
C.,
Kaukiainen,
A.,
Kaistaniemi,
L.,
& Lagerspetz,
K.
M.
J.
(1999).
Self-Evaluated
Self-Esteem,
Peer-Evaluated Self-Esteem,
and Defensive Egotism as Predictors of
Adolescents’
Participation in Bullying Situations.
Personality and Social
Psychology Bulletin
,
25
(10),
1268–
1278.
doi:
10.1177/0146167299258008
Salmivalli, C., Lagerspetz, K., Bj¨orkqvist, K.,
¨
Osterman, K., & Kaukiainen, A.
(1996).
Bullying as
a group process:
Participant roles and their relations to social status within the group.
Aggressive
Behavior
,
22
(1), 1–15.
Salmivalli,
C.,
Voeten,
M.,
& Poskiparta,
E.
(2011).
Bystanders Matter:
Associations Between
Reinforcing, Defending, and the Frequency of Bullying Behavior in Classrooms.
Journal of Clinical
Child & Adolescent Psychology
,
40
(5), 668-676.
doi:
10.1080/15374416.2011.597090
Slonje,
R.,
& Smith,
P.
K.
(2008).
Cyberbullying:
Another main type of bullying?
Scandinavian
Journal
of Psychology
,
49
(2), 147–154.
Slonje,
R.,
Smith,
P.
K.,
& Fris´en,
A.
(2013).
The Nature of
Cyberbullying,
and Strategies for
Prevention.
Compututers in Human Behavior
,
29
(1), 26–32.
Smith, P. K., Mahdavi, J., Carvalho, M., Fisher, S., Russell, S., & Tippett, N. (2008). Cyberbullying:
its nature and impact in secondary school
pupils.
Journal
of
Child Psychology and Psychiatry
,
49
(4), 376–385.
Stenetorp,
P.,
Pyysalo,
S.,
Topi´c,
G.,
Ohta,
T.,
Ananiadou,
S.,
& Tsujii,
J.
(2012).
brat:
a Web-
based Tool for NLP-Assisted Text Annotation.
In
Proceedings of the Demonstrations Session at
EACL 2012
(pp. 102–107).
Avignon, France.
Stone,
P.
J.,
Dunphy,
D.
C.
D.,
Smith,
M.
S.,
& Ogilvie,
D.
M.
(1966).
The General
Inquirer:
A
Computer Approach to Content Analysis
.
The MIT Press.
Sui, J.
(2015).
Understanding and Fighting Bullying with Machine Learning
(PhD thesis).
Depart-
ment of Computer Sciences, University of Wisconsin-Madison.
Tokunaga,
R.
S.
(2010).
Following You Home from School:
A Critical
Review and Synthesis of
Research on Cyberbullying Victimization.
Computers in Human Behavior
,
26
(3), 277–287.
Van Cleemput,
K.,
Bastiaensens,
S.,
Vandebosch,
H.,
Poels,
K.,
Deboutte,
G.,
DeSmet,
A.,
&
De Bourdeaudhuij,
I.
(2013).
Zes jaar onderzoek naar cyberpesten in Vlaanderen,
Belgi¨e en
daarbuiten:
een overzicht van de bevindingen. (Six years of research on cyberbullying in Flanders,
Belgium and beyond:
an overview of
the findings.) (White Paper)
(Tech.
Rep.).
University of
Antwerp & Ghent University.
Vandebosch, H., & Van Cleemput, K. (2008). Defining cyberbullying:
a qualitative research into the
perceptions of youngsters.
Cyberpsychology and behavior:
the impact of the Internet, multimedia
and virtual
reality on behavior and society
,
11
(4), 499–503.
Vandebosch,
H.,
& Van Cleemput,
K.
(2009).
Cyberbullying among youngsters:
profiles of bullies
and victims.
New Media & Society
,
11
(8), 1349–1371.
Vandebosch,
H.,
Van Cleemput,
K.,
Mortelmans,
D.,
& Walrave,
M.
(2006).
Cyberpesten bij
jongeren in Vlaanderen:
Een studie in opdracht van het viWTA (Cyberbullying among youngsters
in Flanders:
a study commissoned by the viWTA). Brussels:
viWTA
(Tech. Rep.).
Retrieved from
http://www.samenlevingentechnologie.be/ists/nl/pdf/rapporten\/rapportcyberpesten.pdf
20
van de Kauter,
M.,
Coorman, G.,
Lefever,
E.,
Desmet,
B.,
Macken,
L.,
& Hoste,
V.
(2013).
LeTs
Preprocess:
The multilingual
LT3 linguistic preprocessing toolkit.
Computational
Linguistics in
the Netherlands Journal
,
3
, 103–120.
Van Hee,
C.,
Lefever,
E.,
Verhoeven,
B.,
Mennes,
J.,
Desmet,
B.,
De Pauw,
G.,
. . .
Hoste,
V.
(2015a).
Automatic detection and prevention of cyberbullying.
In P. Lorenz & C. Bourret (Eds.),
International
conference on human and social
analytics, proceedings
(pp. 13–18).
IARIA.
Van Hee, C., Lefever, E., Verhoeven, B., Mennes, J., Desmet, B., De Pauw, G., . . .
Hoste, V. (2015b).
Detection and fine-grained classification of cyberbullying events.
In G.
Angelova, K.
Bontcheva,
& R.
Mitkov (Eds.),
Proceedings of
recent advances in natural
language processing,
proceedings
(pp. 672–680).
Van Hee,
C.,
Verhoeven,
B.,
Lefever,
E.,
De Pauw,
G.,
Daelemans,
W.,
& Hoste,
V.
(2015).
Guidelines for the Fine-Grained Analysis of Cyberbullying,
version 1.0
(Tech.
Rep.
No. LT3 15-
01).
LT3, Language and Translation Technology Team–Ghent University.
Van Royen,
K.,
Poels,
K.,
Daelemans,
W.,
& Vandebosch,
H.
(2014).
Automatic monitoring of
cyberbullying on social networking sites:
From technological feasibility to desirability.
Telematics
and Informatics
.
doi:
10.1016/j.tele.2014.04.002
Willard, N. E. (2007).
Cyberbullying and Cyberthreats:
Responding to the Challenge of Online Social
Aggression, Threats, and Distress
(2nd ed.).
Research Publishers LLC.
Wilson,
T.,
Wiebe,
J.,
& Hoffmann,
P.
(2005).
Recognizing Contextual
Polarity in Phrase-level
Sentiment Analysis.
In
Proceedings of the Conference on Human Language Technology and Em-
pirical
Methods in Natural
Language Processing
(pp.
347–354).
Association for Computational
Linguistics.
doi:
10.3115/1220575.1220619
Wolak,
J.,
Mitchell,
K.
J.,
& Finkelhor,
D.
(2007).
Does Online Harassment Constitute Bullying?
An Exploration of
Online Harassment by Known Peers and Online-Only Contacts.
Journal
of
Adolescent Health
,
41
(6, Supplement), S51–S58.
Xu,
J.-M.,
Jun,
K.-S.,
Zhu,
X.,
& Bellmore,
A.
(2012).
Learning from Bullying Traces in Social
Media.
In
Proceedings of the 2012 Conference of the North American Chapter of the Association
for Computational
Linguistics:
Human Language Technologies
(pp.
656–666).
Stroudsburg, PA,
USA: Association for Computational Linguistics.
Yin,
D.,
Davison,
B.
D.,
Xue,
Z.,
Hong,
L.,
Kontostathis,
A.,
& Edwards,
L.
(2009).
Detection
of
Harassment on Web 2.0.
In
Proceedings of
the Content Analysis in the Web 2.0 (CAW2.0).
Madrid, Spain.
Zhao, R., Zhou, A., & Mao, K.
(2016).
Automatic Detection of Cyberbullying on Social Networks
Based on Bullying Features.
In
Proceedings of the 17th International
Conference on Distributed
Computing and Networking
(pp.
43:1–43:6).
New York,
NY,
USA:
ACM.
doi:
10.1145/2833312
.2849567
Zijlstra,
H.,
Van Meerveld,
T.,
Van Middendorp,
H.,
Pennebaker,
J.
W.,
& Geenen,
R.
(2004).
De Nederlandse versie van de ‘linguistic inquiry and word count’
(LIWC).
Gedrag Gezond
,
32
,
271–281.
21
