Hierarchical
Taxonomy Based Topic
Recommendation in Informal
Learning
Mikko Vilenius
Submitted in partial fulﬁllment
of the requirements for the degree of
Doctor of Engineering
Graduate School of Information Systems
The University of Electro-Communications
December 2012
Hierarchical Taxonomy Based Topic
Recommendation in Informal Learning
Approved by supervisory committee:
Chairperson:
Prof.
Toshio OKAMOTO
Member:
Prof.
Toshinori WATANABE
Member:
Prof.
Toshihiko KATO
Member:
Prof.
Tadashi OHMORI
Member:
Prof.
Maomi UENO
Copyright
Mikko Vilenius
2012
インフォーマルラーニングにおける協同学習を支援するオンライン
議論トピックの推薦に関する技術の研究
Mikko Vilenius
概要
本研究では web 上におけるインフォーマル・ラーニングを支援するために
オンライン議論におけるトピック推薦手法を提案する。Web 上でのイン
フォーマル・ラーニングを対象とした場合、教育機関で行なわれるフォ－
マルな教育を対象とした e ラーニングとは異なり、学習履歴や学習者の成
績など学習者に関する情報が少ないことが問題になる。
一般的なフォーマルラーニングと異なり、教育的な目標が指定されない
ために学習者はそれぞれ自己管理によって自分自身が目的を定め、関心が
あることを自発的に学習する必要がある。そこで、インフォーマル・ラー
ニング環境における学習者を支援するために、学習者がオンラインフォー
ラムで投稿した発言を基に、学習者が特に関心があるトピックを抽出し、
そのトピックについて詳しい他のユーザーを推薦する。従来のトピック推
薦手法ではインフォーマル・ラーニングのような環境に適応するのが難し
く、従来の手法を拡張した階層分類ツリートピックモデルを提案し、より
インフォーマル・ラーニングに適応した推薦を行っている。
インフォーマル・ラーニングでの学習は構成主義教育理論と共同学習理
論に基づく。構成主義教育によると、学習者が自分の経験により理解を組
み立てる。そのため、共同学習は一つ方法として非常に適切であり、学習
者が自分の意見を表し、互いとの議論を反映しながらより良い理解ができ
る。共同学習理論に基づき、本研究ではより適切な推薦手法を提案する。
従来の研究では、推薦手法はコンテンツベースの推薦、協調フィルタリン
グによる推薦、ハイブリッド型協調フィルタリング（コンテンツベースと
協調フィルタリングの機能を組み合わせて推薦する手法）の三種類が提案
する。しかし、インフォーマル・ラーニングの場合は、協調フィルタリング
を適用するために必要な情報が少ないため、本研究ではコンテンツベース
の推薦手法を適用する。本研究ではコンテンツベースの推薦システムを実
現するための 3 種類の手法を活用する。すなわち、Centroid Based と Naive
Bayesian と Latent Dirichlet Allocation の手法を、階層分類ツリートピック
モデルによって拡張して、インフォーマル・ラーニングに適切な推薦手法
を開発した。
そして、提案した階層分類ツリートピックモデルによる推薦手法を評価
するために、2 種類のオンライン議論データを用いて、従来の手法と比較し
た推薦の機能評価実験を行なった。さらに、本研究で提案する手法により
教育的にどのような効果があるかを検証するため、学習者へのアンケート
により評価を行なった。その結果、本研究で提案する階層分類ツリーのト
ピックモデルによって、従来の 3 種類の推薦手法のいずれにおいても精度
が上がることが示された。学習者のアンケート結果によれば、本研究で提
案する推薦手法で推薦したオンライン議論のトピックは、従来の手法で推
薦されるトピックよりも関心があり、より積極的に議論に参加するトピッ
クであることが示された。
Hierarchical Taxonomy Based Topic Recommendation in Informal Learning
Mikko Vilenius
Abstract
In this research we present a novel and eﬀective method of topic recommen-
dation and group forming to support informal
learning in online discussions,
centred in the use of the so-called Web 2.0 applications, such as social media
and discussion boards.
While topic recommendation and group forming have
been researched in a well constructed and clearly deﬁned environment, the ill-
constructed and sporadic nature of informal learning poses new challenges for
the typically used methods.
Our method is especially suitable for encouraging
active and knowledgeable people to join a discussion.
The nature of
informal
learning is very diﬀerent from traditional,
formal
learning.
Formal
learning usually uses well
deﬁned sources and happens in
a certain space or environment,
like in a school,
university,
or on a speciﬁc
course.
In terms of e-Learning this refers to,
for example,
Learning Manage-
ment Systems where we have detailed information on the learners and clearly
set goals and learning items.
Informal learning on the other hand uses a wide variety of sources – both on-
and oﬄine – such as books, television, direct person to person communication,
the Internet,
etc.
and can happen in a wide variety of
places and social
environments.
Furthermore,
informal
learning is by nature self-directed and
self-centred, and is centred on collaborative and social activities.
It is usually
ill-constructed,
i.e.
there is no clear structure to the order learning happens
in and it changes dynamically in irregular patterns.
In terms of
e-Learning
this means that we cannot grasp the whole learning process, and cannot deﬁne
clear learning goals.
We can only give support to the learners by providing
them with useful resources.
In the course of this study we are especially interested in providing the learn-
ers with other learners to discuss with.
We will
recommend learner initiated
discussions to people who are interested and knowledgeable on that speciﬁc
topic.
This means creating groups in which learners collaborate within an
open time frame.
Due to the more open ended and uncontrolled nature of
these discussions conventional recommendation methods do not provide with
accurate recommendation and are not as such suitable for multifaceted and
noisy online discussions.
Therefore we added a Hierarchical Topic Taxonomy
to our recommendation model.
The hierarchical
taxonomy acts as a sort of
“divide-and-conquer”algorithm increasing the accuracy of
recommendation
/ group forming.
We use two data sets from online discussions to evaluate our method’s rec-
ommendation accuracy in comparison to conventional
methods.
In addition
we compared our method with existing methods by means of user question-
naire in order to evaluate the quality of the recommendation and its suitability
for informal learning.
Evaluation of our method shows an increase in topic recommendation preci-
sion in comparison to other, often-used methods.
The results of our question-
naire show that the method is especially suitable for recommending knowledge-
able people to discuss a given topic, which in turn indicates a better suitability
for supporting informal learning.
As a secondary result, our research also yielded some interesting insights on
user participation patterns in online discussions.
Contents
1
Introduction
1
1.1
The Organization of This Thesis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1
1.2
Informal Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1.3
Scope of Our Research .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
1.4
Deﬁnitions .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
1.5
Group Forming and Document Recommendation .
.
.
.
.
.
.
.
8
1.6
Research Problem .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
1.7
Hierarchical Taxonomy Based Recommendation
.
.
.
.
.
.
.
.
14
1.8
Our Contribution .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
16
2
Theoretical
Background
18
2.1
Informal Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
2.1.1
Informal Learning in the Modern Society
.
.
.
.
.
.
.
.
18
2.1.2
Scope of Informal Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20
2.1.3
Many Faces of Informal Learning .
.
.
.
.
.
.
.
.
.
.
.
.
20
2.1.4
Informal Online Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
2.2
Constructive Learning Theories and Collaborative Learning
.
.
22
2.2.1
Constructive and Collaborative Learning .
.
.
.
.
.
.
.
.
23
2.2.2
Existing Methods for Group Forming and Recommen-
dation in e-Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
24
2.2.3
Learner Modelling in e-Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
27
2.3
Theoretical
Basis of
Group Forming / Document Recommen-
dation in Informal Learning .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
31
2.4
Skills Frameworks and Skill Standards
.
.
.
.
.
.
.
.
.
.
.
.
.
.
35
3
Methodology
37
3.1
Recommender systems
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
3.1.1
Content-Based Recommender Systems
.
.
.
.
.
.
.
.
.
.
39
3.1.2
Collaborative Recommender Systems .
.
.
.
.
.
.
.
.
.
.
41
3.1.3
Hybrid Recommender Systems
.
.
.
.
.
.
.
.
.
.
.
.
.
.
42
3.1.4
Discussion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
43
3.2
Text Mining .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
44
3.2.1
Document Representation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
46
3.2.2
Classiﬁcation and Clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
56
3.3
Hierarchical Topic Taxonomy Recommender .
.
.
.
.
.
.
.
.
.
.
66
3.4
Recommender for e-Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
68
3.5
Hierarchical Topic Taxonomy Based Recommender
.
.
.
.
.
.
.
69
3.5.1
Intuition for the Hierarchical
Topic Taxonomy Recom-
mender
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70
3.5.2
The Hierarchical Topic Taxonomy Recommender
.
.
.
.
73
3.5.3
Deﬁnition of the Hierarchical
Topic Taxonomy Recom-
mender
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
74
3.5.4
Domain Model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
75
3.5.5
Learner Model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
75
3.5.6
Recommendation Model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
76
3.5.7
Recommendation Algorithm .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
77
4
Experiments
79
4.1
Experiment 1:
Martial Arts Discussion Forum .
.
.
.
.
.
.
.
.
.
80
4.1.1
Data Description .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
80
4.1.2
Training and Evaluation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
83
4.1.3
Results and Discussion .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
90
4.2
Experiment 2:
Programming Discussion Forum .
.
.
.
.
.
.
.
.
105
4.2.1
Data Collection .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
106
4.2.2
The Hierarchical Topic Taxonomy
.
.
.
.
.
.
.
.
.
.
.
.
107
4.2.3
Process for the Experiment
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
107
4.2.4
Results
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
109
4.3
User Questionnaire .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
109
4.3.1
Setting Up the User Questionnaire
.
.
.
.
.
.
.
.
.
.
.
.
111
4.3.2
The Questions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
112
4.3.3
Questionnaire Results
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
116
4.4
Discussion on the Experiment Results
.
.
.
.
.
.
.
.
.
.
.
.
.
.
121
5
Conclusion
123
5.1
Result Signiﬁcance
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
124
5.2
Future Research
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
127
5.3
Closing Remarks
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
130
List of Figures
1.2.1 Collaborative Topic Recommendation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
1.4.1 A simple example of
Hierarchical
Topic Taxonomy from Soft-
ware Engineering .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
1.5.1 Recommending a new topic to a learner
.
.
.
.
.
.
.
.
.
.
.
.
.
9
1.5.2 Connection between recommendation and group forming .
.
.
.
10
1.6.1 Overlapping sub-categories
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
2.2.1 Overlay Learner Model .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
28
2.2.2 Diﬀerential Learner Model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
2.2.3 Perturbation Learner Model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
30
2.2.4 Learner Modelling in Informal Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
31
2.3.1 The SECI-model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
32
2.3.2 The SECI-model and online Discussions
.
.
.
.
.
.
.
.
.
.
.
.
.
34
3.2.1 The bag-of-words document generation.
.
.
.
.
.
.
.
.
.
.
.
.
.
46
3.2.2 The document generation in pLSA.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
52
3.2.3 The document generation in LDA.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
54
3.2.4 The UPGMA-algorithm.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
61
3.2.5 Support Vector Machine .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
65
3.2.6 Classiﬁcation with the SVM .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
67
3.4.1 A simple recommender .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
69
3.5.1 Typical Recommender to recommend discussions in a learning
environment.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70
3.5.2 Learner Participation to discussions in Informal
learning envi-
ronment
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
71
3.5.3 Failed recommendation .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
72
3.5.4 Separating the domain to hierarchical categories
.
.
.
.
.
.
.
.
73
3.5.5 HTT based recommendation algorithm .
.
.
.
.
.
.
.
.
.
.
.
.
.
78
4.1.1 Hierarchical Topic Taxonomy for Martial Arts .
.
.
.
.
.
.
.
.
.
84
4.1.2 Evaluation results (Average, all learners) .
.
.
.
.
.
.
.
.
.
.
.
.
92
4.1.3 Evaluation results (Average, all learners), human labelled data
96
4.1.4 Evaluation results (Average, 5 best learners) .
.
.
.
.
.
.
.
.
.
.
99
4.1.5 The ratio of participation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
101
4.1.6 httCD models threshold performance .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
102
4.1.7 Traditional CD model’s threshold performance
.
.
.
.
.
.
.
.
.
103
4.2.1 The HTT for Experiment 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
108
4.3.1 Screen Capture of the Questionnaire
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
110
4.3.2 Answer distribution for Question 1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
116
4.3.3 Answer distribution for Question 2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
117
4.3.4 Answer distribution for Question 3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
118
4.3.5 Answer distribution for Question 4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
119
5.2.1 Learner Portfolio .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
129
List of Tables
2.1
Comparing e-Learning 1.0 and e-Learning after the popularity
of Web 2.0 applications.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
4.1
Participation recommendation results
.
.
.
.
.
.
.
.
.
.
.
.
.
.
89
4.2
Evaluation results (Average, all learners) .
.
.
.
.
.
.
.
.
.
.
.
.
91
4.3
Evaluation results (Average, all learners), human labelled data
95
4.4
Evaluation results (Average, best 5 learners) .
.
.
.
.
.
.
.
.
.
.
97
4.5
Evaluation results for Experiment 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
109
1 Introduction
In this Chapter we provide an overview of this thesis and the research re-
lated to it and most importantly introduce the basic idea behind our new
method.
We will discuss the scope of the research and the motivation behind
the research.
We deﬁne some important terms and concepts for our research.
1.1 The Organization of This Thesis
This thesis is organized as follows:
In the First Chapter we will
ﬁrst discuss the domain of informal
learning,
the challenges it poses and the motivation of
choosing informal
learning as
our domain.
Next we will
present the research problem and postulate our
hypothesis.
After that we will introduce our method of recommendation and
discuss the contribution of this research.
In the Second Chapter we will
go through the theoretical
background for
our research.
We will
discuss the learning process in informal
learning and
its implications to e-Learning.
We will
go through learning theories closely
related to informal learning, namely socio-constructivist learning theories.
We
will
also discuss the Skills Framework and Skill
Standards,
and their use in
informal learning.
In the Third Chapter we will
introduce the methodology behind our re-
search.
We will
discuss diﬀerent document presentation,
classiﬁcation and
1
clustering methods and – most importantly – introduce our own method in
detail.
In the Fourth Chapter we will describe the empirical experiments related to
our research and introduce their results.
In this chapter we will compare our
methodology to other methodologies and show evidence on the suitability of
our method in the domain of informal learning.
In the Fifth Chapter we will
conclude our research.
We will
discuss the
implications,
the relevance and the signiﬁcance of
our results.
We will
also
analyse our results and discuss secondary results from our research.
In the
end, we will also discuss future directions for our research.
1.2 Informal
Learning
When speaking of education and learning, we usually associate these terms
with institutions such as schools and universities.
These institutions oﬀer well-
designed and pedagogically sound education in a very organized manner with
clear goals.
They usually have a clearly deﬁned curricula,
professional
and
enthusiastic staﬀ and have set a clear goal for their education.
The students, or
learners, join these institutions in order to study towards that clearly deﬁned
goal.
These goals include,
for example,
acquiring a diploma,
completion of
mandatory education,
or fulﬁlling requirements for a job or a position.
The
learners are expected to graduate the institution at some point, showing that
they have acquired required amount of
knowledge on what ever subject the
curriculum was aimed for.
However, in the modern “knowledge society” learning does not stop after a
person graduates from school or university.
Furthermore, even during the in-
stitutionalized education not all learning happens during scheduled activities.
2
Outside the formal, institutionalized education there is learning that happens
through our experiences at work,
in our free time,
through our hobbies,
and
so forth.
This kind of learning is usually referred to as informal learning, and
it is very diﬀerent in nature to formal learning.
We learn as we watch TV, read a book or newspaper, surf the Internet, or
have lunch or dinner with our friends and colleagues.
This kind of informal
learning happens in very irregular patterns and uses a a wide variety of sources.
Because of this ambiguous nature,
informal
learning is very diﬃcult to both
manage and assess.
One important part of the informal
learning is the social
interaction with
our peers.
Indeed,
one can say that this kind of learning is centred on social
activities and discussions.
Through social activities we are constantly challenged by diﬀerent points of
view,
needing to adjust our own understanding.
When we describe our own
ideas and concepts to other people,
we need to construct and re-construct
these ideas and knowledge into structures that will
allow us to explain them
so that the other person can understand them.
In this sense, informal learning
is closely connected with the theories of
collaborative-constructive learning,
which we will discuss in more detail in Section 2.2.
One modern form of social interaction is using the Internet.
Online forums,
blogs,
Wikis and other discussion media and social
networking services are
possible avenues of sharing knowledge, arguing opinions and exchanging ideas.
These media oﬀer us a large resource of documents to browse and study and
peers to interact with.
In fact,
a problem lies in the huge volume of
the
documents and the sheer amount of
these discussion media and the large
number of people taking part in the discussions on them.
In order to support informal
learning online eﬃciently we require diﬀerent
3
Figure 1.2.1: Collaborative Topic Recommendation:
A learner
has
a
question or problem that he or she wants to learn or get clar-
iﬁcation,
opinions on,
etc.
By ﬁnding suitable peers to discuss
the topic with, we can support the learning eﬃciently.
4
techniques to index, organize and ﬁlter all the information.
Therefore, the aim
of this research is on one side to help people to ﬁnd discussions on their topics
of interest, and on the other side ﬁnd appropriate people to discuss a topic in
a speciﬁc area of knowledge (see Figure 1.2.1).
Our method is especially suited for online fora where we have little explicit
knowledge about the learners themselves.
All we need is to identify the learn-
ers with some kind of
ID and then infer their speciﬁc expertise from their
participation to the discussions.
1.3 Scope of Our Research
As seen in the previous Section,
informal
learning is centred on discussion
and social
interaction.
The Internet oﬀers huge resources for discussion and
sharing ideas and knowledge in form of
diﬀerent social
media,
such as dis-
cussion forums,
Wikis and social
networking services.
We have chosen these
social media as our research area.
While the social
media oﬀer great opportunities for learning,
they are not
without problems.
One obvious problem is the amount of
data and people
available.
Another problem is that we usually do not have explicit information
about the learners.
In formal learning we at least know who we are teaching.
We have a general idea on the starting level of the learners, their background
and age.
We can asses the learners with tests and follow their progress.
On the other hand, in discussion forums we have a lot of implicit knowledge
of
the learners.
As they participate in discussions and produce their own
material, we can collect information about their interests.
The problem then
lies on deducing the latent information on learners’
interest and knowledge
from these documents.
5
Therefore,
in the course of
this thesis we propose a method for collabo-
rative topic recommendation / collaborative group forming in an informal,
ill-constructed online learning environment.
Until
now,
most of the research
on collaborative group forming and topic recommendation has been concen-
trating on formal learning, or at least for environments where a lot of speciﬁc
data about the learners are readily provided.
We selected informal, ill-constructed learning as our domain, as we feel that
such often quoted beneﬁts of
e-Learning as “anybody,
anytime,
anywhere”
are ideal for informal learning.
In Section 2.1 we discuss the theoretical back-
ground and scope of
informal
learning in general
and its implications for e-
Learning.
In Section 2.2 we will discuss collaborative learning from the view
point of informal learning.
1.4 Deﬁnitions
In this thesis we use the terms “collaborative topic recommendation” and
“collaborative group forming” interchangeably,
as in our view these are “two
sides
of
the same coin”.
From the system’s
perspective the problem can
be stated as ﬁnding appropriate people for collaborative activities,
in which
case we are basically talking about group forming,
while from an individual
learner’s point of view our system recommends topics of interest to collaborate
on.
In this thesis the term “document” refers to an online document written by
a human.
In general,
we mean such things as a Wikipedia page,
a post to a
online discussion forum, or a Tweet or similar short text in social media.
More
speciﬁcally,
in our empirical
experiment we are concentrating on discussions
in an online discussion forum.
6
Figure 1.4.1: A simple example of Hierarchical Topic Taxonomy from
Software Engineering:
Each document may belong to a more
generic or more speciﬁc topic and these topics are hierarchically
related.
For example a topic discussing class structures and in-
heritance would belong to a more general Software Development
topic,
while discussion on memory allocation would belong into
more speciﬁc topic.
In this context, the term “document” refers to any one post by a learner.
It
is worth noting here that in this case the “documents” are not written in any
formal language or format, but the documents that we use come from people
with wide variety of backgrounds, diﬀerent levels in education and are written
by people with diﬀerent levels of writing skills.
We use the term “topic” to describe the theme of a post.
While one post
may touch several topics, in the course of this research we assume that there
is one main theme in each document and use this as the topic.
A topic can be very general, like “Programming” or more focused and well-
deﬁned, like “Memory allocation in Ansi-C language”.
In fact, this distinction
between more general
and focused topics allows us to create a hierarchical
topic taxonomy,
which we use successfully in our method (See Figure 1.4.1).
7
We describe our method in detail in the Section 3.3.
1.5 Group Forming and Document Recommendation
As seen in the previous Section,
our research can be seen from two points
of view.
From the larger point of view we are trying to form groups in which
individual learners may learn from each other.
From individual learner’s point
of view the problem is similar to a typical problem in the ﬁeld of information
retrieval:
trying to ﬁnd documents that are similar to the interests of
an
individual
learner.
The interest of
a learner is inferred from the discussions
in which the learner has taken part of,
or from the documents he or she has
produced.
See Figures 1.5.1 and 1.5.2.
Group forming is generally researched in the ﬁeld of e-Learning.
According
to collaborative-constructive learning theories, communication between peers
is beneﬁcial
to learning and in order to support this some Learning Man-
agement Systems have introduced a wide variety of more or less elaborative
collaborative tools.
These tools vary from simple communication tools to
collaborative document edition,
and to complex virtual
reality environments
where peers interact together via their avatar characters.
We will have a closer
look at the research in group forming and examples of used methods in Section
2.2.
Another closely related ﬁeld is the so-called “expert recommendation”.
In-
deed, our method manages to recommend knowledgeable and active people in
a very speciﬁc topic in an informal learning environment.
Document recommendation is rather well
research area.
Usual
methods
include using diﬀerent machine learning methods to classify documents into
recommendable and non-recommendable ones, or other methods to deﬁne doc-
8
Figure 1.5.1: Recommending a new topic to a learner:
As a new topic
T is detected,
learners that have been active in the category
(marked by solid line) to which the new topic T belongs to are
recommended the new topic.
9
Figure 1.5.2: Connection between recommendation and group form-
ing:
Comparing to the Figure 1.5.1, Topic recommendation can
also be interpreted as a group forming.
A group is formed from
the person posting the initial topic, and the people to whom that
topic is recommended to.
10
ument similarity and using this is criterion to recommend documents.
We will
take a look at the research related to these methods more closely in Section
3.1.
1.6 Research Problem
The need for a new type of approach came up when we tried to apply the
well-known methods to a martial
arts discussion forum.
We soon noticed
several problems in trying to achieve good recommendations.
Most of the group forming systems are built for well-deﬁned learning.
That
is, the systems form groups from people who are logged in to the system that
has detailed,
explicit information about the people.
In formal
learning the
students may be part of
the same course,
studying exactly the same mate-
rial
in very linear manner.
Needless to say that this gives a good basis for
group forming.
Unfortunately, in an informal learning environment we do not
have this kind of luxury and from the outset most of these methods proved
impossible to use.
Next we turned into information retrieval methods to try to infer the learn-
ers’
preferences and interests and to form groups or recommend documents
based on that.
However, most of the research on methods of information retrieval consider
already well-deﬁned and well-formed documents – such as news or academic
articles – on which meta-data is sometimes available.
In informal learning we
cannot expect our data to be well-formed or noise-free.
Furthermore,
most
information retrieval methods cannot distinguish learners interest in very so-
phisticated manner.
They may have very coarse separation of
topics,
but
usually this is not good enough for recommendation in terms of learning.
11
Most recommendation systems take a corpus and infer users’ interests within
that corpus.
However, the corpus might have a subtle structure not immedi-
ately apparent.
We have a concrete example from our data:
The term “martial arts” used to be almost synonymous with the term “East
Asian martial
arts” in the 1970’s and 1980’s.
However,
as these martial
arts
came popular in the Western main stream, and knowledge of them deepened,
it became clear that diﬀerent people may have very narrow interest in martial
arts.
Also traditional western combat sports as boxing and wrestling became
included under the term “martial arts”.
At ﬁrst we can clearly separate martial
arts based on diﬀerent cultures
they emerged from.
For example, Chinese, Japanese, Philippine, Indian, and
Western martial arts.
There are also big diﬀerences between the competitive
martial arts, traditional martial arts and modern self-defence.
While each of these sub-categories contain a lot of similar vocabulary, per-
sonal interests may vary greatly.
This makes more traditional recommendation
methods unsuitable.
Our data are collected from a martial
arts discussion forum.
To a person
who is not familiar with the domain, the topics may seem very homogeneous
and traditional recommendation methods may seem like good solution.
However, as one looks at the data and the behaviour of the users,
one can
distinguish patterns that seem to make sense only to person very familiar with
the martial arts.
Most people who are not familiar with the subject probably
do not see any diﬀerence between Kick Boxing and Thai
Boxing,
while an
enthusiast may be very interested in one, and despise the other.
Even people who are interested in western sports martial arts, such as boxing
and wrestling, most East Asian martial arts seem the same, whether they come
12
Figure 1.6.1: Overlapping sub-categories:
A corpus may contain several
sub-categories and the learner’s interest may focus on a very
small
sub-set of
apparently similar documents making recom-
mendation in typical
methods
diﬃcult.
In this
example the
sub-category “classical” shares vocabulary with the sub-category
“aikido” through the common parent categories “Martial
Arts”
and “Japanese” making typical recommendation diﬃcult if a per-
son is interested only in classical Japanese Martial Arts.
from China,
Okinawa or Japanese main islands.
However,
for an enthusiast
there is a big diﬀerence in,
for example,
Japanese and Chinese martial
arts
and he or she may have a considerable diﬀerence in interest for each of them.
This kind of structure makes typically used methods less suitable for rec-
ommending topics for an individual
and asks for a diﬀerent approach.
Our
method of dividing the corpus into a hierarchical taxonomy makes the recom-
mendation more accurate and meaningful.
A simple way to understand this is that when classifying documents by tra-
ditional methods, one class can contain several sub-categories and the learner
may be interested in a sub-set of them.
These sub-categories can be semanti-
cally separate and very well deﬁned, but share a lot of same vocabulary within
13
the same class.
For example,
the modern Japanese martial art “aikido” shares a lot of vo-
cabulary, not only with any other martial arts (e.g.
“punch”, “throw”, “kick”)
but also with,
for example,
other Japanese martial
arts (e.g.
“uke”,
“irimi”,
“kokyu”).
A learner’s interest may be in classical Japanese martial arts (styles
and schools established before the Meiji Restoration) and recommending mod-
ern martial art related topics may be considered undesirable (See Figure 1.6.1).
Therefore,
being unable to directly apply existing methods,
we needed to
overcome these problems.
We implemented this inherent hierarchical
topic
structure to our recommendation model
in order to achieve more suitable
recommendations.
We postulated our hypothesis as follows:
Claim.
Due to the noisy and fragmented nature of informal learning, we can
achieve more meaningful and accurate results in document recommendation /
collaborative group forming through the use of Hierarchical Topic Taxonomy.
While the exact details of
our method will
be discussed and compared to
existing methods in the Chapter 3,
we next discuss the contribution of
our
research.
1.7 Hierarchical
Taxonomy Based Recommendation
We suggest a two stage recommendation method that performs better than
conventional
recommendation methods.
The idea behind the method is that
rather than ﬁnding people who are interested in a more general manner in the
discussion topic, we pin-point active and knowledgeable people on a more spe-
ciﬁc discussion topic.
This can be achieved by adding a separate step before
the actual recommendation.
In this initial step we ﬁrst deﬁne a more detailed
14
category for the topic of discussion using a hierarchical
topic taxonomy.
For
example,
instead of
recommending people who are interested in “program-
ming” we recommend people who are active and knowledgeable in the speciﬁc
language (e.g.
Java, C++, etc.) or a more speciﬁc area of programming (e.g.
graphics programming).
Therefore the ﬁrst step of our method is to ﬁnd the more detailed category
of the topic.
This is done by using a classiﬁer that pin-points the given topic
to a category in the hierarchical
topic taxonomy.
Through our empirical
ex-
periments we found that the K-Nearest Neighbour method is the method best
suited for this task.
The second step is to recommend people who are active in that speciﬁc cat-
egory of the discussion.
Our method is successful because by ﬁrst dividing the
discussions into more speciﬁc categories,
we can concentrate on recommend-
ing to people who are highly active in a speciﬁc category and at the same
time ignore categories which they may sometimes participate, but are not ex-
actly interested in.
These speciﬁc categories share a lot of
the vocabulary
with each other causing erroneous recommendation.
As a simple example,
all
discussions on programming share a lot of
vocabulary.
However,
diﬀer-
ent programmers may be more interested in a certain aspect of programming
or a certain programming language.
A Java “guru” may indeed know a lot
about programming in general,
especially object-oriented programming,
but
may have more speciﬁc knowledge on the standard Java libraries.
A C++
“guru” on the other hand maybe equally knowledgeable on object-oriented
programming,
but may not feel
as comfortable commenting on Java-related
questions, as he or she is not as familiar with the standard Java libraries.
On
the other hand,
he maybe more knowledgeable and conﬁdent in giving ad-
vice on memory management,
as Java programmers do not need to concern
15
themselves with the memory management as much due to the Java Virtual
Machine’s “Garbage Collection” memory handling.
Our experiments indeed show that our method recommends more precisely
the topics in which the users are actually active and our questionnaire shows
that our recommendation method successfully recommends topics that the
learners are more interested in, are more likely to participate in, and feel more
conﬁdent on their knowledge in.
1.8 Our Contribution
Our method has two main contributions:
1.
By applying a Hierarchical Topic Taxonomy we can achieve more precise
recommendation in noisy, multifaceted and open-ended discussions.
2.
The Hierarchical
Topic Taxonomy is similar to Skills Frameworks and
Skill Standards (explained in the Section 2.4) and gives our research an
educational context suitable for informal learning.
Besides these two main contributions,
our model
also gives an interesting
insight to the participation patterns of
some of
the learners in an Internet
discussion forum.
These will be discussed in the Chapter 5.
The ﬁrst main contribution is related to the fragmentation of user interest
within a certain category.
While most of
the recommendation methods can
judge learners’ interest in very broad areas (e.g.
between “Computer Science”
and “Martial
Arts”) they cannot distinguish between categories that have
very delicate latent structure.
These kinds of sub-categories usually share a
lot of
vocabulary and the diﬀerences between sub-categories might at ﬁrst
look irrelevant to a person not familiar with the subject.
Still there might be
16
a critical
diﬀerence in keeping up the motivation for discussion for a person
who is more knowledgeable in the area,
and whose interest lies in a speciﬁc
sub-category of the domain.
17
2 Theoretical
Background
In this Chapter we will review the previous research relevant to our thesis.
At ﬁrst we will have a look at theories on informal learning.
Next we will take
a look at collaborative learning and its relevance to our research.
Finally we
will
formulate the theoretical
basis for applying our method in an informal
learning environment and discuss the concept of
Skills Frameworks or Skill
Standards that work as a basic framework to which we based our document
recommendation model.
2.1 Informal
Learning
In this Section we will review the essential research about informal learning
to our thesis.
First, we will discuss the importance of informal learning in the
modern society, and then we will discuss the scope of informal learning.
2.1.1 Informal
Learning in the Modern Society
Modern society has been labelled as a “knowledge society”.
To put this in
simple terms, knowledge is the most important capital in the modern society.
As manufacturing and other physical
labour become more and more auto-
mated, knowledge becomes the essential possession for the work force in this
kind of society.
18
One implication of
this is that learning does not stop after elementary or
even after higher education,
but learning (acquiring knowledge) is a lifelong
process.
As Fischer [13] puts it:
Lifelong learning is an essential challenge for inventing the future of
our societies; it is a necessity rather than a possibility or a luxury
to be considered.
Furthermore,
Marsick and Watkins [31]
note that informal
learning is in the
“heart of adult learning”.
The nature of
informal
learning is very diﬀerent from traditional,
formal
learning.
Formal
learning usually uses well-deﬁned sources – for example,
the materials provided by a lecturer – and happens in a certain space or
environment,
like in a school,
university,
or on a speciﬁc course.
Informal
learning,
on the other hand,
uses a wide variety of
sources,
such as books,
television,
face-to-face discussions,
the Internet,
etc.
and can happen in a
wide variety of places and social environments [31, 39].
From the e-Learning
point of view this means that a lot of the actual learning happens undetected
to us,
and information about the learning process or the learners themselves
cannot be collected easily.
Furthermore,
informal
learning is by nature self-directed and self-centred,
and is centred on collaborative and social activities.
It is usually ill-constructed,
i.e.
there is no clear structure to the order learning happens in and it changes
dynamically according to the whim of the learner in irregular patterns.
There-
fore,
it is also very diﬃcult to distinguish when exactly informal
learning is
actually happening.
For similar reasons, evaluation of the learning is also very
diﬃcult.
[8, 31, 39]
The underlying theory for informal learning lies in the socio-constructivist
learning theories, which will be discussed in Section 2.2.
[28]
19
2.1.2 Scope of Informal
Learning
While informal
learning might be diﬃcult to assess,
it cannot be ignored,
as it aﬀects human societies signiﬁcantly.
A large scale survey conducted in
Canada found that over 95% of adults are involved in explicit informal learning
[30].
The average number of
hours spent in informal
learning was 15 hours
per week, which was contrasted by the average of 4 hours spent on organized
courses.
To underline the importance of informal learning in the Internet age,
the rise in time spent for informal
learning between years 1996-1998 was on
average 3 hours per week.
In [30] it is also reported that people in less advantaged social positions and
lower levels of formal
schooling spend as much time on informal
learning as
the socially advantaged and well
educated.
While young adults spent more
time on informal learning per week, the older people get the more they prefer
self-directed learning and older people consider independent eﬀorts as the most
important source of knowledge.
2.1.3 Many Faces of Informal
Learning
Informal
learning can happen in multitude of
situations and locations,
as
noted above.
For example:
• Autonomous learning
• Organizational learning
• Social / tacit learning
• Online learning
By autonomous learning we mean – in the context of informal learning – that
the learner is learning by him- or herself, using, for example, books, television,
20
browsing documents on the Internet, etc.
Organizational learning usually refers to a company or similar entity learn-
ing as whole, not only individuals within the organization.
This kind of learn-
ing has been discussed widely in the literature and its connection to Informal
Learning is well recognized [35].
Social or tacit learning usually refers to learning we are not necessarily aware
of ourselves.
For example,
transferring values and sometimes even prejudice
from one generation to another fall into this category.
As the name hints, this
kind of learning mostly happens through social interaction, as we try to adapt
into the society we are living in.
[30, 39]
In the course of
this study,
we are mostly interested in the third kind of
informal
learning,
online learning.
In the next subsection we will
discuss in-
formal online learning in more detail.
2.1.4 Informal
Online Learning
As the learning paradigm shifts towards informal learning, researchers have
become more and more interested in the informal learning happening online.
Especially the introduction of the so-called Web 2.0 applications, such as Social
Networking Services, Wikipedia, and online communities.
In [11] this paradigm shift is expressed as seen in the Table 2.1.
As can be seen, the paradigm shift is mostly towards more informal learning,
away from well-deﬁned systems and strict courses.
In this research, we aim especially to support three of the above mentioned
features:
1.
Participation process
2.
Communication
21
e-Learning 1.0
e-Learning 2.0
Learning Manag.
Syst.
Personal Learn.
Env.
Acquisition process
Participation process
Multimedia
SNS and Communities
Provided content
Learner content
Curricula
Portfolios / diaries
Course Structure
Communication
Tutor availability
Learner interaction
Expert assessment
Peer assessment
Table 2.1: Comparing e-Learning 1.0 and e-Learning after the popularity of
Web 2.0 applications.
3.
Learner interaction
This is achieved via recommending discussions to learners,
so that they can
interact and share ideas,
and in the process learn something.
Our method
is eﬀective due to the fact that it can recommend people who are especially
active in the topic and feel conﬁdent in sharing their knowledge on it.
As can be seen, our research touches the realms of collaborative and coop-
erative learning theories, as well as socio-constructivism.
In the next Section
we will take a closer look at these.
2.2 Constructive Learning Theories and Collaborative
Learning
In this Section we will
discuss the collaborative learning and constructive
learning theories.
We will also review published research on group formation
in e-Learning environments and discuss their suitability for informal learning.
22
2.2.1 Constructive and Collaborative Learning
According to constructive learning theories, learning is an active process and
relies highly on social activities.
The theory contrasts the behaviouristic view
of
learning,
in which facts given to the learner by instructor or teacher are
memorized through education.
The supporters of the constructivist learning
theories consider learning as a process where an individual
constructs their
understanding of the world surrounding them via activities and social interac-
tion.
Through these activities the learners constantly construct new informa-
tion and adjust their understanding actively based on their past and recent
experiences.
[26, 12]
Constructivist learning theories are well-supported and researchers are ba-
sically unanimous on the fact that learners beneﬁt greatly from collaborative
activities during a learning process.
The term “collaborative activities” can
include a wide variety of activities, where two or more learners work together
towards a common goal.
Besides the common goal each learner may also have
their individual
agenda,
e.g.
acquiring certain skills,
arguing their point of
view,
etc.
Therefore,
as [10]
argues,
collaboration comes in a large variety
of
diﬀerent forms,
from short period tasks to processes that may span gen-
erations,
from two people having a discussion up to a whole society working
together.
The basis of
the concept of
collaborative learning is in Vygotsky’s theory
of social
constructivism,
or socio-constructivism as it is often expressed.
Ac-
cording to Vygotsky,
knowledge is mediated via social
activities.
According
to Vygotsky’s concept of “Zone of Proximal Development” (ZPD) learners can
learn ideas and concepts by collaborating with more advanced and knowledge-
able peers.
[6]
From a pedagogical
point of
view collaboration is by no means a trivial
23
thing and aspects like group dynamics, roles of the participants in the collab-
oration and so forth aﬀect the results of the activity.
However, because of the
ambiguous nature of the term “collaborative learning” we can also note that
collaborative learning happens spontaneously every day in the Internet,
for
example.
People share ideas, hold discussions, ask questions, and form groups
via bulletin board systems,
chats and online communities like MySpace and
Facebook.
In the course of this study we are interested in creating groups of an unde-
ﬁned number of people – sharing interest in similar subjects – collaborating
within an open time frame.
We can look at the system from two points of view:
from the system-wide perspective,
we are interested in group forming,
while
from each individual learner’s point of view the system can be seen as a rec-
ommendation system.
Traditionally,
collaborative groups in an educational
surrounding have been formed intuitively by the teacher or by the learners
themselves.
We are especially interested in creating groups from active and knowledge-
able people that can share their knowledge with their peers.
In other words,
we create an online “zone of proximal development”.
2.2.2 Existing Methods for Group Forming and Recommendation
in e-Learning
Some systems and methods have been developed for forming collaborative
groups automatically in an e-Learning environment.
For example,
[17]
uses
the heterogeneity as deﬁned by the personality traits of the learners.
Groups
are selected so that heterogeneity is maximized within the group.
According
to some research they cite, heterogeneity of a group works towards the quality
of learning.
While not explicitly stated, their research obviously assumes that
24
the learners are working within the framework of the same curriculum.
In [43] learners’ relative progress in the course material is used as a criterion
for group formation:
when a suitable number of
students reach a so-called
“point of cooperation”, a collaborative activity is automatically triggered.
The
members in each collaborative group – selected from the students that have
reached a point of cooperation – are then decided automatically by the system
or intuitively by an administrator or a teacher.
In [42],
what they label
as “opportunistic group forming”,
is used.
This
is fundamentally similar to the approach in [43].
In [42]
the points of
col-
laboration are not scheduled in the learning material, but the system decides
when a learner is in need of collaborative activities (e.g.
has trouble under-
standing a certain part in the course) and assigns roles to other learners based
on their progress and success in the learning material.
In other words,
other
learners might be recommended for collaboration if they can help a learner in
trouble (in a tutor or mentor kind of role), or if they have problems with the
same learning object as the person for whom the collaboration was originally
initiated.
In [45]
a method for self-organizing e-Learning communities is introduced.
In short the system clusters learners into communities based on information
– such as taking part in courses,
submitting questions and assessments,
and
performing exercises – collected about the learners.
While the authors of [45]
manage to show results on successful forming of the learning communities, the
actual interaction of learners as a part of a community is not discussed.
In [14], user participation frequency in online discussions is predicted using
Weighted Non-Negative Factorization.
This prediction measure gives an ap-
proximation on users’
over-all participation on a discussion forum.
However,
the study does not categorize the topics of discussions, but simply diﬀerenti-
25
ates the learners based on their activity.
Therefore the method is diﬃcult to
use directly with recommendation.
As seen above, most group formation methods are concentrated on bringing
learners together in a closed,
well-deﬁned learning environment [17,
42,
43].
This kind of model usually relies on a certain explicit representation of under-
lying components.
For example, a learner model may need to contain explicit
descriptions of student knowledge and goals.
In addition,
if the assumptions
(upon which the model
is based on) change,
the model
may not be able to
automatically adapt and may again require additional resources for needed ad-
justments.
Others either loosely group learners together without any explicit
implementation for collaboration within the group [45], or only predicting the
frequency of participation to a discussion [14].
For typical
examples of learning object and document recommendation in
e-Learning we reviewed three relatively recent publications.
These systems or
methods represent in our opinion well typical recommendation in e-Learning:
all
are aimed for more or less closed and formal
environments,
where well-
constructed meta-data is readily available from both the learners and the
recommended objects.
Middleton et al [32] describe a recommendation system for academical pa-
pers.
The system uses hybrid recommendation method (see Section 3.1) to
recommend articles to users.
The recommendation uses document meta-data
to represent user proﬁles in ontological
terms.
The recommendation is then
done using collaborative ﬁltering.
The system has been tested in a formal
learning environment in a University, recommending computer science related
papers to 14 and 24 users in a computer science laboratory.
Huang et al [23] describe a SCORM compatible learning object recommen-
dation system.
The recommendation system calculates Jaccard’s coeﬃcient
26
based on the learning object meta-data and then uses collaborative ﬁltering to
recommend learning objects between users.
Needles to say, that the SCORM
learning objects contain well-structured meta-data, which can be used during
the recommendation process.
Yu et al [46] introduce a system that is closest to our concept of Hierarchi-
cal
Topic Taxonomy.
They use a hierarchical
ontology model
to recommend
learning objects to learners.
However,
unlike our system,
their recommenda-
tion happens in a formal
learning environment.
All
recommended items are
learning objects that contain the ontological information as meta-data.
Users
are recommended learning objects in a context of a well-constructed course.
It is surprising to see, that while the necessity of informal learning has been
acknowledged in the literature, very little research has been actually done on
systems directly supporting this kind of learning, and taking into account the
scope and the special nature of how it is conducted.
In our approach we pay
special attention to take into account the nature and the diﬃculties associated
with informal learning.
2.2.3 Learner Modelling in e-Learning
When recommending learning objects or tutoring to learners,
the usual
method is to model
the learners’
knowledge in the target domain in order
to recommend reasonably interesting and not too diﬃcult materials to the
learners.
We will
next review three typical
models used in e-Learning and
argue that they are not as such applicable to informal learning.
Overlay Model
The overlay model
(Figure 2.2.1) maps the learner’s knowledge to the do-
main knowledge in order to recommend suitable tutoring or learning materials
27
Figure 2.2.1: Overlay Learner Model:
Overlay Learner Model
represents
the learners knowledge in relation to the domain knowledge and
maps the learners current knowledge in order to oﬀer suitable
tutoring or learning object recommendation.
to the learner.
After oﬀering the new knowledge to the learner, it is expected
to be learned and the learner model is updated.
Diﬀerential
Model
The diﬀerential
model
(Figure 2.2.2) is an extension to the overlay model.
In addition to the overlay model the diﬀerential model maps also the knowl-
edge to which the learner has been exposed to, but which the learner has not
necessarily yet fully grasped.
Perturbation Learner Model
The Perturbation Model (Figure 2.2.3) includes any misconceptions that the
learner may have to the learner model.
Recommender system or an intelligent
tutoring system may then oﬀer materials and tutoring to remedy the learner’s
28
Figure 2.2.2: Diﬀerential Learner Model:
Diﬀerential Learner Model rep-
resents the learners knowledge in relation to the domain knowl-
edge and maps the learners current knowledge in order to oﬀer
suitable tutoring or learning object recommendation.
In contrast
to the Overlay Model the Diﬀerential Model maps the knowledge
to which the learner has been exposed separately to the knowl-
edge that the learner is expected to have fully learned.
29
Figure 2.2.3: Perturbation Learner Model: In addition to learner’s knowl-
edge the Perturbation Learner Model presents the learner’s mis-
conceptions or “bugs”.
These misconceptions can then be cor-
rected via tutoring or teaching.
misconceptions.
Learner Models in Informal
Online Learning
In informal learning none of the above learner models are usable as such (see
Figure 2.2.4).
We do not have any clearly deﬁned domain and therefore map-
ping the learners knowledge and detecting any misconceptions is impossible.
The learner’s knowledge may well
be fragmented into smaller sub-categories
of the domain and trying to force the learner to learn what we consider worth-
while, rather let the learner him- or herself decide, is futile.
Therefore, in the course of this research we do not limit ourselves into any
strict concept of
learner model.
We collect information about the activities
of the learners and infer from that activity narrowly deﬁned sub-categories of
the domain in which the learner is active and knowledgeable.
30
Figure 2.2.4: Learner Modelling in Informal
Learning:
Due to the am-
biguous nature of informal
learning none of the learner models
are as such applicable.
Next we are going to discuss the theoretical
basis for group forming and
document recommendation in informal learning.
2.3 Theoretical
Basis of Group Forming / Document
Recommendation in Informal
Learning
Considering that informal learning is happening mostly invisible to us, we do
not have explicit information about the learners’
learning process.
However,
when a learner needs help in a particular problem,
or needs to reﬂect his or
her own ideas via discussion with other learners,
we can provide the learner
potential collaborators.
While concentrating on “organizational
learning”,
i.e.
dissemination and
creation of
knowledge in an organization – especially a business one – we
ﬁnd Nonaka and Konno’s [35]
model
of
knowledge acquisition to be a clear
31
Socialization
Externalization
Combination
Internalization
T
a
c
i
t
K
n
.
Tacit Knowledge
E
x
p
l
i
c
i
t
K
n
.
Explicit Knowledge
Figure 2.3.1: The SECI-model:
Nonaka and Konno explain learning as a
spiralling process between tacit and explicit knowledge,
recog-
nizing four phases in this process:
Socialization,
Externaliza-
tion, Combination, and Internalization.
A person may have tacit
knowledge,
knowledge he or she has internalized and is able to
use practically.
However,
the person may not be able to for-
mally explain or teach his or her knowledge to other people.
Through socialization and externalization the knowledge gets a
formal presentation and can then be combined to each individu-
als personal
tacit knowledge,
continuing the SECI-cycle (image
based on Nonaka and Konno [35]).
32
example of how knowledge is transferred and created in an informal learning
environment.
Indeed,
organizational
learning is usually considered to be of
the informal
kind.
This model
is basically based on the socio-constructivist
learning theories explained in Section 2.2.1 and combining it to the concept of
tacit knowledge by Polanyi [37].
Nonaka and Konno [35]
deﬁned knowledge acquisition (i.e.
learning) as
a spiralling process between tacit and explicit knowledge.
They recognized
four phases in this process:
Socialization, Externalization, Combination, and
Internalization (the SECI-model, see Figure 2.3.1).
Tacit knowledge refers to knowledge that a person has, but can not explicitly
formulate into coherent explanation or is not necessarily explicitly conscious
of.
As an example of tacit knowledge, we may have through our experience at
work become ﬂuent in certain work related processes.
We might not be able
to exactly explain to a peer how or why those processes work,
or be able to
teach them to a colleague.
Through Socialization,
daily interaction with our peers,
and Externaliza-
tion, using and sharing our knowledge and explaining our opinions and ideas
to others, we formulate this tacit knowledge into explicit knowledge.
Through
feedback and receiving new information – which sometimes challenges and con-
ﬂicts with our previous beliefs – we combine it with our previous information
and slowly internalize it, creating new tacit knowledge.
In terms of online forums, people acquire diﬀerent skills and knowledge from
multiple sources and social contacts.
They then share their knowledge online,
in a somewhat limited social environment.
Our method supports this process most notably in the Externalization and
Combination phases via online socialization (Figure 2.3.2).
Due to the am-
33
Figure 2.3.2: The SECI model and online Discussions: Knowledge acqui-
sition and the SECI-model
in relation to our method.
Informal
learning happens from a wide variety of
sources,
and therefore
is diﬃcult to assess from e-Learning perspective.
However, part
of
the learning process can be grasped when learners commu-
nicate their ideas online,
externalizing their knowledge,
sharing
and arguing their ideas with online peers,
and combining new
knowledge with the help of them.
34
biguous nature of informal
learning we can not “see” nor assess most of the
learning process,
i.e.
the part of
the learning that happens oﬄine.
We can
only detect the activity of the learners according to their participation in on-
line discussion.
We can then use this information to form groups to socialize
and to share knowledge with.
In more concrete terms, when learners run into a situation in their learning,
where they require help or want feedback on their ideas, we can provide them
with multiple peers, who are active and familiar with the subject in question
and motivated to discuss about it.
Obviously we need precise recommenda-
tions for people who are both active in discussing the speciﬁc topic and able
to give a worthwhile contribution to the discussion.
2.4 Skills Frameworks and Skill
Standards
In this Section we discuss Skills Frameworks,
or Skill Standards,
and their
relation to our research.
Both terms,
“Skills Framework” and “Skill
Stan-
dards” have been used in the literature to describe similar kind of concepts.
In the course of this article we use the term “Skills Framework” from now on.
Skills Frameworks are usually developed from an governmental
initiation,
and they deﬁne a presentation standard for skill
areas,
skills,
sub-skills,
and
competency (Figure 1.4.1).
The motivation behind them is to ensure suit-
able Human Resources Development for,
for example,
industry needs.
They
can be used for learning eﬀectiveness and quality management in the Human
Resources Development.
[20]
Some examples of
Skills Frameworks are the SFIA Skills Framework by
the Skills Framework for the Information Age (SFIA) Foundation and the
Chief Information Oﬃcer Council in The United Kingdom, and the Embedded
35
Technology Skill Standards,
and the Information Technology Skill Standards
developed by the Japanese Ministry of Economy, Trade and Industry.
36
3 Methodology
In this Chapter we present the methodology used in our research.
At ﬁrst
we take a general look at recommender systems.
Next we will discuss formal
methods of
presenting documents.
Then we will
have a look at the classi-
ﬁcation and clustering methods relevant to our research.
Finally,
and most
importantly, we will introduce our own method.
3.1 Recommender systems
History of
recommender systems can be traced to research on cognitive
sciences,
information retrieval,
and approximation theory.
Actual
ﬁrst inde-
pendent papers published on recommender systems were in the mid 1990’s.
The ﬁrst systems were almost exclusively concerned with collaborative ﬁlter-
ing.
These systems were based on estimating user ratings of
the objects in
the object space that they had not yet observed,
based on the ratings of the
objects they had rated so far.
[3]
In the current literature the recommender systems are be deﬁned as systems
that ﬁnd and recommend to the users relevant objects from a large object
space.
The term “relevant” here can be understood as interesting or useful for
the individual
user.
The aim is to help the users with information overload
and give them personalized content or services [3, 36, 5].
In more formal terms,
if we consider O as the space for the objects,
the U
37
as the space for users and the function t :
U × O → R as a utility function
that measures the the usefulness or desirability of objects o ∈ O to the users
u ∈ U .
We then have generally two options:
• Recommend a user the top n objects from the R
• Recommend all items for which the t(u, o) = r ≥ r
t
, where r
t
∈ R is the
recommendation threshold
In order to do this, both the objects and the users are proﬁled using diﬀerent
characteristics.
For users, included characteristics range from very simple, like
only a user ID, to very elaborate, including age, gender, profession, nationality
and other demographic information.
This also includes information of
the
users’
interaction with the system,
such as,
purchases,
accepting or rejecting
a recommendation,
etc.
Similarly the objects may have any combination of
characteristics ranging from very simple to very elaborate.
For example, if the
objects are movies,
besides genre and director they may include information
on year of release, popularity, leading actors, etc.
[3, 36]
One big problem is that the utility is not deﬁned in the whole U × O space.
The solution in general
is to specify heuristics for the utility and using em-
piricist methods to validate the utility or estimating a utility function that
optimizes some performance measure [3].
While there are several ways of classifying recommender systems, the most
common in modern literature can be split into three categories:
1.
Collaborative Recommender systems
2.
Content-based Recommender systems
3.
Hybrid Recommender systems
38
Another way of classifying the recommender systems is based on the method-
ology they use.
In this way they are usually split into two separate categories
that overlap the above three categories [3, 36]:
1.
Memory based systems
2.
Model based systems
Memory based systems in practice keep track of
the whole history of
the
users’
preference of
the objects and use some sort of
heuristics to predict
preference for new objects, while model based systems usually try to capture
the likelihood of users preference based on the object characteristics.
First we are going to have a short look at the content-based recommender
systems,
the collaborative recommender systems,
and ﬁnally the hybrid rec-
ommender systems.
In the course of this thesis we will be biased towards the
content-based recommender systems and the methodology related to them, as
they are more relevant for our research.
3.1.1 Content-Based Recommender Systems
In a strict sense,
content-based recommender systems in general
recom-
mend objects to the users based solely on the individual
user preferences on
the object characteristics.
In the memory based systems the information on
preferences can be acquired explicitly,
for example by users rating the ob-
jects, or implicitly by observing users interaction with the objects.
In practice
most of the modern content-based recommender systems base their methods
in information retrieval and various machine learning methods.
[36, 3]
In the model based systems the problem usually comes down to a classiﬁca-
tion problem.
The user model in general yields a likelihood or probability on
the unseen object based on the training on the objects seen by the user.
The
39
classiﬁcation problem is then, in general, a question of binary classiﬁcation of
the objects into the category of “the user prefers the object” or “the user does
not prefer the object”.
While this seems a rather simple task, the fact is that most of the times we
do not have explicit information on the users’ preferences in the object space.
The implicit information we can collect is usually very noisy,
and we require
a lot of it to make good decisions on the users’ preferences.
[36]
As most information online is text information, one important part of rec-
ommendation is document or text recommendation.
These include for exam-
ple web-page recommendation or news article recommendation.
In e-Learning
these also include learning materials and – what we are especially interested
in – online discussions.
As most of the document data that are the object of recommendation are
in unstructured form,
for example as unrestricted texts,
in order to do doc-
ument recommendation,
the documents ﬁrst need to be represented in some
structured form.
The information retrieval
and data mining community have developed a
variety of methods that are used for document recommendation.
These include
classiﬁcation and clustering methods,
diﬀerent data representation methods,
etc.
In fact, these are such an important part of our research, that rather than
going through them here,
we will
dedicate a whole Section to them (Section
3.2).
Next we are going to have a look at the systems where the recommender
systems ﬁrst started oﬀ as an independent research area,
the collaborative
recommender systems.
40
3.1.2 Collaborative Recommender Systems
The collaborative approach in general
means that the system tries to ﬁnd
similar patterns in user behaviour (for example, style of music, preferred books,
etc.) and then recommends objects across users with similar patterns.
Using
the formalism discussed in Section 3.1,
rather than using a utility function
t(u, o), we use the function t
�
(u
�
, o), where u
�
∈ U are users that are similar to
the user u.[3, 5]
The greatest strength of the collaborative recommendation is that we really
do not need to be concerned what the objects are,
or how we can represent
them formally.
We just need to have information about users’ preferences on
them.
[5]
A simple example of
this is the Rating Matrix.
It is used as a part of
a
system in which the users can rate the objects (e.g.
movies, music, books).
A
matrix of the users and the objects is then formed:
o
1
o
2
· · ·
o
n
u
1
r(u
1
, o
1
)
r(u
1
, o
2
)
· · ·
r(u
1
, o
n
)
u
2
r(u
2
, o
1
)
r(u
2
, o
2
)
· · ·
r(u
2
, o
n
)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
u
m
r(u
m
, o
1
)
r(u
m
, o
2
)
· · ·
r(u
m
, o
n
)
where r(u
i
, o
j
) is the rating given to the object o
j
by the user u
i
.
As a very much simpliﬁed example,
if
user u
r
then,
for example,
gives a
good rating to objects o
k
, o
l
and o
m
and user u
s
gives good ratings to objects
o
k
and o
l
and has not yet observed the object o
m
,
the system recommends
object o
m
to the user u
s
.
In practice the utility of the object o
m
is calculated
over several users who are similar to user u
s
.
Similarity between users can be decided by object ratings (e.g.
cosine sim-
ilarity or Pearson correlation coeﬃcient of the object rating vectors between
41
the users),
or demographics (e.g.
gender,
age).
Most of
the modern collab-
orative recommender systems however use the former option,
object ratings.
[3]
The recommender system research community has come up with several
performance increasing methods for the typical
cosine and correlation meth-
ods,
such as default voting,
inverse user frequency,
case ampliﬁcation,
and
weighted-majority prediction.
Other methods, besides similarity, include clus-
tering the users based on their ratings,
Markov decision process and proba-
bilistic modelling.
[3, 5]
3.1.3 Hybrid Recommender Systems
As both methods, the content-based recommendation and the collaborative
recommendation,
have their strengths and weaknesses,
a natural
step is to
combine them.
The hybrid recommender systems try to utilize the “best of
both worlds” and over come the inherited weaknesses of either of the methods.
The hybrid systems can be classiﬁed into four categories [3]:
1.
Combining the inference from separate collaborative and content-based
methods.
2.
Including content-based characteristics in a system generally using a
collaborative recommendation.
3.
Including collaborative characteristics in a system generally using a content-
based recommendation.
4.
Unifying both content-based and collaborative characteristics into a sep-
arate, new model.
[5] recognized seven diﬀerent types of hybrid recommendation:
42
Weighted:
The scores (or votes) of several recommendation tech-
niques are combined together to produce a single recommendation.
Switching:
The system switches between recommendation tech-
niques depending on the current situation.
Mixed:
Recommendations from several
diﬀerent recommenders
are presented at the same time
Feature combination:
Features from diﬀerent recommendation
data sources are thrown together into a single recommendation
algorithm.
Cascade:
One recommender reﬁnes the recommendations given
by another.
Feature augmentation:
Output from one technique is used as an
input feature to another.
Meta-level:
The model
learned by one recommender is used as
input to another.
3.1.4 Discussion
As the potential for recommender systems in terms of marketing and busi-
ness use is high, they are used a lot in the commercial world.
In e-Learning rec-
ommender systems have been used to recommend the learners suitable learn-
ing materials and peers to collaborate with.
In the latter sense recommender
systems are sometimes closely related to group forming.
As seen in the Section 2.2.2 most of the group forming methods in e-Learning
are concentrating on well constructed learning in a well deﬁned domain.
There-
fore,
we will
take an approach more towards the recommender systems.
As
our target environment (online discussions) has a lot of data produced by the
learners but very little direct information about the learners themselves,
we
43
will choose content-based recommendation as our method.
We also discussed that one important part of
modern content-based rec-
ommendation is document recommendation.
As document recommendation
methodology comes essentially from text mining and this research area is the
most important for our research,
in the next Section we will
go through the
text mining methodologies essential for our research more thoroughly.
3.2 Text Mining
In this Section we will
review the text mining methods that are relevant
to our research.
As seen in the previous Section,
the approach most suitable
for us is the Content-based recommendation,
due to the ambiguous nature
of
our target domain,
the informal
learning.
To be more precise,
as we are
dealing with text documents submitted by the learners, our methodology also
naturally uses the methodologies widely applied in text mining.
Text mining is essentially a collection of methods for retrieving information
from a large corpus of unstructured text.
As the name suggests, text mining is
closely related to data mining and uses a lot of similar kind of methods applied
to textual sources.
But where data mining is involved with well structured data
from databases, text mining is more considered with unstructured data from
(usually) natural language sources.
Text mining also overlaps with the research
ﬁelds of information retrieval, machine learning, statistics and computational
linguistics.
[40, 22, 18]
Trying to ﬁnd hidden patterns and connections in naturally language text
documents can be an arduous task.
The methods used must be scalable to
large amount of
words and structures in natural
language and at the same
time be able to handle vagueness and fuzziness.
[40, 22]
44
Some common text mining tasks are [40]:
• Using supervised and semi-supervised learning to classify documents into
a set of speciﬁed topic areas.
• Using unsupervised and clustering methods to group documents in such
manner that each member of each group has similar characteristics.
• Finding documents that satisfy some search criteria (information re-
trieval).
• Figuring out if the opinion expressed in a document is for or against a
claim (sentiment analysis)
• Summarization of the contents of document.
We can recognize that text mining can have two levels:
1.
Finding information within each individual document
2.
Finding information about inter-document relations
In the ﬁrst case we are more interested in what each individual
document
is expressing as it is.
While this might include domain information from a
corpus,
the focus is in the contents of
each of
the documents individually.
This might include sentiment analysing and summarization, for example.
In the second case the focus is more in the relation between the documents
in the corpus and on how the contents of the documents relate to each other.
In the course of this research we concentrate exclusively on the second case
in order to limit the scope and to be able to focus on relevant methodology.
In the next Sections we are going to take a look at the text mining methods
relevant to our research.
First we are going to discuss the document presenta-
tion methods for text mining.
Next we are going to take a look into document
45
c
w
W
D
Figure 3.2.1: The bag-of-words document generation:
Each word is cho-
sen from the word distribution W for the corpus D with the
corpus representing one uniﬁed topic c.
classiﬁcation and clustering.
We are going to especially take a look at the fol-
lowing clustering and classiﬁcation methods:
Centroid Based Classiﬁcation,
Naive Bayes, K Nearest Neighbours and Support Vector Machines.
3.2.1 Document Representation
In this Section we discuss the diﬀerent ways of formally representing a doc-
ument.
These representation methods are necessary for formal
calculations
within the document space.
Document Vector Space
The most used method for document representation in text mining is the
document vector space model.
The basic idea is to transfer each text document
in the corpus into a vector of numerical values to allow automatic processing
46
of the corpus.
The common way of
viewing documents in this case is the bag-of-words
approach (see Figure 3.2.1) [22].
This means that the documents are seen as
collection of individual words, and the relation and order between the words is
not considered important.
This is a huge simpliﬁcation in terms of document
representation and allows the use of
vector space models.
While we might
lose a lot of semantic and syntactic information doing this assumption, as we
are more interested in the relation of the documents between each other than
the exact contents of each individual documentation, this approach has been
proven to be much simpler in terms of
computational
complexity and still
eﬀective enough for the purpose.
To express the document vector space representation in slightly more formal
terms,
each of the documents in the document space D are generally formu-
lated as a n-dimensional ﬁnite vector,
�
d
i
= (w
i1
, w
i2
, . . . , w
in
), where n is the
size of the vocabulary V of the document space including all words occurring
in the document space.
The terms w
ij
in the document vector for document d
i
are generally called word or term weights and there are several diﬀerent ways
of calculating them.
As in most cases when dealing with natural language documents, usual the
ﬁrst step before the vector presentation is to preprocess the documents.
Next
we will ﬁrst brieﬂy discuss typical preprocessing methods and then go through
the most common vector representation methods in the order of simplicity.
Document Preprocessing
As seen in the previous Subsection 3.2.1, the length of each document vector
is the size of the vocabulary in the whole corpus.
If the we have a large corpus,
the vocabulary can be huge.
Therefore one aim of the document preprocessing
47
is to reduce the dimensionality of the document vector space [22].
Typical preprocessing methods to reduce the dimensionality include [22, 40]:
• Removing of stop words
• Stemming
• Lemmatization
• Use of synonym and concept dictionaries
Stop word removal deals with removing the words that do not contribute in the
features of any document in a meaningful manner.
In practice, the stop word
list might vary greatly,
as depending on the domain,
some words might be
relevant to that domain, but irrelevant to other domains.
Typical stop words
for almost every domain include for example prepositions,
postpositions and
conjunctions.
Stemming is the process of ﬁnding the body of a word.
This method is very
much dependent on the target language in question,
as some languages have
more inﬂections for words than others.
As a simple example in the English
language, we can consider removing the -ing aﬃxes from the progressive verb
forms or removing the -s aﬃx for plural nouns.
Lemmatization tries to return each verb form in to the inﬁnite tense and
nouns to the singular form.
This diﬀers from stemming in the sense that where
stemming is more of mechanical
nature (simply recognize and remove aﬃx),
lemmatization requires recognizing the part-of-speech of
a word and ﬁnding
the lemma through this.
As this in a more complex and error prone method
stemming is more often used.
[22]
Another – though not so often used method due to the intensive and costly
human labour involved in creating the dictionaries – method is the use of
48
synonym and concept dictionaries.
Synonymous words are replaced with one
similar word and some words can be replaced with a word expressing the
concept of the words.
An example of concept dictionary would be replacing
all geographical place names with a word “PLACE”.
Binary Vector Representation
The simplest way of presenting a document in the vector space,
is to form
the document vector as a binary vector
�
d
bin
i
= (w
bin
i1,
, w
bin
i2,
, . . . , w
bin
in,
), where:







w
bin
ij
= 1,
if w
j
∈ d
i
w
bin
ij
= 0,
if w
j
/
∈ d
i
Each of
the weights w
bin
ij
are 1 if
the word w
j
occurs at least once in the
document d
i
, or 0 if the word w
j
does not occur in the document.
The beneﬁts of the binary vector representation are obvious:
the simplicity
of this representation method makes computation much easier.
However, the
importance of
each word in a document cannot be captured by this model.
Therefore,
the next step is to include the information on how important a
word is to a document.
Term Frequency Vector Representation
The easiest way of
including the weight of
a word in a document is the
number of
times a word appears in a document.
Logic behind this is,
of
course, that the more times the a word is mentioned within a document, more
relevant it is to the contents of that document.
If we have a document vector
�
d
i
= (w
i1
, w
i2
, . . . , w
in
) for document d
i
, each
element w
ij
of the vector is the number of times the word w
j
appears in the
document i.
This representation catches the importance of each word for an individual
49
document rather well, but does not tell anything about the importance of each
word in the context of the whole corpus.
If
we consider as an example a situation where we have two corpora,
one
on the domain of politics and another in the domain of detective stories.
The
word “murder” might be very important and even critical
in diﬀerentiating
between diﬀerent documents (e.g.
news) within the domain of politics and –
at least hopefully – appears very rarely.
However,
in most detective stories
(documents) the word “murder” is likely to be very common,
and does not
really give any information to tell the detective stories apart from each other.
Therefore the next natural step is to weight each of the words in more global
context.
TF-IDF Vector Representation
Probably the most used document vector presentation is the Term Fre-
quency / Inverted Document Frequency (TF-IDF) method.
In TF-IDF each
word is assigned a weight according to the relevance (frequency) of the word
to the document versus its frequency in the corpus.
While there are several
formal presentations of this, in the course of this research we use the following:
#w
ij
#w
i
· log(
#d
#d
j
),
where #w
ij
is the frequency of the word j in the document i,
#w
i
is the
number of words in document i, #d is the total number of documents in the
corpus, and #d
j
is the number of documents containing the word j.
While the TF-IDF document representation model is essentially an ad-hoc
method and does not have clear theoretical structure, it has been empirically
and practically proven to be a very good representation method and is applied
widely.
50
Latent Semantic Analysis
Latent Semantic Analysis (LSA) is a method that further reduces the doc-
ument vector space.
The method was introduced in the late 1980s – early
1990s by [15, 9].
It is an application of Singular Value Decomposition (SVD)
method to the TF-IDF document vector representation.
[21]
If we consider the corpus as a matrix
ˆ
D formed from the document vectors
�
d
i
in the corpus D, then the SVD can be formulated as:
ˆ
D = UΣV
T
,
where the columns of U and V are the left and right singular vectors corre-
sponding to the diagonal elements of Σ called the singular values δ
1
, δ
2
, . . . , δ
r
of the matrix
ˆ
D.
Further, U and V are orthogonal so that U
T
U = V
T
V = I
r
, where r is the
rank of
the matrix
ˆ
D.
We can then use the ﬁrst k columns of
U and V to
calculate a rank-k approximation of the matrix
ˆ
D:
ˆ
D
k
= U
k
Σ
k
V
k
.
In practice this method eliminates some of the noisy, irrelevant dimensions
from the corpus,
yielding a more compact representation of
the document
vector space.
The claimed practical advantages of LSA are:
• Synonymy
• Polysemy
• Term dependence
51
d
c
w
W
D
Figure 3.2.2: The document generation in pLSA:
Each word is chosen
from the word distribution W for the latent topics d.
In terms of
the word weights LSA captures synonymy by representing syn-
onymous terms in similar weighted combination of the SVD variables.
Same
applies for polysemic words,
i.e.
words with multiple meaning (e.g.
mole:
an animal
and a small
raised brown spot on human skin).
The SVD also
to some extent works as an “antidote” against the bag-of-word presentation
nature of
the documents as it includes word-word,
document-document and
word-document correlations into the presentation.
Probabilistic Latent Semantic Analysis
Probabilistic Latent Semantic Analysis (pLSA) or Probabilistic Latent Se-
mantic Indexing (pLSI) is an extension of
the LSA including,
as the name
hints, probabilistic framework.
It models each document as a mixture of top-
ics, further reducing the dimensionality of the document representation.
[21]
The basic idea is that each of the documents consists of several topics.
For
example, an article in the general ﬁeld of environmental politics may include
topics about environmental
activism,
globalization,
global
warming,
energy-
sources, etc.
52
The pLSA has strong theoretical foundation in statistics.
The basic idea is
that a document consists of latent classes c ∈ C.
Each of the co-occurrence of
word and document (w, d) is an mixture of multinomial probability distribu-
tions:
P (d, w) = P (d)P (w, d),
where
P (w, d) =
∑
c∈C
P (w | c)P (c | d).
The distribution for each document is then usually estimated with Expec-
tation Maximization (EM) algorithm [33, 4].
However, as [4] note, pLSA is not without problems:
The pLSI model
attempts to relax the simplifying assumption
made in the mixture of
unigrams model
that each document is
generated from only one topic.
In a sense, it does capture the pos-
sibility that a document may contain multiple topics since p(z | d)
[Note:
[4] uses the symbol z for the topic classes, for which we use
the symbol c above] serves as the mixture weights of the topics for a
particular document d.
However, it is important to note that d is a
dummy index into the list of documents in the training set.
Thus,
d is a multinomial
random variable with as many possible values
as there are training documents and the model
learns the topic
mixtures p(z | d) only for those documents on which it is trained.
For this reason,
pLSI
is not a well-deﬁned generative model
of
documents; there is no natural way to use it to assign probability
to a previously unseen document.
A further diﬃculty with pLSI,
53
α
θ
β
c
w
W
D
Figure 3.2.3: The document generation in LDA: First a topic c is chosen
from a Dirichlet distribution θ and then each word is chosen from
the word distribution W for that topic.
which also stems from the use of a distribution indexed by training
documents, is that the number of parameters which must be esti-
mated grows linearly with the number of training documents.
The
parameters for a k-topic pLSI model
are k multinomial
distribu-
tions of size V and M [Note:
[4]] use the annotation M to denote
number of documents in the corpus.
We have used the symbol
#D
above]
mixtures over the k hidden topics.
This gives kV + kM
parameters and therefore linear growth in M .
The linear growth
in parameters suggests that the model is prone to over ﬁtting and,
empirically, over ﬁtting is indeed a serious problem.
To answer these problems a Latent Dirichlet Allocation (LDA) model has been
suggested.
In the next subsection we will take a closer look at the LDA.
54
Latent Dirichlet Allocation
Latent Dirichlet Allocation (LDA) is logically a direct descendant of
the
LSA, pLSA development line.
Like pLSA, LDA assumes that each document
consists of multiple topics.
It is a three level hierarchical Bayesian model (see
Subsection 3.2.2), in which each document is a mixture of ﬁnite set of topics.
Each topic is in turn modelled as a distribution of topic probabilities.
[4]
LDA can be thought as a bag-of-words generative model:
At ﬁrst we have an empty document for which we know the probability
distribution of the topics.
For each topic we know the probability distribution
of words.
We then ﬁrst select a topic c from the topic distribution p(c | d) and
then words w from the word distribution for that topic p(w | c).
The complete process to generate a document is [4]:
1.
Choose W ∼ P oisson(ξ)
2.
Choose θ ∼ Dir(α).
3.
For each of the words w ∈ W
a) Choose a topic c ∼ M ultinomial(θ).
b) Choose a word w
i
from p(w
i
|
c
t
, β),
a multinomial
probability
conditioned on the topic c
t
.
This is indeed close to the process with the pLSA.
There are few notable
diﬀerences though:
• The documents d are not modelled explicitly.
• Topic mixtures are not for each document,
but are treated a hidden
random variable with a Dirichlet distribution.
55
Therefore,
if
we have k topics,
the LDA method only uses k + kV parame-
ters and does not grow with the #D,
providing a solution to the over-ﬁtting
problem of pLSA.
In the case of
document presentation we of
course use an inference as a
reverse process to the document generation.
In order to ﬁnd the parameters
for the topic and word distributions, usually a Gibbs sampler is used [4].
The Gibbs sampler is a Markov Chain Monte Carlo (MCMC) method that
does a random walk sampling the target distribution in order to approximate
a multivariate probability distribution.
The details of a Gibbs sampler are out
of
the scope of
this thesis,
but for people interested in MCMC-methods we
recommend the book [44].
3.2.2 Classiﬁcation and Clustering
After the documents have been transferred into a selected document rep-
resentation and the document vectors have been calculated with one of
the
methods discussed in the Section 3.2.1, the next task is to ﬁnd inter-document
relations.
Typical
situations here are ﬁnding similar documents to a query (of
key
words),
ﬁnding similarity between documents,
classiﬁcation of documents to
a pre-deﬁned classes (for example categories),
or clustering the documents
together into non-deﬁned groups.
In this Section we will
discuss the classiﬁcation and clustering methods
relevant to our research.
Our research has two diﬀerent kind of classiﬁcation problems:
1.
Classiﬁcation of a document into a Hierarchical Topic Taxonomy in the
whole corpus.
56
2.
Classiﬁcation of a document into recommendable and non-recommendable
ones for each individual learner.
Therefore we concentrate on these two speciﬁc problems in our methodology.
As we can see, the ﬁrst problem is a more generic classiﬁcation problem with
multiple categories (classes).
Therefore, for the ﬁrst problem we will introduce
two typical methods:
1.
Hierarchical clustering
2.
K-Nearest Neighbours algorithm
The second classiﬁcation problem is basically a binary classiﬁcation problem
(i.e.
a classiﬁcation problem with only two classes).
For this we will
discuss
three classiﬁcation methods:
1.
Centroid-based Classiﬁcation
2.
Naive Bayesian Classiﬁcation
3.
Support Vector Machines
Before our discussion on clustering and classiﬁcation is possible,
we need to
introduce two more generic concepts:
Bayesian probability and Document
similarity.
Bayesian Probability
Bayesian statistical
approach is a result of
the research by an 18th cen-
tury Presbyterian minister, Reverend Thomas Bayes.
The diﬀerence between
Bayesian inference and other statistical
inferences is,
in essence,
that while
other statistical inferences are based on retrospective analysis of the data, the
conclusions in the Bayesian inference about the parameter θ,
or unobserved
57
data y are made in terms of
probability statements that are conditioned on
the collected data [16].
The usability of Bayesian methods has been under criticism throughout their
history.
Most of this critique comes from the view that the Bayesian methods
can be seen as “subjective”, because the it is the statisticians job to deﬁne the
prior probabilities before any data is available[16, 29].
Nonetheless, the Bayesian inference remains a useful and practical tool for
several statistical problems.
Peter M. Lee implies that the 5% or 0.1% signif-
icance tests used frequently by the advocates of classical
statistical
methods
– who criticize Bayesian methods for lack of
objectivity – are fact equally
subjective [29].
Generally speaking, in a random experience where we can have any results
y = {y
1
, y
2
,
3
, . . . , y
n
},
y
i
is called an element.
Classical
probability for y
i
is
P (y
i
) =
n
y
i
n
.
The problem with classical approach is that when there is only
little data the results are not reliable.
For example, if we consider a die, then
y = 1, 2, 3, 4, 5, 6.
After one throw the probability for the number rolled is 1,
while it is 0 for all
other numbers.
Yet we do not expect the die to be this
biased.
The way that the Bayesian statistics handle this problem is through the
use of
a prior distribution p(θ) describing the prior expectations about the
parameters.
The posterior distribution for the θ can then be calculated from
Bayes’ formula:
p(θ|y) =
p(y|θ)p(θ)
p(y)
,
p(y) =
∑
θ
p(y|θ)p(θ),
Or in the case of continuous θ:
p(y) =
´
p(y|θ)p(θ)dθ
In a shorter form,
the Bayes’
formula can be put in the form p(θ|y)
∝
p(y|θ)p(θ).
Here the p(y|θ) is called likelihood function.
In other words the
58
posterior is proportional to likelihood multiplied by the prior and considering
this the likelihood can be multiplied by any constant.
Bayes’ theorem can be applied sequentially as long as the set of observations
are independent:
p(θ|Y, X) ∝ p(θ)p(Y, X|θ)
p(Y, X|θ) = p(Y |θ)p(X|θ)
p(θ|Y, X) ∝ p(θ|X)p(Y |θ)
In practical terms, this means that once the posterior is calculated for data
y, the posterior can be used as a prior if further data is calculated
p(θ|y
2
) =
p(y
2
|θ)p(θ|y
1
)
p(y
2
)
While Bayesian methodology is a popular method for diﬀerent classiﬁcation
problems,
in general it does not scale well to high dimensional problems and
therefore has some problems when applied to document recommendation with
a large vocabulary.
Cosine Similarity
Cosine distance measure is used to calculate the distance between two vec-
tors.
It is used commonly in text mining to ﬁnd how similar the documents
d
a
and d
b
are.
While there are several
diﬀerent similarity measures,
cosine
similarity has empirically proven to be very good measure in text mining
[3, 5, 22, 36, 40].
For documents d
a
and d
b
represented in the document vector space as
�
d
a
and
�
d
b
respectively, the cosine similarity is deﬁned as:
59
Sim
cos
(d
a
, d
b
) = cos θ =
�
d
a
·
�
d
b
�
�
d
a
��
�
d
b
�
.
In other words, the cosine distance gives the cosine of the angle between the
two document vectors and yields a real value on the range (0, 1).
The smaller
the cosine is, the more similar the two documents are.
Hierarchical
Clustering
Hierarchical clustering is a clustering method that uses a recursively iterative
method of
connecting documents to a cluster (agglomerative) or dividing a
cluster of documents into two separate clusters (divisive) [34].
Typical basic hierarchical clustering algorithm is the agglomerative UPGMA-
algorithm.
It uses the cosine distance to agglomeratively combine the two
closest existing clusters (or data vectors in the ﬁrst step) to a new cluster in
each step, until the set number of clusters is achieved [41]:
1.
The mean distance between all clusters is calculated.
2.
The clusters for which the distance is the shortest are combined into a
new cluster and the calculation is continued from 1.
The mean distance between clusters A and B is calculated as follows:
1
�A� · �B�
∑
d
a
∈A
∑
d
b
∈B
Sim(d
a
, d
b
)
The hierarchical clustering is easy to implement and intuitive, but has not
proved to be the most accurate clustering method.
K-Nearest Neighbours
The K-Nearest Neighbours (KNN) classiﬁcation was introduced already in
the 1960s and is one of the very basic, but most useful classiﬁcation methods.
60
Figure 3.2.4: The UPGMA-algorithm:
The clusters are formed iteratively
by combining two closest (in terms of centroid cosine distance)
clusters into a one cluster until
desired number of
clusters has
been achieved.
61
The k-Nearest Neighbours classiﬁcation method algorithm is as follows:
1.
Calculate the distance between the target vector and all
vectors in all
classes
2.
Select k documents, for which the class is known, that are nearest to the
target document
3.
The class for the target document is the one that has most members in
the k documents.
Or in more formal terms:
y(d
a
) = arg max
k
∑
d
j
∈kNN
y(d
j
, c
k
),
where d
a
is the document to be classiﬁed, y(d
j
, c
k
) ∈ {0, 1} indicates if docu-
ment d
j
belongs to the class c
k
or not.
Centroid Based Classiﬁcation
Centroid based classiﬁcation is a very simple method of document classiﬁ-
cation using the cosine distance [19].
The basic idea is to calculate the centroid vector for each class and then
classify the target document into the class for which the distance is the short-
est.
The centroid vector for a class is:
C =
1
|S|
∑
�
d∈S
�
d,
where S is the collection of documents belonging to the class and |S| is the
number of documents in the class.
62
According to some studies, Centroid Based classiﬁcation outperforms both
Naive Bayesian and KNN-classiﬁcation in document
classiﬁcation in some
cases.
For further details, please see [19].
Naive Bayes
One popular method for classiﬁcation is the Naive Bayesian method.
Being
an intuitive and easy to implement and a relatively accurate classiﬁer it has
been both used and researched quite extensively in document classiﬁcation.
[27, 22]
Naive Bayesian classiﬁcation is based on the Bayesian statistical
methods,
discussed in the Subsection 3.2.2.
In the most simplest form, the multivariate Bernoulli Naive Bayes classiﬁer
uses the binary word document vector, as explained in the Section 3.2.1.
The
multivariate Naive Bayesian method assigns a probability for a document d
i
belonging to a class c
t
, based on the multivariate distribution the words in the
vocabulary W being part of that class:
p(c
t
| d
i
) =
p(d
i
|c
t
)p(c
t
)
p(d
t
)
∼ p(c
t
)
∏
j∈W
p(w
j
| c
t
), where
p(w
j
| c
t
) =
#d
t
j
#d
t
, where #d
t
j
is the number of documents in class c
t
in which
the word w
j
appears,
and #d
t
is the total
number of documents in class c
t
,
and p(c
t
) =
#d
t
#d
, where #d is the total number of documents in the corpus.
However, as the multivariate method does not take into account any weight
in the importance of the words to a document, a more sophisticated multino-
mial method is usually used.
The multinomial method uses the term frequency
vector discussed in the Section 3.2.1 adding information on each words impor-
tance to a document.
The word probabilities for the multinomial method can be formulated as:
p(w
j
| c
t
) =
#w
t
j
#W
t
, where #w
t
j
is the number of times that the word occurs in
63
all documents in the class c
t
and #W
t
is the number of words in all documents
belonging to the class c
t
.
Binary classiﬁcation for the NB model can be calculated in very convenient
form using log-likelihood ratio:
ln
p(c
1
| d
i
)
p(c
0
| d
i
)
= ln
p(c
1
)
p(c
0
)
+
∑
i∈V
ln
p(w
i
| c
1
)
p(w
i
| c
0
)
If
then ln
p(c
1
|d
i
)
p(c
0
|d
i
)
> 0 the document
d
i
belongs
to the class
c
1
,
and if
ln
p(c
1
|d
i
)
p(c
0
|d
i
)
< 0, the document d
i
belongs to the class c
0
.
The multinomial method has been empirically proven better than the multi-
variate method.
This is especially true the larger the vocabulary in the corpus
gets.
[36]
As the Naive Bayes method is easy to implement,
intuitive and at best
performs relatively good when compared to other methods,
it has become a
very popular method.
Support Vector Machines
Support Vector Machine (SVM) is a popular classiﬁcation algorithm.
In its
original form it is a linear binary classiﬁer and has proven to be very eﬃcient
in multidimensional data classiﬁcation.
[25]
The basic idea for the linear binary classiﬁcation is simple,
but the SVM
can also be extended to multi-class classiﬁcation and by using the so-called
kernel
methods,
beyond linear classiﬁcation.
In the course of
this thesis we
are however interested in the linear binary classiﬁer.
Intuition behind the SVM method is easy to understand.
The method ﬁnds
two hyper-planes in a vector space that separate the data so that there are no
data points between them.
These hyper-planes deﬁne an area between them,
64
w · x − b = 0
w · x − b = 1
w · x − b = −1
Figure 3.2.5: Support Vector Machine:
The SVM method ﬁnds the hyper-
plane w · x − b = 0 between the two classes y
i
= 1 and y
i
= −1
so that the hyper-plane is equally far from the closest point in
each class.
65
called the margin.
More formally, given data point set D = {(x
i
, y
i
)} | x
i
∈ R
P
, y
i
∈ {−1, 1}}
n
i=1
with n points,
with each point belonging to either class y
i
= 1 or y
i
= −1 ,
the hyper-planes can be described with a normal vector w as:
w · x − b = 1 and
w · x − b = −1.
The distance between these hyper-planes is then
2
�w�
(see Figure 3.2.5).
We
then want to maximize this distance,
or in other words minimize �w�,
while
we do not want any of the data points between the hyper-planes, so that:
w · x
i
− b ≥ 1 for y
i
= 1, and
w · x
i
− b ≤ −1 for y
i
= −1.
This in turn can be summarized as inequalities:
y
i
(w · x
i
− b) ≥ 1, for all 1 ≤ i ≤ n.
Therefore we need to solve the optimization problem:
min �w�, subject to the constraint
y
i
(w · x
i
− b) − 1 ≥ 0 ∀i.
When classifying, we can then use the hyper-plane w ·x−b = 0 and calculate
on which side of the hyper-plane the candidate point is to ﬁnd its class.
3.3 Hierarchical
Topic Taxonomy Recommender
In this section we describe our new methodology to recommend discussions
to learners in Informal Learning environment.
As seen in Section 1.6 the problem with applying the existing methods
directly to Informal
Learning lies in the fact that the data is fragmented,
multi-faceted and noisy.
In order to overcome this problem we include in our
recommender a Domain Model using a Hierarchical Topic Taxonomy, discussed
66
n
w · x − b = 0
w · x − b = 1
w · x − b = −1
Figure 3.2.6: Classiﬁcation with the SVM: The new observed document is
classiﬁed to the class on which side of the hyper-plane w·x−b = 0
it resides.
67
in more detail in the next Subsection.
This section is organized as follows:
First we are going to discuss the intuition behind our Hierarchical
Topic
Taxonomy Recommender.
Next we are going to introduce the system archi-
tecture, and the major components of our Recommender.
Then we will discuss
the intuition behind our method, give a formal presentation of our method and
the recommendation algorithm.
3.4 Recommender for e-Learning
Typically recommendation and group forming in e-Learning consists of three
main parts:
1.
The Document Model
2.
The Domain Model
3.
The Learner Model
4.
The Recommender Model
The Document Model
is used to represent the documents (in our case,
the
posts to the online discussion which we are recommending to learners).
As
seen in Sub-section 3.2.1, there are several ways of presenting documents in the
vector space.
By far the most popular has been the TF-IDF presentation, but
lately the Latent Dirilechlet Allocation has proven to be especially popular,
due to the good and compact representation method it provides.
The Domain Model models the domain to be taught.
It models the learning
objects (or documents in our case),
the subject of the domain,
and so forth
using suitable parameters depending on the target objects of modelling.
68
Figure 3.4.1: A simple recommender to recommend documents in a learn-
ing environment.
The documents are represented by the Docu-
ment Model
in a document vector space.
The recommendation
model then uses the learner preferences or other requirements to
select suitable recommendation candidates from the document
space.
The Learner Model
contains structured information on the learners.
Such
features
as
learner
competence,
learner
preferences,
learning history,
age,
grades, possibly models of the learners knowledge in the domain, etc.
The recommender model usually then uses the features of the learners and
the learning objects, deﬁned by the Learner and Domain models respectively
to recommend either new learning objects or to form groups for discussion, for
example.
A simple recommender to recommend discussions can be seen in the Figure
3.4.1
3.5 Hierarchical
Topic Taxonomy Based Recommender
In this Subsection we describe our novel recommender.
As seen in the Section 2.1,
Recommendation in an informal
learning envi-
ronment diﬀers from regular recommendation.
The main diﬀerences are:
69
Figure 3.5.1: Typical
Recommender to recommend discussions in a learning
environment.
1.
The document space is much more ambiguous and multi-faceted,
than
in formal learning.
2.
We do not have information about the learners readily available.
3.
We do not
have clear
curriculum structure,
but
learning is
learner-
centred,
which means that each learner selects his or her own path of
learning.
Because of
this the traditional
recommending methods usually do not per-
form very well.
Therefore we suggest augmenting the recommendation by
introducing a Hierarchical Topic Taxonomy-based Domain Model to the rec-
ommendation process (See Figure 3.5.1).
3.5.1 Intuition for the Hierarchical
Topic Taxonomy Recommender
In this Section we introduce the intuition behind our method.
As seen in Figure 3.5.3,
one of
the major problems recommending in In-
formal
Learning environment,
such as online discussions is the nature of the
data we are trying to recommend to learners.
As the data is created by the
70
Figure 3.5.2: Learner Participation in discussions in Informal learning
environment:
The black asterisks represent the discussions in
the environment and the red asterisks learner participation.
learning community itself, it is usually very noisy and multi-faceted, unlike the
well-deﬁned, well-constructed and limited data that is usually seen in Formal
Learning.
Furthermore, as we have no clear data on the learners themselves,
we have to infer their interest from their discussion activity.
In the Figure 3.5.2,
we can see the activity of
a learner in an informal
learning environment.
At ﬁrst sight the participation seems well deﬁned and
applying a regular recommender may seem like a good idea.
However, due to the multi-faceted nature of the data, the recommender mis-
takenly recommends documents from more general categories that the learner
is interested in (see Figure 3.5.3).
One contributing factor to this is general
noise in the target documents, as is usually seen in informal online discussions,
while another problem lies in the learner behaviour, to whom the recommen-
71
Figure 3.5.3: Failed Recommendation: When using a regular classiﬁcation,
the classiﬁer may recommend from more general
category,
of
which the learner is not at all interested in.
In this ﬁgure, the red
asterisks represent learner participation, and the blue line repre-
sents a recommender that has been trained on the participation
data.
The green asterisks represent mistakenly recommended
documents.
The diﬀerent coloured areas separated by the dash-
line can are unobserved subcategories and sub-subcategories of
the domain.
As can be seen, the activity of the learner has been
focused on very speciﬁc subcategories of the whole domain, caus-
ing the recommendation to fail.
72
Figure 3.5.4: Separating the domain to hierarchical
categories:
The
data is separated into a more detailed category structure, allow-
ing more focused recommendation within each of the categories.
dation is done.
Our learner is very “picky” about the discussions he or she
wants to participate.
In other words,
he or she is interested only in a very
limited subcategory of the whole domain.
In the next Subsection we propose
a solution to this problem.
3.5.2 The Hierarchical
Topic Taxonomy Recommender
In order to solve the problem stated in the previous Section,
our system
takes advantage of
a hierarchical
structure so that each potential
topic for
collaboration is classiﬁed into a speciﬁc category in the structure, and recom-
mendation of a topic is decided based on the category which it belongs to (see
Figure 3.5.4).
The idea is to eﬀectively separate the data to smaller and more
focused sub-sets,
in which the recommendation can be more easily inferred.
73
The idea of
a tree-structure is based on the concept of
Skills Frameworks,
which are expert-designed to help planning human resources development and
education.
As these structures are readily available for several ﬁelds, and are
designed with education in mind, we feel that they are the most suitable way
of describing the categories.
3.5.3 Deﬁnition of the Hierarchical
Topic Taxonomy Recommender
The System architecture of the Hierarchical Topic Taxonomy Recommender
can be seen in the Figure 3.5.1
We will use the following annotation to deﬁne our HTT Recommender:
X = {x
1
, x
2
, x
3
, . . . , x
n
}:
Discussion (document)
space.
Each discussion is
represented by a document (initial post to the discussion) feature vector.
C = {c
1
, c
2
, c
3
, . . . , c
k
}:
Topic categories for the discussions.
T = (C, E):
The Hierarchical Topic Taxonomy tree of whose nodes correspond
to the topic categories and the most generic category is the root node.
E
is the set of undirected edges e between categories so that e
a,b
∈ E =<
c
a
,
c
b
>.
First we will deﬁne the Domain Model for our Recommender.
74
3.5.4 Domain Model
The Domain Model is deﬁned as follows:
D = (X, C):
The domain model
is a pair of the discussion space X and the
categories C,
where each discussion is mapped to one of the categories
c ∈ C in the tree T .
We then use the labelled discussions mapped by the Domain Model as training
data to learn a function γ : X → C that classiﬁes any new discussions X
�
to
the categories in the HTT tree.
I.e.
γ(x
�
) = c.
3.5.5 Learner Model
We deﬁne the Learner Model as follows:
U = (u
r
, u
s
, u
p
, . . . ) :
Set of learners.
75
X
u
= (x
i
, x
j
, x
k
, . . . ):
A set of the discussions x
u
∈ X in which the learner u
has participated in.
Note that the discussions X
u
can be mapped to the
HTT tree using the the classiﬁer γ.
L
u
= (X
u
, u):
The learner model
is a pair of the learner info u ∈ U and the
participated discussions X
u
for that learner.
3.5.6 Recommendation Model
Recommendation Model
R
u
= (L
u
, D, Φ
u
):
The recommendation model is a triplet of the Learner Model
L
u
for the learner u ∈ U , the Domain Model D and a participation pre-
dictors Φ
u
.
Φ
u
= {φ
u,c
1
, φ
u,c
2
, φ
u,c
3
, . . . }:
A set
of
participation predictors
φ
u,c
for
the
learner u ∈ U , one for each of the categories c ∈ C.
φ
u,c
: (x
�
c
) → {0, 1}:
In practice a binary classiﬁer,
which classiﬁes any new
discussions x
�
,
belonging to a category c ∈ C to either 1 which means
that the document is recommended to the learner u, or 0, which means
that the document is not recommended.
76
Algorithm 3.1 The recommendation algorithm for our HTT Recommender.
Input New discussions X
�
Output Participation prediction results the learners u ∈ U
1.
For each new incoming discussion x
�
∈ X
�
:
2.
Classify the discussion x
�
with the γ so that the x
�
can be mapped to
exactly one category c ∈ C in the tree T :
γ(x
�
) = c.
3.
For each learner u ∈ U that has been active in the category c that the
discussion was classiﬁed to:
a) Use the learner’s u participation predictor φ
u,c
to decide whether
to recommend the discussion to learner u.
3.5.7 Recommendation Algorithm
The algorithm for deciding whether a discussion should be recommended to
a learner or not, is deﬁned in the Algorithm 3.1.
Basic idea of the algorithm is to ﬁrst classify any new incoming discussions
to the Hierarchical Topic Taxonomy, and then decide the recommendation in
each of the categories separately,
The algorithm can be seen as a ﬂow chart in the Figure 3.5.5.
We tested the recommendation with the following classiﬁcation methods for
our participation predictor:
• Centroid-based method
• Multinomial Naive Bayesian classiﬁer
• LDA-representation based SVM-model
The results for the empirical experiment can be seen in the Chapter 4.
77
Figure 3.5.5: Hierarchical Topic Taxonomy based recommendation al-
gorithm.
78
4 Experiments
In this Chapter we describe our empirical
experiments for the document
recommendation.
We used two separate sets of
data and also conducted a
questionnaire to evaluate the recommendation.
We are going to describe the
Hierarchical
Taxonomies used for each of the experiments.
We will
describe
the results for each of
the existing conventional
methods that we compared
to our own approach, and discuss the results with them.
Then ﬁnally we will
discuss the results using the Hierarchical Taxonomy with each of the methods.
The ﬁrst experiment is more extensive and more important in terms of eval-
uating the recommendation accuracy of our method.
The second experiment
is more limited and aimed to add extra evidence for the suitability of
our
recommendation method.
The questionnaire was used in the context of
the ﬁrst experiment.
The
questionnaire was performed to compare the conventional
method and our
Hierarchical Topic Taxonomy based recommender from four aspects:
How do the conventional
and the Hierarchical
Topic Taxonomy Methods
compare in terms of
1.
Recommending interesting discussions
2.
Recommending discussions in which the learner is more likely to partic-
ipate actively
79
3.
Recommending discussions that oﬀer new information to the learner
4.
Recommending discussions on which the learner is more knowledgeable
about
First we will
take a look at the ﬁrst experiment.
We will
describe the data
and the experiment with the Martial Arts discussion forum in detail.
We will
present the results of the experiments and discuss their implications.
As the second experiment is very similar to the ﬁrst one and indeed only
veriﬁes the results we got with the ﬁrst experiment,
we will
not discuss it in
equal length.
Finally we will discuss the results of the questionnaire and the
implications of the results.
4.1 Experiment 1:
Martial
Arts Discussion Forum
Our ﬁrst experiment was done with discussion data from a Martial Arts dis-
cussion forum.
While for general public the Martial Arts appear as rather cor-
poral activity, and sometimes even unsophisticated physical match of strength,
for the enthusiasts there are subtle diﬀerences between diﬀerent types of arts,
and besides the physical
side,
most practitioners ﬁnd discussion on training
methods, history, diet and even techniques or competition rules very interest-
ing.
4.1.1 Data Description
In this Section we describe the data source and any pre-processing for the
data.
80
Data Source and Raw Data
The data was collected from a discussion forum of Finnish martial arts com-
munity [1]
boasting as one of the largest martial
arts online communities in
Europe with over 4,000 registered members and over 11,000 individual
visi-
tors per week (including,
naturally,
a lot of non-registered members).
While
the community oﬀers articles (for example, on nutrition and training) and or-
ganized meetings twice a year,
the heart of the community is the discussion
forums, from which our data was retrieved.
The forum is an open forum and community designed to connect people
over diﬀerent martial arts background.
Participants in the discussions on the
forum range from hobbyists,
history aﬁcionados to professional
athletes,
and
to Finnish military, public police and private security company employers.
Discussions range from the most popular Olympic sports martial arts, such
as boxing, fencing and judo to most esoteric and exotic martial arts all over the
world.
Besides discussions on the martial
arts themselves,
discussions touch
on topics such as politics (for example,
gun and self-defence laws),
personal
security and self-defence, personal health and nutrition, physical ﬁtness, and –
as with any active community – discussions completely unrelated to the area
itself, the martial arts.
The forum is moderated by a several
moderators,
making sure that un-
suitable posts (such as spam and derogatory posts or personal
attacks) are
removed and that each discussion thread stays on topic.
If discussions in any
one topic drifts into a diﬀerent topic the moderators move the unrelated posts
to an existing (if one exists on the topic that the discussion has drifted to) or
to a new discussion thread.
The discussion forum uses an open source phpBB forum software,
which
uses a relational
database.
The data were extracted directly from the forum
81
database excluding all private information of the users.
Each user was recog-
nized by an id number (an unique integer for each user).
The document data consisted of the actual posts, a time stamp for each of
the posts and the participants to the discussion.
We considered each discussion
thread opener as an individual topic, and each member that had written a post
into the thread as a participant in the topic.
Selecting the Data and Pre-Processing
As the forum also has some non-martial arts related discussions, these were
ﬁltered out at ﬁrst.
Doing so we ended up with around 4,000 topics.
Next all
topics that had only one participant were removed.
Most of these were simple
advertisements of training seminars and selling adverts for used martial
arts
equipments and not proper discussions per se.
As the phpBB forum allows a tag-based formatting scheme for the poster,
these tags and all
hyper links and email
addresses were cleaned out.
Next
all
documents were tokenized to individual
words and all
stop words were
removed.
The stop words included words that do not contribute to the contents
of the topic, such as conjunctions, interjections and particles.
Finally all words
were stemmed using a open source stemmer Snowball [2].
Stemming of the data proved to be somewhat diﬃcult.
First of all Finnish
language is a highly inﬂected language and the data contains a lot of spoken
language words, slang and typing errors.
We ended up with 2,230 topics.
To assure that we had enough data for train-
ing the model
and evaluating it,
we then removed all
users who participated
with less than 30 topics and ended up with 67 users.
All
of the topics were
annotated to being part of one of the 45 classes in our martial arts Hierarchical
Topic Taxonomy (see the Section 3.3).
82
Hierarchical
Topic Taxonomy for Martial
Arts
As an oﬃcial Skills Framework is not available for our domain, the Martial
Arts,
we consulted several
experts in the area,
each with 20-30 years of
ex-
perience in diﬀerent martial arts, ranging from traditional East-Asian martial
arts to modern arts, such as boxing, for suitable categories.
We ended up with
a hierarchy of 45 categories (see Figure 3.5.5).
Creating the Domain Model:
Classiﬁcation to the Hierarchical
Topic
Taxonomy
We tried out several diﬀerent clustering and classiﬁcation techniques to see
how well
the documents could be classiﬁed or clustered to our hierarchical
topic taxonomy.
The clustering proved somewhat problematic.
Most clustering and classiﬁcation algorithms performed poorly when evalu-
ated with our annotated data.
As the data contains a lot of similar vocabulary
between the diﬀerent categories in the taxonomy, most algorithms seem to get
hung up on rather irrelevant terms, like in one case the classiﬁcation was done
based on the word “article” (as in newspaper article),
which is irrelevant for
our purposes.
In the end we got the best results with the KNN-clustering
algorithm,
described in the Section 3.2.
We got the best results with k = 9,
for which the classiﬁcation accuracy was 0.740 and used the KNN-algorithm
in our experiment, when creating the Domain Model.
4.1.2 Training and Evaluation
We selected the following methods for our empirical evaluation:
1.
Centroid Distance with threshold,
using TF-IDF and cosine distance
(CD)
83
Figure 4.1.1: Hierarchical Topic Taxonomy for Martial Arts
84
2.
Binary Naive Bayes (NB)
3.
Latent Dirichlet Allocation with binary SVM classiﬁcation (LDA)
The Centroid Distance Model
and the Naive Bayes Model
was programmed
by ourselves.
For LDA we used the existing library GenSim [38] and for the
SVM we used the LibSVM [7] library.
Each of
the above methods were used in their traditional
form,
i.e.
to
recommend the documents for each user from the whole corpus and then using
the Hierarchical Topic Taxonomy (denoted with “htt”, i.e.
httCD, httNB and
httLDA) and performing the recommendation in each topic node separately.
The evaluation was done using 80% of
the data to train the model,
and
the rest 20% of
the data to cross-evaluate the performance for each learner
in each category.
The cross-evaluation was iterated multiple times (n = 100)
to smoothen the eﬀects of
the random sampling of
the training data.
It is
worth noting here that the distribution of the negative topics in the categories
represents the distribution of
the activity of
the whole community,
and the
distribution of the positive topics represents the distribution of the individual
learner’s interest.
While we were using threading and a 16 core server,
sadly the LDA and
SVM libraries that we were using did not allow multi-threading and therefore
with the traditional version of the LDA algorithm the calculation would have
taken several
weeks for 100 iterations and therefore we needed to settle for
30.
However,
as the results were rather clear,
we do not believe that further
iterations would have brought out much diﬀerent end results on the evaluation,
except some what more accurate estimate to the actual value of the precision
for the LDA method.
Note that this only concerns the traditional
version
of
the LDA algorithm with 80% of
all
topics a person has participated in
85
used for training.
Splitting the training and the evaluation actually makes it
computationally easier, which is an additional plus for our method.
Besides the above methods, we calculated a base-line for scaling the evalua-
tion.
This was done by calculating the precision for recommending randomly
half of the topics for all learners.
Next we are going to describe the detailed procedure for training and rec-
ommendation for each of the models.
Note that the procedure is generally the
same for both conventional models and the HTT extended models.
The only
diﬀerence is that when using the term “category” below, for the conventional
model there is only one category that contains all of the documents, while with
the HTT extended model, each category contains the documents classiﬁed for
that particular category by the KNN classiﬁer.
Pre-Processing
The following steps were the same for all of the models.
1.
Pre-processing described in the Section 4.1.1
2.
Each document was represented as a list of words (note that any word
may appear several times in a document).
The Centroid Model
For each category c:
1.
TF-IDF matrix was calculated for the category c (i.e.
all
documents
were expressed as TF-IDF vectors) as described in the Section 3.2.1.
For each learner u:
a) A mean vector of the participated document vectors for the learner
u in the training data set
for the category c was calculated as
86
�
¯
x
c,u
=
1
|X
c,u
|
∑
�x∈X
c,u
�x,
where X
c,u
is the collection of
TF-IDF
vectors for the participated documents and |X
c,u
|
is the number
of participated documents.
Note that for the Centroid model non-
participated documents are not needed during training.
b) For each document x in the test set for the learner u in the category
c cosine distance was calculated from the mean vector obtained dur-
ing training as Sim
cos
(¯
x
c,u
, x) =
�
¯
x
c,u
·�x
�
�
¯
x
c,u
���x�
.
If the distance was less
than the threshold distance,
the document was considered recom-
mended and if the distance was more than the threshold distance
the document was considered not recommended.
The threshold dis-
tance was set empirically to 0.92 for the httCD model and 0.84 for
the traditional
CD model
(See Section 4.1.3 “Performance of
the
Centroid Distance Based Model” for selecting the threshold).
The Naive Bayes Model
For each category c:
For each user u:
1.
Maximum Likelihood Estimate was used to estimate the model param-
eters:
a) A prior probability for recommendation p(r
c,u
= 1) was calculated
as
#X
p
c,u
#X
c,u
, where #X
p
c,u
is the number of documents in the training
data for the category c, which the learner u had participated in and
#X
c,u
is the number of all
documents in the training data set for
the learner u in category c.
b) Next conditional
parameters p(w |
r
c,u
= 1) were calculated as
W
w
c,u
+1
∑
w
�
∈V
c
(W
w
�
c,u
+1)
, where W
w
c,u
is the number of times the word w ap-
87
pears in all
of
the participated documents for the user u in the
category c,
and the sum
∑
w
�
∈V
c
(W
w
�
c,u
+ 1) denotes the number of
all
words in the participated documents and V
c
is the vocabulary
for the category c.
Note that we also used Laplace smoothing by
adding 1 to the frequency of the words w and w
�
in order to handle
any words that appear in the vocabulary but do not appear in one
of the classes.
c) Prior probability and conditional parameters for not recommending
a document p(w |
r
c,u
= 0) were calculated in similar manner for
the training documents in which the user had not participated in.
2.
For each document x in the test set, the recommendation was decided by
calculating the log-likelihood ratio as follows:
ln
p(r
c,u
=1)
p(r
c,u
=0)
+
∑
i∈D
ln
p(w
i
|r
c,u
=1)
p(w
i
|r
c,u
=0)
,
where D is the collection of words in the document x.
If the log-likelihood
ratio was more than 0 then the document was considered recommended
and if the log-likelihood ratio was equal or less than 0 then the document
was considered not recommended.
The LDA-SVM Model
For each category c:
1.
An LDA model was trained using all the documents in the training data
set.
For training the LDA model,
we used GenSim that automatically
calculates the topic posterior distribution based on the corpus where each
document is presented as a list of words (as formatted above during the
pre-process step).
For each learner u:
88
a) The output of the GenSim (each document as topic distribution vec-
tor) was used to train SVM model using the LibSVM library with
the classes r
c,u
= 1 for the participated documents and r
c,u
= 0 for
the non-participated documents.
As the data was very unbalanced
(in most categories there is usually considerably less participated
than non-participated documents for each user),
we used weight
for the Cost parameter C for the SVM model, as described in [24]:
ω
C
c,u
=
#X
p
c,u
#X
n
c,u
,
where #X
p
c,u
is the number of
participated doc-
uments for the learner u in the training data,
and #X
n
c,u
is the
number of non-participated documents in the training data in the
category c.
b) For each document x in the test set for the learner u in the category
c the document was transferred into topic distribution vector using
the LDA model yielded by the GenSim module and then classiﬁed
to r
c,u
= 1 (recommended) or r
c,u
= 0 (not recommended) using
the model yielded by the LibSVM.
Evaluation Metrics
For each topic in the evaluation data set we predicted participation for the
learner.
From this we constructed a confusion matrix:
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
❤
Participated
Recommended
Positive
Negative
Positive
True Positive
False Negative
Negative
False Positive
True Negative
Table 4.1: Participation recommendation results:
Recommendation er-
rors according to actual participation.
We then calculated precision, recall and F-measure in the following manner:
89
Precision =
TruePositive
TruePositive+FalsePositive
Precision measures to what portion the learners actually participated in the
topics recommended by a method.
Recall =
TruePositive
TruePositive+FalseNegative
Recall
measures the percentage of
the topics that were recommended by a
method from all of the topics that the learners participated in.
F − measure = 2 ∗
Precision∗Recall
Precision+Recall
F-measure is the harmonic mean of precision and recall.
4.1.3 Results and Discussion
In this Section we show and discuss the evaluation results for our experiment.
Results for the Recommendation
The average results over all learners for the three models and the base-line
recommendation turned out as follows:
We can see the results of
our evaluation in the Table 4.2 or Figure 4.1.2.
The most interesting result of the experiment is that all
of the models work
much better with the HTT method than without it,
showing strong support
for our original hypothesis.
According to our evaluation,
there is not a huge diﬀerence between the
performance of
the diﬀerent methods with the HTT extension,
but they all
perform considerably better than the traditional methods.
The httNB method
performs best in terms of
precision,
but worst in terms of
recall,
while the
httCD model
performs best in terms of recall,
but in turn worst in terms of
precision.
The Latent Dirichlet Allocation model with the Hierarchical Topic
90
Precision
Recall
F-measure
HTT
BL
0.212
0.500
0.298
httLDA
0.377
0.477
0.421
httCD
0.357
0.555
0.435
httNB
0.399
0.341
0.368
BL
0.042
0.500
0.077
Trad.
LDA
0.109
0.486
0.178
CD
0.349
0.081
0.131
NB
0.129
0.032
0.051
Table 4.2: Evaluation results (Average, all learners):
Average over all learners for both traditional Centroid Distance
Based (CD), Latent Dirichlet Allocation (LDA) and Naive Bayes (NB) and using the the Hierarchical Topic Taxon-
omy (httCD, httLDA, httNB). The base-line (BL) average was established by recommending topics automatically
with a 50% probability to all learners.
91
��������
���
��
��
�����
���
���
���
���
���
���
���
���������
�����
�����
�����
�����
�����
�����
�����
�����
��������� ��� ��� ��������� ������
�����
���
Figure 4.1.2: Evaluation results (Average, all learners): Average over all learners for both traditional Centroid Distance
Based (CD),
Latent Dirichlet Allocation (LDA) and Naive Bayes (NB) and using the the Hierarchical
Topic
Taxonomy (httCD,
httLDA,
httNB).
The base-line (BL) average was established by recommending all
topics
automatically to all learners.
92
Taxonomy (httLDA) seems to give the best balance between increase in both
precision and recall,
while the Centroid Distance (CD) model’s increase in
precision is very small
when using the HTT extension,
but the increase in
recall is signiﬁcant.
Over all
learners the httNB method recommends topics with precision of
0.40 and recall of 0.34.
The next best model is the LDA model with the HTT.
It yields an precision of 0.38 and recall
of 0.48.
The httCD model
performs
better than the httNB model in terms of recall (0.56, which is the best recall
of all
models),
but looses to both httLDA and httNB models with precision
of 0.35.
For the traditional models (i.e.
without the HTT) the CD model performed
best, achieving a precision of 0.35.
The recall for the traditional LDA model
was by far the best, with a recall of 0.49.
The traditional
CD model
performed terribly on recall:
0.08.
This is due
to the fact that using a reasonably low threshold distance (see Section “Per-
formance of the Centroid Distance Based Model” below) the precision of the
CD model can be very high at the cost of the recall.
The traditional NB model performs the worst overall.
It achieved precision
of only 0.13 and recall of 0.03.
It soon came obvious that each of the model’s performance varied greatly
for diﬀerent learners.
In fact,
the traditional
NB model
did not perform as
bad as it at ﬁrst looks for all of the learners.
For some learners the NB model
worked indeed well.
Looking at the performance of the models for individual
learners,
we soon
noticed that the there are learners for whom each of the models perform well,
some learners to which some of the models perform well,
some poorly.
And
then there is a group of
learners whom none of
the models seem to yield
93
satisfactory results.
We will discuss these results in more detail in the Section
“Further Analysis for the Recommendation” below.
Recommendation Results with an “Ideal” Domain Model
As the recommendation is done in two stages, ﬁrst classifying each topic to
the Topic Hierarchy using the Domain Model,
we also wanted to check how
well
the method would work with ideal
classiﬁcation by the Domain Model.
Therefore we used human annotated Topic Taxonomy labels for the data set.
Originally these human annotated labels were used to train and evaluate the
Domain Model (See Section 4.1.1).
The results are as follows:
We can see the results of
our evaluation in the Table 4.3 or Figure 4.1.3.
The most interesting result of the experiment is that all
of the models work
much better with the HTT method than without it,
showing strong support
for our original hypothesis.
The eﬀect of using the accurate human labelled Hierarchical Topic Taxon-
omy categories for the discussions caused an increase in the HTT extended
models.
This implies that as the multi-category classiﬁcation methods (which
can be used to classify the discussions to the Hierarchical
Topic Taxonomy)
get better, our methods performance also increases.
Further Analysis for the Recommendation
Next we took the average for the ﬁve learners for which each model
per-
formed best.
The results can be seen in the Table 4.4 or Figure 4.1.3.
Selecting only the ﬁve learners for whom a model performed the best yields
much more even results.
The httNB model is still the best in terms of precision,
achieving a precision of 0.893, but both httLDA and the httNB model are not
94
Precision
Recall
F-measure
HTT
BL
0.212
0.500
0.298
httLDA
0.476
0.571
0.519
httCD
0.465
0.535
0.498
httNB
0.472
0.440
0.455
BL
0.042
0.500
0.077
Trad.
LDA
0.109
0.486
0.178
CD
0.349
0.081
0.129
NB
0.129
0.032
0.051
Table 4.3: Evaluation results (Average, all learners):
Average over all learners using human labelled category data for
both traditional
Centroid Distance Based (CD),
Latent Dirichlet Allocation (LDA) and Naive Bayes (NB) and
using the the Hierarchical Topic Taxonomy (httCD, httLDA, httNB). The base-line (BL) average was established
by recommending topics automatically with a 50% probability to all learners.
95
��������
���
��
��
�����
���
���
���
���
���
���
���
���
���������
�����
�����
�����
�����
�����
�����
�����
�����
��������� ��� ��� ��������� ������
�����
���
Figure 4.1.3: Evaluation results (Average, all learners): Average over all learners for both traditional Centroid Distance
Based (CD),
Latent Dirichlet Allocation (LDA) and Naive Bayes (NB) and using the the Hierarchical
Topic
Taxonomy (httCD,
httLDA,
httNB).
The base-line (BL) average was established by recommending all
topics
automatically to all learners.
For these results the human annotated Topic Taxonomy categories were used for
each discussion.
96
Precision
Recall
F-measure
Hierarchical Topic Taxonomy
httCD
0.888
0.704
0.781
httLDA
0.885
0.937
0.910
httNB
0.893
0.828
0.826
CD
0.785
0.068
0.120
Traditional
LDA
0.326
0.596
0.421
NB
0.488
0.078
0.133
Table 4.4: Evaluation results (Average,
best 5 learners):
Average over the 5 learners for whom the recommendation
worked the best, for both traditional Centroid Distance Based (CD), Latent Dirichlet Allocation (LDA) and Naive
Bayes (NB) and using the the Hierarchical Topic Taxonomy (httCD, httLDA, httNB).
97
far behind with precisions of 0.885 and 0.888 respectively.
The httLDA model’s recall is impressive with a score of 0.94, while httCD
and httNB models’
recalls are 0.70 and 0.83 respectively.
The traditional
versions’
performance is much lower for the best 5 learners than the models
with the HTT, showing again strong support for our hypothesis that using the
HTT model increases the recommendation accuracy.
The diﬀerence for the performance over all
learners and only over the ﬁve
best learners has two reasons:
1.
The diﬀerence in the performance exhibits about the models’ robustness
for noise in learner behaviour patterns.
2.
Some models require more training data before they start to perform
well.
It is good to notice here that using the HTT model eﬀectively splits the data
into several
chunks,
and therefore methods that are sensitive to a lack of
training data might have problems performing with the HTT model.
On the
other hand not using the HTT model means greater noise in the user behaviour
patterns, and methods that are more sensitive to noise also perform worse with
it.
In particular, the NB model works very well for some learners, while failing
completely with others.
The CD model
works much more evenly for all
of
the learners,
but does not perform exceptionally good for anyone.
The LDA
model seems to be most robust of the models.
Our data is very noisy.
The language that people use on Internet discus-
sion fora are full
of
slang,
ad-hoc terminology and colloquialisms.
The text
also has a relatively large amount of typing errors.
Some of the foreign mar-
tial
arts terminology can be also transliterated in several
ways.
For example
98
������
�����
�����
���
��
��
��������
�����
���
���
���
���
���
���
��������� � ������
�����
�����
�����
�����
�����
�����
�����
�����
�����
�����
�����
�����
�����
�����
��������� ��� ������ ��� ��� ��������� ������
��� ��� � ���� ������� ���������������
���������
������
Figure 4.1.4: Evaluation results (Average, 5 best learners):
Average over the 5 learners for whom the recommendation
worked best.
Results for both traditional
Centroid Distance Based (CD),
Latent Dirichlet Allocation (LDA)
and Naive Bayes (NB) and using the the Hierarchical
Topic Taxonomy (httCD,
httLDA,
httNB) are shown.
The base-line (BL) average was established by recommending all topics automatically to all learners.
99
the Japanese term for unarmed combat can be seen in the forms “jiu-jitsu”,
“jujutsu”, “jyuujyutu.
Some of these problems could be addressed to with cleaning the data with
spelling error corrector,
synonym dictionaries and stemming,
but this would
mean a very labour intensive pre-processing and – more importantly – would
not give us a realistic result for ill-constructed,
informal
learning,
where the
data may well be noisy.
Next we are going to discuss each of the models in more detail separately.
Performance of the Centroid Distance Based Model
The CD model is very simple.
It recommends all topics that are within an
empirically set threshold distance (cosine distance) from the centroid vector
(see Subsection “Centroid Based Classiﬁcation” in Section 3.2) for the training
documents in which the learner has participated in the category.
The threshold
was set to 0.92 for the httCD model and to 0.84 for the traditional CD model.
While the model does not achieve an impressive precision, it performs very
evenly for most of the learners.
The model is simple and therefore seems very
robust to noise.
On the downside the model
does require quite a lot of training data,
and
the relative performance drops quickly with the lack of training data.
The threshold distance for the model
was set empirically over all
learners.
The performance would have been better if the threshold would have been set
individually for each learner.
The average precision and recall over all learners
for diﬀerent threshold distances can be seen in the Figure 4.1.6 and for the
traditional CD model in the Figure 4.1.7.
100
Figure 4.1.5: The ratio of participation: The solid line shows the average ratio of participation to topics against the cosine
distance (x-axis) from the centroid, while the dotted line shows the percentage of all documents (average over
all learners, over all categories in the HTT). As we can see, around the distance 0.75, the ratio of participation
within the distance suddenly drops.
We can also observe that most of the topics are quite far away from the
centroid vector of the participated topics of the learner, giving an insight why the CD model works relatively
well.
101
����
����
����
����
����
����
����
����
����
����
����
����
������ �������� ���������
���
���
���
���
���
���
���
���
���
��������� � ������
��� �������� ����� ��������� ��� ������ ��������� �� ���������
���������
������
Figure 4.1.6: httCD model’s threshold performance:
Performance of the httCD model for diﬀerent distance thresholds
for recommendation.
102
����
����
����
����
����
����
����
����
����
����
����
����
������ �������� ���������
���
���
���
���
���
���
���
���
���
��������� � ������
������� �������� ����� ��������� ��� ������ ��������� �� ���������
���������
������
Figure 4.1.7: Traditional
CD model’s threshold performance:
Performance of
the CD model
without the HTT for
diﬀerent distance thresholds for recommendation.
103
Performance of the Naive Bayes Model
The traditional Naive Bayes model performed well in the experiment.
While
there has been critique for the NB method in literature,
especially on its
performance with high dimensionality of
the data,
as mentioned in Section
3.2.2, we did found the NB model performing as good as other models.
Looking closer at the performance of
the NB model
it seems to be very
sensitive to diﬀerent learner types.
For some learners, who seem very “logical”
in their participation,
the httNB model
seems to work well.
Then again for
some learners the NB model
performs very poorly.
These learners seem to
have some tendency for “sporadic” participation into topics in which they are
not generally so active.
This kind of activity is natural
in informal
learning and online discussions
and a model should be able to answer the challenge in order to be useful.
We
can see two kinds of participation patterns for this kind of sporadic participa-
tion:
1.
People who are very active and are more interested in discussion or
debate for debate’s sake rather than because they are actually interested
in the topic
2.
Social
reasons.
Sometimes people will
join a discussion,
not because
the discussion is interesting in itself for them, but because other people
participating in the discussion are their friends or opponents.
In both cases it is very diﬃcult to distinguish the reason why people are
participating into the discussion, making recommendation extremely diﬃcult.
Using the Hierarchical Topic Taxonomy helps with this problem rather well
as it makes it easier to ﬁnd easier the actual fragmented topic areas in which
the learners are interested in.
104
Performance of the LDA Model
The LDA model
has been found to be a very good method in general
to
represent the latent topic information in documents and SVM models have
proven to perform well
in binary classiﬁcation.
In our domain,
however,
the
traditional LDA-SVM model did not perform up to par.
While the LDA model
seems to catch the latent features of
the documents well,
the fragmentation
of
interest to very speciﬁc areas by the learners (see Figure 3.5.3 in Section
3.5.1) is not handled too well by the SVM model.
By applying the Hierarchical
Topic Taxonomy to ﬁrst split the topics into
separate independent,
well-focused categories brings out the true strength of
the LDA-SVM method, yielding a considerable increase in performance.
The
LDA-SVM model performs especially well, when the classiﬁcation to the cat-
egories is done correctly, as can be seen with the results for the human anno-
tated data in Table 4.3 in Section 4.1.3.
4.2 Experiment 2:
Programming Discussion Forum
The aim of the second experiment was to give further evidence on the suit-
ability of our method.
The data for the second experiment was obtained by
crawling a popular online programming discussion forum.
We selected the
LDA model for this experiment, because it seemed to give most balanced re-
sults in the ﬁrst experiment in terms of both precision and recall.
The data
itself
is not as extensive as for the ﬁrst experiment and only covers a much
smaller domain with a much simpler Hierarchical Topic Taxonomy.
Even with
these limitations, the experiment yields similar results to the ﬁrst experiment,
providing further evidence of the suitability of our method.
First we will
describe the data used for the second experiment.
Then we
105
will describe the results for the conventional LDA method and compare them
to the Hierarchical Topic Taxonomy extended LDA method.
4.2.1 Data Collection
The data were collected from an online discussion forum using a web crawler.
The discussion forum is a relatively active forum for programmers.
As we do
not have direct access to the database of the forum,
we do not have as clear
a view of
the demographics of
the people participating into the discussions
as for the ﬁrst experiment,
but looking at the discussions,
the participants
seem to range from experienced professional
system engineers to “hobbyist”
and beginner programmers.
Most of
the topics are questions from relative
beginners asking about particular errors, algorithms and design-related issues
in their own programming projects.
The scope of
the discussions is quite narrow and most of
the discussions
are directly related to programming in diﬀerent languages, for example C++,
Java, Visual Basic, etc.
There are also a few more generic categories, such as
project management and algorithms and data and some sub-categories such
as graphics programming with the C-language and .net framework related
C-sharp programming.
The biggest problem was that unlike the Martial Arts forum, the Program-
ming forum was not actively moderated.
There seems to be some amount
of
advertisement (“spam”) threads,
usually discussed so that automatic ﬁl-
ters would not ﬁnd them,
and also some threads that drifted oﬀ from the
original
discussion topic.
While we did our best to ﬁlter these discussions,
their existence in the data is clearly reﬂected in the overall
accuracy of
the
recommendation,
especially because,
as mentioned,
the advertisements were
disguised so that ﬁltering them automatically proved diﬃcult.
Also,
unlike
106
with the Martial Arts discussion forum that has a lot of regular users that ac-
tively participate into the discussions, the Programming forum seems to have
a lot of participants that join the forum only to ask a speciﬁc programming
related question.
Whether they receive a satisfactory answer to that question
or not,
they do not participate in any other discussions at all.
Therefore the
suitable amount of users with whom the recommendation could be tested was
much smaller and we needed to use much less training data for the partici-
pants, aﬀecting the overall performance of the recommendation.
4.2.2 The Hierarchical
Topic Taxonomy
The Hierarchical Topic Taxonomy used in this experiment was based on the
Information Technology Skill Standard by the Japanese Ministry of Economy,
Trade and Industry discussed in Section 2.4.
The taxonomy can be seen in
Figure 4.2.1.
4.2.3 Process for the Experiment
We ﬁltered out all
learners with less than 40 participations and ended up
with 2040 discussion threads and with 13 active forum users.
The data was preprocessed similarly to the data in the ﬁrst experiment:
stop
words were ﬁltered and the remaining words were run through a stemmer.
Next each thread was annotated to the Hierarchical Topic Taxonomy.
The
application of
the LDA model
and the httLDA model
were the same as for
the ﬁrst experiment (see Section 4.1.2).
The model was then trained with 80% of the data for each of the learners and
the remaining 20% was used to evaluate the accuracy of the recommendation.
Similar metrics to the ﬁrst experiment were used.
107
Figure 4.2.1: The HTT for Experiment 2:
The Hierarchical
Topic Taxonomy for the second experiment was based on
the “Information Technology Skill Standard” by the Japanese Ministry of Economy, Trade and Industry.
108
4.2.4 Results
Precision
Recall
F-measure
Hier, Topic Taxonomy
httLDA
0.125
0.132
0.128
Traditional
LDA
0.059
0.337
0.101
Table 4.5: Evaluation results:
For both traditional
Latent Dirichlet Allo-
cation (LDA) and using the the Hierarchical Topic Taxonomy (ht-
tLDA) using the Programming discussions.
The evaluation results can be seen in the Table 4.5.
As mentioned above,
due to the less extensive nature of the data and the
fact that the discussions also include non-related advertisements,
the overall
accuracy of the recommendation is not very high.
Due to low overall user ac-
tivity, we could use only 5 positive topic examples for training the HTT-model
per category (while for the traditional model all users had 40 training exam-
ples), which also in part explains the diﬀerence in recall.
However, the results
indicate that despite of the lower amount positive training topic examples, ex-
tending the recommendation with the Hierarchical Topic Taxonomy increases
recommendation accuracy,
supporting our hypothesis that by applying the
Hierarchical Topic Taxonomy we can achieve more precise recommendation.
4.3 User Questionnaire
In addition to the above experiments analysing the method in terms of how
well
it correlates with the actual
participation activity of
the learners,
we
wanted to evaluate the recommendation methods with a questionnaire to the
learners.
By far the most diﬃcult aspect of evaluating systems designed for informal
learning is the evaluation of the educational value of the system.
As we do not
have a clear curriculum, facts or items that we expect the learners to learn we
109
Figure 4.3.1: Screen Capture of the Questionnaire:
A questionnaire was used to compare the traditional LDA method
(“Thread A” recommendations) to the httLDA method (“Thread B”).
110
cannot construct a test to evaluate the learning process.
As explained in the Section 2.2,
constructivist learning theories state that
learning happens through activities and social interaction.
The learners con-
struct their knowledge by being exposed to diﬀerent kind of information and
reﬂecting their understanding via communication.
For informal learning this
means that we cannot really grasp the learner’s learning process, as it not only
involves the online discussion, but all oﬀ-line activities (e.g.
reading books or
participating to a martial arts class in case of our experiment data) as well.
In order to evaluate at least some educational aspects of our recommenda-
tion method,
we conducted a questionnaire for the users of the Martial
Arts
discussion forum.
The questionnaire and its results will be discussed below.
4.3.1 Setting Up the User Questionnaire
The questionnaire was conducted online and the respondents were the users
of the Martial Arts forum from which the data for the ﬁrst experiment was col-
lected (see Section 4.1).
We selected 18 currently active users and for each of
them we recommended 10 pairs of discussions based on their personal activity
on the forum.
One discussion in the pair was recommended by the conven-
tional LDA method and the other discussion was recommended using the LDA
method extended with the Hierarchical Topic Taxonomy.
Then for each of the
ten pairs, we asked the users to compare the quality of recommendation with
four questions.
The recommendation was done in two separate occasions with
diﬀerent topic pairs.
Of
the 18 users 11 participated in the questionnaire
on both rounds (for the second round two users were diﬀerent from the ﬁrst
round), so we ended up with 220 recommendation pairs altogether.
111
4.3.2 The Questions
The four questions were meant to compare the conventional
and the Hi-
erarchical
Topic Taxonomy methods from four aspects.
The ﬁrst two ques-
tions were to further evaluate the accuracy of the recommendation in terms
of the user participation in discussions and the the other two questions were
to evaluate the suitability of
the methods for informal
learning via simple
self-assessment.
Below,
the term “System A” refers to the traditional
LDA
recommendation method, and the term “Thread A” refers to a recommended
discussion thread recommended by this method.
“System B” and “Thread
B” refer to our novel recommendation method that extends the LDA method
with the Hierarchical Topic Taxonomy.
Before the questionnaire the users were provided with the following instruc-
tions:
The aim of this questionnaire is to get simple user feedback on
two separate recommendation systems (i.e.
systems that recom-
mend online discussions to users based on their participation ac-
tivity).
No personal information of any kind will be included when
publishing the results.
For each question you will
be shown ten topic pairs (each row
of the table represents a pair) from Potku.net.
The topics link to
discussion threads.
Please have a look at the discussion threads
and select the option that most closely represents your opinion
comparing the two discussion threads in each of the pairs.
The questionnaire should not take more than few minutes.
When
you are ready to start, click the link below.
All of the questions also included the following pre-amble:
112
Below,
you are shown ten pairs of
topics from System A and
System B with links to discussion threads.
Take a look at the discussions for each pair and select the best
option for each pair.
Note that there is a small chance that both
A and B threads point to the same discussion.
In that case select
the middle option (3).
For each discussion thread pair the users then selected an answer from a scale
of 1 to 5 with the given options for each question.
Question 1:
Which of the threads (A or B) is more interesting to you consid-
ering your personal interest in martial arts?
Options:
1:
Thread A is much more interesting
2:
Thread A is somewhat more interesting
3:
Both threads are equally interesting
4:
Thread B is somewhat more interesting
5:
Thread B is much more interesting
Results can be seen in Figure 4.3.2.
Question 2:
Which discussion would you most likely participate in?
Options:
113
1:
You are much more likely to participate in thread A
2:
You are somewhat more likely to participate in thread A
3:
You are equally likely to participate in both threads,
or you
would not participate in either
4:
You are somewhat more likely to participate in thread B some-
what more likely
5:
You are much more likely to participate in thread B
Results can be seen in Figure 4.3.3.
Question 3:
Which of the threads do you feel oﬀers you more new information
in the ﬁeld of martial arts?
Options:
1:
Thread A oﬀers much more new information
2:
Thread A oﬀers somewhat more new information
3:
Both threads oﬀer same amount of new information
4:
Thread B oﬀers somewhat more new information
5:
Thread B oﬀers much more new information
Results can be seen in Figure 4.3.4.
Question 4:
Considering your
own knowledge of
martial
arts,
which of
the
threads would you feel
more conﬁdent in contributing accurate
and worthwhile information to?
114
Options:
1:
You could contribute to thread A with much more conﬁdence
2:
You could contribute to thread A with somewhat more conﬁ-
dence
3:
You could contribute to both threads with equal conﬁdence
4:
You could contribute to thread B with somewhat more conﬁ-
dence
5:
You could contribute to thread B with much more conﬁdence
Results can be seen in Figure 4.3.5.
115
4.3.3 Questionnaire Results
Question 1:
��
��
��
��
��
������ �������
�
��
��
��
��
���
���
������ �� �������
�
��
��
��
���
������ ������������ ��� �������� �
Figure 4.3.2: Answer distribution for Question 1:
Comparing the tradi-
tional LDA recommendation method and the Hierarchical Topic
Taxonomy extended LDA method (httLDA) in terms of
which
method yields more interesting discussion recommendations for a
user.
The users considered the threads recommended by the sug-
gested method to be more interesting than the recommendations
by the traditional LDA method.
116
Question 2:
��
��
��
��
��
������ �������
�
��
��
��
��
���
���
������ �� �������
�
��
��
��
���
������ ������������ ��� �������� �
Figure 4.3.3: Answer distribution for Question 2:
Comparing the tradi-
tional LDA recommendation method and the Hierarchical Topic
Taxonomy extended LDA method (httLDA) in terms of
which
method yields discussion recommendations in which a user is
more likely to participate in.The users considered participating
in the threads recommended by the suggested method more likely
than the ones recommended by the traditional LDA method.
117
Question 3:
��
��
��
��
��
������ �������
�
��
��
��
��
��
��
��
������ �� �������
��
��
��
��
��
������ ������������ ��� �������� �
Figure 4.3.4: Answer distribution for Question 3:
Comparing the tradi-
tional LDA recommendation method and the Hierarchical Topic
Taxonomy extended LDA method (httLDA) in terms of
which
method yields discussion recommendations that oﬀer more new
information to a user.The users considered that the threads rec-
ommended by the traditional method oﬀer them more new infor-
mation than the ones recommended by the suggested method.
118
Question 4:
��
��
��
��
��
������ �������
�
��
��
��
��
���
���
���
������ �� �������
�
��
��
��
���
������ ������������ ��� �������� �
Figure 4.3.5: Answer distribution for Question 4:
Comparing the tradi-
tional LDA recommendation method and the Hierarchical Topic
Taxonomy extended LDA method (httLDA) in terms of
which
method yields more interesting discussion recommendations for a
user.The users considered that they could contribute much more
“accurate and worthwhile” information to the threads recom-
mended by the suggested method than to the threads recom-
mended by the traditional LDA method.
119
Discussion on the Questionnaire Results
The results for the questions 1 and 2 reaﬃrm the results for the Experiment
1 and 2:
The Hierarchical
Taxonomy Topic recommendation yields more in-
teresting recommendations and recommendations of discussions in which the
learners are more likely to participate actively.
In terms of oﬀering new information for the learners, the traditional method
seems to work better.
This is not a surprise,
as “false” recommendations
(i.e.
the ones which the user is not interested in) probably contain more new
information to the user.
However,
one needs to be careful
with this kind of
recommendation,
because overtly recommending uninteresting discussions to
the learner may cause frustration and decrease motivation.
The fourth question was to evaluate the users’ assessment on their own level
of
knowledge and the conﬁdence that they could contribute worthwhile and
accurate information on the discussions recommended by the system.
The original aim for our recommendation system was indeed to provide ac-
tive, motivated and knowledgeable people to discuss in topics raised by other
learners.
From this point of
view the questionnaire seems to indicate that
our initial
hypothesis that we can provide more active and knowledgeable
people using our recommendation method in contrast to the traditional
rec-
ommendation methods is indeed correct.
About 79% of
the discussions (10
recommendations for each 22 of the participants, 220 recommendations in to-
tal) recommended by the Hierarchical Topic Taxonomy extended system were
considered to be discussions in which the learners could contribute their own
expertise with more conﬁdence than to the discussions recommended by the
traditional method.
120
4.4 Discussion on the Experiment Results
The main purpose of
this research was to ﬁnd good recommendation for
informal
learning in order to support communication between learners.
As
we discussed in the Section 2.1,
informal
learning is centred around commu-
nication and is closely related to the socio-constructivist learning theories.
According to these theories,
learning is an active process with learners con-
structing their knowledge through diﬀerent sources and social activities.
Online discussion media are one suitable channel
for communication.
As
we noticed in the Section 2.1.4,
there has been a clear paradigm shift from
traditional,
Learning Management System-based learning towards so-called
“e-Learning 2.0”.
This new form of
e-Learning is based on learner-centred
learning,
where learners deﬁne their own goals and act mostly according to
their own motivations.
Online discussion media,
such as Twitter,
discussion
boards and social networking services act as communication tools in this new
kind of learning.
Traditional e-Learning is in its concepts very close to traditional formal edu-
cation and usually provides communication tools that are speciﬁcally tailored
for structured and well-deﬁned courses where information about the learners is
readily available.
Therefore, the systems designed for formal learning are not
directly applicable to the ambiguous and open-ended informal learning where
each learner sets his or her own goals and activation is based on personal
motivations.
In order to support communication in informal online learning, we used Hier-
archical Topic Taxonomy to enhance the traditional recommendation methods
and to form active and knowledgeable groups of people to join a discussion ini-
tiated by another learner.
As seen in the previous Section, our method yields
121
better results in all
experiments than the traditional
methods.
The most
signiﬁcant point is that our method works with diﬀerent traditional
meth-
ods, increasing the recommendation performance of all of the tested methods.
This indicates that the Hierarchical
Topic Taxonomy is independent of
the
underlying recommendation method.
Our method can be said to ﬁnd the latent interest and expertise of
the
learners in open-ended discussions just based on the activity of the learners,
as the questionnaire results indicate.
As discussed in the Section 2.1,
due
to the ambiguous nature of
informal
learning,
it is diﬃcult to grasp when
exactly learning is happening and what is the target for the learning.
Indeed,
sometimes the learners themselves are not fully aware of this.
Furthermore,
it is diﬃcult to assess the level of expertise of the learners just based on their
activity.
Our method eﬀectively “ﬁlters” out random participation in discussions, by
concentrating the recommendation to small,
well-deﬁned sub-categories of
a
domain,
in which the learner is especially interested in and active.
Active
participation and contribution in the form of expertise in the sub-category is
analogous to Vygotsky’s concept of “Zone of Proximal Development” discussed
in the Section 2.2.1.
122
5 Conclusion
In this paper we proposed a method for collaborative topic recommendation
/ collaborative group forming in an informal,
ill-constructed online learning
environment.
Until
now,
most research on collaborative group forming and
topic recommendation has been either designed for formal learning, or at least
for environments where a lot of data about the learners are readily available.
We ﬁrst discussed the necessity of informal
learning,
and the scope of infor-
mal learning and its nature and implications of these to e-Learning.
Next, we
introduced commonly used methodology, including group forming, recommen-
dation systems and text mining.
Then we introduced the Hierarchical
Topic
Taxonomy extension for the recommendation model.
Finally,
we evaluated
the traditional
recommendation methods and our HTT extension using two
separate data sets and a questionnaire.
In general the recommendation is a diﬃcult task using noisy, multi-faceted
data from an open discussion group.
Users use a lot of
informal
language,
ad-hoc terminology and colloquialisms.
Therefore,
methods that work well
with well-deﬁned and formal text corpora (like news) seem to perform poorly
in recommendation.
In order to support informal
and ill-constructed learning we need to ﬁnd
methods that are robust to noise,
and at the same time can distinguish very
tight local
clusters / classes from learners interest.
These local
classes may
123
share a lot of
vocabulary with their parent and sister classes.
We require
better methods to distinguish what kind of narrow topics actually trigger the
learners activity.
To extend the existing methodologies, we deﬁned a Hierarchical Topic Tax-
onomy (HTT). This taxonomy serves two separate purposes:
1.
By applying a Hierarchical Topic Taxonomy we can achieve more precise
recommendation in a noisy, multifaceted and open ended discussions.
2.
The Hierarchical
Topic Taxonomy is similar to Skills Frameworks and
Skill Standards (explained in the Section 2.4) and gives our research an
educational context suitable for informal learning.
We tested three well-known methods, at ﬁrst without the the HTT, perform-
ing recommendation on the whole corpus,
and then by ﬁrst classifying each
individual topic to a category in the HTT and doing the recommendation in
each category separately.
All of the methods performed better with the HTT
extension.
We also conducted a questionnaire for the users,
using both traditional
recommendation method and the HTT extended method.
The results show
that our method is suitable to oﬀer interesting discussion recommendations,
in which the learners are knowledgeable and active.
5.1 Result Signiﬁcance
In this research we introduced a Hierarchical
Topic Taxonomy to extend
existing recommendation methods to support informal
online learning.
By
using our HTT method we can achieve better recommendation in a noisy,
multifaceted domain, where the learners’ interest may be fragmented into sep-
arate independent, narrowly focused sub-categories.
124
As a secondary result in our research we also recognized some of the patterns
in the user activity in open-ended online discussions.
Most users seem to have
very focused interest in a narrow area of a larger domain.
The fragmentation
of the interest may be very diﬃcult to notice except for people who are highly
knowledgeable of the ﬁeld in question.
Sometimes the learners themselves are
not necessarily conscious of this fact.
We also noticed that not all
participation in the online discussion is nec-
essarily motivated by the interest in the subject.
Other factors,
like social
behaviour may well
contribute to the participation activity.
These aspects
pose a big challenge to the recommendation methods, which have mostly been
tested with very formal data, and – dare we say – in very a loose deﬁnition of
the domain (e.g.
only diﬀerentiating between domains like “computers” and
“sports”) without going into more detail within the domain.
In the Chapter 2.1 we established the importance of
informal
learning in
the modern society.
Most of the research in e-Learning is focused on formal
learning,
promising better education and cost-eﬀectiveness by replacing part
of the traditional education with e-Learning.
We believe that rather than replacing traditional learning, e-Learning should
support the learners in their “quest for knowledge”.
Using e-Learning,
we
can oﬀer large amounts of materials,
information and discussion partners to
motivated learners using online resources.
The question then is how can we
ﬁnd relevant information and suitable people, or in other words, how can we
achieve good recommendation.
The importance of informal
learning has been recognized in the literature
(see Section 2.1).
Recently the importance of informal learning happening on-
line has also been recognized by the e-Learning community (see Section 2.1.4).
However, informal online learning oﬀers huge challenges to the e-Learning com-
125
munity.
Formal
learning with its limited goals and well-deﬁned materials is
much easier to handle than open-ended informal
learning where each learner
has his or her own agenda, motivations and goals.
This requires development of more ﬂexible and learner oriented methodolo-
gies from the e-Learning researchers.
As meta-data on the documents and
learning objects is not as readily available,
nor do we have explicit informa-
tion on the learners,
the systems for informal
online learning must infer this
information directly from learner behaviour patterns and the materials they
produce.
As there is no simple “truth” in informal
learning which should be taught
to the learners,
the systems for informal
learning should not hold any pre-
conceptions on what is “correct” information.
The whole idea behind socio-
constructive learning theories is that the learners construct their own facts
through communication, with each each individual learner contributing their
own knowledge to the bigger whole.
Because of
this ambiguous nature,
in-
formal
learning is also very diﬃcult to assess.
Grasping what the learners
actually learn is very diﬃcult.
On the other hand,
it can be argued that assessing informal
learning is
not that important.
As the need for informal
learning is usually born out of
practical necessity or interest, the actual applying of the skills and knowledge
acquired through the learning acts as indicator of how successful the learning
has been.
After all, constructivist learning theory is all about learning through
the experience and social interaction.
However, from a research point of view, assessing the learners is important.
We will discuss this in Section 5.2 below.
Informal learning happens not only after the institutionalized learning ends,
but also during it.
As the learners use the information that they have acquired
126
during the formal learning to discuss with their fellow students and acquain-
tances and possibly browse the Internet for further information, or watch TV,
read a book,
they combine new bits of
information with the knowledge ac-
quired during the formal,
traditional
learning.
We believe that this informal
phase, parallel to the formal learning, is where e-Learning’s forte is.
If
at this point we can recognize and aid the learners in their search for
knowledge, we believe that this will be very beneﬁcial.
Recommendation sys-
tems,
like the one suggested in this thesis are important supportive tools in
this kind of learning.
We believe that taking advantage of the Skills Standards or Skill Frameworks
discussed in Section 2.4 can be beneﬁcial in oﬀering a link between formal and
informal learning.
Research on the matter is needed, but we believe that the
Hierarchical
Topic Taxonomy can act as a form of support for scaﬀolding to
improve informal learning happening parallel to formal learning.
Learning in itself is a complex process involving several aspects, from cogni-
tive to social.
We believe that computational methods are best used to handle
certain manageable parts of this process.
We see e-Learning as oﬀering addi-
tional
– possibly modular – support to the learning process.
By modular we
mean a collection of separate methods and systems that can be applied in a
speciﬁc task on a needs basis to support and to add to the overall
learning
process.
5.2 Future Research
In Chapter 4 we showed evidence that our method is successful
in recom-
mending topics in open-ended online discussions in order to support informal
online learning.
However, the research still leaves several questions open.
127
For example, it is possible that that learners’ interest changes dynamically
as they learn.
As an example,
we can consider a person who is not very
familiar with the target domain joining the discussions and in the beginning
being more active in the more general categories.
However, through learning
his or her interest may change towards more detailed categories, as he or she
learns more about the domain.
The dynamic changes in learner interest on a
longer time-scale need to be researched and the model extended so that it can
dynamically handle any possible changes in learner interest.
Another interesting research question is the relation between diﬀerent cat-
egories in the Hierarchical
Topic Taxonomy.
For example,
if the hierarchy is
extended by introducing a new category,
it may be possible to start recom-
mendation on the new category based on learner activity in the sibling, parent
and child categories of that new category.
As discussed in the Section 5.1 above,
one important issue that needs to
be addressed is the assessment of
individual
learners.
Whether one agrees
on the necessity of assessing the learners from pedagogical
point of view,
for
developing and evaluating new methods assessing the learners is a valuable
tool.
However,
we need to be careful
that we do not by accident include
our own conceptions on what is “correct knowledge” in the assessment,
and
by doing that design methods that are just a form of formal
learning thinly
veiled in an informal exterior.
We also need to keep in mind that what we see
is just a very small and isolated part of the whole learning process, as informal
learning also happens oﬄine in locations and situations of
which we do not
have any information (see Figure 2.3.2 in Section 2.3).
In order to assess the learners we need to add a learner portfolio to our
system (see Figure 5.2.1).
Besides the learner activity in participation,
the
portfolio should include other available information on the learner.
For exam-
128
Figure 5.2.1: Learner Portfolio:
In order to facilitate assessment of
individual
learners,
we need to introduce a learner
portfolio module to our system.
The portfolio will include information about the learner activities and especially
material that the learner him- or herself has produced.
The portfolio can then be used to both assess the learners
and to add information to the learner model to achieve better recommendation.
129
ple,
actual
material
(e.g.
posts on the discussions) produced by the learner
should be included in the portfolio.
However, we need to make sure that col-
lecting the data for the portfolio is as non-intrusive to the learners as possible.
One possible answer to assessing the learners may be in the methodology of
natural language processing.
Applying such techniques as sentiment analysis
on the materials in the learner portfolio may give us valuable information on
the learners.
Another indicator of learning may be found by looking into how interacting
with his or her peers aﬀects the language that a learner uses.
After all, Vygot-
sky’s theory of socio-constructivism (see Section 2.2.1) is very closely related
to the way that language is used to conceptualize the world around us,
and
how we communicate our ideas and knowledge to our peers.
One possible way
to analyse the language use of learners is to construct language models for the
learners and see if the models show similar features over the course of time,
as the learners interact with each other.
5.3 Closing Remarks
E-Learning is by deﬁnition a very complex and wide research ﬁeld.
We
believe that any e-Learning system is only as good as its worst component.
No matter how beautiful and nice user interface an e-Learning system has,
if the pedagogical theory behind it is not solid, the learning experience will not
be very good.
A small thing, like a user interface that is not intuitive or even
at worst interferes with the concentration of the learner can make e-Learning
act counter actively.
Learning and teaching is a cross scientiﬁc discipline that touches not only
pedagogy and cognitive psychology,
but social sciences also.
We believe that
130
when we,
as e-Learning researchers,
need to work in even in closer collabo-
ration with psychologists, usability researchers, educators, and especially the
learners.
131
Bibliography
[1]
Potku.net — An Online Martial
Arts Community.
URL http://www.
potku.net.
[2]
Snowball
Stemmer
for
Finnish Language.
URL http://snowball.
tartarus.org/algorithms/finnish/stemmer.html.
[3]
Gediminas Adomavicius and Alexander Tuzhilin.
Toward the Next Gen-
eration of Recommender Systems:
A Survey of the State-of-the-Art and
Possible Extensions.
IEEE TRANSACTIONS ON KNOWLEDGE AND
DATA ENGINEERING, 17(6):734–749, 2005.
[4]
David M.
Blei,
Andrew Y.
Ng,
and Michael
I.
Jordan.
Latent Dirichlet
Allocation.
Journal
of Machine Learning Research, 3:993–1022, 2003.
[5]
Robin D. Burke. Hybrid Recommender Systems with Case-Based Compo-
nents.
In Advances in Case-Based Reasoning, 7th European Conference,
pages 91–105, 2004.
[6]
Seth Chaiklin. The Zone of Proximal Development in Vygotsky’s Analysis
of Learning and Instruction.
In A.
Kozulin,
B.
Gindis,
V.
Ageyev,
and
S. Miller, editors, Vygotsky’s Educational Theory and Practice in Cultural
Context. Cambridge University Press, 2003.
[7]
Chih-Chung Chang and Chih-Jen Lin.
LIBSVM:
A library for support
132
vector machines.
ACM Transactions on Intelligent Systems and Technol-
ogy, 2:1–27, 2011.
[8]
J.
Collins.
Education Techniques for Lifelong Learning:
Principles of
Adult Learning.
Radiographics, 24(5):1483–1489, 2004.
[9]
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W.
Furnas,
and Richard A.
Harshman.
Indexing by Latent Semantic Anal-
ysis.
Journal
of
the American Society for Information Science,
41(6):
391–407, 1990.
[10]
Pierre
Dillenbourg.
What
do you mean by collaborative
learning?
In Pierre
Dillenbourg,
editor,
Collaborative-learning:
Cognitive
and
Computational
Approaches, pages 1–19. Elsevier, Oxford, 1999.
[11]
Ulf Daniel
Ehlers.
Web 2.0 - e-Learning 2.0 - Quality 2.0?
Quality for
New Learning Cultures.
Quality Assurance in Education, 17(3):296–314,
2009.
[12]
Lyn English.
Constructivism and Education.
Educational
Studies in
Mathematics, 40:93–99, 1999.
ISSN 0013-1954.
[13]
Gerhard Fischer.
Lifelong Learning and Its Support with New Media.
In N.J.
Smelser and B.P.
Baltes,
editors,
International
Encyclopedia of
Social
and Behavioral
Sciences,
volume 13,
pages 8836–8840.
Elsevier,
2001.
[14]
Yik-Hing
Fung,
Chun-Hung
Li,
and
William K.
Cheung.
On-
line
Discussion Participation Prediction Using
Non-negative
Matrix
Factorization.
In
WI-IATW ’07:
Proceedings
of
the
2007
IEEE/WIC/ACM International Conferences on Web Intelligence and In-
133
telligent Agent Technology - Workshops, pages 284–287, Washington, DC,
USA, 2007. IEEE Computer Society.
ISBN 0-7695-3028-1.
[15]
George W.
Furnas,
Scott C.
Deerwester,
Susan T.
Dumais,
Thomas K.
Landauer,
Richard A.
Harshman,
Lynn A.
Streeter,
and Karen E.
Lochbaum.
Information Retrieval using a Singular Value Decomposition
Model
of Latent Semantic Structure.
In Proceedings of the 11th Annual
International
ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 465–480, 1988.
[16]
Andrew Gelman,
John B.
Carlin,
Hal
S.
Stern,
and Donald B.
Rubin.
Bayesian Data Analysis,
Second Edition (Chapman & Hall/CRC Texts
in Statistical
Science).
Chapman and Hall/CRC,
2 edition,
July 2003.
ISBN 158488388X.
[17]
Sabine
Graf
and
Rahel
Bekele.
Forming
Heterogeneous
Groups
for
Intelligent
Collaborative
Learning
Systems
with
Ant
Colony
Optimization.
In Intelligent Tutoring Systems, pages 217–226, 2006.
[18]
Vishal Gupta and Gurpreet S. Lehal. A Survey of Text Mining Techniques
and Applications.
Journal of Emerging Technologies in Web Intelligence,
1(1):60–76, 2009.
[19]
Eui-Hong Han and George Karypis.
Centroid-Based Document Classi-
ﬁcation:
Analysis and Experimental
Results.
In Proceedings of
the 4th
European Conference on Principles of Data Mining and Knowledge Dis-
covery, pages 424–431, 2000.
[20]
K Hirata,
K Seta,
and K Makiuchi.
Skill
and Competency Modelling
Typology.
In Proceedings for the 15th International
Conference on Com-
puters in Education, pages 9–16, Hiroshima, Japan, November 2007.
134
[21]
Thomas Hofmann.
Probabilistic Latent Semantic Indexing.
In Proceed-
ings of the 22nd annual international ACM SIGIR conference on Research
and development in information retrieval,
SIGIR ’99,
pages 50–57,
New
York, NY, USA, 1999. ACM.
ISBN 1-58113-096-1.
[22]
Andreas Hotho,
Andreas N¨
urnberger,
and Gerhard Paass.
A Brief Sur-
vey of
Text Mining.
LDV Forum -
GLDV Journal
for Computational
Linguistics and Language Technology, 20(1):19–62, 2005.
[23]
Yi-Wei
Huang,
Zhe-Nan Lin,
Von-Wun Soo,
Bert Chen,
and Chen-Yu
Lee. Ontology Application to Question Generation in E-learning Systems
in Mobile Industry.
Advances in Computing & Emerging E-Learning
Technologies 2012, March / April 2012.
[24]
Nathalie Japkowicz and Shaju Stephen.
The Class Imbalance Problem:
A Systematic Study.
Intelligent
Data Analysis,
6(5):429–449,
October
2002.
ISSN 1088-467X.
[25]
T. Joachims.
Text Categorization with Support Vector Machines:
Learn-
ing with Many Relevant Features.
In European Conference on Machine
Learning (ECML), pages 137–142, Berlin, 1998. Springer.
[26]
G. Kelly. A Brief Introduction to Personal Construct Theory., pages 1–29.
Academic Press., 1970.
[27]
Sang-Bum Kim,
Kyoung-Soo Han,
Hae-Chang Rim,
and Sung-Hyon
Myaeng.
Some Eﬀective Techniques for Naive Bayes Text Classiﬁcation.
IEEE Trans. Knowl. Data Eng., 18(11):1457–1466, 2006.
[28]
M.S. Knowles, E.F. Holton, and R.A. Swanson.
The Adult Learner:
The
Deﬁnitive Classic in Adult Education and Human Resource Development.
MA. Butterwirth-Heinemann, 5 edition, 1998.
135
[29]
Peter M. Lee.
Bayesian Statistics:
An Introduction.
Arnold Publication,
3 edition, 2009.
[30]
D. W. Livingstone. Exploring the Icebergs of Adult Learning:
Findings of
the First Canadian Survey of Informal Learning Practices. The Canadian
Journal
for the Study of Adult Learning, 13(2):49–72, 2000.
[31]
Victoria J.
Marsick and Karen E.
Watkins.
Informal
and Incidental
Learning.
New Directions for Adult and Continuing Education, 2001(89):
25–34, 2002.
[32]
Stuart E. Middleton, Nigel R. Shadbolt, and David C. De Roure.
Onto-
logical user proﬁling in recommender systems.
ACM Trans. Inf. Syst., 22
(1):54–88, January 2004.
ISSN 1046-8188.
[33]
Todd K. Moon.
The Expectation-Maximization Algorithm.
IEEE Signal
Process. Mag, 13:47–60, 1996.
[34]
F.
Murtagh.
A Survey of
Recent Advances in Hierarchical
Clustering
Algorithms.
Computer Journal, 26(4):354–359, 1983.
[35]
Ikujiro Nonaka and Noburu Konno.
The Concept of
’Ba’:
Building a
Foundation for Knowledge Creation.
California Management Review, 40
(3):40–54, 1998.
[36]
Michael J. Pazzani and Daniel Billsus.
Content-Based Recommendation
Systems.
In The Adaptive Web, pages 325–341, 2007.
[37]
M. Polanyi.
The Tacit Dimension.
Number p. 762 in Terry lectures, Yale
University. Doubleday, 1966.
ISBN 9780385069885.
[38]
Radim
ˇ
Reh˚uˇrek and Petr Sojka. Software Framework for Topic Modelling
with Large Corpora.
In Proceedings of the LREC 2010 Workshop on New
136
Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.
ELRA.
[39]
Daniel
Schugurensky.
The Forms
of
Informal
Learning:
Towards
a
Conceptualization of
the Field.
In NALL Working Papers,
number 19.
NALL, 2000.
[40]
Jeﬀrey Solka. Text Data Mining:
Theory and Methods. Statistics Surveys,
2:94–112, 2008.
[41]
M.
Steinbach,
G.
Karypis,
and V.
Kumar.
A Comparison of Document
Clustering Techniques.
Technical report, University of Minnesota, 2000.
[42]
Thepchai
Supnithi,
Akiko Inaba,
Mitsuru Ikeda,
Junichi
Toyoda,
and
Riichiro Mizoguchi.
Learning Goal
Ontology Supported by Learning
Theories for Opportunistic Group Formation.
In Artiﬁcial
Intelligence
in Education. IOS, 1999.
[43]
Martin
Wessner
and
Hans-R¨
udiger
Pﬁster.
Group
Formation
in
Computer-Supported Collaborative Learning. In Skip Ellis, Tom Rodden,
and Ilse Zigurs, editors, Proceedings of the ACM Group 2001 Conference,
Sep 30 - Oct 3, 2001, Boulder CO, USA, pages 24–31. ACM Press, 2001.
[44]
David Spiegelhalter W.R.
Gilks,
S.
Richardson,
editor.
Markov Chain
Monte
Carlo
in Practice:
Interdisciplinary
Statistics
(Chapman &
Hall/CRC Interdisciplinary Statistics).
Chapman and Hall/CRC, 1 edi-
tion, December 1995.
ISBN 0412055511.
[45]
Fan Yang,
Peng Han,
Ruimin Shen,
Bernd J.
Kramer,
and Xinwei
Fan.
Cooperative Learning in Self-Organizing E-Learner Communities Based
on a Multi-Agents Mechanism. In Tamas D. Gedeon and Lance Chun Che
137
Fung,
editors,
Australian Conference on Artiﬁcial
Intelligence,
volume
2903 of
Lecture Notes in Computer Science,
pages 490–500.
Springer,
2003.
ISBN 3-540-20646-9.
[46]
Zhiwen Yu,
Yuichi
Nakamura,
Seiie
Jang,
Shoji
Kajita,
and Kenji
Mase.
Ontology-Based Semantic Recommendation for Context-Aware
E-Learning.
In Jadwiga Indulska,
Jianhua Ma,
Laurence.
Yang,
Theo
Ungerer, and Jiannong Cao, editors, Ubiquitous Intelligence and Comput-
ing,
volume 4611 of Lecture Notes in Computer Science,
pages 898–907.
Springer Berlin Heidelberg, 2007.
ISBN 978-3-540-73548-9.
138
List of Publications Related to the Thesis
Journal articles:
1.
Mikko Vilenius,
Neil
Rubens,
Fumihiko Anma,
Toshio Okamoto.
Sup-
porting Collaborative Activities in Informal,
Ill-constructed Learning.
The Journal
of Information and Systems in Education,
Vol.
8,
Nr.
1,
pp.
12–24, December, 2009
Conference proceedings:
1.
Mikko Vilenius, Fumihiko Anma, Toshio Okamoto.
Collaborative Group
Forming in Grid Environment.
The 11th IASTED International Confer-
ence Computers and Advanced Technology in Education (CATE2008),
pp.
55–60, October, 2008
2.
Mikko Vilenius, Toshio Okamoto.
Supporting Cooperative Group Form-
ing in a Learning Grid.
The 33rd Conference of Japanese Society for In-
formation and System in Education (JSiSE2008), pp.
458–459 , Septem-
ber 2008
3.
Mikko Vilenius,
Toshio Okamoto.
Supporting Social
Interaction and
Collaborative Tasks in an Asynchronous Collaborative Learning.
The
32nd Conference of Japanese Society for Information and System in Ed-
ucation (JSiSE2007), pp.
134–135, September 2007
139
Author Biography
Mikko Vilenius was born in Tampere,
Finland on the 30th of
June 1975.
He received his Master’s Degree in Computer Science from the University of
Jyv¨askyl¨
a in 2006.
He received the Japanese Ministry of Education, Culture,
Sports, Science and Technology scholarship for Ph.D. studies at the University
of Electro-Communications for the years 2006–2009.
His research interests in-
clude educational technology, informal learning, machine learning and natural
language processing.
Mr.
Vilenius is currently working as a researcher in The Japan Institute for
Educational Measurement, Inc.
140
