BigARTM: Open Source Library for
Regularized Multimodal
Topic Modeling
of Large Collections
Konstantin Vorontsov, Oleksandr Frei, Murat Apishev,
Peter Romov, Marina Dudarenko
Yandex
•
CC RAS
•
MIPT
•
HSE
•
MSU
Analysis of Images, Social
Networks and Texts
Ekaterinburg
•
9–11 April
2015
Contents
1
Theory
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
2
BigARTM implementation — http://bigartm.org
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
3
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
What is “topic”?
Topic is a special
terminology of a particular domain area.
Topic is a set of coherent terms (words or phrases)
that often occur together in documents.
Formally,
topic is a probability distribution over terms:
p(w
|
t) is (unknown) frequency of word w in topic t.
Document semantics is a probability distribution over topics:
p(t
|
d ) is (unknown) frequency of topic t in document d .
Each document d consists of terms w
1
,
w
2
, . . . ,
w
n
d
:
p(w
|
d ) is (known) frequency of term w in document d .
When writing term w in document d author thinks about topic t.
Topic model
tries to uncover latent topics from a text collection.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
3 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Goals and applications of Topic Modeling
Goals:
Uncover a hidden thematic structure of the text collection
Find a compressed semantic representation of each document
Applications:
Information retrieval
for long-text queries
Semantic search in large scientiﬁc document collections
Revealing research trends and research fronts
Expert search
News aggregation
Recommender systems
Categorization, classiﬁcation, summarization, segmentation
of texts, images, video, signals, social
media
and many others
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
4 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Probabilistic Topic Modeling: milestones and mainstream
1
PLSA — Probabilistic Latent Semantic Analysis (1999)
2
LDA — Latent Dirichlet Allocation (2003)
3
100s of PTMs based on Graphical
Models & Bayesian Inference
David Blei. Probabilistic topic models // Communications of the ACM, 2012.
Vol. 55. No. 4. Pp. 77–84.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
5 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Generative Probabilistic Topic Model
(PTM)
Topic model
explains terms w in documents d by topics t:
p(w
|
d) =
P
t
p(w
|
t)p(t
|
d )
Разработан спектрально-аналитический подход к выявлению размытых протяженных повторов 
в геномных последовательностях. Метод основан на разномасштабном оценивании сходства 
нуклеотидных последовательностей в пространстве коэффициентов разложения фрагментов 
кривых GC- и GA-содержания по классическим ортогональным базисам. Найдены условия 
оптимальной аппроксимации, обеспечивающие автоматическое распознавание повторов 
различных видов (прямых и инвертированных, а также тандемных) на спектральной матрице 
сходства. Метод одинаково хорошо работает на разных масштабах данных. Он позволяет 
выявлять следы сегментных дупликаций и мегасателлитные участки в геноме, районы синтении 
при сравнении пары геномов. Его можно использовать для детального изучения фрагментов 
хромосом (поиска размытых участков с умеренной длиной повторяющегося паттерна). 
( |!) 

(
"
|
)
: 
,…, 
#
$
"
, … , "
#
$
: 
0.018 распознавание 
0.013 сходство 
0.011 паттерн 
… … … … 
0.023 днк 
0.0
16 геном 
0.009 нуклеотид 
… … … … 
0.014 базис 
0.0
09 спектр 
0.006 ортогональный 
… … … … 
! 
" 
"
#" 
$ 
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
6 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
PLSA: Probabilistic Latent Semantic Analysis [T. Hofmann 1999]
Given: D is a set (collection) of documents
W is a set (vocabulary) of terms
n
dw
= how many times term w appears in document d
Find:
parameters
φ
wt
=p(w
|
t ),
θ
td
= p(t
|
d ) of the topic model
p(w
|
d) =
X
t
φ
wt
θ
td
.
The problem of log-likelihood maximization under
non-negativeness and normalization constraints:
X
d,w
n
dw
ln
X
t
φ
wt
θ
td
→
max
Φ,Θ
,
φ
wt
>
0
,
P
w∈W
φ
wt
= 1;
θ
td
>
0
,
P
t∈T
θ
td
= 1
.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
7 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Topic Modeling is an ill-posed inverse problem
Topic Modeling is the problem of stochastic matrix factorization:
p(w
|
d) =
X
t∈T
φ
wt
θ
td
.
In matrix notation
P
W×D
= Φ
W×T
·
Θ
T×D
,
where
P =
p(w
|
d )
W×D
is known term–document matrix,
Φ =
φ
wt
W×T
is unknown term–topic matrix,
φ
wt
=p(w
|
t ),
Θ =
θ
td
T×D
is unknown topic–document matrix,
θ
td
= p(t
|
d ).
Matrix factorization is not unique, the solution is not stable:
ΦΘ = (ΦS)(S
−1
Θ) = Φ
′
Θ
′
for all
S such that Φ
′
= ΦS,
Θ
′
= S
−1
Θ are stochastic.
Then, regularization is needed to ﬁnd appropriate solution.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
8 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
ARTM: Additive Regularization of Topic Model
Additional
regularization criteria R
i
(Φ
,
Θ)
→
max,
i
= 1
, . . . ,
n.
The problem of regularized log-likelihood maximization under
non-negativeness and normalization constraints:
X
d,w
n
dw
ln
X
t∈T
φ
wt
θ
td
|
{z
}
log-likelihood
L
(Φ,Θ)
+
n
X
i =1
τ
i
R
i
(Φ
,
Θ)
|
{z
}
R(Φ,Θ)
→
max
Φ,Θ
,
φ
wt
>
0;
P
w∈W
φ
wt
= 1;
θ
td
>
0;
P
t∈T
θ
td
= 1
where
τ
i
>
0 are regularization coefficients.
Vorontsov K. V., Potapenko A. A. Tutorial
on Probabilistic Topic Modeling:
Additive Regularization for Stochastic Matrix Factorization // AIST’2014,
Springer CCIS, 2014. Vol. 436. pp. 29–46.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
9 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
ARTM: available regularizers
topic smoothing (equivalent to LDA)
topic sparsing
topic decorrelation
topic selection via entropy sparsing
topic coherence maximization
supervised learning for classiﬁcation and regression
semi-supervised learning
using documents citation and links
modeling temporal
topic dynamics
using vocabularies in multilingual
topic models
and many others
Vorontsov K. V., Potapenko A. A. Additive Regularization of Topic Models //
Machine Learning. Special
Issue “Data Analysis and Intelligent Optimization
with Applications”. Springer, 2014.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
10 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Probabilistic Topic Modeling
Given a text document collection Probabilistic Topic Model
ﬁnds:
p(t
|
d ) — topic distribution for each document d ,
p(w
|
t) — term distribution for each topic t.
Topics of documents
Words and keyphrases of topics
doc1:
doc2:
doc3:
doc4:
...
Text documents
Topic
Modeling
D
o
c
u
m
e
n
t
s
T
o
p
i
c
s
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
11 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Topic Model
ﬁnds topical
distribution for terms p(w
|
t ),
authors p(a
|
t), time p(y
|
t ),
Topics of documents
Words and keyphrases of topics
doc1:
doc2:
doc3:
doc4:
...
Text documents
Topic
Modeling
D
o
c
u
m
e
n
t
s
T
o
p
i
c
s
Metadata:
Authors
Data Time
Conference
Organization
URL
etc.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
12 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Topic Model
ﬁnds topical
distribution for terms p(w
|
t ),
authors p(a
|
t), time p(y
|
t), objects on images p(o
|
t ),
Topics of documents
Words and keyphrases of topics
doc1:
doc2:
doc3:
doc4:
...
Text documents
Topic
Modeling
D
o
c
u
m
e
n
t
s
T
o
p
i
c
s
Metadata:
Authors
Data Time
Conference
Organization
URL
etc.
Images
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
13 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Topic Model
ﬁnds topical
distribution for terms p(w
|
t ),
authors p(a
|
t), time p(y
|
t), objects on images p(o
|
t ),
linked documents p(d
′
|
t ),
Topics of documents
Words and keyphrases of topics
doc1:
doc2:
doc3:
doc4:
...
Text documents
Topic
Modeling
D
o
c
u
m
e
n
t
s
T
o
p
i
c
s
Metadata:
Authors
Data Time
Conference
Organization
URL
etc.
Images
Links
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
14 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Topic Model
ﬁnds topical
distribution for terms p(w
|
t ),
authors p(a
|
t), time p(y
|
t), objects on images p(o
|
t ),
linked documents p(d
′
|
t), advertising banners p(b
|
t ),
Topics of documents
Words and keyphrases of topics
doc1:
doc2:
doc3:
doc4:
...
Text documents
Topic
Modeling
D
o
c
u
m
e
n
t
s
T
o
p
i
c
s
Metadata:
Authors
Data Time
Conference
Organization
URL
etc.
Ads
Images
Links
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
15 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Topic Model
ﬁnds topical
distribution for terms p(w
|
t ),
authors p(a
|
t), time p(y
|
t), objects on images p(o
|
t ),
linked documents p(d
′
|
t), advertising banners p(b
|
t), users p(u
|
t ),
Topics of documents
Words and keyphrases of topics
doc1:
doc2:
doc3:
doc4:
...
Text documents
Topic
Modeling
D
o
c
u
m
e
n
t
s
T
o
p
i
c
s
Metadata:
Authors
Data Time
Conference
Organization
URL
etc.
Ads
Images
Links
Users
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
16 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multimodal
Topic Model
ﬁnds topical
distribution for terms p(w
|
t ),
authors p(a
|
t), time p(y
|
t), objects on images p(o
|
t ),
linked documents p(d
′
|
t), advertising banners p(b
|
t), users p(u
|
t ),
and binds all
these modalities into a single topic model.
Topics of documents
Words and keyphrases of topics
doc1:
doc2:
doc3:
doc4:
...
Text documents
Topic
Modeling
D
o
c
u
m
e
n
t
s
T
o
p
i
c
s
Metadata:
Authors
Data Time
Conference
Organization
URL
etc.
Ads
Images
Links
Users
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
17 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multi-ARTM: combining multimodality with regularization
M is the set of modalities
W
m
is a vocabulary of tokens of m-th modality, m
∈
M
W = W
1
⊔ · · · ⊔
W
M
is a joint vocabulary of all
modalities
The problem of multimodal
regularized log-likelihood
maximization under non-negativeness and normalization constraints:
X
m∈M
λ
m
X
d∈D
X
w∈W
m
n
dw
ln
X
t∈T
φ
wt
θ
td
|
{z
}
modality log-likelihood
L
m
(Φ,Θ)
+
n
X
i =1
τ
i
R
i
(Φ
,
Θ)
|
{z
}
R(Φ,Θ)
→
max
Φ,Θ
,
φ
wt
>
0
,
P
w∈W
m
φ
wt
= 1
,
m
∈
M;
θ
td
>
0
,
P
t∈T
θ
td
= 1
.
where
λ
m
>
0,
τ
i
>
0 are regularization coefficients.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
18 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Multi-ARTM: multimodal
regularized EM-algorithm
EM-algorithm is a simple-iteration method for a system of equations
Theorem. The local
maximum (Φ
,
Θ) satisﬁes the following
system of equations with auxiliary variables p
tdw
= p(t
|
d
,
w ):
p
tdw
= norm
t∈T
φ
wt
θ
td

;
φ
wt
= norm
w∈W
m

n
wt
+
φ
wt
∂
R
∂φ
wt

;
n
wt
=
X
d∈D
λ
m(w)
n
dw
p
tdw
;
θ
td
= norm
t∈T

n
td
+
θ
td
∂
R
∂θ
td

;
n
td
=
X
w∈d
λ
m(w)
n
dw
p
tdw
;
where norm
t∈T
x
t
=
max{x
t
,0}
P
s∈T
max{x
s
,0}
is nonnegative normalization;
m(w ) is the modality of the term w , so that w
∈
W
m(w)
.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
19 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Fast online EM-algorithm for Multi-ARTM
Input: collection D split into batches D
b
, b = 1
, . . . ,
B;
Output: matrix Φ;
1
initialize
φ
wt
for all
w
∈
W , t
∈
T ;
2
n
wt
:= 0,
˜
n
wt
:= 0 for all
w
∈
W , t
∈
T ;
3
for all batches D
b
, b = 1
, . . . ,
B
4
iterate each document d
∈
D
b
at a constant matrix Φ:
(˜
n
wt
) := (˜
n
wt
) + ProcessBatch (D
b
,
Φ);
5
if (synchronize) then
6
n
wt
:= n
wt
+ ˜
n
dw
for all
w
∈
W , t
∈
T ;
7
φ
wt
:= norm
w∈W
m
n
wt
+
φ
wt
∂R
∂φ
wt

for all
w
∈
W
m
, m
∈
M, t
∈
T ;
8
˜
n
wt
:= 0 for all
w
∈
W , t
∈
T ;
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
20 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
Fast online EM-algorithm for Multi-ARTM
ProcessBatch iterates documents d
∈
D
b
at a constant matrix Φ.
matrix (˜
n
wt
) := ProcessBatch (set of documents D
b
, matrix Φ)
1
˜
n
wt
:= 0 for all
w
∈
W , t
∈
T ;
2
for all d
∈
D
b
3
initialize
θ
td
:=
1
|T |
for all
t
∈
T ;
4
repeat
5
p
tdw
:= norm
t∈T
φ
wt
θ
td

for all
w
∈
d , t
∈
T ;
6
n
td
:=
P
w∈d
λ
m(w)
n
dw
p
tdw
for all
t
∈
T ;
7
θ
td
:= norm
t∈T
n
td
+
θ
td
∂R
∂θ
td

for all
t
∈
T ;
8
until
θ
d
converges;
9
˜
n
wt
:= ˜
n
wt
+
λ
m(w)
n
dw
p
tdw
for all
w
∈
d , t
∈
T ;
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
21 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
Probabilistic Topic Modeling
ARTM — Additive Regularization for Topic Modeling
Multimodal
Probabilistic Topic Modeling
ARTM approach: beneﬁts and restrictions
Beneﬁts
Single EM-algorithm for many models and their combinations
PLSA, LDA, and 100s of PTMs are covered by ARTM
No complicated inference and graphical
models
ARTM reduces barriers to entry into PTM research ﬁeld
ARTM encourages any combinations of regularizers
Multi-ARTM encourages any combinations of modalities
Multi-ARTM is implemented in BigARTM open-source project
Under development (not really restrictions):
3-matrix factorization P = ΦΨΘ, e.g. Author-Topic Model
Further generalization of hypergraph-based Multi-ARTM
Adaptive optimization of regularization coeﬃcients
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
22 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
The BigARTM project: main features
Parallel
online Multi-ARTM framework
Open-source http://bigartm.org
Distributed storage of collection is possible
Built-in regularizers:
smoothing, sparsing, decorrelation, semi-supervised learning,
and many others coming soon
Built-in quality measures:
perplexity, sparsity, kernel
contrast and purity,
and many others coming soon
Many types of PTMs can be implemented via Multi-ARTM:
multilanguage, temporal, hierarchical, multigram,
and many others
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
23 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
The BigARTM project: parallel
architecture
Concurrent processing of batches
Simple single-threaded code for ProcessBatch
User controls when to update the model
in online algorithm
Deterministic (reproducible) results from run to run
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
24 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
BigARTM vs Gensim vs Vowpal
Wabbit
3.7M articles from Wikipedia, 100K unique words
procs
train
inference
perplexity
BigARTM
1
35 min
72 sec
4000
Gensim.LdaModel
1
369 min
395 sec
4161
VowpalWabbit.LDA
1
73 min
120 sec
4108
BigARTM
4
9 min
20 sec
4061
Gensim.LdaMulticore
4
60 min
222 sec
4111
BigARTM
8
4.5 min
14 sec
4304
Gensim.LdaMulticore
8
57 min
224 sec
4455
procs = number of parallel
threads
inference = time to infer
θ
d
for 100K held-out documents
perplexity is calculated on held-out documents.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
25 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
Running BigARTM in Parallel
3.7M articles from Wikipedia, 100K unique words
Amazon EC2 c3.8xlarge (16 physical
cores + hyperthreading)
No extra memory cost for adding more threads
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
26 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
How to start using BigARTM
1
Download links, tutorials, documentation:
http://bigartm.org
2
Linux: compile and start examples
Windows: start examples
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
27 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
How to start using BigARTM
1
Download links, tutorials, documentation:
http://bigartm.org
2
Linux: compile and start examples
Windows: start examples
BigARTM community:
1
Post questions in BigARTM discussion group:
https://groups.google.com/group/bigartm-users
2
Report bugs in BigARTM issue tracker:
https://github.com/bigartm/bigartm/issues
3
Contribute to BigARTM project via pull
requests:
https://github.com/bigartm/bigartm/pulls
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
28 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
BigARTM: parallel
architecture
BigARTM: time and memory performance
How to start using BigARTM
License and programming environment
Freely available for commercial
usage (BSD 3-Clause license)
Cross-platform — Windows, Linux, Mac OS X (32 bit, 64 bit)
Simple command-line API — available now
Rich programming API in C++ and Python — available now
Rich programming API in C# and Java — coming soon
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
29 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
Combining Regularizers: experiment on 3.7M Wikipedia collection
Additive combination of 5 regularizers:
smoothing background (common lexis) topics B in Φ and Θ
sparsing domain-speciﬁc topics S = T
\
B in Φ and Θ
decorrelation of topics in Φ
Φ
W×T
Θ
T×D
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
30 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
Combining Regularizers: experiment on 3.7M Wikipedia collection
Additive combination of 5 regularizers:
smoothing background (common lexis) topics B in Φ and Θ
sparsing domain-speciﬁc topics S = T
\
B in Φ and Θ
decorrelation of topics in Φ
R(Φ
,
Θ) = +
β
1
P
t∈B
P
w∈W
β
w
ln
φ
wt
+
α
1
P
d∈D
P
t∈B
α
t
ln
θ
td
−
β
0
P
t∈S
P
w∈W
β
w
ln
φ
wt
−
α
0
P
d∈D
P
t∈S
α
t
ln
θ
td
−
γ
P
t∈T
P
s∈T \t
P
w∈W
φ
wt
φ
ws
where
β
0
,
α
0
,
β
1
,
α
1
,
γ
are regularization coeﬃcients.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
31 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
Combining Regularizers: LDA vs ARTM models
P
10k
,
P
100k
— hold-out perplexity (10K, 100K documents)
S
Φ
,
S
Θ
— sparsity of Φ and Θ matrices (in %)
K
s
,
K
p
,
K
c
— average topic kernel
size, purity and contrast
Model
P
10k
P
100k
S
Φ
S
Θ
K
s
K
p
K
c
LDA
3436
3801
0.0
0.0
873
0.533
0.507
ARTM
3577
3947
96.3
80.9
1079
0.785
0.731
Convergence of LDA (thin lines) and ARTM (bold lines)
1 · 10
6
2 · 10
6
3 · 10
6
0.34
0.52
0.69
0.87
1.04
1.22
·10
4
Perplexity
0
20
40
60
80
100
Sparsity
Perplexity
Phi
Theta
1 · 10
6
2 · 10
6
3 · 10
6
0
0.25
0.5
0.75
1
1.25
·10
3
Kernel size
0
0.2
0.4
0.6
0.8
1
Purity and contrast
Size
Purity
Contrast
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
32 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
EUR-Lex corpus
19 800 documents about European Union law
Two modalities: 21K words, 3 250 categories (class labels)
EUR-Lex is a “power-law dataset” with unbalanced classes:
Left: # unique labels with a given # documents per label
Right: # documents with a given # labels
Rubin T. N., Chambers A., Smyth P., Steyvers M. Statistical
topic models for
multi-label
document classiﬁcation // Machine Learning, 2012, 88(1-2).
Pp. 157–208.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
33 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
Multi-ARTM for classiﬁcation
Regularizers:
Uniform smoothing for Θ
Uniform smoothing for word–topic matrix Φ
1
Label
regularization for class–topic matrix Φ
2
:
R(Φ
2
) =
τ
X
c∈W
2
ˆ
p
c
ln p(c )
→
max
,
where
p(c) =
P
t∈T
φ
ct
p(t) is the model
distribution of class c ,
p(t) =
n
t
n
can be easily estimated along EM iterations,
ˆ
p
c
is the empirical
frequency of class c in the training data.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
34 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
The comparative study of models on EUR-Lex classiﬁcation task
DLDA (Dependency LDA) [Rubin 2012] is a nearest analog
of Multi-ARTM for classiﬁcation among Bayesian Topic Models
Quality measures [Rubin 2012]:
AUC-PR (%,
⇑
) — Area under precision-recall
curve
AUC (%,
⇑
) — Area under ROC curve
OneErr (%,
⇓
) — One error (most ranked label
is not relevant)
IsErr (%,
⇓
) — Is error (no perfect classiﬁcation)
Results:
|
T
|
opt
AUC-PR
AUC
OneErr
IsErr
Multi-ARTM
10 000
51.3
98.0
29.1
95.5
DLDA [Rubin 2012]
200
49.2
98.2
32.0
97.2
SVM
43.5
97.5
31.6
98.1
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
35 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
Multi-language ARTM
We consider languages as modalities in Multi-ARTM.
Collection of 216 175 Russian–English Wikipedia articles pairs.
Top 10 words with p(w
|
t) probabilities (in %):
Topic 68
Topic 79
research
4.56
институт
6.03
goals
4.48
матч
6.02
technology
3.14
университет
3.35
league
3.99
игрок
5.56
engineering
2.63
программа
3.17
club
3.76
сборная
4.51
institute
2.37
учебный
2.75
season 3.49
фк
3.25
science
1.97
технический
2.70
scored
2.72
против
3.20
program
1.60
технология
2.30
cup
2.57
клуб
3.14
education
1.44
научный
1.76
goal
2.48
футболист
2.67
campus
1.43
исследование
1.67
apps
1.74
гол
2.65
management
1.38
наука
1.64
debut
1.69
забивать
2.53
programs
1.36
образование
1.47
match
1.67
команда
2.14
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
36 / 38
Theory
BigARTM implementation — http://bigartm.org
Experiments
ARTM for combining regularizers
Multi-ARTM for classiﬁcation
Multi-ARTM for multi-language TM
Multi-language ARTM
Collection of 216 175 Russian–English Wikipedia articles pairs.
Top 10 words with p(w
|
t) probabilities (in %):
Topic 88
Topic 251
opera
7.36
опера
7.82
windows
8.00
windows
6.05
conductor
1.69
оперный
3.13
microsoft
4.03
microsoft
3.76
orchestra
1.14
дирижер 2.82
server
2.93
версия
1.86
wagner
0.97
певец
1.65
software
1.38
приложение
1.86
soprano
0.78
певица
1.51
user
1.03
сервер
1.63
performance
0.78
театр
1.14
security
0.92
server
1.54
mozart
0.74
партия
1.05
mitchell
0.82
программный 1.08
sang
0.70
сопрано
0.97
oracle
0.82
пользователь
1.04
singing
0.69
вагнер
0.90
enterprise
0.78
обеспечение
1.02
operas
0.68
оркестр
0.82
users
0.78
система
0.96
All
|
T
|
= 400 topics were reviewed by an independent assessor,
and he successfully interpreted 396 topics.
Konstantin Vorontsov (voron@yandex-team.ru)
BigARTM: Open Source Topic Modeling
37 / 38
Conclusions
ARTM (Additive Regularization for Topic Modeling)
is a general
framework, which makes topic models
easy to design, to infer, to explain,
and to combine.
Multi-ARTM is a further generalization of ARTM
for multimodal
topic modeling
BigARTM is an open source project for parallel
online
topic modeling of large text collections.
http://bigartm.org
Join BigARTM community!
