Computer-Assisted Text Analysis for Social Science:
Topic Models and Beyond
Ryan Wesslen
College of Computing and Informatics
University of North Carolina at Charlotte
Charlotte,
North Carolina USA
Email: rwesslen@uncc.edu
Abstract—Topic models are a family of
statistical-based al-
gorithms to summarize,
explore and index large collections of
text
documents.
After a decade of
research led by computer
scientists,
topic models have spread to social
science as a new
generation of
data-driven social
scientists
have
searched for
tools to explore large collections of unstructured text.
Recently,
social
scientists have contributed to topic model
literature with
developments
in causal
inference
and tools
for
handling the
problem of multi-modality.
In this paper,
I provide a literature
review on the evolution of topic modeling including extensions for
document covariates,
methods for evaluation and interpretation,
and advances in interactive visualizations along with each aspect’s
relevance and application for social
science research.
Keywords—computational social science, computer-assisted text
analysis,
visual
analytics,
structural
topic model
I.
I
NTRODUCTION
Topic models are a framework of
statistical-based algo-
rithms
used to identify and measure latent
(hidden)
topics
within a corpus of text
documents.
Despite their wide use in
computer
science research [1],
topic models have remained
largely absent
from the average social
scientist’s
analytical
toolkit.
Historically,
most
social
science
text
analysis
has
instead focused on either
human coding or
dictionary-based
methods that are semi-automated but require high pre-analysis
costs before implementation [2].
Moreover,
this problem is
magnified when considering the tremendous increase in the
volume and variety of unstructured text
documents for social
scientists
to study [3].
To overcome
this
problem,
social
scientists have started to adopt computer-assisted text analysis
techniques (like supervised learning and topic models) for their
research with the goal of “amplifying and augmenting” social
science analysis,
not replacing it [3].
Unlike
computer
scientists
who typically use
machine
learning techniques for prediction, social scientists have found
use in machine learning for the analysis of latent variables that
could previously only be measured under untestable and con-
sequential
assumptions [4].
Ultimately,
the rise of computer-
assisted text
analysis tools in social
science research is one
of
the major
drivers of
the emerging field of
computational
social science [5],
[6].
The purpose of this paper is to survey
the literature of
one such computer-assisted text
technique,
topic models,
and provide background on its importance to
social science research. This overview leads itself to the newly
created structural topic model (STM) that extends the general
topic model
framework to estimate causal
effects within text
documents [7],
[8].
In section 1,
I
review the evolution of
topic models by
introducing latent
Dirichlet
allocation (LDA) [9] and related
seminal
models.
I
also consider
computational
methods for
topic models and discuss the extensions to the LDA-based
framework to include document covariates within the model. In
section 2,
I explore tools for the application of topic models
including methods for
evaluation,
measures of
interpretation
and visualization interfaces.
In section 3,
I discuss two major
contributions by social scientists to the topic model literature:
structural topic model (STM) and techniques to handle multi-
modality.
I
provide examples of
social
science applications
that
use these techniques
for
texts
like open-ended survey
responses, political rhetoric, and social media. Last, I consider
ongoing limitations
in the
methodology along with future
opportunities for topic models within social
science research
in relation to computational
social
science and explainable
artificial intelligence (XAI).
II.
E
VOLUTION OF TOPIC MODELS
Topic models identify and measure latent
topics within a
corpus of text documents.
Topic models are called generative
models because they assume that observable data is generated
by joint
probability of
variables
that
are interpreted to be
topics.
LDA is the workhorse topic model
and is a Bayesian
two-tiered mixture model
that
identifies word co-occurrence
patterns,
which are interpreted as topics.
In this section,
I summarize key properties of this frame-
work by reviewing the evolution of topic models starting with
its seminal models. Next, I review model extensions along with
advances in computational methods in the model framework.
A.
LDA and Seminal Papers
As a generalization, there are two approaches to computer-
assisted text
analysis:
natural
language processing (NLP) and
statistical-based algorithms like topic models [13]. Unlike NLP
methods that
tags parts-of-speech and grammatical
structure,
statistical-based models like topic models are largely based
on the “bag-of-words” (BoW)
assumption.
In BoW models,
collection of text
documents are quantified into a document-
term matrix (DTM) that
counts the occurrence of each word
(columns) for each document (rows). In the case of most topic
models like LDA, the DTM is one of two model inputs (along
with the number of topics).
The BoW approach provides
two key advantages
(sim-
plicity,
statistical
properties) at
the expense of ignoring word
arXiv:1803.11045v2 [cs.CL] 3 Apr 2018
order. For example, the BoW approach reduces the information
contained in a collection of text documents into the word and
document
counts.
An implication of
word counts is that
it
ignores word order,
which is opposite of NLP methods that
parse language structure.
Without
accounting for word order,
BoW methods perform poorly on micro-level
problems like
question-and-answer
and others
that
require exact
semantic
meaning. However, for large collections of documents (sample
size), the BoW assumption provides the theoretical foundation
for
a richer
set
of
statistical
methods
(mixture models)
by
the assumption of
exchangeability [9].
Ultimately,
this
ad-
vantage underpins statistical-based methods success in macro,
document-level
summarization problems for
a large enough
collection of inter-related documents.
An early motivation for
topic models
was
the goal
of
dimensionality reduction of the document-term matrix for large
collections of documents or corpora. For example, Deerwester
et
al.
(1990)
presented one of
the first
predecessor
models
(latent
semantic indexing,
or LSI) by applying singular value
decomposition (SVD),
a linear algebra dimensionality reduc-
tion technique,
to reduce the document-term matrix to latent
factors. Their goal was to identify broad semantic (correlation)
structure within the documents by removing noise from unin-
formative factors [11].
Later,
Landauser
and Dumais (1997)
extended the LSI model to create the latent semantic analysis
(LSA) model [12].
Further,
these methods could be improved
by substituting the term-frequency inverse-document frequen-
cies (TF-IDF)
weightings in place of
the raw term counts.
However,
Hofmann (2001)
identified two major
drawbacks
to LSA.
First,
the approach lacked theoretical
foundation as
the method (SVD) relied on a Gaussian noise assumption that
could not be justified for word counts (document-term matrix).
Second, LSA could not account for polysemy, the multiple uses
of words in different contexts [13]. To address these problems,
Hoffman (2001) introduced probabilistic latent semantic index
(pLSI) model through the addition of a probabilistic (mixture)
component
to the LSA model
by assuming each word is
generated by a word probability distribution interpreted to be
a “topic”.
1
These seminal
models set
the stage for Blei
et
al.
(2003)
to extend these predecessor
models to build the workhorse
model
LDA [9].
The key contribution of
LDA was
to ex-
tend Hofmann’s pLSI model
to include a second probability
(mixture)
component
for
the document-level,
thus assuming
documents
are
a
mixture
of
topics.
This
addition yielded
a two-tiered model,
a core component
of
the typical
topic
model
framework,
in which observed words are assumed to
be
generated by the
joint-probability of
two mixtures.
At
the top,
documents are a mixture of
topics.
At
the bottom,
topics are also a mixture of words.
Each topic is defined as a
unique distribution of words and yields the algorithms second
output: the word-topic matrix. The word-topic matrix provides
a conditional
probability for
every word (row)
given each
hidden topic (column). Using these probability distributions, a
researcher can rank-order any word by each topic to determine
what
is
the
most
common word the
author(s)
use
when
1
Alternatively, Ding et al. (2008) show a different but related methodology,
Non-negative Matrix Factorization (NMF),
is theoretically identical
to pLSI
as both approaches are maximizing the same objective function.
The only
difference between the two methods
is
that
each approach differs
in its
inference method [14].
referring to each topic.
2
Similar to the idea of singular value
decomposition (SVD) used in earlier topic models like latent
semantic analysis (LSA),
the probabilistic (mixture) nature of
LDA acts
similarly as
dimensionality reduction process
by
reducing the information about each document from the large
number
of
columns
(words)
to a much smaller
number
of
columns (topics) [15].
One consequence of the introduction of the mixture com-
ponents within the LDA-based framework was the problem of
“intractability” [1]. In fact, like many other Bayesian methods,
the introduction of the mixture components allowed, in theory,
the measurement
of
the latent
variable of
topics but
at
the
expense of the ability to measure precisely the optimal model
because the exponentially large potential
solutions
of
topic
values.
This problem lead to the question:
what
is the best
approach to compute topic models?
B.
Computational Methods
Following the introduction of
LDA,
a major
theme in
topic modeling literature was on computational
methods for
topic models given the problem of intractability of computing
the evidence,
or the marginal
probability of observations [1],
[16]. Without an analytical solution, the goal of computational
methods for topic model inference is to find the most (compu-
tational) efficient method that also best approximates the pos-
terior.
In general,
there are two common approaches for topic
model inference: sampling-based methods (e.g., MCMC/Gibbs
Sampling)
and variational
inference.
Sampling-based algo-
rithms simulate samples of
the posterior
to approximate the
true posterior.
Gibbs sampling,
the most
common sampling
method, was introduced for topic model inference by Griffiths
and Steyvers (2004) and uses a Markov Chain to estimate a
sequence of
dependent
random variables that
asymptotically
serves as the posterior distribution [17]. Ultimately, sampling-
based methods like Gibbs sampling have the advantages that
they are (1) theoretically backed, (2) unbiased, and (3) compu-
tationally convenient to implement [17]. The downside of this
approach is that
it
can be very slow for large inputs (number
of documents,
words or topics).
As an alternative to approximating the posterior
through
sampling, variational inference methods transform the problem
into an optimization problem.
In this
context,
the goal
is
to find families
of
distributions
over
the
hidden variables
(topics)
that
most
closely estimates the actual
posterior
[1].
In other words,
variational
inference attempts to most
tightly
estimate the posterior with a simpler distribution that includes
free variational parameters used as the optimization arguments
[16].
Blei
et
al.
(2003)
introduce
variational
methods
for
topic model
inference by using an Expectation Maximization
(EM) algorithm.
Figure 1 provides the two graphical models,
the
left
the
theoretical
model
for
LDA and the
right
the
simplified model
used in the variational
inference algorithm.
To use variational inference for LDA,
the model is simplified
by removing the edges between
θ
,
w,
and z variables and
introducing the free variational
parameters
γ
and
φ
[9].
The
optimization problem uses Kullback-Leibler (KL) divergence
to best minimize the estimated posterior to the true posterior.
2
As will be discussed in Section 3, normally, the highest probability words
conditioned on each topic serve to aid the researcher in the interpretation of
each topic.
Fig.
1.
(Left)
Graphical
model
for
latent
Dirichlet
allocation (LDA)
and
(Right) a graphical
model
of the variational
distribution used for variational
inference for LDA from Blei et al.
(2003)
The EM algorithm is used as a two step process in which the
variational distribution is estimated with estimated parameters
and then the new variational
parameters are chosen through
the variational inference optimization problem. The process is
repeated until a convergence threshold is met.
Hoffman et al.
(2010) extended the variational inference to introduce a faster
online batch algorithm that
can be used to massively scale
LDA for very large corpora or streaming data [18].
Ultimately,
given that sampling-based or variational infer-
ence methods are estimates and never
ever
exact
solutions,
neither method is perfect and the decision of each depends on
the trades off
of
speed,
complexity,
accuracy and simplicity
required for the problem at hand [16].
In the next section,
I’ll
introduce how the basic LDA framework has been generalized
to include document
metadata variables within the algorithm
which thus require modifications to the computational methods
as well. Later in Section 4, I’ll discuss how such metadata vari-
able extensions has an impact
on the computational
methods
possible when introducing the structural topic model (STM).
C.
Model Extensions for Document Metadata
A second theme in topic modeling literature deals with the
inclusion of metadata variables into the model. One of the first
examples of such a model is the author-topic model proposed
by Rosen-Zvi
et
al.
(2004).
The original
motivation was the
explicit point that who the author is will have a direct impact
on what topics are discussed in a publication (e.g., a biologist
will
more likely write about
topics in biology than sociology
or politics).
In contrast,
LDA does not
take into account
any
document
variables (like author) and thus fails to incorporate
author
into the model.
Using only LDA,
the only way to
analyze the impact
of
author
“post-hoc” was by comparing
how the model outputs (topics) compare relative to author.
A
noted downside of this approach is that the model would likely
be less effective as its omitting a known variable that
affects
the topic proportions.
To address this problem,
Rosen-Zvi
et
al. (2004) introduced the author-topic model to incorporate the
author attribute by modifying LDA’s assumption that
author,
not
documents,
are a multinomial
distribution over the topics
[19].
Soon after,
many metadata topic model extensions were
created for a variety of metadata attributes like time (dynamic
topic model [20]), geography (geographical topic model [21]),
and emotion (emotion topic model [22]).
Given the large collection of
metadata topic model
ex-
tensions,
Mimno and McCallum (2009) categorized metadata
extension models into two groups: down-stream and up-stream
models.
The key difference between each approach is on the
role the metadata variables take in the process to generate the
text.
For instance,
in the down-stream approach,
the metadata
Fig.
2.
Example of
an down-stream model
extension from Mimno and
McCallum (2008)
Fig. 3.
Example of an up-stream model extension from Mimno and McCallum
(2008)
(like the text itself) is assumed to be generated by the hidden
topics.
In this approach,
topics are word distributions as well
as distributions over the metadata variables. Figure 2 provides
an example plate notation for
a down-stream model.
In this
figure,
the metadata variables (m) and the words in the text
(w) are conditionally generated by the hidden topics (z).
The
most common example of this approach is the supervised latent
Dirichlet allocation (sLDA) model [24].
Whereas in the up-stream approach, the algorithm is condi-
tioned on the metadata covariates such that the document-topic
distributions are mixtures of the covariate-specific distribution
[32].
The classic example of
this
approach is
the author-
topic model or dynamic topic model. Essentially, the up-stream
models “learn an assignment
of the words in each document
to one of a set of entities” [32].
In addition to incorporating metadata into the model, other
topic model
extensions
include additional
information like
word context,
graphical (network) relationships and hierarchi-
cal
topic structure.
For
example,
the Hidden Markov Model
(HMM) extended the normal
BoW assumption to facilitate a
consideration of word context into the topic model framework
[34].
In the case of network data,
the link-topic model
[35]
and the relational topic model (RTM) [36] are two additional
topic models that incorporate relational based information into
the
model
for
further
analysis.
Finally,
Teh et
al.
(2006)
introduce a generalized hierarchical structure to the topics that
alleviates
the problem of
the number
of
topics
and allows
analysis
from different
topic levels
[25].
Nevertheless,
one
of
the
main advantages
of
topic
models
is
its
flexibility
to incorporate multiple different
computational
methods and
alternative specifications.
In the next
section,
I’ll
move on
to more practical
considerations
of
how to use and apply
topic models before introducing social science applications in
Section 4.
III.
T
OOLS FOR
M
ODEL
A
PPLICATION
: I
NTERPRETATION
,
M
ODEL
S
ELECTION AND
V
ISUALIZATION
In this
section,
I
review research that
has
focused on
pre-processing, interpretation, evaluation and analysis of topic
models.
While not necessarily changing the underlying model
framework like discussed in Section 2, this research is critical
for the implementation of topic models and their application
for social science research.
A.
Pre-processing
A critical step to analyze text is the process of quantifying
text. This process, called pre-processing, is a series of decisions
a
researcher
makes
to clean and normalize
text
with the
goal
of
removing potential
noise to maximize her
analysis
on underlying signal.
While many researchers overlook these
steps, relying instead of default rules without questioning their
merits,
recent
research has found that
text
analysis methods
like topic modeling are susceptible to potential “forking paths”
that leave results subject to initial coding decisions [26], [27],
[29].
Moreover,
there is no set
rules for what
pre-processing
steps are necessary and the need is defined by the quality,
quantity and style of the underlying text.
B.
Model Selection: Prediction-Interpretability Trade-off
The question of
how to select
and validate topic model
specifications (e.g.
number of topics,
model
framework,
etc.)
depends on the researcher’s objective.
In predictive modeling,
a researcher’s
goal
is
to build a model
that
can best
pre-
dict
out-of-sample or
future documents using log-likelihood
(perplexity)
measures.
Whereas
for
researchers
whose goal
is
exploration and knowledge
discovery,
human judgment
(e.g.
interpretable topics)
may take precedence over
holdout
prediction.
Initially in the topic model
literature,
prediction accuracy
was the key goal
of model
evaluation.
Wallach et
al.
(2009a)
outlined different
evaluation methods on LDA using Gibbs
sampling.
They consider
two approaches to evaluating topic
models:
maximizing the held-out
documents likelihood (per-
plexity) and document
completion in which long documents
are trained on part
of
the document
and evaluated on the
model’s ability to correctly “complete” the document.
They
find that
methods like harmonic mean,
importance sampling
and document completion methods are inaccurate and may dis-
tort the relative advantage of one model versus another model.
Instead, they recommend either the Chib-style estimator or the
“left-to-right” algorithm as more accurate evaluation methods
[40].
3
However,
Chang el
al.
(2009)
explored the trade-off
be-
tween prediction and interpretability.
Through the word intru-
sion tasks,
they found the counter-intuitive result
that
highly
predictive topics tend to be negatively correlated with inter-
pretability.
Semantic coherence was introduced by Mimno et
al. (2011) as a measure for how internally consistent words are
within topics.
On the other hand,
exclusivity is a measure to
identify words that have high probabilities for only a few topics
rather than many topics. Roberts et al. argue that a “topic that
is both cohesive and exclusive is more likely to be semantically
useful” [8].
Finally,
other researchers have studied the effectiveness of
topic modeling while controlling for
other
key factors (e.g.
3
Two limitations to Wallach et
al.
(2009a) is that
these results were only
tested using Gibbs sampling-based inference and on vanilla LDA.
hyperparameters,
number
of
topics,
document
sample size,
document
length).
Wallach et
al.
(2009b) explored the effect
of
relaxing LDA’s
prior
distribution assumptions,
including
using non-symmetric Dirichlet
parameters for the document-
topic or word-topic matrices. They find that asymmetric priors
on the
document-topic
distribution can provide
substantial
advantages
while
asymmetric
priors
do not
provide
much
benefit
to the word-topic distribution.
Taddy (2012) explored
estimation methods for choosing the optimal number of topics
via block-diagonal
approximations to the information matrix
and goodness-of-fit analysis for likelihood-based model selec-
tion [42].
In a more systematic review of topic model
performance,
Tang et al.
(2014) analyzed LDA performance controlling for
four limiting factors: document length,
number of documents,
and the two prior
distribution hyperparameters.
Considering
two simulated and three real
datasets (Wikipedia,
New York
Times,
and Twitter),
they make five recommendations.
First,
they argue that
a sufficient
number of documents is the most
important
factor
to ensure accurate inference.
For
example,
they find LDA is
more difficult
when running on a small
sample (e.g.
less than a thousand documents).
Second,
they
find the length of the document matters as well. This matters in
social media data like Twitter messages in which all messages
are less than 140 characters.
As an alternative,
they cite alter-
natives like aggregating messages to transform the documents
to a user-level
to expand the size of documents [44].
Third,
they find that collections with too many topics lend statistical
inference methods to be inefficient. Fourth, they find that LDA
performance is affected by how well-separated the underlying
topics are relative to a Euclidean measure. Last, they find that
the variability of hyperparameters is important
depending on
the number
of
topics within documents.
For
example,
they
recommend using a lower
alpha (Dirichlet
parameter)
when
documents have few topics,
whereas using a high alpha for
documents with many topics.
C.
Visualizations
In this section,
I review semi-automation methods of an-
alyzing topic models through visualizations.
Hu et
al.
(2013)
argue that topic models suffer from an interpretation problem
that
requires the need for
interactive system for
end users.
Topic results are never
perfect
and often include bad topics
that
do not
perfectly align with an end-user’s judgment
and
intuition (similar to the argument
by Chang et
al.
2009).
To
address this problem, they argue for systems that allow the end
user
to annotate the model
results and incorporate feedback
into the model’s output. Moreover, they identify social science
(in addition to digital
humanities
and information studies)
as
a discipline that
would greatly benefit
from interactive
systems when implementing topic models.
They emphasize
social
science as such a field because of its “take it
or leave
it” problem because many social
scientists
“have extensive
domain knowledge but lack the machine learning expertise to
modify topic model algorithms” [45].
A shortcoming of their
argument
is they omit
the role that
visualizations can play
within such interactive systems. Further, they fail to recognize
the body of
research by the visualization community that
has
extended and proposed many different
applications
for
visualizations to fit into such interactive systems.
Dou and Liu (2016)
argue that
visual
interfaces
allow
decision makers to explore and analyze the model results. This
point
is more important
when considering the application of
topic models for non-computer scientists (like most social sci-
entists) who may not have the programming or computational
training to run the algorithms on their own. A major motivation
for the use of visualizations for analyzing topic models is that
the output
is too large for
a researcher
to absorb manually
[46].
For
example,
LDA’s output
is two large datasets (the
word-topic and the document-topic matrices),
the size of both
are proportional to the number of documents, terms and topics.
Therefore,
the larger the corpus,
the larger the output and the
more difficult it is for a researcher to analyze the results.
In general,
there are two common approaches
to visu-
alizing topic models:
topic-oriented and time-oriented [46].
Each approach differs based on which is the most
important
element
of interest.
In topic-oriented visualizations,
the focus
is
on the relationship between either
the words
and topics
(word-topic matrix)
or
the document
and topics (document-
topic matrix). Such approaches focus on the task of document
summarization, information retrieval and relationships between
documents.
Common examples of
these approaches include
matrix representations
like Termite [47]
and Serendip [48]
(see Figure 4)
as well
as parallel
coordinates visualizations
as
in ParallelTopics
[49].
Chuang et
al.
(2012)
provide a
general design framework for topic-oriented interactive visual
systems
based on how an analyst
makes
inference on the
topics (interpretation) and the actual
and perceived accuracy
of
the analyst’s inference (trust)
[50].
Other
interfaces like
HierarchicalTopics have generalized the model and facilitated
interfaces that focus on a hierarchical structure within the top-
ics that can aid in drill down on multiple levels for document
summarization [51],
[52].
Moreover,
new research has used
(network) graphs to represent the correlations between topics,
especially when coupled with models with a more flexible
correlation structure like CTM [53].
Fig. 4.
Topic-oriented visualizations including (Left)the Termite model from
Chuang et
al.
(2012) and (Right) the Serendip model
from Alexander et
al.
(2014)
On the other hand,
documents that
are time-oriented (e.g.
Twitter
messages or
news articles)
can be aided with time-
dependent
visualizations that
can aid in exploring the trend,
evolution,
lead-lag effect
and event-detection relative to the
topics. TIARA is an interface created to visualize topical trends
by using an enhanced stacked graph [54],
[55].
Similarly,
TextFlow was
introduced with the
goal
of
exploring the
evolution of
topics
including identifying how topics
merge
and split
over
time
[56].
Further,
TextPioneer
is
a
visual
interface that introduces the problem of lead-lag relationships
in exploring topic results
[57].
Lead-lag is
the problem in
which a researcher
needs to understand the relationship be-
tween two corpora,
especially in the case when one corpus
(e.g.
social
media)
may lead the information that
may then
appear
in another
corpus
(e.g.
news
articles
that
recount
information spread through social media).
Last,
another time-
oriented component that has been explored in visual interfaces
is event-detection and analysis.
For
example,
LeadLine is a
visual
analysis
system used to identify and explore events
by detecting the most
common words (topics) used in short,
discrete bursts [58].
Last,
another major consideration in the use of visual
in-
terfaces for topic models includes the type of data used within
the model.
The simplest approach is when homogeneous data
is used, and thus the focus is on a single corpus and the topics
that stem from the corpus.
However,
much deeper insight can
be found with the inclusion of
heterogeneous
data sources
that
append document
metadata to the corpus.
One example
of
a recent
approach to combine such sources
include the
TopicPanorama interface that combines text from multiple data
sources (e.g. news articles and Twitter messages) and provides
a network graph to link across these sources (see Figure 5.
Further,
another avenue of topic analysis includes the impact
of analyzing topics within streaming data sources like Twitter
or other social media platforms [59].
Fig.
5.
The integration of multiple corpora within TopicPanorama from Liu
et al.
(2016)
D.
LDA-based Topic Model Applications in Social Sciences
One of the first applications of topic models within social
science literature comes from Quinn et al.
(2010).
This paper
laid the foundation for social science (mainly political science)
application of
topic models in three ways.
First,
the paper
directly compares topic model
relative to other text
analysis
methods
(reading,
human coding,
dictionary-based and su-
pervised learning),
comparing and contrasting the costs and
benefits of each method.
4
Second, Quinn et al. set out a list of
five criterion-based concepts (goals) based on more traditional
social science content analysis to be used for model evaluation
that,
they argue,
should be considered when applying topic
models.
5
Last,
the authors modified LDA for
their
analysis
by using a continuous time model
that
limited the number
4
As mentioned in the introduction of this paper, this cost-benefit comparison
justified the benefit of computer-assisted text analysis tools like topic models
relative traditional
text
analysis research like human coding and dictionary-
based methods [2].
5
The five criterion are:
semantic validity,
convergent
construct
validity,
discriminant
construct
validity,
predictive validity,
and hypothesis
validity.
Semantic validity is the coherent
meaning from interpreting the word-topic
probabilities.
Convergent construct validity is the extent that the results align
to existing measures or benchmarks of known truths.
Discriminant
construct
validity measures how the results depart from the same existing benchmarks.
Predictive validity is the ability that the results can correctly predict external
events.
Last,
hypothesis validity is the extent
to which the results can be
effectively used to test hypotheses [2].
Fig.
6.
Comparison of LDA,
Dynamic Multitopic Model and Expressed Agenda Model from Grimmer and Stewart (2013)
of
topics
for
each speech to only one (as
opposed to the
mixture assumed in LDA).
The model is named the Dynamic
Multitopic Model and is represented in Figure 5. Unlike LDA,
the model
is a single-membership mixture model
in which
each speech was assigned to a unique topic,
and each day
was assumed to be a mixture of speeches. As shown in Figure
6,
this can be analogous to how LDA assigns each word to a
topic, then assumes each document is a mixture of those topics.
This is important as it was one of the first social science-based
topic model modifications created to test a theoretically driven
hypothesis for social science research.
Similarly, Grimmer (2010) created a modified topic model,
the Expressed Agenda model,
to analyze political
rhetoric
by individual
senators through their press releases [60].
Also
shown in Figure 6, the Expressed Agenda model modified the
LDA framework into a single membership mixture model and
assumed each press release was assigned to a single topic,
while each senator’s press releases corpus were assumed to be
a mixture of those topics. Like the Dynamic Multitopic Model,
the units of measurement (e.g. topic assignment and document-
level) were modified from standard LDA to allow the author
to test
a political
theory on how a senator divides his or her
attention across the multiple political issues as represented by
their explicit press releases [60].
Fig. 7.
Monthly allergy prevalence for Tweets using four methods and Gallup
survey results from Paul and Dredze (2014)
Other
social
scientists
have
considered using LDA to
analyze text
from social
media (in particular
Twitter).
One
novel
example comes from health informatics by Paul
and
Dredze (2014).
Using Twitter
messages,
the authors modify
LDA to create the Ailment Topic Aspect Model (ATAM) with
the goal of identifying health related keywords automatically.
The authors
discover
13 interpretable topics
(e.g.,
seasonal
flu, allergies, exercise,
and obesity) that correlate significantly
with U.S.
geographic survey data [61].
Figure 7 provides an
except of their findings by comparing the normalized frequency
(z-score)
trends
of
external
survey results
on allergies
and
four
approaches
to identify allergy-related Tweets
(by key-
words “allergy” and “allergies”,
LDA and ATAM).
The key
methodological
contribution of this model
was the inclusion
of external
“background words” that
represent
health aspects
that the model will identify as topics that describe the aspect.
Further,
the authors
explore their
output
topics
relative to
document-level
metadata regarding the time and geography
of
the Tweet.
A limitation of
their
approach is that,
while
the results correlate with external
(Gallup) benchmarks,
their
work does
not
demonstrate with statistical
significance the
relationship between Twitter
topics and its effect
on public
awareness (via the Gallup results).
At
its core,
this approach
lacks a causal inference mechanism to explain the significance
of the discussions on Twitter.
Fig.
8.
Differences in the estimated responsiveness of different
constituent
follower groups using F-tests from Barbera et al.
(2014)
One application of LDA on Twitter that attempts to provide
a causal inference mechanism comes from political science by
Barbera et al.
(2014).
In their paper,
the authors analyzed the
responsiveness to members of the United States Congress to
constituent conversations on Twitter. In order to categorize the
topics,
they used LDA on the tweets of both constituents and
the members of Congress.
Next,
they measured the variance
between the topics over time to estimate whether members of
Congress lead or follow their constituents on political issues by
employing Granger-causality testing after running LDA [62].
The key contribution of this paper was the use of a temporal
causal
framework (Granger
causality)
along with LDA to
provide a higher
level
statistical
significance for
the effect
of
covariates (author
and time).
To analyze this result,
they
divided the members of Congress and their constituents into
six groups, three groups per political party (Democrat and Re-
publican):
members of Congress,
“hardcore” constituents and
“uninterested” constituents.
Also,
they categorized the topics
from LDA into four categories: Democrat-owned, Republican-
owned,
non-political
and political
topics.
Then they ran 24
“post-hoc” regressions (four
topics x six groups)
using five
day lags of
topic proportions for
each of
the six groups as
the 50 independent variables. Figure 8 provides standard F-test
results by topic (rows), columns (group) and the relationship.
6
.
They find that
members of
Congress are responsive to their
constituents,
especially prominent
issues by their “hardcore”
constituents. However, they find little evidence of that members
of Congress have influence on what
topics their constituents
discuss publicly [62].
While
being a
novel
contribution to estimating causal
effects
with LDA,
their
approach has
two limiting factors.
First,
to analyze author and time,
they did not directly model.
Instead,
employing the “aggregation” approach suggested by
[44],
they combined Tweets by author and day to modify the
definition of
a document
from a tweet
to the collection of
all
tweets
in a day by each author.
By doing aggregation,
this enables them to control
for
the document
covariates of
time and author but
still
employing LDA that
does not
have
a mechanism to directly control
for these covariates.
Second,
they use regression “post-hoc” and not
within the generative
topic model itself.
As noted in the appendix of Roberts et al.
(2014), the problem with this approach is the measurement of
uncertainty that can lead to spurious results [8].
7
A general theme of these applications represent the flexibil-
ity of how LDA-based framework can be modified to address
a unique theoretical question for a specific document-level co-
variate (e.g.
time,
author,
geography).
However,
Grimmer and
Stewart
(2013)
recognized that
requiring all
social
scientists
to “tune each model to their task is a daunting task”.
Further,
each provides the motivation for deriving a causal
inference
component
within the LDA framework by asking questions
involving document covariates that facilitate hypothesis testing.
In response to these problems, Roberts et al. (2013) introduced
structural
topic model
(STM)
as a general
causal
inference
framework for hypothesis testing for document covariates.
IV.
S
TRUCTURAL
T
OPIC
M
ODEL
& M
ULTI
-M
ODALITY
In this section,
I
review literature by social
scientists to
reconcile two major problems with the standard topic model
framework: the lack of causal inference and multi-modality.
6
The “F
→
MC” column represents F-test
for
measuring the followers
impact
on the member
of
Congress’
topic proportions while the “MC
→
F” column is the opposite (i.e.,
a member
of
Congress’
impact
on his/her
followers’ topic proportions)
7
Roberts et al.
(2014) call this approach two-stage in which in LDA is run
(stage 1) followed by running a regression for topic proportions conditioned
on a covariate of interest (stage 2).
The first
issue I
address is that
standard (down-stream)
metadata
topic
models
(e.g.
Author-Topic,
Dynamic,
etc.)
make point but not standard error estimates to facilitate statisti-
cal hypothesis testing. The next problem is that computational
methods for topic model inference, as it is an NP-hard problem
[68],
[69],
can provide local
optima but
cannot
guarantee
global
optima,
which is termed multi-modality.
This problem
threatens the stability of
a topic model
output
and can lead
researchers to question whether they “did not
stumble across
the result completely by chance” [69].
A.
STM Model
The first
major critique of topic models is that
its output
provides point estimate of word or topic probabilities without
confidence intervals that facilitate statistical hypothesis testing.
This problem is especially important
in down-stream exten-
sions that
include metadata that
affect
the topic proportions.
For example, researchers could estimate that a topic proportion
is different for various covariate levels,
but without statistical
confidence provided by standard error estimates. This problem
is directly contrary to the tradition of
causal
inference that
employs statistical confidence within the social sciences to de-
termine causation (for example [63]).
In an effort to reconcile
this problem,
Roberts et
al.
(2013) and Roberts et
al.
(2014)
introduced the structural topic model (STM) by incorporating
a generalized linear
model
(GLM)
framework for
document
metadata by extending elements of three previous topic model
extensions: CTM,
DMR and SAGE.
Let’s first
start
with CTM.
Blei
and Lafferty (2007)
in-
troduce the correlated topic model
(CTM)
to provide more
realism to the original LDA model which incorporates a more
flexible correlation structure than the independence assumption
assumed in LDA. The model replaces the Dirichlet assumption
for topic proportions as used in LDA with a logistic normal
distribution. The main advantage of the CTM versus the LDA
is improved predictive power:
“A Dirichlet-based model
will
predict
items based on the latent
topics that
the observations
suggest,
but
the
CTM will
predict
items
associated with
additional
topics
that
are correlated with the conditionally
probable topics” [23].
However,
like many other topic model
extensions,
relax-
ation of
model
assumptions comes at
the expense of
model
complexity and even intractability for existing methods. In this
case, simulation techniques like Gibbs sampling are no longer
possible
as
Markov-Chain Monte
Carlo (MCMC)
process
(Metropolis-Hastings) become untenable given their size and
scale [23].
8
As an alternative,
Blei
and Lafferty introduce
a fast
variational
inference that
approximates posterior distri-
butions for CTM.
This approach underpins the computational
framework for STM and STM can be thought
of as identical
to CTM when no metadata covariates are included.
The
other
two
models
(Dirichlet-multinomial
model
(DMR) and sparse additive generative model (SAGE) ) provide
the framework for introducing document metadata covariates.
First,
the DMR model applies to the introduction of metadata
covariates that
can affect
the topic proportions.
The model
8
As noted earlier, simulation approaches are ideal for topic models because
they are (1) theoretically backed; (2) unbiased; (3) computationally convenient
[23].
Fig.
9.
Covariate inference for
LDA and STM on simulated datasets by
Roberts et al.
(2016)
replaces LDA’s assumption of a Dirichlet
prior for the topic
distribution with a Dirichlet-Multinomial
regression for
the
given covariates [32].
On the other
hand,
researchers found
that the same approach (Dirichlet-multinomial regression) was
not feasible for the word distributions. Eisenstein et al. (2009)
identify three main problems
when applying the Dirichlet-
multinomial
framework to the word distribution:
the increase
in parameters,
the computational
complexity and the lack of
sparsity.
This is especially a problem when considering the
word-topic relationship for a large corpus of documents can
have an extensive vocabulary [33].
To address this problem,
they introduce the Sparse Additive Generative Model (SAGE).
The SAGE consists of an alternative framework that uses devi-
ations in log-frequency from a benchmark distribution.
There
are two main advantages of this model.
By cutting down on
the number of parameters, this approach (1) reduces overfitting
issues and (2) can combine generative facets through simple
addition in log space,
avoiding the need for latent
switching
variables [33].
Given the inclusion of these predecessor models, its impor-
tant to review the terminology to distinguish between the two
types of covariates used in the model: prevalence and content.
Prevalence covariates are document-level attributes that affect
which topics are communicated in a document. In other words,
prevalence covariates
impact
the topic proportions
through
the DMR model.
On the other
hand,
a content
covariate is
a
document-level
attribute
that
affects
how the
topics
are
conveyed in a document.
In this
case,
a content
covariate
modifies which words are used to communicate a topic and
thus
the word proportion that
define each topic.
Similarly,
content covariate is used in the SAGE component of the STM
model.
9
For
STM inference,
like its predecessor
models the ex-
act
posterior
is intractable which restricts the computational
methods
possible.
To address
intractability,
Roberts
et
al.
(2016) introduce a partially collapsed variational Expectation-
Maximization algorithm for
inference
estimation.
Another
problem with the
inference
estimation with this
model
is
the non-conjugacy of
the logistic normal
distribution,
which
replaces the Dirichlet
prior distribution assumed in LDA,
to
the posterior
distribution (multinomial).
To account
for
the
non-conjugacy,
they also introduce a Laplace approximation
for the non-conjugate elements of the model.
9
The current STM computation only allows one content covariate. However,
this covariate can have multiple levels and thus a researcher can approximate
this with multiple variables by using the interaction between all of the levels of
the variables. For example, instead of having two binary content covariates for
gender (male or female) and treatment (yes or no),
this can be approximated
with four levels of one covariate (male-yes,
male-no,
female-yes,
female-no).
Fig. 10.
LDA, STM, SAGE and DMR out-of-sample performance by Roberts
et al.
(2016)
Roberts et
al.
(2016)
analyze the estimation benefits of
STM relative to LDA and STMs sub-component
models like
CTM, SAGE and DMR. They find that for metadata-generated
topic processes, STM outperforms LDA in covariate inference
and out-of-sample prediction [10].
Figure 9 provides the per-
formance of
LDA and STM on 50 samples of
a simulated
dataset (including a random component) that is generated along
with a continuous, non-linear covariate. LDA was, on average,
able to identify the non-linear
pattern of
the covariate with
the topic proportion
10
;
however,
due to the noise component,
LDA was
not
robust
to consistently identify the true non-
linear
pattern.
For
example,
there were cases in which the
model’s
estimates
inferred a
reverse
pattern than the
true
shape value.
On the other
hand,
STM was able to correctly
identify the pattern consistently across all
50 datasets.
The
conclusion is by incorporating the covariates directly into the
algorithm (rather
than through a post-hoc analysis),
we can
be confident
that
STM will
consistently detect
the pattern
while on average LDA can but
not
always.
Ultimately,
the
improved performance of
measuring covariate relationships
yield the model
to consistently out
perform (post-hoc) LDA
in out-of-sample prediction.
Figure 10 is an analysis by [10]
on the same dataset
but
using LDA,
SAGE,
DMR and STM
model
separately.
They find that
STM,
largely driven by the
influence of DMR, had the best performance while SAGE and
LDA performed worse across all candidate sets for a different
number of topics.
There
are
three
major
advantages
to the
STM model.
First,
the model
facilitates a statistical-based framework for
facilitating hypothesis testing for the causal
impact
of docu-
ment metadata that affects which and how the topics vary by
document.
In fact,
the model
allows for general
relationship
framework outside of typical linear relationships in which re-
searchers can test for non-linear patterns through log, spline or
interaction terms. Second, the model introduces enhancements
to the computational
methods
in order
to make the model
feasible for modeling as well as methods for model evaluation,
interpretation and handling multi-modality.
Last,
the authors
introduce an open-source R package to accompany the paper
[64]. This package facilitates the implementation of the model
to a much wider audience (e.g.
social scientists) by providing
the model
in a high-level
(R) rather than the traditional
low-
level
languages most
topic models methods have previously
been available (e.g.
Java in Mallet
[65],
Python for
Gensim
[66]).
10
STM is able to model
non-linear patterns through its extension to allow
spline transformations and covariate interactions.
Fig.
11.
Posterior predictive checks (PPC) using instantaneous mutual information from Roberts et al.
(2016)
Despite these advancements, STM has multiple limitations.
First,
the model
remains
intractable for
a large number
of
covariates or
more than one content
covariate.
Second,
the
model
can produce statistical
testing for prevalent
covariates
but
not
content
covariates.
Third,
given topic proportions’
zero sum property (i.e.,
must
sum to one),
interpretation of
covariates’ marginal effects in STM are difficult as an increase
in one topic proportion must
be tied to a decrease in similar
magnitude to the other topic proportions [67]. Last, the model
uses are approximation methods that
are more complex than
simulation-based (e.g.
MCMC or Gibbs Sampling) methods.
Last,
as the model
is more complex than LDA,
so too is its
output
as it
now must
account
for
differences in the input
covariates.
With this last
point
in mind,
I’ll
consider the role
that
explainable interactive visualizations can play in future
research for STM in the conclusion of this paper.
B.
Multi-Modality
The
second problem with topic
models
is
its
lack of
stability due to its inherent
computational
complexity.
Topic
model inference is a NP-hard problem [68], [69]. This hardness
leads to the problem of multi-modality,
i.e.,
an optimization
problem (like maximum likelihood) can be solved locally but
cannot,
with certainty,
be solved globally.
11
Multi-modality
is
important
when an algorithm’s
output
can change with
initialization parameters.
In other
words,
different
starting
parameters can alter results and threatened the legitimacy of
the approach and results.
12
While this implication cannot
be
directly solved (as its a product
of the algorithms hardness),
they argue the solution for
researchers
is
to use improved
initialization (spectral) methods and posterior predictive (sta-
bility)
checks [70]
to achieve the best
of
all
possible local
optima.
Further,
Chuang et
al.
(2015) provide an example of
how a visualization interface can help researchers understand
the effect of multi-modality on the stability of their results.
A good initialization approach needs to balance the trade-
off between the computational cost of implementation and the
11
Roberts et
al.
(2015) write in a footnote that
connecting NP-hard com-
plexity and multi-modality (local modes) is difficult to state easily. They argue
that hardness is sufficient to prove an algorithm is not easily solved with global
convergence.
12
Interestingly,
Roberts et
al.
(2015) note that
such sensitivity to starting
positions is well known by computer scientists yet infrequently discussed.
relative model
improvement
for
optimizing the initial
state
[69].
In the STM model,
a researcher
has
the option to a
spectral
initialization that
provides a quick starting point
that
minimizes
the chance of
finding sub-optimal
local
minima
[69].
One approach for STM is to run standard LDA on the
dataset
and use the LDA results to help determine the initial
state for
STM.
Roberts et
al.
(2015)
find that
using LDA’s
results for initialization not only improves the model’s results
but also (using Gibbs sampling) leads to a faster convergence.
However,
they find that
using LDA for
initialization does
not
help increase
the
average
quality of
the
results
with
more simulations. Instead, they recommend a spectral learning
approach that
provides a more robust
initialization with the
computational complexity of using LDA’s results. The spectral
approach utilizes the connection of
LDA with non-negative
matrix factorization (see [71]) that
provides theoretical
guar-
antees
that
the optimal
parameters
will
be recovered [69].
Essentially,
this approach makes stronger model
assumptions
(matrix decomposition elements must be non-negative) in order
to avoid the problems of
multi-modality.
However,
Roberts
et
al.
(2015)
identify two practical
limitations with spectral
initialization. First, it requires large amounts of data to perform
adequately.
Second,
the modification of the model’s assump-
tions have the potential of leading to less interpretable models.
Posterior predictive checks (PPC) provide insight
on how
well
the model’s assumptions hold [10],
[72].
Figure 11 pro-
vides one such PPC (instantaneous mutual
information from
[72]) for three topics,
each plot representing the top ten most
likely words for each of the three topics.
In these examples,
the model’s assumptions does not hold if there is a significant
difference between what
is observed (the black circle)
and
the simulated reference distribution (the open circles).
For
instance,
in the “SARS/Avian Flu” topic,
the words “sar” and
“bird” are observed to be drastically outside of its reference
distribution.
This indicates that
these words indicate would
most
likely be better
of
split
between two separate topics.
While one deviation like this may not
jeopardize the entire
topic model
results,
the goal
of analyzing PPC is to identify
systematic errors (e.g. caused by a poor local optimal solution
or
initialization)
like
this
across
multiple
topics
that
can
threaten the legitimacy of the model as a whole [72].
Last,
Chuang et
al.
(2015) provide an interactive solution
to the problem of multi-modality: TopicCheck, a visualization
Fig.
12.
TopicCheck with 50 Iterations of a 20 Topic STM to assess topic
stability from Chuang et al.
(2015)
interface to assess the stability of topics across multiple runs
of STM [73]. Figure 12 shows TopicCheck for 50 iterations of
a 20 topic STM for a dataset
of 13,250 political
blogs.
Each
rectangle is a topic, with each column being one run of STM.
Topics are aligned across each STM iteration into horizontal
groups or
rows.
Topics that
are not
aligned to other
topics
(for a different STM run) are included below the baseline “x-
axis”. One of the key goals of this visualization is to determine
how robust (or stable) a topic is for different STM iterations.
For
example,
the top row (including words
like “Barack”,
“Obama”) has topics for all
50 models and thus we can feel
confident this aligned topic is very robust.
On the other hand,
the last
topic row (financial
crisis)
is a topic in only 18 of
the 50 STM iterations.
We can infer that
this topic is not
as
stable as the topics that
have aligned topics in more STM
iterations.
Chuang et
al.
(2015) findings suggest
the need to
consider more than one topic model as one “single topic may
not capture all perspectives on a dataset” [73]. Further, another
contribution of this paper is analyzing the impact of including
or excluding rare-words on the stability of the topics. This is a
novel approach to provide users an interactive understanding of
the impact of a pre-processing step (like sparse word removal)
that they find can potentially have a significant impact on the
final topics.
C.
STM Applications in Social Sciences
Already, STM has been applied to multiple areas of social
science including open-ended surveys, political rhetoric, social
media and massive online open courses (MOOCs).
Roberts et
al.
(2014) provide the first extensive application of STM with
open-ended survey responses.
13
Traditionally, human coding is
the most common methodology for social scientists to analyze
open-ended survey responses.
14
The authors argue STM has
two key advantages.
First,
being an unsupervised algorithm,
STM allows the researcher to “discover topics from the data,
13
Roberts et al. (2013) was the first paper that introduced STM and provided
two simple applications (open-ended surveys and media bias in news articles).
However,
both of these analyses only covered a few paragraphs and did not
provide the full
details of
the model
including model
setup,
selection and
validation.
14
Roberts
et
al.
(2014)
acknowledge that
many survey analyses
simply
ignore open-ended survey responses in favor of closed-ended surveys given
the lack of tools to analyze such results.
rather
than assume them” [8].
Second,
it
provides a formal
way of quantifying the content
and prevalance effects on the
topics, especially with a treatment variable, through a cheaper,
more consistent semi-automated process.
For their application of STM,
they use the ANES (Ameri-
can National Election Studies) survey dataset of 2,323 respon-
dents on questions related to the 2008 election.
In addition to
open-ended survey responses related to the election, the dataset
includes individual
covariates about
the respondent
including
their top voting issue,
party identification,
education and age.
Further,
the survey randomly assigned respondents into test
and control groups in which either group was subjected to dif-
ferent treatment to simulate either intuitive (test) or reflective
(control) thinking. The treatment variable was collected to test
political science theory on the role either intuition or reflection
shapes decision making as demonstrated through their open-
ended responses.
To analyze their results,
the authors compare their finding
with the STM model relative to that of human coders who had
previously analyzed the same dataset. They make three conclu-
sions. First, they find that in aggregate, STM categorized most
of
the same responses into similar
topics as human coders.
Second,
they find that
STM recovers covariate relationships
closely identical
to the ANES human coders.
The first
two
findings suggest that STM can do the job very similar to the hu-
man coders but at a much lower cost through automation rather
than hiring multiple human coders. Last, they do find STM has
the additional
advantage in that
it
required less assumptions
about
the topics themselves as STMs unsupervised approach
naturally found the occurring topics without requiring to know
in advance what topics were likely to be discussed.
However,
they do find one disadvantage to STM is that
low incidence
pre-determined categories are unlikely to be identified in STM
as the unsupervised approach will likely identify these topics.
Instead,
STM may have “writing style” topics using frequent
words like “go” and “get” that
are not
relevant
but
occur in
the unsupervised approach because they have enough volume
and were not used as stop words.
Nevertheless,
they find that
the benefits of STM largely outstrip any such costs relative to
human coding in this example [8].
Another
significant
application of
STM has been by po-
litical
scientists to analyze political
rhetoric.
One of the first
examples come from Milner and Tingley (2015) in which they
analyzed the text within lobbying reports to estimate the impact
of whether the White House was lobbied on that specific issue.
They find that
economic interest
groups tend to have higher
influence in topics that
have high distributional
impacts and
low information asymmetries like military spending. Moreover,
they find in topics
like
military spending,
such economic
interest
groups
are less
likely to directly solicit
the White
House, instead targeting the White House with direct lobbying
efforts for efforts that are more policy focused [74].
A further STM application by political
scientists includes
the measurement
of
media bias
in news
articles
related to
China by five different
international
media outlets.
15
Using
a dataset
of over 14,000 news articles,
Roberts et
al.
(2016)
analyzed the difference in what (prevalence) and how (content)
15
The five media outlets are Xinhua (China),
BBC (Great
Britain),
JEN
(Japan),
AP (United States) and AFP (France)
Fig.
13.
The most
likely words for a topic (“Falungong”) conditioned on the news source (Left) and the estimated topic proportion for each source (Right)
from Roberts et al.
(2016)
topics related to the rise of China were communicated by dif-
ferent media sources. They found that the Chinese-based media
outlet (XIN) had,
in general,
a more positive view of growth-
related topics
than other
international
media sources
while
covering less and using different language for more controver-
sial topics like Chinese dissidents and protesters. For example,
Figure 13 provides two graphics relating the topic relating to
the Falungong movement,
a Chinese religious group that
the
Chinese government outlawed in 1999 leading to many protests
and a government
crackdown on participants.
The left
side
represents the different
words used to characterize this topic
conditioned on each media source (content covariate). Xinhua
(XIN) used words like “illegal”,
“smuggler”,
and “criminal”
to describe the movement as an illegal operation. On the other,
Western media outlets like the U.S.
Associated Press (AP)
and the French Agence France-Presse (AFP) characterized the
movement
with language characterizing the movement
as a
protest rather than a criminal operation.
For example,
AP and
AFP’s conditioned topic words include “protest”,
“dissident”,
“movement” and “crackdown”.
Moreover,
STM could also
be used to measure the difference in topic proportion each
outlet
used each topic.
In the
example
of
the
Falungong
topic,
the right
side of
Figure 13 shows the estimated topic
proportions (along with confidence intervals) for each of the
five media outlets.
Xinhua covered the Falungong topic much
less frequently than did Western media outlets like AP and
AFP with statistical significance.
In addition to political and news rhetoric, other applications
of
STM by social
scientists include social
media messages
(in particular
Twitter)
and online course feedback.
Lucas et
al.
(2015) analyzed bilingual
social
media messages through
automated machine translation to estimate the effect language
(Arabic or
Chinese)
has
on Twitter
users’
topics
regarding
Edward Snowden [75].
Sachdeva et
al.
(2016)
used STM
to analyze
smoke-related tweets
and the
potential
spatial-
temporal
effects of
wildfires have on users’
tweets relative
to those individuals
who reside or
work close to affected
areas [76].
Reich et
al.
(2015)
analyzed the use of
massive
online (MOOC) course feedback to scale open-ended course
responses.
Reich et al.
(2016) extended this work in MOOCs
and connected it
with political
rhetoric to identify political
discussions
within these
courses.
Ultimately,
given STM’s
recent
introduction,
these examples represent
just
a handful
of the early applications.
As the technique begins to mature
and more researchers learn about the methodology, there is no
doubt
the number and the range of applications will
begin to
increase rapidly in the near future.
V.
C
ONCLUSION AND
F
UTURE
R
ESEARCH
While used rarely by most
social
scientists,
topic mod-
els
offer
social
scientists
an innovative and objective way
to measure
latent
qualities
on large,
unstructured datasets
like social
media,
open-ended surveys and news or research
publications.
Topic models are one of
many machine learn-
ing frameworks that,
when combined with causal
inference
tools,
represent
a significant
opportunity for social
scientists
to answer
many of
society’s
large-scale
problems
[4].
In
particular,
STM represents an example of integrating machine
learning (topic
models)
with causal
inference
mechanisms
in a
generalized framework that
can be
applied to many
social
science problems.
Nevertheless,
a major
impediment
to the expansion of
STM (and machine learning algorithms
in general)
for
most
social
scientists is the high knowledge
barriers to use these models.
Given the quick development of
such models,
many social scientists may lack the training and
experience in machine learning and computational
program-
ming to implement and to analyze topic models. To address this
concern, a potential research opportunity with the STM model
is
through an explainable user
interface (visualization)
that
can aid social scientists in hypothesis testing large collections
of
text
documents.
The
importance
of
this
opportunity is
exemplified through DARPA’s
recent
Explainable Artificial
Intelligence (XAI) program [80].
An inherent
problem with most
machine learning tech-
niques and tasks for decision makers is the lack of explanation
Fig.
14.
Explainable Artificial Intelligence (XAI) from DARPA (2016)
of
why a
specific
prediction or
choice
was
made
by the
algorithm.
Instead,
the goal of the XAI program is to provide
explainable features to traditional machine learning techniques
along with an explainable user interface (e.g., visualization) to
combine human insight
with the predictions of the model
to
answer the question of why.
Structural
topic modeling would
directly fit
under
the
program’s
approach of
interpretable
models that “that learn more structured, interpretable, or causal
models” [80].
16
Figure 14 provides a graphic representing the
role of explainable user interfaces for machine learning tasks
like STM inference.
Therefore,
a key research opportunity for STM is the de-
velopment of an explainable, intelligent interactive system [81]
to analyze for interpretation, model evaluation, multi-modality,
pre-processing steps
17
and validation.
Such an interface could
be built
integrating high level
visualization tools like Shiny
[82] and Vega-lite [83],
[84] that
can provide a robust
set
of
tools with minimal amount of coding. Further, if written in R,
such an interface could easily combine with R packages like
stm for widespread use (see Appendix 1).
18
A
PPENDIX
A
A
PPENDIX
1:
STM
R
PACKAGE
The
R package
stm was
introduced by Roberts
et
al.
(2014b) to facilitate the widespread use of STM for R users
[64].
A key advantage to the stm R package is that
includes
multiple methods of posterior predictive checks, interpretation,
data preprocessing,
model
selection and static visualizations.
Figure 15 provides an outline of the functions within the stm
package categorized by their different functionalities.
A
PPENDIX
B
A
PPENDIX
2: W
ORD
E
MBEDDING
M
ODELS
-
WORD
2
VEC
AND
G
LO
V
E MODELS
More generally,
topic models are part
of a larger frame-
work of language models called vector space models.
In this
approach, language is encoded by vector representations based
16
In addition to interpretable models,
the program cites
two other
ap-
proaches: deep explanation and model induction.
17
Interactive
interfaces
could be
used to measure
the
impact
of
pre-
processing (stemming,
stop words,
etc.)
to help researchers
determine its
impact on the model’s results.
18
Currently,
there are two STM-based visualization packages (stmViz and
stmBrowser) but their functions are limited.
Fig.
15.
Functions within stm package from Roberts et al.
(2015b)
on the Distributional
Hypothesis of language,
i.e.,
words are
used in similar context
to words with similar meanings [85].
Under this interpretation,
bag-of-words models (like LSA and
LDA)
are considered as count-based models as they encode
language as a series of vector counts.
As mentioned earlier,
one downside of this approach is that it ignores the context in
which words are used, an obvious deficiency when considering
semantic meaning.
Alternatively,
context-based models are a
new generation of vector space models that encode word con-
text
within a vector framework.
These models are sometimes
called word embedding,
neural
language models
or
simply
predictive models.
The first example of context based models was introduced
by Mikolov et
al.
[86] as the word2vec models:
Continuous
Bag of Words (CBOW) and Skip grams. Unlike the traditional
bag-of-words approaches that treated each word like an atomic
unit,
this approach considered a rolling window context
(e.g.
five words) to build a vector space model that provided deeper
semantic meaning while facilitating scalability to millions of
words and documents.
The difference between the models is
a reversal of the model inputs and outputs. The CBOW model
uses a list
of words (e.g.
five words) to best
predict
a word
that will most likely be used in a similar context. Alternatively,
the skip gram model uses a given word to predict what are the
most likely words that will be used in a similar context.
Building off
of
this framework,
Pennington,
Socher
and
Manning [87]
unified vector
space
models
by combining
features
of
the
count-based models
(like
LSA and LDA)
and context-based models (like word2vec)
to a more robust
model named GloVe, or global vectors of word representations.
Acknowledging the deficiencies
of
the count-based models
that
motivated the word2vec model,
Pennington,
Socher and
Manning argue that word2vec models have the opposite prob-
lem that
by only analyzing the (local) context
of words,
this
approach fails
to utilize
the
statistical
properties
provided
through a count-based approach.
Ultimately,
their
approach
resulted in not
only a better
predictive model
that
produced
deeper semantic meanings,
the neural language structure [88]
connected directly with related breakthroughs in the applica-
tion of deep learning to text analysis [89].
R
EFERENCES
[1]
D.
M.
Blei.
Probabilistic topic models.
Communications of the ACM,
55(4): 7784,
2012.
[2]
K.
M.
Quinn,
B.
L.
Monroe,
M.
Colaresi,
M.
H.
Crespin,
and D.
R.
Radev. How to analyze political attention with minimal assumptions and
costs.
American Journal of Political Science,
54 (1): 209-228,
2010.
[3]
J.
Grimmer,
and B.
Stewart.
Text
as data:
The promise and pitfalls of
automatic content analysis methods for political texts. Political Analysis,
21(3): 267-297,
2013.
[4]
J.Grimmer.
We are all
social
scientists now:
How big data,
machine
learning,
and causal
inference work together.
PS:
Political
Science and
Politics.
48(1): 80-83,
2015.
[5]
D.
Lazer et
al.
Life in the network:
the coming age of computational
social science.
Science (New York,
NY),
323(5915): 721-723,
2009.
[6]
H.
Wallach.
Computational
Social
Science:
Toward a
Collaborative
Future.
Computational Social Science: Discovery and Prediction,
2016.
[7]
M.
E.
Roberts,
B.
M.
Stewart,
D.
Tingley,
and E.
M.
Airoldi.
The
structural
topic model
and applied social
science.
Advances in Neural
Information Processing Systems Workshop on Topic Models: Computa-
tion,
Application,
and Evaluation,
1-4,
2013.
[8]
M.
E.
Roberts
et
al.
Structural
topic models
for
open ended survey
responses.
American Journal
of
Political
Science,
58(4):
1064-1082,
2014.
[9]
D.
M.
Blei,
A.
Y.
Ng,
and M.
I.
Jordan.
Latent
Dirichlet
allocation.
Journal of Machine Learning Research,
3: 9931022,
2003.
[10]
M.E.
Roberts,
B.
M.
Stewart,
and E.
M.
Airoldi.
A model
of text
for
experimentation in the social sciences. Journal of the American Statistical
Association,
111(515): 988-1003,
2016.
[11]
S.
Deerwester,
S.
T.
Dumais,
G.
W.
Furnas,
T.
K.
Landauer,
and R.
Harshman. Indexing by latent semantic analysis. Journal of the American
Society of Information Science,
41(6): 391-407,
1990.
[12]
T. Landauer, and S. Dumais. A Solution to Platos Problem: The Latent
Semantic
Analysis
of
Acquisition,
Induction,
and Representation of
Knowledge.
Psychological Review,
104(2): 211-240,
1997.
[13]
T.
Hofmann,
Unsupervised learning by probabilistic latent
semantic
analysis,
Machine Learning,
42(1): 177-196,
2001.
[14]
C. Ding, T. Li, and W. Peng. On the equivalence between non-negative
matrix Factorization and probabilistic latent semantic indexing. Comput.
Stat.
Data Analysis 52: 39133927,
2008.
[15]
S.P. Crain, K. Zhou, S.-H. Yang, and H. Zha. Dimensionality reduction
and topic modeling:
From latent
semantic indexing to latent
Dirichlet
allocation and beyond.
In Mining Text Data,
C.
Aggarwal and C.
Zhai,
Eds.,
Springer: 129162,
2012.
[16]
D.
Blei
and J.
Lafferty.
Topic
models.
Text
Mining:
Theory and
Applications,
2009.
[17]
T.
Griffiths,
and M.
Steyvers.
Finding scientific topics.
Proc.
of
the
National Academy of Science,
2004.
[18]
M.
D.
Hoffman,
D.
M.
Blei,
and F.
Bach.
Online learning for latent
Dirichlet allocation.
Neural Information Processing Systems,
2010.
[19]
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The author-topic
model for authors and documents. UAI ’04 Proc. of the 20th conference
on Uncertainty in Artificial Intelligence: 487-494,
2004.
[20]
D.
Blei
and J.
Lafferty.
Dynamic topic models.
In Proc.
of
the 23rd
International Conference on Machine Learning,
ACM,
2006.
[21]
J.
Eisenstein,
B.
O’Connor,
N.
A.
Smith,
and E.
P.
Xing.
A latent
variable model for geographic lexical variation.
In EMNLP,
1277-1287,
2010.
[22]
Y.
Rao,
Q.
Li,
X.
Mao,
and L.
Wenyin.
Sentiment
topic models for
social emotion mining,
Information Science,
266: 90100,
2014.
[23]
D.
Blei,
and J.
Lafferty.
Correlated topic models.
Neural
Information
Processing Systems,
2006.
[24]
D. Blei, and J. McAuliffe. Supervised topic models. Neural Information
Processing Systems 21,
2007.
[25]
Y.
W.
Teh,
M.
I.
Jordan,
M.
J.
Beal,
and D.
M.
Blei.
Hierarchical
Dirichlet processes. Journal of the American Statistical Association, 101
(476): 1566-1581,
2006.
[26]
A.
Gelman,
and E.
Loken.
The Statistical Crisis in Science.
American
Scientist,
102 (6): 460-465,
2014.
[27]
M.
J.
Denny,
and A.
Spirling.
Text
preprocessing for
unsupervised
learning:
why it
matters,
when it
misleads,
and what
to do about
it.
Unpublished manuscript, Dep. Polit. Sci, Stanford Univ and Inst. Quant.
Soc.
Sci.,
Harvard Univ.
https://ssrn.
com/abstract,
2017.
[28]
A.
Schofield,
L.
Thompson,
and D.
Mimno.
Quantifying the Effects of
Text Duplication on Semantic Models.
EMNLP,
2017.
[29]
A.
Schofield,
M.
Magnusson,
L.
Thompson,
and D.
Mimno.
Un-
derstanding Text
Pre-Processing for
Latent
Dirichlet
Allocation.
ACL
Workshop for Women in NLP (WiNLP),
2017.
[30]
A.
Schofield,
M.
Magnusson,
and D.
Mimno.
Pulling Out
the Stops:
Rethinking Stopword Removal for Topic Models.
EACL,
2017.
[31]
A.
Schofield,
and D.
Mimno.
Comparing Apples to Apple: the Effects
of Stemmers on Topic Models.
TACL Vol.
4,
2016.
[32]
D.
Mimno,
and A.
McCallum.
Topic models conditioned on arbitrary
features with Dirichlet-multinomial
regression.
In Uncertainty in Artifi-
cial Intelligence [UAI],
2008.
[33]
J.
Eisenstein et
al.
Sparse
additive
generative
models
of
text.
In
International Conference on Machine Learning [ICML], 10411048, 2011.
[34]
A. Gruber, M. Rosen-Zvi, and Y. Weiss. Hidden Topic Markov Models.
In Proc. of the Conference on Artificial Intelligence and Statistics, 2007.
[35]
Y.
Liu,
A.
Niculescu-Mizil,
and W.
Gryc.
Topic-link lda: joint models
of
topic
and author
community.
In Proc.
of
the
26th International
Conference on Machine Learning,
ACM,
665672,
2009.
[36]
J.
Chang,
and D.
Blei.
Hierarchical
relational
models for
document
networks.
Annals of Applied Statistics,
4 (1):124150,
2010.
[37]
J. Chang, J. Boyd-Graber, C. Wang, S. Gerrish, and D. M. Blei. Reading
tea leaves:
How humans
interpret
topic models.
Neural
Information
Processing Systems,
2009.
[38]
D.
Mimno,
H.
M.
Wallach,
E.
Talley,
M.
Leenders,
and A.
McCallum.
Optimizing semantic
coherence
in topic
models.
Proc.
of
the
2011
Conference
on Empirical
Methods
in Natural
Language
Processing,
262272,
2011.
[39]
J.
Bischof,
and E.
M.
Airoldi.
Summarizing topical content with word
frequency and exclusivity.
In ICML,
201208,
2012
[40]
H.
Wallach,
I.
Murray,
R.
Salakhutdinov,
and D.
Mimno.
Evaluation
methods for topic models,
In Proc.
of the 26th International Conference
on Machine Learning,
2009.
[41]
H. Wallach, D. Mimno, and A. McCallum. Rethinking LDA: why priors
matter.
In Neural Informational Processing Systems,
2009.
[42]
M.A.
Taddy.
On estimation and selection for topic models.
In Proc.
of
the 15th International Conference on Artificial Intelligence and Statistics
(AISTATS): 1184-1193,
2012.
[43]
J.
Tang,
and J.
L.
Zhao.
Understanding the limiting factor
of
topic
modeling via posterior contraction analysis. Journal of Machine Learning
Research,
32:190-198,
2014.
[44]
L.
Hong,
and B.
D.
Davison.
Empirical
study of
topic modeling in
twitter. In Proc. of the First Workshop on Social Media Analytics. ACM,
80-88,
2010
[45]
Y.
Hu,
J.
Boyd-Graber,
B.
Satinoff,
and A.
Smith.
Interactive topic
modeling,
Machine Learning,
95 (3),
423469,
Oct.
2013.
[46]
W. Dou, and S. Liu. Topic- and time-oriented visual text analysis. IEEE
Computer Graphics and Applications,
36(4): 8-13,
2016.
[47]
J.
Chuang,
C.
D.
Manning,
and J.
Heer.
Termite:
Visualization tech-
niques for assessing textual
topic models.
Proc.
International
Working
Conference on Advanced Visual Interfaces,
ACM,
74-77,
2012.
[48]
E. Alexander, J. Kohlmann, R. Valenza, M. Witmore, and M. Gleicher.
Serendip: Topic model-driven visual exploration of text corpora. Proc. of
the IEEE Conference Visual Analytics Science and Technology (VAST),
173-182,
2014.
[49]
W.
Dou,
X.
Wang,
R.
Chang and W.
Ribarsky.
ParallelTopics:
A
probabilistic approach to exploring document collections. In IEEE Visual
Analytics Science and Technology,
IEEE,
231240,
2011.
[50]
J.
Chuang,
D.
Ramage,
C.
D.
Manning,
and J.
Heer.
Interpretation and
trust: Designing model-driven visualizations for text analysis. Proc. of the
2012 ACM annual conference on Human Factors in Computing Systems,
ACM,
443-452,
2012.
[51]
W. Dou, L. Yu, X. Wang, Z. Ma, and W. Ribarsky. HierarchicalTopics:
Visually exploring large text
collections using topic hierarchies.
IEEE
TVCG,
19(12):20022011,
2013.
[52]
W. Cui, S. Liu, Z. Wu, and H. Wai. How Hierarchical Topics Evolve in
Large Text Corpora.
IEEE Trans.
Vis.
Comput.
Graphics,
20(12): 2281-
2290,
2014.
[53]
X.
Wang,
S.
Liu,
J.
Liu,
J.
Chen,
J.
Zhu,
and B.
Guo.
TopicPanorama:
A Full Picture of Relevant Topics.
IEEE Trans.
Vis.
Comput.
Graphics,
22(12): 2508-2521,
2016.
[54]
F. Wei et al. TIARA: A visual exploratory text analytic system. Proc. of
the 16th ACM SIGKDD International Conference Knowledge Discovery
and Data Mining: 153-162,
2010.
[55]
S.
Liu,
M.
X.
Zhou,
S.
Pan,
Y.
Song,
W.
Qian,
W.
Cai,
and X.
Lian.
TIARA: Interactive,
topic-based visual text summarization and analysis.
ACM Trans.
Intelligent Systems and Technology,
3(2): 25,
2012.
[56]
W. Cui et al. Textflow: Towards better understanding of evolving topics
in text.
IEEE Trans.
Vis.
Comput.
Graphics,
17(12):24122421,
2011.
[57]
S. Liu, Y. Chen, H. Wei, J. Yang, and K. Zhou. Exploring topical lead-
lag across corpora. IEEE Trans. Knowl. Data Eng., 27(1): 115-129, 2015.
[58]
W.
Dou,
X.
Wang,
D.
Skau,
W.
Ribarsky,
and M.
X.
Zhou.
LeadLine:
Interactive visual
analysis of text
data through event
identification and
exploration.
Proc.
of IEEE Conference on Visual Analytics Science and
Technology (VAST): 93-102,
2012.
[59]
S.
Liu,
J.
Yin,
X.
Wang,
W.
Cui,
K.
Cao,
and J.
Pei.
Online visual
analytics of text streams.
IEEE Trans.
Vis.
Comput.
Graphics,
2015.
[60]
J.
Grimmer,
A Bayesian hierarchical
topic model
for
political
texts:
measuring expressed agenda in Senate press releases, Political Analysis,
18 (1),
2010.
[61]
M.
J.
Paul,
and M.
Dredze.
Discovering health topics in social
media
using topic models,
PLoS ONE,
9(8),
e103408,
2014.
[62]
P.
Barbera,
R.
Bonneau,
P.
Egan,
J.
T.
Jost,
J.
Nagler,
and J.
Tucker.
Leaders or
followers? Measuring political
responsiveness in the U.S.
congress using social media data.
Working paper,
2014.
[63]
W.
H.
Greene,
Econometric analysis,
7th ed.
Harlow:
Pearson Educa-
tion,
2011.
[64]
M.
Roberts,
B.
Stewart,
and D.
Tingley.
stm:
R package for structural
topic models.
R package version 1.1.3,
2014.
[65]
A.
McCallum.
MALLET:
A machine learning for
language toolkit.
http://mallet.cs.umass.edu,
2002.
[66]
R.
Rehurek and P.
Sojka.
Software framework for
topic modelling
with large corpora,
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks,
45-50,
2010.
[67]
C. Fong and J. Grimmer. Discovery of Treatments from Text Corpora, In
Proceedings of the Annual Meeting of the Association for Computational
Linguistics,
2016.
[68]
D.
Sontag,
and D.M.
Roy.
Complexity of
inference in topic models,
Advances in Neural Information Processing: Workshop on Applications
for Topic Models: Text and Beyond,
2009.
[69]
M.
Roberts,
B.
Stewart,
and D.
Tingley.
Navigating the Local
Modes
of
Big Data:
The Case of
Topic Models.
In Data Analytics in Social
Science,
Government,
and Industry.
New York:
Cambridge University
Press,
2016.
[70]
A. Gelman et al.
Bayesian data analysis.
Boca Raton,
FL: Chapman &
Hall/CRC,
1995.
[71]
S.
Arora et al.
A practical algorithm for topic modeling with provable
guarantees.
Proc.
of the 30th Intl.
Conf.
on Machine Learning,
ACM,
145-162,
2013.
[72]
D.
Mimno,
and D.
Blei.
Bayesian checking for
topic models.
Proc.
of
the 2011 Conference on Empirical
Methods
in Natural
Language
Processing,
227237,
2011.
[73]
J.
Chuang et
al.
TopicCheck:
Interactive alignment
for assessing topic
model stability. In Y. Weiss, B. Schlkopf, & J. C. Platt (Eds.), Proceedings
of NAACL-HLT,
Cambridge,
MA: MIT Press,
175-184,
2015.
[74]
H.
V.
Milner,
and D.
Tingley.
Sailing the waters edge:
The domestic
politics of American foreign policy.
United States: Princeton University
Press,
2015
[75]
C.
Lucas,
R.
A.
Nielsen,
M.
E.
Roberts,
B.
M.
Stewart,
A.
Storer,
and D. Tingley. Computer assisted text analysis for comparative politics.
Political Analysis,
23(2): 254277,
Feb.
2015
[76]
S.
Sachdeva,
S.
McCaffrey,
and D.
Locke.
Social media approaches to
modeling wildfire smoke dispersion: spatiotemporal and social scientific
investigations.
Information,
Communication & Society,
2016.
[77]
J.
Reich,
D.
H.
Tingley,
J.
Leder-Luis,
M.
E.
Roberts,
and B.
Stewart.
Computer-assisted reading and discovery for
student
generated text
in
massive open online courses,
Journal of Learning Analytics,
2015.
[78]
J.
Reich,
B.
Stewart,
K.
Mavon,
and D.
Tingley.
The civic mission of
MOOCs:
measuring engagement
across political
differences in forums,
Association for Computing Machinery: Learning at Scale,
2016.
[79]
J.
Chuang et
al.
Computer-assisted content
analysis:
Topic models
for exploring multiple subjective interpretations.
Conference on Neural
Information Processing Systems (NIPS). Workshop on Human-Propelled
Machine Learning.
Montreal,
Canada,
2014.
[80]
DARPA.
Explainable Artificial
Intelligence (XAI),
DARPA-BAA-16-
53,
http://www.darpa.mil/attachments/DARPA-BAA-16-53.pdf,
2016.
[81]
T.
Kulesza,
M.
Burnett,
W.-K.
Wong,
and S.
Stumpf.
Principles of
explanatory debugging to personalize interactive machine learning.
In
Proc. of the 20th International Conference on Intelligent User Interfaces,
126–137,
2015.
[82]
Shiny by RStudio.
http://shiny.rstudio.com,
June 2016
[83]
K.
Wongsuphasawat,
D.
Moritz,
A.
Anand,
J.
Mackinlay,
B.
Howe,
and J.
Heer.
Voyager:
Exploratory analysis
via
facted browsing of
visualization recommendations.
IEEE Trans.
Visualization & Comp.
Graphics,
2015.
[84]
A.
Satyanarayan,
D.
Moritz,
K.
Wongsuphasawat,
and J.
Heer.
Vega-
Lite:
A grammar of interactive graphics,
IEEE Transactions on Visual-
ization and Computer Graphics,
23 (1),
341-350,
Jan.
2017.
[85]
M.
Baroni,
G.
Dinu,
and G.
Kruszewski.
Don’t
count,
predict! A sys-
tematic comparison of context-counting vs.
context-predicting semantic
vectors.,
ACL,
1,
238–247,
2014.
[86]
T.
Mikolov,
K.
Chai,
G.
S.
Corrado,
and J.
Dean.
Efficient
estimation
of word representations in vector space, arXiv preprint arXiv:1301.3781,
2013.
[87]
J.
Pennington,
R.
Socher,
and C.
D.
Manning.
Glove:
Global
Vectors
for Word Representation,
In EMNLP,
14,
1532–1543,
2014.
[88]
Y.
Bengio,
R.
Ducharme,
P.
Vincent,
and C.
Janvin.
A neural
proba-
bilistic language model,
In JMLR,
3,
1137–1155,
2003.
[89]
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning, Nature, 521 (7553),
436–444,
2015.
