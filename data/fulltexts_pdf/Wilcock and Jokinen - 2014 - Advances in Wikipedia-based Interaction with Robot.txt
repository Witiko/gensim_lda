Advances in Wikipedia-based Interaction with Robots
Graham Wilcock
University of Helsinki
Finland
graham.wilcock@helsinki.fi
Kristiina Jokinen
University of Helsinki
Finland
kristiina.jokinen@helsinki.fi
ABSTRACT
The paper describes advances in Wikipedia-based human-
robot interaction.
After reviewing the current capabilities of
talking robots that use Wikipedia as an information source,
the paper presents methods that support new capabilities.
These include language-switching and multimodal behaviour-
switching when using Wikipedias in many languages, robot
initiatives to suggest new topics from Wikipedia based on
semantic similarity to the current topic,
and the capability
of
the robot to listen to the user talking and to recognize
entities mentioned by the user that have Wikipedia links.
Categories and Subject Descriptors
I.2.9 [Robotics]:
Commercial robots and applications
Keywords
Human-robot interaction,
Wikipedia-based information ac-
cess, spoken dialogue systems
1.
INTRODUCTION
Natural language interaction clearly requires robust speech
recognition and language understanding components, but in
addition it includes issues related to multimodal
dialogue
management.
The conversational robot needs to cope with
rich information exchanges in mixed-initiative interactions
and it also needs to be able to understand various multi-
modal signals that tell
about the partner’s interest and at-
tention in dynamic and complex environments.
A unique challenge in human-robot interactions is that the
robot is situated in the same environment as the user,
the
robot occupies physical space quite unlike desktop comput-
ers or ambient intelligence applications.
Physical
location
imposes requirements on how the space is shared between
robots and humans and especially how a common ground
is created between robots and humans.
In human-human
interactions, common experiences and cultures mediate this
kind of common ground creation, and also allow reasonable
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page.
Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
MMRWHRI’14, November 16, 2014, Istanbul, Turkey.
Copyright 2014 ACM 978-1-4503-0551-8/14/11 ...$15.00.
http://dx.doi.org/10.1145/2666499.2666503.
expectations to function as the basis of communication.
Ef-
fective interactions between robots and humans must thus
also support establishing and maintaining common ground.
This requires creation of cognitive models of human reason-
ing and behaviour so that the robot can identify the human’s
cognitive states and adjust its information exchange accord-
ingly.
Moreover, the robot’s own behaviour is to be planned
and generated according to models that the human partners
can understand [3, 10].
It is expected that talking robots will make important con-
tributions by serving as assistive companions,
for instance
by reading web news or weather forecasts for blind people, or
providing support for those who have age-related challenges
[16, 18].
Other research in the use of robots for companion-
ship or entertainment include robotic story-tellers [15].
Another important area where robots may provide novel
opportunities is in interactive therapy.
For instance,
con-
cerning interaction with autistic children,
[2]
and [20]
have
shown that many of these children respond weakly or not at
all to social cues by the human partner, but respond well to
mechanical devices such as robots.
In these cases robots can
have a therapeutic role in helping children to improve social
interactions [24].
1.1
Talking Encyclopedias
The Aldebaran (http://www.aldebaran.com) Nao robot
has
a high-quality text-to-speech component
provided as
standard.
Among the applications that can be downloaded
from the Nao Apps Store there are apps that combine Nao’s
speech capability with wireless internet access to provide
spoken information directly from Wikipedia.
Two of these
“talking encyclopedia” applications are Naopedia and Wiki
Voice (https://store.aldebaran-robotics.com).
These practical applications enable robots to talk fluently
and at length about more or less any topic by reading out the
content of
Wikipedia articles.
They use different methods
for identifying the desired Wikipedia topic in order to get
started:
Naopedia uses a button-clicking input method for
spelling the name of the topic, and Wiki Voice uses Google’s
cloud-based speech engine (http://research.google.com/
pubs/SpeechProcessing.html) to perform open-vocabulary
speech recognition.
Reading out a Wikipedia article is a monologue rather
than a dialogue.
In this paper we will describe the WikiTalk
spoken information access system that integrates Wikipedia-
based talking into the framework of
an interactive spoken
dialogue system based on theoretical foundations.
13
Figure 1:
Constructive Dialogue Modelling in human-robot interaction, from [9].
The paper is structured as follows.
Section 2 gives a sum-
mary of the existing capabilities of the WikiTalk system [9]
that have been previously published.
The rest of the paper
describes new directions that go beyond the previous capa-
bilities.
Section 3 focusses on multilinguality and language-
switching when using Wikipedias in many languages,
but
also discuses multimodal
issues,
when switching languages
requires
switching behaviours
due to cultural
differences.
Section 4 introduces other new directions:
methods which
enable robots to simulate natural
language understanding
by suggesting new topics based on awareness of
semantic
similarity between Wikipedia articles,
and methods which
enable robots to follow the changing interests of the user by
listening to the humans talking and recognizing new enti-
ties mentioned by the humans that have Wikipedia links.
Section 5 draws conclusions.
2.
THE WIKITALK SYSTEM
WikiTalk [8, 25] is an interactive spoken dialogue system
for Wikipedia-based open-domain information access.
The
human partner listens to the system talking about a given
topic,
and can then choose to hear more about the current
topic, shift to a related topic that is linked from the current
Wikipedia entry,
or go back,
repeat,
etc.
As well
as user-
initiated topics,
the system suggests new topics using the
daily ”Did you know?” items in Wikipedia.
When the WikiTalk system was implemented on the Nao
humanoid robot it was extended with multimodal behaviours
[5, 9].
Multimodal aspects of WikiTalk include the integra-
tion and synchronisation of the robot’s speech with a range
of gestures for specific communicative functions [12] (for re-
cent related work see [21]), and experiments on recognising
non-verbal cues using vision for gesture recognition and us-
ing sonar sensors for speaker proximity detection [6].
WikiTalk has theoretical foundations in the framework of
Constructive Dialogue Modelling [7], which integrates topic
management, information flow, and the mutual construction
of shared knowledge in the conversation using a set of four
enablements (Contact, Perception, Understanding, and Re-
action).
The implementation of the Constructive Dialogue
Modelling architecture on the Nao robot for multimodal in-
teraction is shown in Figure 1.
An evaluation of the WikiTalk system based on user tests
with the Nao robot at the eNTERFACE 2012 summer school
is presented in [1].
Videos of the user tests are included in the
eNTERFACE 2012 Nao-Human Interaction Data corpus.
A
video of
the demo in 2012 can be seen at https://docs.
google.com/file/d/0B-D1kVqPMlKdOEcyS25nMWpjUG8.
2.1
Smooth Topic Shifts
One of the features that enables the WikiTalk system to
manage interactive dialogues rather than spoken monologues
is its ability to handle smooth topic shifts by predicting what
the user will want to talk about next.
The mechanism that
supports smooth topic shifts is described in [25], where more
details are given, but is summarised here.
The dialogue management in WikiTalk is based on a state
transition network.
Once the first topic is selected in the
selectNewTopic state,
the system switches to the start-
NewTopic state which downloads the Wikipedia article and
divides it into paragraph-sized chunks.
The hyperlinks in the
text are extracted and saved, to be used as potential smooth
topic shifts.
The extracted links are called NewInfos (pieces
of new information) and the system predicts that the user
will often want to shift the topic to one of the NewInfos.
Starting with the first chunk, one or more chunks are spo-
ken aloud under control
of the continueTopic state.
This
state allows the user to control whether to hear more chunks
about the same topic, to change to a related topic (a smooth
topic shift),
to change to an unrelated topic (an awkward
topic shift),
or to end the dialogue.
The user can make a
smooth topic shift at any time by mentioning one of
the
recent NewInfos.
In that case,
the selected NewInfo imme-
diately becomes the new topic.
The system switches back to
14
the startNewTopic state,
downloads the Wikipedia article
for the new topic, extracts a new set of NewInfos, and starts
reading the first chunk about the new topic.
This smooth topic shift loop can be repeated any num-
ber of times,
so the dialogue can follow topic chains of any
length,
and the user
can navigate freely to any topic in
Wikipedia.
Examples of smooth topic shifts and topic chains
in human-robot dialogues are given in [8]
(Shakespeare →
Shakespeare’s
sexuality → Shakespeare’s
son Hamnet
→
Shakespeare’s play Hamlet) and in [25]
(Metz → Celts →
Celtic languages → Indo-European languages).
3.
MULTIMODAL MULTILINGUALITY
As there are many different language versions of Wikipedia
it is possible to make Wikipedia-based talking robot systems
for many different languages.
It is a prerequisite that speech
components for each specific language are available for the
robot,
both for text-to-speech and speech recognition.
For
the Nao robot, the number of languages supported by Alde-
baran has increased from 8 to 19.
As Finnish is one of the new languages that have speech
components for the robot, we have made a Finnish version of
WikiTalk using the extensive Finnish Wikipedia (although
Finnish is not in the top 100 languages by number of speak-
ers,
it is in the top 20 by number of
Wikipedia articles).
However,
instead of making multiple separate monolingual
versions of WikiTalk, we naturally wish to develop an inte-
grated WikiTalk system with multilingual capabilities.
In WikiTalk, there are two distinct types of language use
that
need to be properly handled in different
languages.
First, there are the sentences and paragraphs extracted from
the Wikipedia articles.
These sentences and paragraphs are
already in the specific language,
because they are down-
loaded from the specific version of
Wikipedia.
There are
complexities due to different writing systems,
but most of
the difficulties are resolved by systematic use of
unicode
character encodings.
More serious difficulties arise in the
process of extracting the text from the HTML tags, images,
infoboxes and other items that are embedded in the articles,
due to the many different ways that Wikipedia articles are
structured and formatted in different language versions.
Second,
there are the relatively fixed words and phrases
that the user and the robot both use in order to manage the
flow of the dialogue.
These include user commands such as
continue,
repeat,
enough,
short phrases such as Oh,
sorry!
(when the user interrupts the robot while it is talking), and
multiple sentences such as I cannot access Wikipedia.
Please
check the Internet connection.
The phrases of
the second type need to be localized for
each specific language, that is, the phrases need to be trans-
lated.
We are following the well-established methodologies
for
software localization,
such as
GNU gettext
(http://
www.gnu.org/software/gettext/).
However, there are ad-
ditional challenges in translating phrases for use in human-
robot interaction with different human partners and in dif-
ferent situations, as discussed in Section 3.2.
3.1
Language Switching
Instead of producing localized versions of Wikipedia-based
talking robot applications for individual
languages,
we are
designing a multilingual
system for a robot that can speak
many different languages.
In this case,
the robot needs to
find out which language to use before starting to speak.
The
state transition network for dialogue management described
by [25]
is extended by adding a new state for selecting the
language, as shown in Figure 2.
After the robot performs preparatory actions in the Hello
state, such as standing up, the initial language is selected in
a new selectLanguage state.
To prompt the user to specify
the preferred language, the robot cycles through the names
of the available languages.
This is done by In English? (for
English) and Suomeksi?
(for Finnish),
but in future will
include for example En Fran¸
cais?
(French),
Auf
Deutsch?
(German) and Nihongo de?
(Japanese).
When the user
selects one of the available languages, it is set as the current
language.
The robot then says the equivalent of I’ll
tell
you
something interesting in the selected language and switches
to the selectNewTopic state.
The language selection capability is also being further ex-
ploited to add a language switching capability.
The user
may wish to change language, for example because another
user has arrived who has a different language preference.
Switching language can be done in the continueTopic state
by saying the name of the preferred language.
The system
then switches to the selectLanguage state to confirm the
choice of
language and set it as the current language,
as
shown in Figure 2.
Because WikiTalk is interactive and multimodal,
one of
the responsibilities of the continueTopic state is to monitor
the interest level
and engagement of
the user,
in order to
decide whether to continue with the current topic or ask the
user whether something else would be preferred.
One of the
possible reasons for a lack of engagement would be that the
user finds the currently selected language difficult,
and in
such a case the system could take the initiative by asking
the user if switching to another language would help.
3.2
Multimodal Behaviour Switching
There are additional challenges in translating phrases for
human-robot interactions, as these are situated interactions
with different human partners in different situations.
Lin-
guistic challenges include deciding on translations when dif-
ferent levels of politeness or familiarity are required for dif-
ferent human partners.
An example of this would be decid-
ing when to use vous or tu in French.
For Japanese, there are not only different linguistic forms
for different politeness levels, but also different required robot
behaviours such as bowing.
Language-switching will need to
be extended to include multicultural
behaviour-switching.
For example, the depth and duration of bowing will need to
be parameterized for the perceived status of
different dia-
logue partners.
In the case of
English and Finnish,
there are not such
major differences that are required in robot behaviour com-
pared with Japanese,
for example bowing is not required.
Nevertheless there are more subtle differences that have been
identified in human-human multimodal
dialogue research.
The NOMCO Multimodal
Nordic video corpus [17]
shows
significant differences in conversational head-nodding when
comparing Finns [22]
with Swedes and Danes.
The idea of
parameterizing robot head nodding depths and frequencies
for Finnish and other European languages is planned for
future development.
The WikiTalk system includes a functional
classification
of gesture types,
as well
as parameterization of gestures to
manage their synchronization with speech [12].
We are ex-
15
H
ello
st
art
se
lectLanguage
se
lectNewTopic
st
artNewTopic
c
ontinueTopic
sa
yGoodbye
I
n English?
En Fran¸
cais?
Auf Deutsch?
Nihongo de?
I
’ll
tell you
something
interesting
Wh
at topic
do you want
to hear about?
OK
fi
rst
chunk
Co
ntinue:
next chunk
S
omething
else
N
ewInfo
(smooth topic shift)
La
nguage
switch
Th
at’s
en
ough
Figure 2:
Extending the state diagram in [25] by adding language-switching.
tending both the classification and the parameterization in
order to manage multiple dimensions of politeness, friendli-
ness, gender-appropriateness and verbosity.
Another parameter in situated interaction is the distance
between the participants.
It is well-known that in different
cultures there are different norms for speakers’
proximity
to each other in human-human conversations,
but it is not
yet known whether the same norms also apply in human-
robot interactions.
Using sonar sensors in the Nao robot’s
chest,
empirical
measurements were made of
how far from
the robot people positioned themselves when interacting in
user tests of
WikiTalk.
Initial
results reported in [6]
were
made during interactions in English with international par-
ticipants.
As WikiTalk becomes more multilingual,
it will
become possible to measure user proximities during interac-
tions in a range of languages and make comparisons across
different language communities.
4.
OTHER NEW DIRECTIONS
The main restriction on classical spoken dialogue systems
has been their inability to move out of
a limited domain
such as flight reservations, for which the system has a dedi-
cated database and a carefully constructed set of rules and
algorithms.
Open-domain dialogues are usually believed to
require unrestricted world knowledge and general-purpose
artificial
intelligence.
The most famous open-domain dia-
logue system ELIZA [23] maintains on-going dialogues with
no restriction on the topics,
but ELIZA does not use any
domain knowledge.
As a result,
the user soon notices that
the dialogue lacks a goal and there is no global coherence or
structure in the replies.
However, without attempting to solve the problem of open-
domain general-purpose artificial intelligence, we can design
methods that enable a robot to follow the topic of the con-
versation and to know what other topics are related to the
current topic, and to listen apparently intelligently to what
the human is talking about so that the robot’s replies show
some level
of
coherence.
The first method is based on se-
mantic similarity distances between Wikipedia articles.
The
second method is based on Wikipedia-based entity recogni-
tion and entity linking.
4.1
Wikipedia-based Topic Modelling
The WikiTalk system can make smooth topic shifts from
one Wikipedia topic to another by predicting what topic the
user is likely to be interested in hearing about next.
The
predicted topics are passed to the speech recognizer to be
included in the recognition vocabulary for the user’s next
utterance.
If the user mentions one of the predicted topics,
the robot immediately shifts to this new topic and starts
talking about it.
The basic mechanism in WikiTalk for predicting topics is
simply to collect all the hyperlinks in the current article and
use the set of links as the predictions.
Each link is associ-
ated with one or more words and phrases in the text of the
article, and these words and phrases are used by the speech
recognizer in the recognition vocabulary.
One limitation of
the existing WikiTalk system is that it cannot recognize top-
ics which are not linked from the current Wikipedia entry,
because the language model is limited to the current set of
hyperlinks and a small set of commands.
This problem could be overcome by calculating semantic
similarities of Wikipedia articles using vector space models.
The intention is that with these models, a list of Wikipedia
articles will be produced in which the articles are ranked in
order of
semantic similarity to the current dialogue topic.
It can be predicted that the topics of
semantically similar
articles are more likely to be mentioned by the human,
so
the robot will be ready to recognize them and make smooth
topic shifts,
even when there are no hyperlinks from the
current article.
Articles with high similarity will
also be
used for system initiatives to suggest new topics that might
be interesting for the dialogue partner.
However,
in practice this has not so far been successful.
The full English Wikipedia is so large that the calculations
could not be done on a standard laptop.
It is hoped that
the use of computing clusters will solve this problem, using
for example the Gensim [19]
software for topic modelling
with large corpora.
An experiment using the much smaller
16
Simple English Wikipedia easily performed the calculations,
but the results were disappointing as the similarity ranking
was quite unconvincing.
Further work is needed.
Another approach is to use an external
service to obtain
similarity rankings.
We are currently using the cloud-based
service provided by the Wikipedia Miner project [14],
with
promising results.
4.2
Wikipedia-based Listening
In text-based natural
language processing,
named entity
recognition is a long-standing research topic.
Gazetteers, or
lists of known entities, have long been the basis of named en-
tity recognition techniques.
However, fixed lists are rapidly
out-of-date and need constant efforts to keep them regu-
larly updated.
Recent progress in named entity recognition
has therefore investigated techniques for using Wikipedia.
“Since Wikipedia aims to be an encyclopedia, most articles
are about named entities and they are more structured than
raw texts.
. . . It is also important that Wikipedia is updated
every day and therefore new named entities are added con-
stantly.” [11]
WikiTalk and the “talking encyclopedia” robot applica-
tions described in Section 1.1 are practical
working exam-
ples of Wikipedia-based open-domain talking.
A parallel ca-
pability for Wikipedia-based open-domain listening is now
needed in order to develop dialogue systems in which the
robot and the human can take turns in talking more and lis-
tening more in changing proportions.
This capability is now
becoming feasible due to advances both in open-vocabulary
speech recognition and in Wikipedia-based named entity
recognition.
Given an approximate transcription of the hu-
man partner’s speech,
the entities mentioned can be iden-
tified and linked to Wikipedia topics using existing meth-
ods for entity linking or wikification [13].
The robot can
then continue the dialogue by talking about the new top-
ics introduced by the human,
using the Wikipedia articles
as its knowledge source.
This maintains the coherence of
the dialogue and gives an appearance of
natural
language
understanding.
Previously,
open vocabulary speech recognition was not
feasible as there are too many possible words.
However,
continuous improvements in speech recognition techniques
mean that open vocabulary speech recognition is gradually
becoming effective.
Our ongoing experiments in Wikipedia-
based listening currently use two cloud-based services.
For
open-vocabulary speech recognition we use Google’s cloud-
based speech engine [4].
For Wikipedia-based entity recog-
nition (wikification) we use the web service provided by the
Wikipedia Miner project [14].
5.
CONCLUSIONS
The paper
describes
new directions
in spoken interac-
tion with robots.
After reviewing the current capabilities
of
Wikipedia-based talking robots,
we described methods
that support new capabilities,
focussing on multilinguallty
and language switching when using Wikipedias in many lan-
guages, and multimodal aspects of multilinguality.
We also discussed methods that can help the robot to take
initiatives in suggesting new topics from Wikipedia based on
their semantic similarity to the current topic, and methods
through which the robot can achieve a capability to listen
to the humans talking and recognize new Wikipedia-linked
entities that they mention,
in order to follow the humans’
changing topics when the robot is not directly involved.
6.
ACKNOWLEDGMENTS
We thank the reviewers for their valuable comments.
7.
REFERENCES
[1]
D. Anastasiou, K. Jokinen, and G. Wilcock.
Evaluation of WikiTalk - user studies of human-robot
interaction. In Proceedings of 15th International
Conference on Human-Computer Interaction (HCII
2013), Las Vegas, 2013.
[2]
A. Billard, B. Robins, J. Nadel, and K. Dautenhahn.
Building robota, a mini-humanoid robot for the
rehabilitation of children with autism. RESNA
Assistive Technology Journal, 2006.
[3]
C. Breazeal. Toward sociable robots. Robotics and
Autonomous Systems, 42:167–175, 2003.
[4]
C. Chelba, D. Bikel, M. Shugrina, P. Nguyen, and
S. Kumar. Large scale language modeling in automatic
speech recognition.
http://static.googleusercontent.com/media/
research.google.com/en//pubs/archive/40491.pdf,
2012.
[5]
A. Csapo, E. Gilmartin, J. Grizou, J. Han, R. Meena,
D. Anastasiou, K. Jokinen, and G. Wilcock.
Multimodal conversational interaction with a
humanoid robot. In Proceedings of 3rd IEEE
International
Conference on Cognitive
Infocommunications (CogInfoCom 2012), pages
667–672, Kosice, 2012.
[6]
J. Han, N. Campbell, K. Jokinen, and G. Wilcock.
Investigating the use of non-verbal cues in
human-robot interaction with a Nao robot. In
Proceedings of 3rd IEEE International
Conference on
Cognitive Infocommunications (CogInfoCom 2012),
pages 679–683, Kosice, 2012.
[7]
K. Jokinen. Constructive Dialogue Modelling:
Speech
Interaction and Rational
Agents. John Wiley & Sons,
2009.
[8]
K. Jokinen and G. Wilcock. Constructive interaction
for talking about interesting topics. In Proceedings of
Eighth International
Conference on Language
Resources and Evaluation (LREC 2012), Istanbul,
2012.
[9]
K. Jokinen and G. Wilcock. Multimodal open-domain
conversations with the Nao robot. In J. Mariani,
S. Rosset, M. Garnier-Rizet, and L. Devillers, editors,
Natural
Interaction with Robots, Knowbots and
Smartphones:
Putting Spoken Dialogue Systems into
Practice), pages 213–224. Springer, 2014.
[10]
T. Kanda, T. Hirano, D. Eaton, and H. Ishiguro.
Interactive robots as social partners and peer tutors
for children:
A field trial. Human-Computer
Interaction, 19(1–2):61–84, 2004.
[11]
J. Kazama and K. Torisawa. Exploiting Wikipedia as
external knowledge for named entity recognition. In
Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural
Language Processing and
Computational
Natural
Language Learning, pages
698–707, Prague, 2007.
17
[12]
R. Meena, K. Jokinen, and G. Wilcock. Integration of
gestures and speech in human-robot interaction. In
Proceedings of 3rd IEEE International
Conference on
Cognitive Infocommunications (CogInfoCom 2012),
pages 673–678, Kosice, 2012.
[13]
R. Mihalcea and A. Csomai. Wikify!
Linking
documents to encyclopedic knowledge. In Proceedings
of the 16th ACM Conference on Information and
Knowledge Management (CIKM 2007), pages
233–242, Lisbon, 2007.
[14]
D. Milne and I. Witten. An open-source toolkit for
mining Wikipedia. Artificial
Intelligence, 194:222–239,
2013.
[15]
J. Montemayor, H. Alborizi, A. Druin, J. Hendler,
D. Pollack, J. Porteous, L. Sherman, A. Afework,
J. Best, J. Hammer, A. Kruskal, and A. Lal. From
pets to storykit:
Creating new technology with and
intergenerational design team. In Proceedings of the
Workshop on Interactive Robots and Entertainment
(WIRE), Pittsburgh, 2000.
[16]
M. Montemerlo, J. Pineau, N. Roy, S. Thrun, and
V. Verma. Experiences with a mobile robotic guide for
the elderly. In Proceedings of AAAI National
Conference on Artificial
Intelligence, 2002.
[17]
P. Paggio, J. Allwood, E. Ahls´en, K. Jokinen, and
C. Navarretta. The NOMCO Multimodal Nordic
Resource - Goals and Characteristics. In Proceedings
of Seventh International
Conference on Language
Resources and Evaluation (LREC 2010), pages
2968–2973, Valletta, Malta, 2010.
[18]
M. E. Pollack. Intelligent technology for an aging
population:
The use of AI to assist elders with
cognitive impairment. AI Magazine, 26(2):9–24, 2005.
[19]
R.
ˇ
Reh˚uˇrek and P. Sojka. Software Framework for
Topic Modelling with Large Corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for
NLP Frameworks, pages 45–50, Valletta, Malta, 2010.
[20]
B. Robins, P. Dickerson, P. Stribling, and
K. Dautenhahn. Robot-mediated joint attention in
children with autism:
A case study in robot-human
interaction. Interaction Studies, 5(2):161–198, 2004.
[21]
M. Salem, S. Kopp, and F. Joublin. Generating finely
synchronized gesture and speech for humanoid robots:
a closed-loop approach. In Proceedings of the 8th
ACM/IEEE international
conference on Human-robot
interaction, pages 219–220. ACM Press, 2013.
[22]
E. Toivio and K. Jokinen. Multimodal Feedback
Signaling in Finnish. In Proceedings of the Human
Language Technologies - The Baltic Perspective, 2012.
[23]
J. Weizenbaum. Eliza - a computer program for the
study of natural language communication between
man and machine. Communications of the ACM,
9(1):36–45, 1966.
[24]
I. Werry, K. Dautenhahn, B. Ogden, and W. Harwin.
Can social interaction skills be taught by a social
agent? the role of a robotic mediator in autism
therapy. In Proceedings of Fourth International
Conference on Cognitive Technology, pages 371–376,
Warwick, 2001. Springer.
[25]
G. Wilcock. WikiTalk:
A spoken Wikipedia-based
open-domain knowledge access system. In Proceedings
of the COLING 2012 Workshop on Question
Answering for Complex Domains, pages 57–69,
Mumbai, 2012.
18
