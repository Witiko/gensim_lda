Structured semantic knowledge can emerge automatically from 
predicting word sequences in child-directed speech 
Philip A. Huebner*, Interdepartmental Neuroscience Graduate Program, University of California 
Riverside, Riverside, CA, USA 
Jon A. Willits, Department of Psychology, University of California Riverside, Riverside, CA, 
USA 
Correspondence: 
Philip A. Huebner 
phueb001@ucr.edu
Abstract
Previous research has suggested that a distributional learning mechanism may bootstrap 
acquisition of semantic knowledge. However, it remains unclear how such a mechanism might 
work exactly, and how rich and structured the knowledge produced by such a mechanism might 
be. Previous research with neural networks has shown that prediction-based models that 
instantiate distributional learning principles have the potential to develop rich semantic 
representations. It remains unknown, however, whether these models can learn semantic 
knowledge and structure from the complex noisy input that children receive, and whether the 
knowledge they would acquire resembles that of children. We investigated these questions by 
training three classes of neural networks (Simple Recurrent Network, Long Short-Term Memory, 
and Skip-gram) to predict word sequences in a 5-million-word corpus of speech directed to 
children ages 0 to 3 years old. We assessed the models qualitatively, by assessing semantic 
neighborhoods and hierarchical clustering solutions derived from the internal representations 
after training. We also assessed the models quantitatively in a semantic categorization task for 
which models received no training. Both qualitative and quantitative analyses demonstrated that 
the models developed rich semantic structure, including hierarchical organization and category 
structure, without having been explicitly trained to perform those tasks. The structure within the 
internal representations produced by the models mirrored a number of interesting properties of 
child semantic development, such as a bias for taxonomic relations over thematic relations, 
complex differences between artifacts and natural kinds, and interesting differences between 
easy-to-learn and hard-to-learn categories and words. Comparing the representations produced 
by the three different models, we found that the more advanced models (LSTM and Skip-gram) 
provided better learning of semantic structure compared to the Simple Recurrent Network. Our 
findings show that a learning system that derives abstract, distributed representations for the 
purpose of predicting sequential dependencies in naturalistic language may provide insight into 
emergence of many properties of the developing semantic system. 
Keywords​
: semantic development, language learning, neural networks, statistical learning 
1. Introduction 
The development of semantic memory is an extremely complex phenomenon, requiring 
input from all perceptual modalities and making use of many psychological processes. In recent 
years, one focus of research has been on the ways in which the development of semantic memory 
is aided by - or even driven by - acquiring information about the distributional and syntactic 
structures in which words occur. The linguist Harris (1954) was among the earliest to formalize 
how the distributional similarity of two elements of language can be a basis for judging them to 
be members of the same class or category, and Firth (1957) explored this idea with regard to 
semantic knowledge, noting that “you shall know a word by the company it keeps”. This 
perspective emphasizes how words’ meanings are inherently contextual, and can be defined in 
terms of the words around them. 
The perspective has come to be known as the distributional hypothesis, specifically the 
claim that the similarity or class membership of linguistic units can be inferred from the 
statistical or structural contexts in which those units occur. In the 1990s and 2000s, these ideas 
were formalized in a range of different computational models of adult semantics, such as LSA 
and HAL (Jones & Mewhort, 2006; Landauer & Dumais, 1997; Lund & Burgess, 1996; 
Steyvers & Tenenbaum, 2005). Considerable research has since shown that these and related 
procedures for representing distributional information can be used to form very sophisticated 
semantic spaces, and that those semantic spaces predict a wide range of adult psycholinguistic 
variables (Bullinaria & Levy, 2007; Burgess, 1998; Jones, Mewhort & Kintsch, 2006; Olney, 
Dale, & D’Mello, 2012; Pereira et al., 2016). 
At the same time, researchers in child language acquisition were studying the question of 
whether children are sensitive to distributional information, and whether they can use it to infer 
word meanings. Gleitman (1990) suggested that syntactic bootstrapping (i.e., inferring a word’s 
meaning from its syntactic structure) may be an important mechanism by which children begin to 
learn the meanings of words. Using syntactic bootstrapping, children may, for example, learn 
about the difference between transitive and intransitive verb meaning in part by noting that these 
verbs co-occur with different nouns or noun phrases. Recent studies have shown that infants and 
children are sensitive to the distributional structure of words, and do seem to infer aspects of 
word meaning from lexical and syntactic distributional structure (Fisher et al., 2010; Lany & 
Saffran, 2010; Syrett & Lidz, 2010; Wojcik, & Saffran, 2013). 
Thus, formal, computational, and experimental work in linguistics, psychology, and 
computer science has shown both that substantial semantic information exists in words’ 
distributions, and that human learners are sensitive to this information. However, there remains 
an unanswered question with regard to this potential mechanism of the acquisition of semantic 
knowledge. What is the specific learning and representational mechanisms that is capable of 
supporting such a process, and what is the degree of complexity, structure, and detail that the 
mechanism can derive? 
By age three, children already demonstrate having developed rich semantic knowledge. 
For example, by this age most children already know an extremely wide range of facts about the 
world (Clark, 2009). They also appear to have developed a significant amount of meta-semantic 
knowledge, such as appearing to demonstrate a bias for assuming that labels refer to taxonomic 
(within-category) relationships, as opposed to thematic (within-event) relationships (Waxman & 
Markow, 1995), and that the relationships between words’ meanings are organized into 
structured, hierarchical representations (Keil, 1992). It is not yet clear what learning and 
representational mechanisms are required to explain the development and acquisition of semantic 
knowledge. Some researchers favor the hypothesis that the mind begins with a fairly rich set of 
semantic structures and learning mechanisms, because the information in the world is too noisy, 
and the resultant knowledge too complex, to be explained through simple learning mechanisms 
operating over the information in children’s environment (Chomsky, 2006; Jackendoff, 1992; 
Fodor, 1965; Pinker, 1984). Others, however, argue that statistical learning mechanisms are 
much more powerful than detractors claim, and that the information in the environment is much 
richer and more structured than it is often assumed to be. According to this view, the complex 
semantic knowledge that children acquire can be explained as an emergent property of simple 
learning mechanisms that process this complex but highly structured environmental information 
over time (Rogers & McClelland, 2004; Xu & Tenenbaum, 2007). 
To address the question of what kind of semantic knowledge can be obtained from such 
statistical learning mechanisms, we need two things. First, we need an accurate and 
representative model of the linguistic input that children receive. Many have argued about the 
“poverty of the stimulus” for speculations and anecdotes, without actual data about the nature of 
children’s experiences. Second, we need a computational model that makes specific claims about 
how that information is learned and represented. Consider Firth’s distributional hypothesis, that 
two elements shall be considered similar if they tend to occur in similar contexts. Within this 
hypothesis, there are nonetheless many details that need to be specified. What does “similar 
context” mean and how is it quantified? Without constraints, a learning or representational 
theory will not be useful due to theoretical or computational intractability. In addition to asking 
what
​
relations are tracked and represented by a system, one must ask 
​how
​
those relations are 
tracked and represented. There are dozens of learning theories that make different predictions 
about the kinds of semantic structures that might emerge from such principles. 
In this paper, we explicitly test the idea that models that learn and represent the linguistic 
context in which words occur, while operating over the noisy naturalistic input that children 
receive, can learn rich and highly complex semantic structure. We test this hypothesis using the 
CHILDES corpus, a dataset containing thousands of transcribed episodes of interactions with 
children of various ages and backgrounds (MacWhinney, 2000). The general class of learning 
and memory models we investigated were artificial neural networks, focusing on three different 
architectures: the Simple Recurrent Network (SRN) (Elman, 1990), the Long Short-term 
Memory (LSTM) (Hochreiter & Schmidhuber, 1997), and Skip-gram, which is one variant of the 
Word2Vec family of models (Mikolov, 2013a). These models allow us to assess whether neural 
networks that track information about word order (SRN, LSTM) or co-occurrences (Skip-gram) 
can learn semantic knowledge from noisy speech to children, what kind of semantic knowledge 
they acquire, and whether the structure of this knowledge qualitatively reflects children’s 
semantic memory development. 
The earliest models of semantic knowledge focused on simulating semantic processing in 
terms of computations over semantic features (Collins & Quillian, 1969; Rosch, 1975; Smith et 
al., 1974). Neural networks that connect concepts to features in an attractor network, learn their 
correlational structure, and have been used to model a wide range of phenomena, including 
semantic priming (McRae, de Sa, & Seidenberg, 1997; Plaut & Booth, 2000), the effects of brain 
damage on semantic processing (Hinton & Shallice, 1991; Rogers et al., 2004), and semantic 
effects on sentence processing (Christiansen & MacDonald, 2009; McClelland & Kawamoto, 
1986; McClelland, St. John, & Taraban, 1989; Tabor, Juliano, & Tanenhaus, 1997). 
Feedforward, feature-based neural networks have also been used to simulate semantic 
development. Rogers and McClelland (2004; 2008) showed that a feedforward neural network 
learning about concepts in terms of the correlational structure of their shared features or 
propositional content (such as 
​canaries
​
“
​are yellow
​
” and “
​have wings
​
”) can be used to explain a 
number of phenomena in semantic development, including how concepts differentiate and 
reorganize over the course of development, how semantic knowledge and causal knowledge are 
related, and how properties come to be important and define concepts. 
Feedforward neural networks have been used to simulate and understand an impressive 
range of phenomena relating to semantic knowledge, semantic processing, and semantic 
development. Given a set of primitives, learning the correlational structure can explain much 
about the development of semantic structure. However, one open question about these models 
relates to the source of their primitives (i.e., the semantic features or propositions the model is 
tracking). Is it reasonable to assume the primitives can be derived from the input or must they be 
posited as innate knowledge built into the system? Critics of Rogers and McClelland have argued 
that this is actually one of the most difficult questions about semantic development that needs to 
be explained, and that their model used a simplistic, idealized view of what children’s input is 
actually like. They claim that the “poverty of the stimulus” would prevent such a model from 
explaining real-world semantic development (Marcus & Keil, 2008; Robbins, 2008; Snedeker, 
2008). 
The distributional hypothesis directly addresses the question of where these primitives 
might come from and whether they can be learned. It proposes that the features 
​are
​
the contexts 
in which they occur. Semantic models that learn from distributional information using realistic 
and large datasets have already shown that semantic knowledge can emerge from learning the 
statistical structure of language. Overall, existing methods for deriving word meanings from 
large corpora can generally be divided into two classes: matrix-factorization methods and 
prediction-based methods. 
Matrix-factorization methods decompose matrices containing frequency or co-occurrence 
statistics. For example, in the Latent Semantic Analysis (LSA) model (Landauer & Dumais, 
1997), a word-by-document frequency matrix is created, indexing how frequently each word 
occurs in each document. This matrix is then factorized and latent variables extracted using 
Singular Value Decomposition (SVD), resulting in a set of vectors for words whose values 
represent the meanings of the words. The similarity between words is typically defined as the 
distance between the vectors representing those words. A similar method is the Hyperspace 
Analogue to Language (HAL) model (Lund & Burgess, 1996), in which a word-by-word 
co-occurrence matrix is created, indexing how often two words co-occur within a window of a 
predefined size. Like with LSA, the similarity between two co-occurrence vectors (or their 
SVD-abstracted vectors, Bullinaria & Levy, 2007; Rohde, Gonnerman, & Plaut, 2004) can be 
used as a measure of the similarity of the words they represent. A number of more complex 
frequency and co-occurrence based models have been proposed, including those that use 
advanced techniques such as random vector accumulation (Jones & Mewhort, 2006) and 
Bayesian inference about the likely latent variables or “topics” that generated an observed corpus 
of speech or text (Steyvers & Tenenbaum, 2005). 
It is worth pointing out that the distributional hypothesis – and specifically its 
instantiation in models like HAL and LSA – has been criticized for suggesting that meaning is 
derived purely from linguistic relations, and that it is not grounded in nonlinguistic information 
(e.g., Glenberg & Robertson, 2000). However, there is nothing in the distributional hypothesis 
that suggests that nonlinguistic, grounded perceptual contexts could not also be treated as 
contexts. For example, Sadeghi, McClelland, and Hoffman (2015) showed that an LSA-like 
algorithm could be implemented using sets of co-occurring objects within images, rather than 
words within documents. Thus, there is nothing inherently problematic with distribution-based 
models that use only linguistic information to test how much semantic information can be 
derived from this modality alone. In fact, one could argue that the absence of nonlinguistic input 
makes it all the more impressive that such models can learn as much as they do. 
In contrast to matrix-factorization-based models like HAL and LSA, prediction-based 
models learn representations by predicting words given their context or vice versa. For example, 
a feed-forward neural network can be used to predict a word given its co-occurrence context, and 
the resulting representations the network learns in order to do this contain surprisingly rich 
semantic information (Bengio et al., 2003; Mikolov et al., 2013a; Pennington et al., 2014). The 
most popular of these approaches, a family of models often referred to as Word2Vec (Mikolov et 
al., 2013a), has become a popular off-the-shelf tool for learning word representations from text 
in machine learning applications. The representations learned by models in the Word2Vec family 
(such as Skip-gram, the algorithm we will focus on in this paper) outperform a number of 
publically available word representations in a benchmark test that includes 8869 semantic and 
10675 syntactic questions (Mikolov et al., 2013a). 
Systematic comparisons between these two model classes have shown that 
prediction-based models generally provide a better fit to behavioral data (Mandera et al., 2012) 
and achieve superior performance on computational linguistic benchmarks (Baroni et al., 2014, 
but see Levy et al., 2015) than models based on matrix-factorization. However, both methods 
suffer shortcomings with regards to using them as theories of semantic development. Methods 
utilizing matrix-factorization typically disregard information about word order completely (in the 
case of LSA), or a word’s position in the sentence (in the case of HAL). Moreover, HAL, LSA, 
as well as Word2Vec, represent words’ meanings as single vectors, and are thus effectively 
prototype models of conceptual meaning. This means that they do not easily have the capability 
of representing ambiguity and polysemy, and are in conflict with an ever-growing literature in 
concept and category learning showing that purely prototype-based models are generally a poor 
fit to behavioral data about human learning (Goldstone & Kersten, 2003; Murphy, 2004; 
Nosofsky & Johansen, 2000). 
Models in the Word2Vec family have additional problems with regard to being taken 
seriously as a cognitively plausible model of semantic development. Word2Vec implementations 
contain a number of optimizations to speed training on large corpora, and has therefore become 
the most widely used tool for obtaining word vectors. But some of these optimizations seem 
unlikely to be the way the human brain performs prediction-based learning. One requirement for 
training Word2Vec’s Skip-gram model is knowing beforehand the frequency of words in the 
corpus (such that relatively frequent words can be downsampled), which is knowledge that is 
inaccessible to online learning circumstances. Another concern is Skip-gram’s negative sampling 
procedure (Mikolov et al., 2013b), where for each prediction, only a subset of possible words are 
sampled from the vocabulary, including the correct next word, and others drawn from a 
distribution that does not include the correct word. This procedure requires knowing the correct 
prediction before the outcome of the prediction is computed. While this speeds training, and 
increases performance in a machine learning context, we think it is unlikely that neural resources 
are allocated to making predictions for which the correct answer is already available. Moreover, 
because there are no recurrent connections, multiple predictions have to be made to update the 
representation of a single word (one for each word in the context window). Not only does this 
require making more update steps per word relative to a recurrent network, but the bidirectional 
context window which is critical to Skip-gram’s success requires that words which should have 
already been processed by a human language learner, must be predicted too. While researchers 
agree that lexical-level prediction indeed play an important role in the human language 
comprehension system (Kutas & Hillyard, 1984; Maess et al., 2016, Willems et al., 2016), most 
would suggest that the purpose of prediction is to facilitate comprehension of upcoming words in 
the speech stream (Lau et al., 2008, Kuperberg 2016 for a review). Predicting backwards in time 
- as is currently the norm for training Skip-gram - would likely slow, rather than facilitate 
comprehension. We cannot, however, exclude the possibility that a Skip-gram like prediction 
mechanism operates in addition to a separate lexical-level prediction process involved in 
language comprehension, but this would require less efficient utilisation of neural resources than 
an all-in-one mechanism. And there is no evidence for such a complex process. 
There are other prediction-based neural networks that might serve as more plausible 
candidates for theories of semantic knowledge acquisition than Word2Vec. For example, the first 
studies using Simple Recurrent Networks (SRNs) showed they could learn to predict sequences, 
and that models that do this learn a lot about the structure of the items in those sequences 
(Elman, 1990; Cleeremans & McClelland, 1990). For example, Elman (1991) investigated an 
SRN, a three-layer network, with an input layer, an output layer, and an intervening hidden layer 
that was connected to itself via recurrent connections. He showed that the SRN could learn the 
regularities of an artificial linguistic corpus composed of thousands of sentences constructed 
following an extremely simplified English grammar composed of nouns, verbs, articles, and 
prepositions. Elman showed that the SRN could learn to predict the “correct” words in terms of 
following the grammatical rules and semantic constraints that were used to generate the corpus, 
such as noun-verb plural agreement, even in cases where the verb was separated from the noun 
by multiple embedded clauses. Furthermore, its ability to track number agreement diminished as 
the length of intervening words grew larger, and this reflects experimental observations in 
humans. 
The SRN’s success at this task was due to its ability to compress sequential information 
into a compact distributed representation at the hidden layer. Elman analyzed the representations 
of words learned by the hidden layer, and showed that their similarity structure can be interpreted 
as a measure of grammatical and semantic similarity between the words. Thus, Elman’s work 
demonstrated that the SRN has high potential to serve as a model of learning semantic structure 
from distributional information. However, like previous researchers investigating feedforward 
models, Elman used an artificial and simplified corpus, and therefore left open the question of 
whether the SRN can scale up to noisy naturalistic language input. Recent large-scale language 
modeling efforts using written language corpora suggest that SRNs can reach prediction 
performance equal to, and in some cases surpassing, n-gram models (until recently the most 
widely used language modeling tool, and now largely replaced by recurrent neural networks) 
(Mikolov et al. 2014), but its success on noisy naturalistic language input, such as speech to 
children has not been investigated. 
As a preliminary comparison of Word2Vec and the SRN, Mikolov et al. (2013a) showed 
that both variants of Word2Vec (Skip-gram and CBOW) trained on three news corpora with 320 
million total tokens achieve substantially greater accuracy on a semantic relationship test than a 
recurrent neural network (RNN) trained on the same input. Nonetheless, a proper comparison of 
the SRN and Word2Vec models in the context of instantiating theories of semantic development 
requires training both models on naturalistic child-directed speech instead of professionally 
composed news corpora. Moreover, performance on benchmarks should not be the only criterion 
when evaluating cognitive models. Considering the numerous optimizations built into Word2Vec 
to make training more efficient, a slight performance increase over models without such 
optimizations should not be the only evidence in favor of Word2Vec as a more appropriate 
model of semantic development. 
A third model which we consider in this paper is the Long Short-term Memory (LSTM) 
(Hochreiter & Schmidhuber, 1997). While the term strictly refers to a special purpose RNN unit, 
we will use LSTM to refer to a network with a hidden and output layer, where conventional 
sigmoidal units in the hidden layer are replaced by LSTM units. As we shall describe below, the 
LSTM unit differs from the traditional sigmoidal (or hyperbolic tangent) unit found in the SRN 
in the addition of gating units which modify the flow of information both within and across time 
steps, vastly increasing the ability of RNNs to learn long-distance dependencies in sequential 
data. The LSTM is of interest because it has proved successful on a variety of sequence 
modeling tasks such as learning context free languages (Gers & Schmidhuber 2001), and 
recalling high precision real numbers over long and noisy sequences (Hochreiter & 
Schmidhuber, 1997), tasks which are very difficult, if not impossible, for the SRN to learn. 
Furthermore, the LSTM reached substantially greater accuracy on a variety of number agreement 
tasks compared to the SRN (Linzen et al., 2016), and was used recently by Microsoft Research to 
reach human parity in conversational speech recognition (Xiong et al., 2016). By training both 
the SRN and LSTM on the same input, we can get an idea of how the ability to track sequential 
dependencies might influence the semantic structure that emerges. If differences are observed, 
they will be due to the specific architectural improvements of the LSTM relative to the SRN, and 
as such will provide insight into the design of a model of semantic development. 
In this paper, we use three different neural networks to test the distributional hypothesis, 
and to ask three specific questions about the relationship between these models and the 
distributional hypothesis. First, can the neural networks learn semantic structure from predicting 
the word sequences of noisy, naturalistic speech to children? Second, if so, do the semantic 
structures that the models acquire reflect the semantic structures that children acquire? Third, do 
different neural network architectures (and the different theories of learning and memory that 
they represent) perform qualitatively or quantitatively differently? Specifically, how do more 
“state-of-the-art” models, such as the LSTM and Skip-gram, compare to the classical SRN? 
2. Methods 
2.1. Corpus 
The first step in testing the potential of the distributional hypothesis is choosing a 
representative sample of naturalistic speech to children. For this, we used the CHILDES corpus, 
a collection of transcripts of interactions with children in various situations (MacWhinney, 
2000). CHILDES contains a mixture of transcriptions of structured in-lab activities (such as 
book-reading, mealtime, and playing with toys), free play in the lab, and in-home recordings. 
We used all transcripts involving typically-developing children 0 to 3 years of age from 
American-English-speaking households. This resulted in a corpus containing 2873 documents, 
22,448 word types, and 5,308,679 word tokens, collected from 52 different studies of 
parent-child interactions. We randomly split the documents into separate training (5,244,672 
word tokens) and testing (64,007 word tokens) corpora, where the former will be used for 
training, and the latter will be used to assess generalization to input not encountered during 
training. Considering that a typical working-class American child receives approximately 6.5 
million words per year (Hart & Risley, 2004), the training corpus represents approximately 
4%-10% of the amount of lexical input of a 3-year-old child (there are large individual 
differences largely predictable by socio-economic status). The documents of the corpus were 
organized in an age-ordered manner, such that each model experienced the input in an 
age-appropriate way, receiving the input of a 6-month-old, then a 7-month-old, then an 
8-month-old, etc. 
The transcribed corpus was tokenized (split on spaces) with sentence-boundary 
punctuation (periods, exclamation marks, commas, and question marks) left in the corpus 
(intended to serve as a very crude way for representing the pauses and prosody that tend to 
accompany utterance boundaries). Spelling was regularized (e.g., differently spelled forms of the 
same word like “
​play-doh
​
” and “
​playdoh
​
” were converted to the same form). Next, all nouns and 
verbs in the corpus were morphologically parsed, splitting off plural (-
​s
​
, -
​es
​
), possessive (-
​’s
​
, 
-
​s’
​
), and diminutive forms (-
​ie
​
and -
​y
​
) from nouns, and splitting off plural (-
​s
​
,–
​es
​
), past-tense 
(-
​ed
​
) and ongoing (-
​ing
​
) forms from verbs. These morphological endings were left in the corpus 
(effectively treating them as words of their own), due to previous research showing a beneficial 
effect on statistical learning models of natural language (Willits, Seidenberg, & Saffran, 2009). 
Proper names were replaced with tokens signifying the gender of the person in question 
(FNAME and MNAME). 
CHILDES is not perfect as a representative sample of the full range of activities that 
parents participate in with their children or the variety of language used during those activities, 
but is instead a useful approximation. Indeed, the relatively constrained set of activities that 
occur in CHILDES ought to hinder learning of useful semantic structure, and thus make positive 
results all the more impressive. 
2.2. Vocabulary, Probe-Words, and Categories 
To reduce training time and simulate the fact that children are unlikely to know the 
lexical form of the lowest frequency items in the corpus, we limited the model’s vocabulary to 
the 4096 most frequent words. The other 18,352 word types (making up 41,532 tokens, or 0.8% 
of the training corpus) were replaced with the word UNKNOWN. 
To test the semantic structure, we selected a subset of the vocabulary words to serve as 
test words for all subsequent analyses. These words (we will refer to them as probe-words) were 
obtained by 1) choosing the subset of word forms which could be nouns (even if, in practice they 
appear more often in verb form, such as 
​jump
​
), 2) choosing the subset of those that refer to a 
concrete object, and 3) choosing the subset of those that unambiguously belong to a semantic 
category to which at least 6 other words belong, according to a set of human raters. For example, 
apple
​
, 
​orange
​
, and 
​banana
​
(along with many others) were included because they belonged to a 
large category of items that contained at least six items. The result was a set of 720 words 
belonging to 29 categories. This set of 29 categories used for and analysing the models is shown 
in Table 1. 
Table 1. ​
The set of categories, the number of word types in each category, and the number of 
occurrences of word types in each category in the training corpus. 
Category 
Word types 
Word tokens 
Category 
Word types 
Word tokens 
Bathroom 
22 
5533 
Mammal 
72 
35781 
Bird 
27 
8384 
Meat 
18 
2914 
Body 
62 
42601 
Months 
13 
1897 
Clothing 
48 
16022 
Music 
14 
1845 
Days 
14 
8163 
Numbers 
27 
41048 
Dessert 
20 
9048 
Plants 
15 
6006 
Drink 
14 
9880 
Shape 
13 
3355 
Electronics 
18 
5347 
Space 
14 
3042 
Family 
32 
52539 
Times 
11 
7731 
Fruit 
28 
7719 
Tools 
28 
7665 
Furniture 
28 
11131 
Toys 
30 
25339 
Games 
6 
1222 
Vegetable 
21 
3271 
Household 
32 
10930 
Vehicles 
34 
15559 
Insect 
18 
4755 
Weather 
11 
4082 
Kitchen 
29 
7767 
2.3. Model Implementation 
All three models were trained on a machine with 32 GBs of RAM, a 8-core 3.0GHz Intel 
Xeon processor, and a NVIDIA GTX 1080 GPU. To train the recurrent neural networks, we used 
the open-source machine-learning framework TensorFlow (Abadi et al., 2016), and to train 
Skip-gram, we used Gensim(Rehurek & Sojka, 2010), a free Python library which provides APIs 
for a wide variety of semantic models. The code, including the training corpus, and test materials 
are available at https://github.com/phueb/rnnlab. Using a mini-batch size of 64, training one 
LSTM and one SRN on the GPU takes approximately 3 hours and 2.5 hours, respectively. Using 
4 CPU cores in parallel, Skip-gram completed training in less than five minutes.
2.4. Model Architecture and Training 
Figure 1.​
A simple comparison of model architectures showing the Simple Recurrent Network 
(1A), a single Long Short Term Memory unit (1B), and Word2Vec’s Skip-gram. Note that 1B 
depicts a single LSTM unit, rather than the full LSTM model. The complete LSTM architecture 
consists of three layers as in 1A but with LSTM units instead of conventional sigmoidal or 
hyperbolic tangent units at the hidden layer. Colors depict the level of activation from 0 to 1 
(yellow to red). Skip-gram does not have the full vocabulary represented at the output layer as in 
1A, as Skip-gram is typically trained using negative sampling, where the objective is to predict 
the correct answer amongst a small selection of “negative” words. 
2.4.1. Simple Recurrent Network Architecture 
The Simple Recurrent Network (SRN) is an artificial neural network that contains an 
input, a hidden, and an output layer, in addition to recurrent connections linking the hidden layer 
to itself at the previous time step (Elman, 1990). The hidden layer learns distributed internal 
representations of the input, and the recurrent connectivity allows these representations to encode 
information from previous time steps. This means that the hidden layer’s pattern of activations is 
not a simple representation of the input stimulus, but rather the input stimulus in the context in 
which it occurred. 
A schematic of the SRN’s architecture is shown in Figure 1A. For each time step, the 
SRN received as input a localist representation of a single word drawn sequentially from the 
training corpus (which is ordered by the age of the children who are being spoken to). The size 
of the input vector was equal to the size of the vocabulary (4096), and contained zeros at every 
position except for the position uniquely assigned to the current input word. We opted for a 
localist representation not only because it is the most commonly used coding strategy in 
language modeling, but also because it ensures that the model does not have access to any prior 
knowledge (phonological, semantic, etc.) about the words in the input. The goal of this scheme is 
not to claim that children do not utilize additional sources of information, but to test just how 
rich a child’s semantic knowledge could become based on lexical distributional information 
alone. 
The activations at the hidden layer (512 units) are the result of multiplying each input 
unit’s activation by the weighted connections from that input unit to each hidden unit. Critically, 
the pattern of activation from the hidden units at the previous time step is added, weighted by the 
recurrent connections. Lastly, each hidden unit activation is squashed by the hyperbolic tangent 
nonlinearity to constrain its activation between -1 and +1. 
The hidden layer activations are then sent via a third set of weighted connections to the 
output layer (again containing 4096 units, one for each word including UNKNOWN). To 
transform the output layer’s weighted inputs into a posterior probability distribution of the next 
word in the corpus, we use the softmax operation. First, each unit’s weighted input is 
transformed by exponentiating e with the weighted input, and the result is divided by the sum of 
all weighted inputs at the output layer. After the softmax operation, each output unit’s activation 
represents the probability that the word it codes is the next word given the input. 
2.4.2. Long Short-term Memory Architecture 
Because the amount of information in the hidden layer representing distant past 
information decreases with each time step, learning dependencies across longer distances 
becomes increasingly difficult for the SRN. This problem has been referred to as “the vanishing 
gradient problem” (Hochreiter et al., 2001), referring to the vanishing across time steps of the 
signal carrying information that specifies how to update the weighted connections. Numerous 
workarounds have been suggested, and the most successful of them is the Long Short-term 
Memory (LSTM) unit introduced by Hochreiter and Schmidhuber (1997). The LSTM unit, rather 
than being a single unit, is a set of three multiplicative gating units which control the flow of 
information to and from a central unit (termed the “memory cell”) whose activation does not 
undergo nonlinear transformation. The architectural details of a single LSTM unit are shown in 
Figure 1B. Note that this is not the complete LSTM; instead, we will refer to the LSTM as a 
three-layer neural network similar to the SRN, with LSTM units replacing the conventional 
sigmoid (or hyperbolic tangent) units at the hidden layer. 
To illustrate the advantage of the gating units, consider that in the SRN (which does not 
employ gating units), long-term memory is stored in the hidden layer where new information is 
integrated with little regard to whether it is relevant to the model’s objective or might instead 
mask already existing (and possibly useful) information. To prevent this, long-term information 
in the LSTM is stored in the memory cell where it can only be written to or read from by first 
being gated by an input (i
​
t
​
) and forget unit (f
​
t
​
), respectively. More precisely, incoming 
information can only be added to the memory cell if the input gate is not set to zero. Similarly, if 
information already contained in the memory cell at the previous time step is not useful at the 
next timestep, the forget unit can flush the contents of the memory cell by being set to zero. The 
output unit (o
​
t
​
) gates the content of the memory cell after being transformed by a nonlinear 
function (hyperbolic tangent in our case), and it is the output of this gating operation which is 
typically fed into the next layer. Thus, the LSTM unit contains two distinct outputs which are 
reused across timesteps: the content of the memory cell (c
​
t
​
), and the result of the output gating 
operation (h
​
t
​
). The former can preserve gradient information across more timesteps relative to 
conventional units due to preserving only the information that is most useful and because it does 
not undergo any nonlinear transformation, while the latter allows for learning of nonlinear 
relationships. The gating units receive information in the input (x
​
t
​
) and from h
​
t-1
​
, which are 
multiplied by a unique set of weights, and are then squashed through the sigmoid function to 
result in an activation bounded between 0 and 1. Because the weighted connections from x and 
h
​
t-1 
​
to the gating units are trainable, the network can learn to modify the flow of information into 
and out of the memory cell, and it will do so independently at every timestep. 
2.4.3. Recurrent Neural Network Training Regime 
Contrary to conventional training methodology in which input sequences are presented to 
the model for multiple iterations over the whole corpus (epoch training), our models iterated over 
small partitions of the corpus, presented in order. As a simplified example, for a corpus of 3 
documents iterating 4 times, a typical training procedure would have the documents presented to 
the model in the order: 1,2,3,1,2,3,1,2,3,1,2,3, and we instead presented the documents in the 
order 1,1,1,1,2,2,2,2,3,3,3,3. There are at least two potential advantages of this approach: First, 
given that our corpus contains transcribed speech ordered by the age of the children addressed, 
our models are sensitive to any change over time in the structure of child-directed speech. 
Secondly, local iterations lend cognitive and biological plausibility to our training regime 
because it is more likely that children consolidate linguistic experiences across time periods 
spanning hours or days rather than months or years. We split the corpus into 256 partitions, so 
that each partition contains an average of 26,000 words, approximately the average number of 
words heard by children in one day (Hart & Risley, 2003). During training, iterations pass over 
each partition 20 times, guaranteeing that no input will be re-learned by the model past a day’s 
worth of linguistic input. We would argue that this is a better model for language learning in 
children, although the disadvantage of this approach is the increased propensity for overfitting 
and catastrophic interference compared to a model that is trained in the traditional (albeit less 
cognitively plausible) manner. 
We trained ten SRNs and ten LSTMs using the same hyperparameters, but a different 
random seed during weight initialization. Weights were initialized with a truncated normal 
distribution with mean at zero and standard deviation
, where 
​m
​
is the number of units in
/
1
√
m
the layer above. A bias unit was used at the output layer and its weights were initialized at zero. 
The input to the recurrent neural networks consisted of sequences containing 7 words drawn 
sequentially from the corpus. The input was fed through the networks (as described above) and 
resulted in a probability distribution of predictions for the subsequent word. We used the 
cross-entropy operation to compare a model’s predictions to the correct answer, which is 
equivalent to the negative log of the probability assigned by a model to the correct answer. We 
used truncated backpropagation through time (Werbos, 1990; Williams & Peng, 1990) to 
compute the partial derivative of each layer’s activations with respect to the cross-entropy, and 
used these to update the weights in the direction that minimized the cross-entropy. This 
procedure was followed sequentially for each input sequence. We set the learning rate to 0.01 
and used Adagrad optimization (Duchi et al., 2011) to adapt the learning rate so that infrequently 
changed weights received a greater update than those changed more frequently. 
We used mini-batch training in which weight updates only occurred after the 
accumulation of prediction errors from 64 words. In this way, the weight update reflects the 
average prediction error computed for all 64 sequences in the mini-batch. While the primary 
motivation for using mini-batching is to speed training, the cognitive and neural plausibility of 
mini-batch learning is contestable. However, we did test a range of different mini-batch sizes, 
finding that sizes greater than 64 led to slightly worse results, with no noticeable differences 
(other than in training time) for smaller mini-batch sizes, including a size of 1. Thus, we believe 
this detail in the model to be a benefit with regard to training time without a cost in terms of 
qualitatively or quantitatively changing the model’s behavior and thus calling into question its 
cognitive or neural plausibility. 
2.4.4. Word2Vec’s Skip-gram 
Skip-gram is one of two members of the Word2Vec family of neural networks introduced 
by Mikolov et al. (2013a) as an efficient solution for obtaining word representations from very 
large corpora. Word2Vec models consist of an input, hidden, and output layer, and weighted 
connections to and from the hidden layer. While Skip-gram’s sister algorithm, CBOW, generates 
word representations by predicting a word from its context, Skip-gram predicts the context from 
the word in which it occurred (shown in Figure 1C). We opted for Skip-gram because it 
produced superior results compared to CBOW on our corpus. In contrast to the LSTM and SRN, 
both Word2Vec models lack recurrent connections, and therefore do not learn contextual word 
representations (one for each occurence of a word), or the precise order of words in the corpus. 
One reason for including Skip-gram in a paper about child language learning is to 
compare the representations learned by the SRN and LSTM to those generated by the current 
state-of-the-art model in machine learning. Skip-gram’s performance will help to contextualize 
our results when making inferences about the SRN’s achievements. Secondly, and perhaps more 
importantly, because Skip-gram is blind to the order of words (when Skip-gram predicts a 
context word, it is never told what distance it was located away from the input word), we can 
gain insight into how using only statistics which do not consider the precise order of words 
influences the learning of semantic structure. As described above, Skip-gram also includes a 
number of optimizations that increase its performance but vastly decrease its plausibility (or at 
least its parsimony) as a cognitive model. Despite these problems, we examined Skip-gram due 
to its popularity in the machine learning community as a semantic model, and to see if it varies in 
performance on our dataset or tasks. 
We trained Skip-gram with a hidden layer size of 512, 20 epochs, and a bidirectional 
window size of 3. All other model properties were left as the default values specified by Gensim. 
Our choice of hyperparameters was driven by our goal of matching as closely as possible the 
hyperparameters used by the recurrent neural networks. Gensim did not provide the option of 
using the traditional softmax function at the output layer; so we were limited to using either 
hierarchical softmax or negative sampling. We opted for the hierarchical softmax as it is a more 
reliable estimate of the true softmax (Goldberg & Levy, 2014) which we employed in the LSTM 
and SRN. We found no difference in performance, however, when we used negative sampling. 
3. Results 
Due to the simplicity and parsimony of the SRN compared to the other two models, we 
focus our analysis on its performance, and include results from the other two models only in 
cases where they differ substantially. We begin by confirming that the recurrent neural networks 
have learned the sequential structure of the language input. Next, we present two kinds of results 
to assess the semantic knowledge that emerged after training. First, we demonstrate in a 
qualitative fashion the semantic structures that the neural networks acquired. Secondly, we show 
the results of a quantitative analysis of a model’s abilities to infer, from its learned 
representations, that two words belong to the same category. 
3.1. Sequential Structure Prediction 
Before and after training, we calculated the average per-word perplexity, a measure of the 
model’s ability to correctly predict the next word, on a subset of the CHILDES corpus not used 
during training (64,000 words). On a single prediction trial, the mean per-word perplexity score 
is equal to the number of times the model would have to sample from a uniform and independent 
probability distribution to guarantee that one of the guesses is correct. Thus, a perfect prediction 
would result in a per-word perplexity score of 1, and a perplexity score of 50 (for example) 
implies that the model thinks that about 50 different words are equally likely. 
At the end of training, the mean per-word perplexity score of the ten SRNs was 43.8 ± 
0.05 (M ± SEM), and that of the ten LSTMs was 42.6 ± 0.01 (M ± SEM). Compared to the same 
score before training (4102.2 
2.4 for SRN, 4095.9 
0.6 for LSTM), this is a significant
±
±
reduction, and strong evidence that learning of word sequences has taken place. In other words, 
the average number of equally likely predictions across all words in the test data were reduced 
from approximately the total number of words in the vocabulary before training to only about 
42-44 at the end of training for the SRN and LSTM. Because Skip-gram cannot predict word 
sequences there is no way to directly compare Skip-gram on this measure. 
3.2. Analyses of semantic structure 
We performed several analyses to assess the semantic structure that the neural networks 
acquired. These analyses were all based on comparing how a model judged the similarity of a 
pair of words. In the case of the recurrent neural networks (the SRN and LSTM), we calculated 
similarity using the distributed pattern of activations in the hidden layer as the model’s 
representation of that word. For example, Figure 2 shows the hidden layer activations for the 
word 
​helicopter
​
, for all 175 of its occurrences in the training corpus. A hierarchical clustering 
analysis on each unit and occurrence was performed, so as to arrange the rows and columns in 
terms of their similarity. A number of interesting patterns emerge from this analysis. First, for 
each occurrence of 
​helicopter
​
in different context, the network generates slightly different hidden 
activations each time. The general pattern of similarity across the columns show that there is a 
definite central tendency with regard to the pattern of activation that 
​helicopter
​
generates (driven 
by the weights from its input unit to the hidden layer), but that the weights from the recurrent 
layer to itself create subtle differences in the way 
​helicopter
​
is represented, resulting in a number 
of different context-dependent senses of the word 
​helicopter
​
. This nicely demonstrates the way 
that SRNs and LSTMs can accommodate and explain effects of polysemy and ambiguity. But it 
does pose a problem with regard to attempts to simplistically compare the similarity of words. 
For the following analyses, we computed the pairwise similarity between all 720 
probe-words and all 4096 words in the vocabulary in the following manner. First, we re-fed the 
corpus into the model (after training and without updating the weights) and saved the hidden 
layer activations at the end of every sequence. To obtain a single representation for each 
vocabulary word, we averaged across those activations obtained for sequences in which the word 
in the last position matched. For example, the representation of 
​helicopter
​
was obtained by 
averaging the activation patterns across all 175 sequences in which 
​helicopter
​
was the last word. 
This corresponds to averaging across all 175 columns in Figure 2. We then obtained similarity 
scores by computing the correlations between each word’s 512-element “average” 
representation. 
In the case of Skip-gram, the representation of a word was obtained simply from the 
weights connecting that word’s input unit to the hidden layer. For example, the 
​dog
​
and 
​cat
​
input 
units each have 512 weighted connections projecting from their input units to the 512 hidden 
units. Thus, to the extent to which two words make similar predictions about what their 
surrounding contexts are, they tend to have similar values for the weighted connections 
projecting to the hidden layer. 
Figure 2​
. A Heatmap showing the hidden activations for the word 
​helicopter
​
associated with all 
its occurrences in the training corpus. The data was hierarchically clustered such that rows and 
columns which are more similar are located together more closely. 
3.2.1. Overall Similarity Between Models 
Our first analysis concerned how similar the overall semantic spaces were in the ten 
different randomly initialized runs of each model, and also how similar the overall semantic 
spaces were between models. In other words, if the first SRN thought 
​dog
​
’s similarity with 
​cat
​
, 
shoe
​
, 
​cloud
​
, and 
​car
​
were 0.95, 0.76, 0.81, and 0.91, respectively, and the second SRN thought 
the scores were 0.94, 0.77, 0.80, and 0.90, this would reflect high agreement between the models. 
We computed this quantitatively as one would inter-rater reliability (Shoukri, 2010), by taking 
each model’s 720 by 4096 matrix of similarity scores and correlating them with one another, 
resulting in a r-value measuring the similarity of different models’ semantic spaces. These 
analyses showed that the different instances of the SRN were most similar to one another, 
(r=0.967 ± 0.0002, mean ± standard error), that the LSTM instances were less similar (r=0.959, ± 
0.002), and that the different instances of Skip-gram were the least similar to one another 
(r=0.889, ± 0.0005). Moreover, the semantic spaces of the SRNs and LSTMs were much more 
similar to each other (r=0.92, ± 0.0004) than those of either the SRN and Skip-gram (r=0.248, ± 
0.0003) or the LSTM and Skip-gram (r=0.284, ± 0.0005). 
3.2.2. Nearest Neighbors 
The first qualitative analysis we performed was to obtain a list of the most similar words 
(nearest neighbors in high dimensional semantic space) for each of the 720 probe-words. Table 2 
shows the five nearest neighbors for five probe-words for all three models. The words were 
chosen to illustrate typical semantic relations the model has acquired. Full lists of neighbors for 
all 720 probe-words are available at 
​
http://languagelearninglab.org/materials/childes-models/
​
. 
A number of notable facts are worth pointing out about the nearest neighbors. First, the 
nearest neighbors of most probe-words share clear semantic relations, and this is evidence that 
these models have acquired general knowledge about semantic similarity, which strongly 
supports the distributional hypothesis. Second, this fact was true for all three models, with 
obvious qualitative differences in the relatedness or type of relatedness between a probe-word 
and its neighbors. 
A third detail is that the there seems to be a general trend in the types of semantic 
relations the models thought were similar, as a function of the type of word. If the word is an 
example of a relatively well-defined or rule-based category, its neighbors tend to be members of 
the same category, even if such pairs do not co-occur in the corpus and thus are not likely to be 
thematic relations (such as 
​dog
​
and 
​tiger
​
). In contrast nearest neighbors for many human artifact 
categories (like ‘tools’, ‘household rooms’, and ‘furniture’), while still including mostly 
taxonomic relations, also include some thematically related neighbors. This pattern of data that 
shows an interesting resemblance to an observed bias by toddlers to use words to group 
taxonomically related things for well defined categories (Waxman & Markow, 1995; Keil, 
1992). The model predicts this bias may be less strong for artifact categories than for natural 
kinds and rule-based categories like numbers, months, and days. 
Table 2. ​
Nearest semantic neighbors after training for 1 of the 10 models for selected words, in 
terms of the average hidden activation state of the network (for SRNs and LSTMs) and in terms 
of the weight matrix (for Skip-gram).
SRN 
dog 
squirrel 0.95 
fox 0.95 
horse 0.95 
tiger 0.95 
wolf 0.95
bed 
crib 0.93 
room 0.92 
desk 0.92 
pouch 0.92 
house 0.92
shoe 
sock 0.97 
sneaker 0.95 
boot 0.95 
sandal 0.95 
jacket 0.94 
banana 
carrot 0.97 
pretzel 0.96 
cracker 0.96 
cheerio 0.96 
lemon 0.96 
five 
six 0.95 
four 0.95 
three 0.94 
ten 0.93 
seven 0.93
LSTM 
dog 
wolf 0.95 
fox 0.95 
horse 0.95 
mouse 0.95 
penguin 0.94 
bed 
desk 0.94 
crib 094 
shade 0.93 
bedroom 0.93 
room 0.93 
shoe 
sock 0.98 
sneaker 0.96 
sandal 0.95 
boot 0.94 
sweater 0.94 
banana 
cheerio 0.96 
carrot 0.96 
pretzel 0.96 
hamburger 0.96 
peach 0.95 
five 
four 0.97 
six 0.96 
eight 0.94 
seven 0.94 
three 0.94 
Skip-gram 
dog 
pup 0.76 
collie 0.62 
kitten 0.57 
woggy 0.56 
bark 0.53 
bed 
sleep 0.63 
crib 0.59 
blanket 0.54 
bedroom 0.53 
nap 0.47 
shoe 
sock 0.77 
sneaker 0.77 
sandal 0.64 
pant 0.63 
shoelace 0.58 
banana 
pear 0.58 
raisin 0.56 
frozen 0.55 
cereal 0.55 
oatmeal 0.54 
five 
six 0.88 
four 0.83 
seven 0.77 
three 0.74 
eight 0.69 
3.2.3. Dimensionality Reduction
In order to visualize distances between probe-word representations in the model’s 
512-dimensional hidden activations space, we used a t-SNE dimensionality reduction algorithm 
(van der Maaten & Hinton, 2008), available via the Python package Scikit-Learn. We ran the 
algorithm using the average hidden activations for each probe-word as input (shown in Figure 3). 
Due to space constraints, we only show the t-SNE for a single SRN, because no significant 
qualitative differences existed between different instances of the model. t-SNE figures for all 10 
runs of all three models are available at 
​
http://languagelearninglab.org/materials/childes-models/
​
. 
Several qualitative patterns emerged from this two-dimensional representation. When 
inspecting the category labels which are positioned at the average location across category 
members, an organizational scheme between categories becomes noticeable. First, words 
belonging to the categories ‘times’, ‘months’, and ‘days’ occupy a section of the similarity space 
that is distinct from all other probe-words. It makes sense that these are separate from items 
referring to concrete objects, and given that they all relate to timekeeping in some form, that they 
should be positioned closer to each other. Probe-words in the categories ‘bird’, ‘insect’, and 
‘mammals’ also form a distinct super-category cluster in the bottom portion of the figure. 
Categories containing non-living objects, and edible objects also occupy distinct portions of the 
similarity space. 
Lastly, of note is the location of the probe-words belonging to the category ‘body’ in the 
top-right portion of the figure. It is obvious that body parts are not human-made objects like 
those referred to by words belonging to the categories ‘toys’ and ‘games’, and that they aren’t 
living objects with agency either, so they are not close to ‘birds’, ‘mammals’, or even ‘family’ 
clusters. In fact, the ‘body’ category occupies its own distinct space along with ‘clothing’, even 
though clothes are far from being the same kind of object, physically, as well as conceptually. 
However, when thinking about the relationship in terms of interactions taking place in the real 
world, their adjacency in the figure becomes understandable. The human body is dressed more 
often than anything else we might do to it. Notice also that probe-words belonging to the 
categories ‘kitchen’ and ‘bathroom’ are located separately from ‘household’ and ‘furniture’. This 
spatial arrangement does not hold reliably when rerunning the t-SNE algorithm, which is not 
deterministic. While these two categories are often located more closely to ‘household’ and 
furniture’, we show this arrangement because we think that the model is picking up on an 
important difference in the way objects in the kitchen and bathroom are used compared to those 
in other places of the household. We tend to interact more often with objects in the former two, 
either for cooking or tending to our hygiene. Objects in the ‘household’ and ‘furniture’ category, 
in contrast, are less frequently interacted with. The above two observations suggest that the 
model organizes objects not only taxonomically, but also pragmatically. 
Figure 3. ​
A t-SNE dimensionality reduction projection, showing a 2-D representation of the 
relative similarities of the 720 probe-words learned by the SRN. 
3.2.4. Similarities between Categories 
To get a more complete understanding of the extent to which the model’s semantic 
structures are taxonomically-driven and hierarchically-structured, we constructed a dendrogram 
heatmap reflecting the similarities between probe-words within the same category and between 
different categories, shown in Figure 4. To do this, we started with a 720 by 720 matrix 
containing similarity scores for each pair of probe-words. Next, we removed the diagonal of this 
matrix (all ones, reflecting that each word was perfectly similar to itself), and then computed the 
average similarity of words within and between each of the 29 categories. This resulted in a 
29x29 matrix of similarity scores. The rows and columns of this matrix were re-arranged by 
performing hierarchical clustering on the resulting 29x29 matrix. The resulting heatmap has 
higher values on the diagonal, indicating words in the same category have strongly correlated 
activation states. But in addition to this main effect of words being more similar, on average, to 
other words of the same category, there were also off-diagonal clusters which indicate cases 
where the model has learned a set of closely related categories. For example all of the categories 
containing ‘food’ probe-words are found in the lower left portion of the figure. A smaller cluster 
is obtained for categories containing living items, and another large grouping in the upper-most 
right portion of the figure includes categories containing non-living objects like ‘toys’, ‘tools’, 
and ‘vehicles’. 
The dendrogram on the right side of each heatmap in Figure 4 gives a sense of the 
hierarchical organization underlying the model’s similarity judgements. The categories 
‘numbers’, and ‘months’ occupy a distinct branch, indicating that these categories are used in a 
fairly distinct way (paralleling their distinct clusters in Figure 3). These results reflect an 
interesting property of the models. As shown in earlier work (Elman, 1990; Rogers & 
McClelland, 2004), neural networks, while not explicitly encoding or representing hierarchical 
structure, nonetheless produce a set of activations whose similarity encodes hierarchical structure 
in a latent way. This fact remains true even when the input is noisy naturalistic data, where these 
patterns are not explicitly built into the model’s training data. The qualitative nature of these 
taxonomic and hierarchical structures presents an intriguing set of testable hypotheses, namely 
whether children acquire a semantic structure like that acquired by the model, and whether these 
structures are a quantitative and qualitative fit to behavioral data. 
More clearly than in the neighborhood or t-SNE analyses, there do seem to be interesting 
differences between the three types of models, in terms of their between- and within-category 
relationships. Looking at the LSTM’s heatmap dendrogram in Figure 4, we noticed that the most 
separate cluster, determined by the dendrogram to the right, contains all the food categories, 
rather than time-related categories. Instead, the LSTM has a distinct fourth cluster for 
timekeeping categories, including ‘days’ and also ‘numbers’, although the latter category can be 
more broadly used. Figure 4 also shows the heatmap dendrogram for Skip-gram, which is 
different in many respects compared to the two recurrent neural networks. First, the similarities 
are globally lower, which is an artefact of the difference in the two training algorithms, rather 
than a difference in the global similarity structure. While the minimum and maximum 
similarities are shifted, we kept the size of the range the same across all diagrams to enable 
comparing relative similarities between categories across models. Another important difference 
is that the cluster of categories referring to human-made artifact categories is much less distinct. 
Indeed similarities within these categories are much higher relative to similarities across 
categories. One might conclude from this that Skip-gram has learned more about the differences 
in the probe-words referring to human-made objects than their similarities. Its four most 
prominent clusters include animal and food categories, as in the SRN, but also a time-keeping 
category cluster, as in the LSTM, and a unique cluster referring to objects or concepts typically 
found outside, including ‘space’, ‘weather’, and ‘plants’. It is interesting that Skip-gram didn’t 
acquire a human-made categories cluster, but was able to cluster categories based on the concept 
of ‘outside’. A final important difference to the recurrent neural networks is the grouping of 
‘kitchen’ with the food categories instead of with its more taxonomically related categories 
‘bathroom’, ‘household’, or ‘furniture. From this analysis, it has become clear that Skip-gram 
tends to group categories more thematically than the two recurrent neural networks. 
Figure 4. ​
Dendrogram heatmap diagrams showing the average similarity of words (i.e. the 
Pearson correlations of the words’ hidden state activations) within and between categories for A) 
SRN, B) LSTM and C) Skip-gram. Row ordering mirrors that of columns. Note color scale is 
different for skip-gram.
3.2.5. Hierarchical Structure Within Categories 
To demonstrate the extent to which the internal representations have latent hierarchical 
organization 
​within
​
each category, we used the same clustering algorithm as above, this time 
restricting the analysis to similarity scores from within one category. Because space is limited, 
Figure 5 only shows examples of the categories ‘family’, ‘kitchen’ and ‘space from the SRN, but 
the dendograms for other categories and models are available at 
http://languagelearninglab.org/materials/childes-models/
​
. 
Many interesting details about the semantic structures learned by the models become 
apparent from these figures. Beginning with ‘family’ (Figure 5, left), the most closely related 
word pairs (i.e. pairs which share a branching point with the shortest distance from 0) are 
grandfather
​
next to 
​grandmother
​
, and 
​father
​
nearest to 
​mother
​
. It is notable that synonym-like 
words for the same role do not tend to group together, instead grouping with their 
opposite-gender counterpart ( 
​father
​
and 
​mother
​
, not 
​ma
​
and 
​mother, 
​
are more closely related). 
This happens because contexts that follow the word 
​mother
​
tend to be very similar to those that 
follow the word 
​father
​
, resulting in the model learning that these two words are very similar or 
substitutable. Because the SRN is explicitly learning which words are substitutable with one 
another, it ends up with semantic organization that reflects contextual and pragmatic factors such 
as that 
​mother
​
and 
​ma
​
tend to be used in very different situations (predicted by formality), rather 
than a semantic organization that reflects dictionary definitions or feature-based synonyms. 
Again, this forms an intriguing testable hypothesis about the macro- and micro-organization of 
children’s semantic knowledge, namely that these pragmatic and contextual factors may play a 
larger role than has been supposed. 
Similar insights are provided by the hierarchical clustering of words in ‘kitchen’ (Figure 
5, middle). The largest two clusters seem to be separated according to objects used to prepare 
food (bottom cluster) and objects which are associated with the eating of food (top cluster). 
Microwaves and toasters, referred to by words in the bottom cluster, modify food by changing 
their temperature, whereas most words in the top cluster refer to objects that do not modify, but 
instead are present during consumption of food, such as teapots, silverware, napkins, etc. 
The third clustering is illustrative because it shows that hypernyms such as ‘world’, 
‘planet’, ‘star’ are distinctly separated from hyponyms, such as ‘venus’, ‘mars’ (Figure 5, right). 
In other words, the hypernym-containing cluster on the bottom of the figure contains words that 
do not refer to any particular object in space, whereas those in the top cluster do. This provides 
evidence that the model can learn to separate between concrete objects and categories containing 
those objects. 
Figure 5. ​
Hierarchical clustering dendrogram of words in the categories “family members”, 
“kitchen items”, and “space”. 
3.2.6. Quantitative Analyses of Semantic Category Knowledge 
Next we asked to what extent the internal representations can be used in a semantic 
classification task, in which two probe-words are judged to be in the same category. Judgements 
are based on a 720 by 720 matrix of the similarity of all probe words with one another. In this 
task, all word pairs’ similarity scores (S) were compared against a decision threshold and used to 
guess if the two words belonged to the same semantic category. We analyzed these results in a 
signal detection framework, computing hits, misses, correct rejections, and false alarms for each 
probe-word pair at multiple similarity thresholds (r, between 0.0 to 1.0 with step size 0.001). In 
other words, if two probe-words i and j belong to the same category, and S
​
i,j 
​
> r, a hit is recorded, 
whereas if S
​
i,j
​
< r, a miss is recorded. On the other hand, if the two probe-words do not belong to 
the same category, either a correct rejection or false alarm is recorded, depending on whether S
​
i,j
< r or S
​
i,j
​
> r. For each probe-word, we calculated the sensitivity and specificity, and averaged the 
two to produce the balanced accuracy. This procedure eliminates bias resulting from the fact that 
a vast majority of word pairs do not belong to the same category. The measure of interest was the 
average of all the probe-words’ balanced accuracies at the similarity threshold which yielded the 
highest value. 
We repeated this process for each of the ten SRN, LSTM, and Skip-gram models to 
obtain an average balanced accuracy of 70.0% ± 0.05% (mean ± standard error) for the SRNs, 
73.4% ± 0.05% for the LSTMs, and 73.7% ± 0.03% for Skip-grams. With such large differences 
between models and low variances within models, t-tests comparing differences between models 
result in very large differences: 
​t
​
(18) = 45.88, 
​p
​
< 0.0001, 
​r
​
2
​
= 0.992 for the difference between 
the SRN and LSTM; 
​t
​
(18) = 77.38, 
​p
​
< 0.0001, 
​r
​
2
​
= 0.997 for the difference between the SRN 
and Skip-gram; 
​t
​
(18) = 13.56, 
​p
​
< 0.0001, 
​r
​
2
​
= 0.911 for the difference between the LSTM and 
Skip-gram. 
In the case of the SRN, this means that on average a probe-word pair has a 70.0% chance 
of being correctly classified as belonging or not belonging to the same category. This is well 
above 50%, the score that an untrained model would be expected to receive. It is not surprising 
that both the LSTM and Skip-gram outperform the SRN on this task, given previous research 
demonstrating their improved performance on sequence learning and semantic tasks, 
respectively. It is somewhat surprising that Skip-gram did not achieve superior performance 
compared to the LSTM, given that Skip-gram’s architecture is uniquely optimized to produce 
high quality word representations, whereas the LSTM’s objective is to learn sequential 
dependencies. 
Inspecting the balanced accuracy for individual pairs sorted by category membership 
(shown in Figure 6), we found a large range across different categories, ranging from just above 
50% (for words in the ‘weather’ category, like 
​cloud
​
, 
​rain
​
, 
​sunshine
​
, 
​snow
​
), to just over 90% (for 
words in the ‘day’ category, like 
​monday
​
, 
​tuesday
​
, and 
​wednesday
​
). These differences are a 
quantitative assessment of which words’ internal representations form more cohesive categories, 
and which words the models would have difficulty determining belong to the same category. 
This is important because it allows us to make testable predictions about infant language 
development for future behavioral experiments. Comparing the balanced accuracy of the SRN to 
those of the other two models, reveals that the LSTM achieves slightly better scores for each 
category, whereas Skip-gram’s superior performance is due to its increased performance on a 
few categories only (‘plants’, ‘meat’, ‘months’, ‘fruit’). On other categories, Skip-gram’s 
balanced accuracy drops slightly (‘toys’, ‘electronics’, ‘vegetable’) or well below (‘furniture’, 
‘household’) that of the SRN. 
There are many different reasons for variability across words and categories. It is 
important to note that the categories were chosen by the authors and revised based on the explicit 
judgments of adult experimental participants. It could be the case that the categories with lower 
scores are less “real” in either a natural or psychological sense, and thus these lower scores 
reflect exactly how we would expect the model to perform. It could also be the case that these 
categories, while quite real to adults, are less important to children and thus not frequent or 
consistent in child-directed speech. Follow-up corpus analyses and behavioral experiments (with 
children and adults) can test these predictions. 
Learning outcomes, as with previous connectionist models, depend on how frequently 
and how consistently words are used (Seidenberg & McClelland, 1989). By performing worse on 
some items, the model is making specific predictions about how frequency and consistency may 
affect the categories and conceptual structure that children acquire, which can be tested in future 
research. 
Figure 6. ​
Balanced accuracy when using a model’s similarity scores to judge whether two words 
belong to the same or to different categories. The grey area represents the standard error of the 
mean. LSTM is colored orange, SRN is colored green, and skip-gram is colored black. 
Table 3. ​
Nearest semantic neighbors from SRN, LSTM, and Skip-gram for two words in the 
categories ‘weather’, ‘meat’, and ‘months’. 
SRN 
snow 
LSTM 
snow 
Skip-gram 
snow 
treasure 0.87 
log 0.86 
motorcycle 0.86 
taxi 0.86 
mail 0.86 
rocket 0.90 
fish 0.88 
snail 0.88 
cloud 0.88 
mole 0.88 
man 0.57 
flake 0.55 
white 0.52 
baum 0.50 
melt 0.46 
rain 
rain 
rain 
flash 0.92 
dust 0.91 
land 0.90 
steam 0.90 
crowd 0.90 
dark 0.91 
daytime 0.90 
dust 0.89 
steam 0.88 
colder 0.88 
spout 0.49 
outside 0.46 
bitsy 0.45 
itsy 0.45 
spider 0.44 
meat 
meat 
meat 
salad 0.97 
bread 0.97 
pizza 0.96 
oatmeal 0.96 
cereal 0.96 
broccoli 0.96 
oatmeal 0.95 
salad 0.95 
bread 0.95 
macaroni 0.95 
soup 0.57 
carrot 0.56 
broccoli 0.55 
cheese 0.53 
vegetable 0.53 
fish 
fish 
fish 
whale 0.91 
hay 0.91 
goldfish 0.91 
goose 0.91 
turkey 0.91 
penguin 0.92 
snail 0.91 
goldfish 0.91 
whale 0.91 
bug 0.91 
angler 0.53 
turtle 0.49 
glub 0.48 
swim 0.46 
fins 0.46 
april 
april 
april 
buster 0.93 
harvey 0.93 
hank 0.93 
september 0.93 
january 0.93 
harvey 0.93 
darling 0.93 
abba 0.93 
correct 0.92 
america 0.92 
fifth 0.59 
february 0.56 
twenty 0.53 
saturday 052 
october 0.51 
month 
month 
month 
year 0.97 
degree 0.93 
ounce 0.93 
dollar 0.93 
thousand 0.92 
year 0.97 
thousand 0.92 
hour 0.92 
week 0.92 
hundred 0.92 
year 0.80 
week 0.68 
twenty 0.64 
ounce 0.58 
thirty 0.56 
To investigate further the differences between the models, we printed nearest neighbors 
for three categories that showed the greatest learning outcome differences. The category 
‘weather’ is an example where the SRN is outperformed by both models, the ‘meat’ category is 
an example of where Skip-gram achieves superior performance over both recurrent neural 
networks, and lastly, ‘months’ is an example where each model achieves very different scores. 
Inspecting the neighbors that Skip-gram produced (Table 3, right column), we noticed a strong 
thematic bias. For example, Skip-gram’s nearest neighbors of ‘snow’ include ‘man’, ‘white’, and 
’melt’ which are not objects related to weather conditions, but instead are thematically associated 
with ‘snow’. In contrast, neither of the recurrent neural networks (Table 3, left and middle 
columns) produced neighbors referring to the properties of snow such as its color, or its ability to 
melt. Nearest neighbors produced by the recurrent neural network instead appear to be 
constrained by being a noun; no other organization became apparent to us. More evidence for a 
thematic bias can be found by inspecting Skip-gram’s neighbors for the word ‘fish’, which 
include ‘swim’, which describes a property of fish, and ‘glub’, a sound produced by fish (or at 
least, the way that such sounds are described in interactions with children). In contrast, the 
LSTM and SRN produce nearest neighbors that refer to fish-like objects and none refer to 
properties of fish. These differences between the models and their relative tendencies to produce 
taxonomic vs. thematic/co-occurring neighbors is consistent with previous research, which has 
shown that models strictly tracking co-occurrences between words within a window tend to 
define words’ similarities in terms of their substitutability, whereas models tracking 
co-occurrences between words and a context tend to define words similarities in terms of more 
thematic-like relationships. Due to way that Skip-gram presents word pairs in manner that 
removes information about word order within the context window, its behavior is more like a 
bag-of-words model that would be expected to show this bias (Rubin et al., 2015). 
4. Discussion 
We tested the hypothesis that a distributional learning mechanism might account for the 
acquisition of meaningful semantic structure from noisy naturalistic language input. This 
proposal has a long history (Fisher et al., 2010; Gleitman, 1990; Lany & Saffran, 2010; Syrett & 
Lidz, 2010; Wojcik & Saffran, 2013), but until recently it has been difficult to evaluate how 
effective such a process might be in the development of the human semantic system. However, 
due to the increasing usage of more sophisticated computational models, large naturalistic 
datasets, and computer power, this hypothesis can now be tested on a large scale. To this end, we 
trained three classes of neural networks (SRNs, LSTMs, and Word2Vec’s Skip-gram) on over 
5-million words of child-directed speech and examined both qualitatively and quantitatively the 
semantic structure underlying the representations that emerged for 720 probe-words. We 
compared the results to better understand the advantages and limitations of each model. We used 
these analyses to answer three specific questions. 
The first questions is whether recurrent neural networks can learn semantic structure from 
predicting the word sequences of noisy, naturalistic speech to children. We found that all three 
neural network models learned complex semantic relationships, demonstrating learning of 
complex internal representations such as those found in the models of Elman (1990) and Rogers 
and McClelland (2004). This research shows that the principles demonstrated by those models do 
not depend on the cleanliness of their artificial datasets. To the contrary, this work shows that the 
structure of the input that children receive is in fact highly organized and capable of supporting 
learning in neural or cognitive system without strong priors about the organizational structure 
that might be learned. 
Secondly, we asked whether the semantic structures the models acquired reflect the 
semantic structures that children acquire. The words that the recurrent neural network models 
considered to be the most similar tended to be 
​taxonomically
​
related (i.e. from the same 
category) rather than 
​thematically
​
related (i.e. co-occurring within the same situation or event). 
For example, it was very rare for those models to group semantically related words that were 
different parts of speech (
​bounce
​
and 
​ball
​
, for example). This was true for the two recurrent 
models (the SRN and LSTM), because the stronger grammatical constraints (predicting word 
order) on these models led to more grammatically constrained semantic neighborhoods. 
Skip-gram, on the other hand, is less constrained by the grammar inherent in the input, and 
therefore produces less taxonomically constrained neighbors. The bias in the recurrent neural 
networks towards taxonomic relations is notable, given that children have been shown to assume 
that noun labels refer to groups of kinds of things, rather than groups of words that co-occur 
within a situation or event (Waxman & Markow, 1995). All three models also appeared to learn a 
latent hierarchical structure, capable of producing the kinds of behavior observed in children 
(Keil, 1992). Words tended to be most similar to other words of the same category, and then 
most similar to words belonging to a superordinate category. 
Third, do different neural network architectures (and the different theories of learning and 
memory that they represent) perform qualitatively or quantitatively differently with regard to 
their ability to learn semantic information? We found that, in general, the three models perform 
in qualitatively similar ways. All three models showed strong evidence of developing rich, 
structured semantic knowledge. Aligning with previous research, we found that Skip-gram and 
LSTM models tended to be slightly better in quantitatively evaluated situations (such as 
predicting whether two words belong to the same category). However, there is a very important 
caveat to make here. This task is arguably artificial, and even if real, we do not have data on how 
children or adults would perform on a similar task. The higher performance reached by LSTM 
and Skip-gram on this task is evidence these are better machine learning models, but not that 
they are better models of human cognition. Ongoing work is investigating how children perform 
on a number of semantic related tasks, and these models will serve as sources of quantitative and 
qualitative predictions of children’s performance in those tasks. 
One particularly important distinction in our work is that between sequence prediction 
and semantic classification. Our primary interest is in modeling semantic development and how 
it can emerge from sequence prediction, rather than the actual performance on sequence 
prediction itself. There is a sense in which these two tasks, while mutually supporting one 
another to an extent, may compete with one another in the limit. Perfect (or as perfect as 
possible) sequence prediction on new input involves needing to learn abstract generalizations in 
the model’s hidden layer that can predict new situations. The abstractions necessary for doing 
this in language are very likely not identical to the abstractions necessary for interacting with 
world knowledge (Willits, Amato, & MacDonald, 2015). As such, one could view getting very 
good at predicting language sequences as a case of overfitting to the specifics of that task or goal, 
resulting in a slight decrease in performance when making predictions about real world object 
and events. As a consequence, models like Skip-gram which do not perfectly capture word order, 
may be providing a slight protection against such overfitting, resulting in slightly better semantic 
performance. 
Many questions and future research directions remain. The most obvious is the extent to 
which these models correctly predict behaviors about semantic development, beyond those 
qualitative matches we have shown here. In addition to further experimental validation, there 
remain follow-up analyses about the specific semantic structures of the models. Can the precise 
extent to which models develop taxonomic or hierarchical representations be quantified, and 
used to adjudicate between models? How do the models respond to perturbations in the input, 
and how do differences in quantity and quality of input, or learning more than one language at 
once affect acquisition of semantic knowledge? 
Other questions involve the cognitive and neurobiological plausibility of the proposed 
learning and representational mechanisms. While models that are capable of capturing a greater 
number of the statistical properties of language exist (LSTM), we must keep in mind that 
language comprehension is a demanding process, and requires decoding of highly structured 
input within very short time (Frederici, 2002; Pylkkanen and Marantz, 2003). It is unlikely that 
humans use all the statistical information available in the input, as such a learning mechanism 
would be costly in terms of the expenditure of neural resources. Many language statistics contain 
overlapping information, and many may not be relevant to language comprehension at all. 
Moreover, language learning typically occurs well before the brain has achieved maturity, so 
developmental constraints further limit the learning mechanism. 
While laboratory experiments and small-scale simulations have introduced numerous 
candidate mechanisms scaffolding semantic development, little is known whether such 
mechanisms are capable of scaling to the noisy naturalistic language environment of real 
children. Our modeling results show that complex and highly organized semantic structure 
emerges automatically from learning the statistical regularities of child directed speech, 
supporting the idea that a neural network-like model of statistical learning might explain aspects 
of semantic development. We hope that further computational modeling efforts will continue to 
combine realistic models of children’s input with cognitively realistic models to contribute to our 
understanding of a wide range of phenomena in semantic memory and its development. 
References
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., ... & Ghemawat, S. (2016). 
Tensorflow: Large-scale machine learning on heterogeneous distributed systems. 
​arXiv 
preprint arXiv:1603.04467
​
. 
Baroni, M., Dinu, G., & Kruszewski, G. (2014). Don't count, predict! A systematic comparison 
of context-counting vs. context-predicting semantic vectors. In 
​ACL (1)
​
(pp. 238-247). 
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language 
model. 
​Journal of Machine Learning Research
​
, 
​3
​
, 1137-1155. 
Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations from word 
co-occurrence statistics: A computational study. 
​Behavior Research Methods
​
, 
​39
​
, 510-526. 
Burgess, C., & Lund, K. (1998). Modeling cerebral asymmetries in high-dimensional semantic 
space. 
​Right Hemisphere Language Comprehension
​
, 215-244. 
Chomsky, N. (2006). 
​Language and Mind
​
. Cambridge University Press. 
Christiansen, M. H., & MacDonald, M. C. (2009). A usage
-
based approach to recursion in 
sentence processing. 
​Language Learning
​
, 
​59
​
, 126-161. 
Clark, E. V. (2009). 
​First language acquisition
​
. Cambridge University Press. 
Cleeremans, A., & McClelland, J. L. (1991). Learning the structure of event sequences. 
​Journal 
of Experimental Psychology: General
​
, 
​120
​
, 235. 
Collins, A. M., & Quillian, M. R. (1969). Retrieval time from semantic memory. 
​Journal of 
verbal learning and verbal behavior
​
, 
​8
​
, 240-247. 
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and 
stochastic optimization. 
​Journal of Machine Learning Research
​
, 
​12
​
, 2121-2159. 
Elman, J. L. (1990). Finding structure in time. 
​Cognitive
​
​Science
​
, 
​14
​
, 179-211. 
Elman, J. L. (1991). Distributed representations, simple recurrent networks, and grammatical 
structure. 
​Machine Learning
​
, 
​7
​
, 195-225. 
Firth, J. R. (1957). 
​Studies in Linguistic Analysis
​
. Oxford: Blackwell. 
Fisher, C., Gertner, Y., Scott, R. M., & Yuan, S. (2010). Syntactic Bootstrapping. 
​Wiley 
Interdisciplinary Reviews: Cognitive Science
​
, 
​1
​
(2), 143-149. 
Fodor, J. A. (1965). Could meaning be an r
​
m
​
? 
​Journal of Verbal Learning and Verbal Behavior
​
, 
4, 73-81. 
Friederici, A. D. (2002). Towards a neural basis of auditory sentence processing. 
​Trends in 
Cognitive Sciences
​
, 
​6
​
, 78-84. 
Gers, F. A., & Schmidhuber, J. (2001). Long Short-Term Memory learns context free and 
context sensitive languages. In 
​Artificial Neural Nets and Genetic Algorithms
​
(pp. 134-137). 
Springer Vienna. 
Gleitman, L. (1990). The structural sources of verb meanings. 
​Language Acquisition
​
, 
​1
​
, 3-55. 
Glenberg, A. M., & Robertson, D. A. (2000). Symbol grounding and meaning: A comparison of 
high-dimensional and embodied theories of meaning. 
​Journal of Memory and Language
​
, 
​4
​
, 
379-401. 
Goldberg, Y., & Levy, O. (2014). word2vec explained: Deriving mikolov et al.'s 
negative-sampling word-embedding method. 
​arXiv preprint arXiv:1402.3722
​
. 
Goldstone, R. L., & Kersten, A. (2003). Concepts and categorization. 
​Handbook of Psychology
​
. 
Harris, Z. S. (1954). Distributional structure. 
​Word
​
, 
​10
​
, 146-162. 
Hart, B., & Risley, T. R. (2003). The early catastrophe: The 30 million word gap by age 3. 
American Educator
​
, 
​27
​
, 4-9. 
Hinton, G. E., & Shallice, T. (1991). Lesioning an attractor network: Investigations of acquired 
dyslexia. 
​Psychological Review
​
, 
​98
​
, 74. 
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. 
​Neural computation
​
, 
​9
​
, 
1735-1780. 
Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recurrent 
nets: the difficulty of learning long-term dependencies. 
Jackendoff, R. (1992). 
​Semantic structures
​
(Vol. 18). MIT press. 
Jones, M. N., Kintsch, W., & Mewhort, D. J. (2006). High-dimensional semantic space accounts 
of priming. 
​Journal of Memory and Language
​
, 
​55
​
, 534-552. 
Keil, F. C. (1992). 
​Concepts, Kinds, and Cognitive Development
​
. Cambridge, MA: MIT Press. 
Kuperberg, G. R., & Jaeger, T. F. (2016). What do we mean by prediction in language 
comprehension?. 
​Language, Cognition and Neuroscience
​
, 
​31
​
(1), 32-59. 
Kutas, M., & Hillyard, S. A. (1984). Brain potentials during reading reflect word expectancy and 
semantic association. 
​Nature
​
. 
Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato's problem: The latent semantic 
analysis theory of acquisition, induction, and representation of knowledge. 
​Psychological 
Review
​
, 
​104
​
, 211. 
Lany, J., & Saffran, J. R. (2010). From statistics to meaning: Infants’ acquisition of lexical 
categories. 
​Psychological Science
​
, 
​21
​
, 284-291. 
Lau, E. F., Phillips, C., & Poeppel, D. (2008). A cortical network for semantics: (de)constructing 
the N400. 
​Nature reviews. Neuroscience
​
, 
​9
​
, 920. 
Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving distributional similarity with lessons 
learned from word embeddings. 
​Transactions of the Association for Computational 
Linguistics
​
, 
​3
​
, 211-225. 
Linzen, T., Dupoux, E., & Goldberg, Y. (2016). Assessing the Ability of LSTMs to Learn 
Syntax-Sensitive Dependencies. 
​arXiv preprint arXiv:1611.01368
​
. 
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical 
co-occurrence. 
​Behavior Research Methods, Instruments, & Computers
​
, 
​28
​
, 203-208. 
MacWhinney, B. (2000). 
​The CHILDES Project: Tools for analyzing talk. Third Edition. 
Mahwah, NJ: Lawrence Erlbaum Associates. 
Maess, B., Mamashli, F., Obleser, J., Helle, L., & Friederici, A. D. (2016). Prediction signatures 
in the brain: Semantic pre-activation during language comprehension. 
​Frontiers in Human 
Neuroscience
​
, 
​10
​
. 
Mandera, P., Keuleers, E., & Brysbaert, M. (2017). Explaining human performance in 
psycholinguistic tasks with models of semantic similarity based on prediction and counting: 
A review and empirical validation. 
​Journal of Memory and Language
​
, 
​92
​
, 57-78. 
Marcus, G. F., & Keil, F. C. (2008). Concepts, correlations, and some challenges for 
connectionist cognition. 
​Behavioral and Brain Sciences
​
, 
​31
​
, 722-723. 
McClelland, J. L., & Kawamoto, A. H. (1986). Mechanisms of sentence processing: Assigning 
roles to constituents of sentences. 
​Parallel distributed processing
​
, 
​2
​
, 318-362. 
McClelland, J. L., St. John, M., & Taraban, R. (1989). Sentence comprehension: A parallel 
distributed processing approach. 
​Language and cognitive processes
​
, 
​4
​
, SI287-SI335. 
McRae, K., de Sa, V. R., & Seidenberg, M. S. (1997). On the nature and scope of featural 
representations of word meaning. 
​Journal of Experimental Psychology: General
​
, 
​126
​
, 99. 
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word 
representations in vector space. 
​arXiv preprint arXiv:1301.3781
​
. 
Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., & Ranzato, M. A. (2014). Learning longer 
memory in recurrent neural networks. 
​arXiv preprint arXiv:1412.7753
​
. 
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed 
representations of words and phrases and their compositionality. In 
​Advances in neural 
information processing systems
​
(pp. 3111-3119). 
Murphy, G. (2004). 
​The big book of concepts
​
. MIT press. 
Nosofsky, R. M., & Johansen, M. K. (2000). Exemplar-based accounts of" multiple-system" 
phenomena in perceptual categorization. 
​Psychonomic Bulletin and Review
​
, 
​7
​
, 375-402. 
Olney, A. M., Dale, R., & D’Mello, S. K. (2012). The world within Wikipedia: an ecology of 
mind. 
​Information
​
, 
​3
​
(2), 229-255. 
Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global Vectors for Word 
Representation. In 
​EMNLP
​
(Vol. 14, pp. 1532-1543). 
Pereira, F., Gershman, S., Ritter, S., & Botvinick, M. (2016). A comparative evaluation of 
off-the-shelf distributed semantic representations for modelling behavioural data. 
​Cognitive 
Neuropsychology
​
, 
​33
​
, 175-190. 
Pinker, S. (1984). 
​Language Learnability and Language Development
​
. Cambridge, MA: Harvard 
University Press. 
Plaut, D. C., & Booth, J. R. (2000). Individual and developmental differences in semantic 
priming: Empirical and computational support for a single-mechanism account of lexical 
processing. 
​Psychological Review
​
, 
​107
​
, 786. 
Pylkkänen, L., & Marantz, A. (2003). Tracking the time course of word recognition with MEG. 
Trends in Cognitive Sciences
​
, 
​7
​
, 187-189. 
Rehurek, R., & Sojka, P. (2010). Software framework for topic modelling with large corpora. In 
In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks
​
. 
Robbins, S. E. (2008). Semantic redintegration: Ecological invariance. 
​Behavioral and Brain 
Sciences
​
, 
​31
​
, 726-727. 
Rogers, T. T., & McClelland, J. L. (2004). 
​Semantic Cognition: A Parallel Distributed 
Processing Approach
​
. MIT press. 
Rogers,
T.
T.,
& McClelland,
J.
L.
(2008).
Précis of semantic cognition: A parallel distributed
processing approach. 
​Behavioral and Brain Sciences
​
, 
​31
​
, 689-714. 
Rogers, T. T., Lambon Ralph, M. A., Garrard, P., Bozeat, S., McClelland, J. L., Hodges, J. R., & 
Patterson, K. (2004). Structure and deterioration of semantic memory: a neuropsychological 
and computational investigation. 
​Psychological Review
​
, 
​111
​
, 205. 
Rohde, D. L., Gonnerman, L. M., & Plaut, D. C. (2004). An improved method for deriving word 
meaning from lexical co-occurrence. 
​Cognitive Psychology
​
, 
​7
​
, 573-605. 
Rosch, E. (1975). Cognitive representations of semantic categories. 
​Journal of Experimental 
Psychology: General
​
, 
​104
​
, 192. 
Rubin, T. N., Kievit-Kylar, B., Willits, J. A., & Jones, M. N. (2014). Organizing the space and 
behavior of semantic models. In 
​CogSci... Annual Conference of the Cognitive Science 
Society. Cognitive Science Society (US). Conference
​
(Vol. 2014, p. 1329). NIH Public 
Access. 
Sadeghi, Z., McClelland, J. L., & Hoffman, P. (2015). You shall know an object by the company 
it keeps: An investigation of semantic representations derived from object co-occurrence in 
visual scenes. 
​Neuropsychologia
​
, 
​76
​
, 52-61. 
Seidenberg, M. S., & McClelland, J. L. (1989). A distributed, developmental model of word 
recognition and naming. 
​Psychological Review
​
, 
​96
​
, 523. 
Smith, E. E., Shoben, E. J., & Rips, L. J. (1974). Structure and process in semantic memory: A 
featural model for semantic decisions. 
​Psychological Review
​
, 
​81
​
, 214. 
Snedeker, J. (2008). Reading Semantic Cognition as a theory of concepts. 
​Behavioral and Brain 
Sciences
​
, 
​31
​
, 727-728. 
Steyvers, M., & Tenenbaum, J. B. (2005). The Large
-
scale structure of semantic networks: 
Statistical analyses and a model of semantic growth. 
​Cognitive Science
​
, 
​29
​
, 41-78. 
Syrett, K., & Lidz, J. (2010). 30-month-olds use the distribution and meaning of adverbs to 
interpret novel adjectives. 
​Language Learning and Development
​
, 
​6
​
, 258-282. 
Tabor, W., Juliano, C., & Tanenhaus, M. K. (1997). Parsing in a dynamical system: An 
attractor-based account of the interaction of lexical and structural constraints in sentence 
processing. 
​Language and Cognitive Processes
​
, 
​12
​
, 211-271. 
van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine 
Learning Research, 9(Nov), 2579-2605. 
Waxman, S. R., & Markow, D. B. (1995). Words as invitations to form categories: Evidence 
from 12-to 13-month-old infants. 
​Cognitive
​
​Psychology
​
, 
​29
​
, 257-302. 
Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. 
​Proceedings 
of the IEEE
​
, 
​78
​
, 1550-1560. 
Willems, R. M., Frank, S. L., Nijhof, A. D., Hagoort, P., & Van den Bosch, A. (2016). Prediction 
during natural language comprehension. 
​Cerebral Cortex
​
, 
​26
​
, 2506-2516. 
Williams, R. J., & Peng, J. (1990). An efficient gradient-based algorithm for on-line training of 
recurrent network trajectories. 
​Neural computation
​
, 
​2
​
, 490-501. 
Willits, J. A., Amato, M. S., & MacDonald, M. C. (2015). Language knowledge and event 
knowledge in language use. 
​Cognitive Psychology
​
, 
​78
​
, 1-27. 
Willits, J. A., Seidenberg, M., & Saffran, J. (2009). Verbs are looking good in language 
acquisition. In 
​Proceedings of the 31st Annual Conference of the Cognitive Science Society
​
. 
Wojcik, E. H., & Saffran, J. R. (2013). The ontogeny of lexical networks toddlers encode the 
relationships among referents when learning novel words. 
​Psychological science
​
, 
​24
​
(10), 
1898-1905. 
Xiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M., Stolcke, A., ... & Zweig, G. (2016). 
Achieving human parity in conversational speech recognition. 
​arXiv preprint 
arXiv:1610.05256
​
. 
Xu, F., & Tenenbaum, J. B. (2007). Word learning as Bayesian inference. 
​Psychological review
​
, 
114
​
, 245. 
