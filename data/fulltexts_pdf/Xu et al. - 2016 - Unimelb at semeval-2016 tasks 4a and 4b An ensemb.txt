Proceedings of SemEval-2016, pages 183–189,
San Diego, California, June 16-17, 2016.
c
2016 Association for Computational Linguistics
UNIMELB at SemEval-2016 Tasks 4A and 4B: An Ensemble of
Neural Networks and a Word2Vec Based Model for Sentiment
Classification
XingYi Xu
HuiZhi Liang
Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne VIC 3010, Australia
stevenxxiu@gmail.com
oklianghuizi@gmail.com
tb@ldwin.net
Abstract
This paper describes our sentiment clas-
sification system for microblog-sized doc-
uments,
and documents where a topic is
present.
The system consists
of
a soft-
voting ensemble of
a word2vec language
model adapted to classification, a convolu-
tional neural network (CNN), and a long-
short term memory network (LSTM). Our
main contribution consists of a way to in-
troduce topic information into this model,
by concatenating a topic embedding, con-
sisting of the averaged word embedding for
that topic, to each word embedding vector
in our neural
networks.
When we apply
our models to SemEval
2016 Task 4 sub-
tasks A and B,
we demonstrate that the
ensemble performed better than any sin-
gle classifier,
and our method of
includ-
ing topic information achieves a substan-
tial
performance gain.
According to re-
sults on the official
test sets,
our model
ranked 3rd for
𝐹
PN
in the
message-only
subtask A (among 34 teams) and 1st for
accuracy on the topic-dependent subtask
B (among 19 teams).
1
Introduction
The
rapid growth of
user-generated content,
much of which is sentiment-laden, has fueled an
interest
in sentiment
analysis (Pang and Lee,
2008;
Liu,
2010).
One popular form of
senti-
ment analysis involves classifying a document
into discrete classes,
depending on whether it
expresses positive or negative sentiment (or nei-
ther).
The classification can also be dependent
upon a particular topic.
In this work,
we de-
scribe the method we used for
the sentiment
classification of tweets, with or without a topic.
Our approach to the document classification
task consists of an ensemble of 3 classifiers via
soft-voting, 2 of which are neural network mod-
els.
One is
the convolutional
neural
network
(CNN) architecture of Kim (2014), and another
is a Long Short Term Memory (LSTM)-based
network (Hochreiter
and Schmidhuber,
1997).
Both were first tuned on a distant-labelled data
set.
The third classifier adapted word2vec to
output classification probabilities using Bayes’
formula,
a slightly modified version of
Taddy
(2015).
Despite the word2vec classifier being in-
tended as a baseline, and having a small weight
in the ensemble, it proved crucial for the ensem-
ble to work well.
To adapt our models to the
case where a topic is present, in the neural net-
work models,
we concatenated the embedding
vectors for each word with a topic embedding,
which consisted of
the element-wise average of
all word vectors in a particular topic.
We applied our
approach to SemEval
2016
Task
4,
including
the
message-only
subtask
(Task A) and the topic-dependent subtask (Task
B) (Nakov et al., to appear).
Our model ranked
third for
𝐹
PN
in the
message-only subtask A
(among 34 teams) and first for accuracy
1
on the
topic-dependent subtask B (among 19 teams).
1
There
were
some
issues
surrounding
the
evalua-
tion metrics.
We
only got
7th for
𝜌
PN
and 2nd for
𝐹
PN
officially,
but
when we retrained our model
using
𝜌
PN
as the subtask intended, we place first across all met-
rics.
183
The source code for our approach is available
at https://github.com/stevenxxiu/senti.
2
Models
We now describe the classifiers we used in de-
tail, our ensemble method, and our motivations
behind choosing these classifiers.
2.1
Convolutional neural network
We used the dynamic architecture of Kim (2014)
for our convolutional neural network.
This con-
sists of a single 1-d convolution layer with a non-
linearity,
a max-pooling layer,
a dropout layer,
and a softmax classification layer.
This model
was chosen since it was a good
performer empirically.
However,
due to max-
pooling,
this
model
is
essentially
a
bag-of-
phrases model,
which ignores important order-
ing information if the tweet contains a long ar-
gument,
or 2 sentences.
We now give a review
of the layers used.
2.1.1
Word embedding layer
The input to the model is a document, treated
as a sequence of
words.
Each word can possi-
bly be represented by a vector of occurrences, a
vector of counts, or a vector of features.
A vec-
tor of
learnt,
instead of
hand-crafted features,
is also called a word embedding.
This tends to
have dimensionality 𝑑 ≪ |𝑉 |,
the vocabulary
size.
Hence the vectors are dense,
which allows
us to learn more functions of features with lim-
ited data.
Given an embedding of dimension 𝑑, 𝑊
emb
∈
R
𝑑×|𝑉 |
,
we mapped map each document 𝑠 to a
matrix 𝑊
emb,𝑠
∈ R
𝑑×|𝑠|
,
with each word corre-
sponding to a row vector in the order they ap-
pear in.
𝑊
emb
can be trained.
2.1.2
1-d convolution layer
A 1-d convolution layer aims to extract pat-
terns useful for classification, by sliding a fixed-
length filter along the input.
The convolution
operation for an input matrix 𝑆 ∈ R
𝑑×|𝑠|
and
a single filter 𝐹 ∈ R
𝑑×𝑚
of
width 𝑚 creates a
feature 𝑦
conv
∈ R
|𝑠|+𝑚−1
by:
𝑦
conv,𝑖
=
∑︁
𝑘,𝑗
(𝑆
[:,𝑖:𝑖+𝑚−1]
⊙ 𝐹 )
𝑘,𝑗
+ 𝑏
conv
where
⊙ is
element-wise
multiplication,
and
𝑏
conv
is a bias.
There are typically 𝑛 > 1 filters,
which by stacking the feature vectors, results in
𝑌
conv
∈ R
𝑛×(|𝑠|+𝑚−1)
.
Each filter has its own
separate bias.
We used a common modification to the fil-
ter sliding, by padding the document embedding
matrix with 𝑚 − 1 zeroes on its top and bottom.
This is done so that every word in the document
is covered by 𝑚 filters.
2.1.3
Max pooling layer
There may be very few phrases targeted by a
feature map in the document.
For this reason,
we only need to know if
the desired feature is
present in the document, which can be obtained
by taking the maximum.
Formally, we obtain a
vector 𝑑 ∈ R
𝑛
, such that:
𝑦
pool,𝑖
= max
𝑗
𝑌
conv,𝑖,𝑗
2.1.4
Softmax layer
To
convert
our
features
into
classification
probabilities, we first use a dense layer, defined
by:
𝑦
dense
= 𝑊
dense
· 𝑦
pool
+ 𝑏
dense
with a softmax activation function:
softmax(𝑥)
𝑖
=
𝑒
𝑥
𝑖
∑︀
𝑖
𝑒
𝑥
𝑖
such that the output dimension is the same as
the number of classes.
Note that output values
are non-negative and sum to 1,
which form a
discrete probability distribution.
2.1.5
Regularization
To regularize our CNN model,
dropout (Sri-
vastava et al., 2014) is used after the max pool-
ing layer.
Intuitively,
dropout assumes that we
can still
obtain a reasonable classification even
when some of the features are dropped.
To do
this,
each dimension is randomly set to 0 using
a Bernoulli
distribution 𝐵(𝑝).
In order to have
the training and testing to be of the same order,
the test outputs can be scaled by 𝑝.
The softmax layer for the CNN model
also
uses a form of
empirical
Bayes regularization,
where each row of 𝑊
soft
is restricted using an ℓ
2
184
norm,
by re-normalizing the vector if the norm
threshold is exceeded.
2.2
Long short term memory network
We used an LSTM for our recurrent architec-
ture,
which consisted of
an embedding layer,
LSTM layer, and a softmax classification layer.
A recurrent neural
network is a neural
net-
work designed for sequential
problems.
Even
simple RNNs are Turing complete, and they can
theoretically obtain information from the entire
sequence instead of
only an unordered bag of
phrases.
But finding good architectures which
can capture this and training them can be diffi-
cult.
Indeed,
there were many instances where
our LSTM failed to capture important ordering
information.
We now give a brief review of the
LSTM.
Given an input sequence 𝑥 = [𝑥
1
, . . . , 𝑥
𝑇
],
a
recurrent network defines an internal state func-
tion 𝑓
𝑐
and an output function 𝑓
𝑦
to iterate over
𝑥, so that at time step 𝑡:
𝑐
𝑡
= 𝑓
𝑐
(𝑥
𝑡
, 𝑦
𝑡−1
, 𝑐
𝑡−1
)
𝑦
𝑡
= 𝑓
𝑦
(𝑥
𝑡
, 𝑦
𝑡−1
, 𝑐
𝑡
)
where 𝑐
0
and 𝑦
0
are initial bias states.
The sim-
plest RNN, where:
𝑓
𝑦
(𝑥
𝑡
, 𝑦
𝑡−1
, 𝑐
𝑡
)
= tanh(𝑊 ·
[︂
𝑥
𝑡
𝑦
𝑡−1
]︂
)
suffers
from the
gradient
vanishing
and ex-
ploding problem (Hochreiter and Schmidhuber,
1997).
In particular, products of saturated tanh
activations can vanish the gradient,
and prod-
ucts of 𝑊 can vanish or explode the gradient.
The LSTM is a way to remedy this (Hochre-
iter and Schmidhuber, 1997).
It sets:
𝑖
𝑡
= 𝜎
𝑖
(𝑊
𝑥𝑖
𝑥
𝑡
+ 𝑊
ℎ𝑖
ℎ
𝑡−1
+ 𝑤
𝑐𝑖
⊙ 𝑐
𝑡−1
+ 𝑏
𝑖
)
𝑓
𝑡
= 𝜎
𝑓
(𝑊
𝑥𝑓
𝑥
𝑡
+ 𝑊
ℎ𝑓
ℎ
𝑡−1
+ 𝑤
𝑐𝑓
⊙ 𝑐
𝑡−1
+ 𝑏
𝑓
)
𝑐
𝑡
= 𝑓
𝑡
⊙ 𝑐
𝑡−1
+ 𝑖
𝑡
⊙ 𝜎
𝑐
(𝑊
𝑥𝑐
𝑥
𝑡
+ 𝑊
ℎ𝑐
ℎ
𝑡−1
+ 𝑏
𝑐
)
𝑦
𝑡
= 𝜎
𝑜
(𝑊
𝑥𝑜
𝑥
𝑡
+ 𝑊
ℎ𝑜
ℎ
𝑡−1
+ 𝑤
𝑐𝑜
⊙ 𝑐
𝑡
+ 𝑏
𝑜
) ⊙ 𝜎
ℎ
(𝑐
𝑡
)
Here,
𝑊
∙
is
made
up of
weights;
𝑖
𝑡
, 𝑓
𝑡
, 𝑐
𝑡
, 𝑦
𝑡
are
the
input
gates,
forget
gates,
cell
states
and output states.
Cell
states have an identity
activation function.
The gradient will
not van-
ish if
input 𝑥
𝑖
needs to be carried to 𝑥
𝑗
,
since
this can only happen when the forget gates are
near 1.
However,
gradient explosion can still
be present.
We use the common approach of
cutting off gradients above a threshold for the
gradients inside 𝜎
𝑖
, 𝜎
𝑓
, 𝜎
𝑐
, 𝜎
𝑜
.
To use the LSTM,
we first used a document
embedding matrix in the same manner as our
convolutional neural network architecture.
This
was fed into to an LSTM layer.
The output for
the final
timestep of the LSTM layer was then
fed into a final softmax layer with the appropri-
ate output size for classification.
We also experimented with a similar and com-
monly used network, the Gated Recurrent Net-
work (GRU),
but it was not used due to lower
results compared to the LSTM.
2.3
word2vec Bayes
Our
word2vec
Bayes
model
is
our
baseline
model, and described in Taddy (2015), with the
inclusion of
a class prior.
Taddy (2015) uses
Bayes formula to compute the probabilities of a
document belonging to a sentiment class.
Given
a document 𝑑,
its words {𝑤}
𝑖
,
label
𝑦,
Bayes
formula is:
𝑝(𝑦|𝑑) =
𝑝(𝑑|𝑦)𝑝(𝑦)
𝑝(𝑑)
For classification problems,
we can ignore 𝑝(𝑑)
since 𝑑 is fixed.
𝑝(𝑑|𝑦) is estimated by first train-
ing word2vec on a subset of
the corpus with
label 𝑦, then using the skipgram objective com-
posite likelihood as an approximation:
log 𝑝(𝑑|𝑦) ≈
∑︁
𝑠∈𝑑
|𝑠|
∑︁
𝑗=1
|𝑠|
∑︁
𝑘=1
1
1≤|𝑘−𝑗|≤𝑏
log 𝑝(𝑤
𝑘
|𝑤
𝑗
, 𝑦)
We estimated 𝑝(𝑦) via class frequencies, i.e. the
MLE for the categorical distribution, compared
to Taddy (2015), who used the discrete uniform
prior.
This model was chosen since it provides a rea-
sonable baseline,
and also appears to be inde-
pendent
enough from our
neural
networks
to
provide a performance gain in the ensemble.
The word2vec based model benefits from being
185
a semi-supervised method,
but it also loses or-
dering information outside a word’s context win-
dow,
and the prediction of
neighboring words
also ignores distance to that word.
The limited
amount of data for each class is also an issue.
2.4
Ensemble
If the errors made by each classifier are indepen-
dent enough, then combining them in an ensem-
ble can reduce the overall
error rate.
We used
soft voting as a method to combine the outputs
of the above classifiers.
We define soft voting as:
𝑦
vote
=
∑︁
𝑖
𝑤
𝑖
𝑦
𝑖
, s.t.
∑︁
𝑖
𝑤
𝑖
= 1, ∀𝑖 : 𝑤
𝑖
≥ 0
where 𝑦
𝑖
is the output of classifier 𝑖.
3
Topic dependent models
To
adapt
our
neural
networks
to
topic-
dependent sentiment classification, in our neural
network models, we augmented each embedding
vector by concatenating it with a topic embed-
ding.
The motivation behind this approach is
to allow the model to interpret each word to be
within the context of some topic.
Our topic embeddings were obtained by the
element-wise average of
word embeddings
for
each word in that topic.
We found that empiri-
cally, this is a simple yet effective way of achiev-
ing a document embedding.
When used directly
as a feature vector in logistic regression for senti-
ment analysis, we have found this to outperform
methods described in Le and Mikolov (2014).
Word embeddings of dimension 𝑑 = 300 pre-
trained over
Google News
were used directly,
without any further tuning.
Words in the tweet
which were not present in this pretrained em-
bedding were ignored.
4
Experiments and evaluation
We evaluated our models on SemEval 2016 Task
4 subtask A, the message-only subtask, and sub-
task B, the topic-dependent subtask.
4.1
Data
Task A consisted of 3 sentiment classes — posi-
tive, neutral and negative — whilst Task B
consisted of 2 sentiment classes — positive and
negative.
We only managed to download 90%
of the entire set of tweets for the 2016 SemEval
data, due to tweets becoming “not available”.
In
addition to the Twitter data for 2016,
for Task
A we also used training data from SemEval 2016
Task 10.
The data is summarized in Table 1.
To pretrain our network using distant learn-
ing (described below),
we took a random sam-
ple of 10M English tweets from a 5.3TB Twitter
dataset crawled from 18 June to 4 Dec,
2014
using the Twitter Trending API.
We then pro-
cessed tweets with a regular expression:
tweets
which contained emoticons
like :)
were con-
sidered positive,
while those which contained
emoticons
like :(
were considered negative;
tweets which contained both positive and nega-
tive emoticons, or others emoticons such as :—,
were ignored.
We extracted 1M tweets each for
the positive and negative classes.
4.2
Evaluation
Evaluation
consisted
of
accuracy,
macro-
averaged 𝐹
1
across
the positive and nega-
tive classes, which we denote 𝐹
PN
, and macro-
averaged recall
across the positive and nega-
tive classes, which we denote 𝜌
PN
.
4.3
Preprocessing
All
methods use the same preprocessing.
We
normalized the tweets by first replacing URLs
with
url
and author methods such as @Ladi-
ibubblezz
with
author .
Casing was preserved,
as
the
pretrained word2vec vectors
included
casing.
The
tweets
were
tokenized
using
twokenize, with it’s being split into it and ’s.
4.4
Training and hyperparameters
4.4.1
Neural networks
For both our models,
we initialized 𝑊
emb
to
word embeddings pretrained using word2vec’s
skip-gram model
(Mikolov et al.,
2013) on the
Google News corpus, where 𝑑 = 300.
Unknown
words were drawn from 𝑈 [−0.25, 0.25] to match
the variance of the pretrained vectors.
For the
CNN model,
we also stripped words so all
doc-
uments had a length ≤ 56.
186
Datset
A total
A used
B total
B used
Twitter 2016-train
6000
5465
4346
3941
(+11340)
Twitter 2016-dev
2000
1829
1325
1210
Twitter 2016-test
2000
1807
1417
1270
Table 1:
Semeval-2016 data.
The negative : neutral : positive split was 16 : 42 : 42 for all of
2016 Task A used.
The negative : positive split was 19 : 81 for all of 2016 Task B used.
For our CNN,
we used used 3 1-d convolu-
tions,
with filter sizes of
3, 4, 5,
each with 100
filters.
Our dropout rate was 0.5,
and our ℓ
2
norm restriction was 3.
For our LSTM, we used
a cell dimension size of 300, and our activations
were chosen empirically to be 𝜎
𝑖
= 𝜎
𝑓
= 𝜎
𝑜
=
sigmoid, 𝜎
𝑐
= tanh, our gradient cutoff was 100.
To train our neural
networks,
we used cross
entropy loss
and minibatch gradient
descent
with a batch size of
64.
For
our
CNN,
we
used the adadelta (Zeiler,
2012)
gradient
de-
scent method with the default settings.
For our
LSTM,
we used the rmsprop gradient descent
method with a learning rate of 0.01.
Due to limited training data, we can use dis-
tant learning (Severyn and Moschitti, 2015), by
initializing the weights of
our neural
networks
by first training them on a silver standard data
set (generated using Twitter emoticons which
we describe below),
then tuning them further
on the gold standard data set (Severyn and Mos-
chitti,
2015).
However,
we did not use this for
our
topic-dependent
models,
as
there was
no
performance gain.
We split the distant data into 10
4
tweets per
epoch,
and took the best epoch on the valida-
tion set as the initial
weights,
using 𝐹
PN
as our
scoring metric.
We repeatedly iterated over the
SemEval data with 10
3
tweets per epoch for 10
2
epochs,
and again took the best epoch on the
validation set as the final weights.
4.4.2
word2vec Bayes
Gensim (
ˇ
Reh˚uˇrek and Sojka,
2010) was used
to train the word embeddings and obtain 𝑝(𝑑|𝑦).
We used the skipgram objective, with a embed-
ding dimension of
100,
window size of
10,
hi-
erarchical
softmax with 5 samples,
20 training
iterations,
no frequent word cutoff,
and a sam-
Dataset
𝐹
PN
Twitter 2013
0.687
7
Twitter 2014
0.706
7
Twitter 2015
0.650
4
Twitter 2016
0.617
3
Twitter Sarcasm 2014
0.449
11
SMS 2013
0.593
10
LiveJournal 2014
0.683
9
Table 2:
Official
test scores and ranks for Task
A.
Dataset
Val metric 𝜌
PN
𝐹
PN
Acc
Twitter 2016
𝐹
PN
0.758
7
0.788
2
0.870
1
Twitter 2016
𝜌
PN
0.807
1
0.806
1
0.867
1
Table 3:
Test scores and ranks for Task B. The
official run incorrectly used 𝐹
PN
as the validation
metric.
ple coefficient of 0.
4.4.3
Soft voting
To find 𝑤
𝑖
,
we first relaxed the sum condi-
tion of
𝑤
𝑖
by setting 𝑤
1
= 1 and noting that
max
𝑘
𝑦
𝑣𝑜𝑡𝑒,𝑖,𝑘
is
invariant
under
scaling.
We
then used the L-BFGS-B algorithm,
with ini-
tial
weights of 1,
combined with basin-hopping
for 1000 iterations.
We optimized for accuracy,
since this most-consistently improved results.
5
Results
The official
evaluation results are shown in Ta-
ble 2 and Table 3.
The results for Task A suggest
that our models are overfitting.
Our best posi-
tion was achieved on the Twitter 2016 dataset,
and indeed,
this is what our parameters were
chosen on.
Our own evaluation of our different classifiers,
using Twitter 2016-test, is shown in Table 4 and
187
Model
Accuracy
𝐹
PN
soft voting all
0.5772
0.6000
lstm
0.5379
0.5869
soft voting cnn + lstm
0.5606
0.5848
cnn
0.5612
0.5841
word2vec Bayes
0.5130
0.4983
Table 4:
Results for Task A, sorted by 𝐹
PN
.
Model
Accuracy
𝜌
PN
soft voting all
0.8008
0.7849
soft voting cnn + lstm
0.7976
0.7846
cnn topic
0.8047
0.7762
lstm topic
0.6756
0.7494
cnn
0.8354
0.7253
word2vec Bayes
0.7654
0.7138
lstm
0.8118
0.6916
Table 5:
Results
for
Task B,
sorted by 𝜌
PN
.
The dashed line separates
topic models
from
message-only models.
The lstm topic model has
poorer accuracy due to being optimized on 𝜌
PN
.
Table 5.
Taking into account all evaluation met-
rics,
we can see that in both tasks,
our CNN
outperforms our LSTM, in Task A slightly and
in Task B substantially.
The word2vec Bayes
model is worse than both, moreso in Task B.
Soft voting outperforms all classifiers, showing
that there is some independence amongst the er-
rors made.
In Task A, there appears to be more
correlation between the CNN and LSTM classi-
fiers, as excluding the word2vec Bayes model re-
duces the performance.
In Task B, the word2vec
Bayes model
appears to perform too poorly to
provide a marked benefit.
From Table 5 we can see that the inclusion of
topic information provides a substantial
boost
to both of our neural networks.
This shows that
our method of incorporating topic information is
a useful way of modifying neural networks, and
provides a strong baseline for alternative ways
of doing this.
6
Conclusions
We described our ensemble approach to senti-
ment analysis both the task of topic-dependent
document classification and document classifi-
cation by itself.
We gave a detailed descrip-
tion of how to modify our classifiers to be topic-
dependent.
The results show that ensembles can
work for neural nets, and that our way of includ-
ing topics achieves performance gains, and forms
a good basis for future research in this area.
References
[Hochreiter and Schmidhuber1997]
Sepp
Hochreiter
and J¨
urgen Schmidhuber.
1997.
Long short-term
memory.
Neural
computation, 9(8):1735–1780.
[Kim2014]
Yoon Kim.
2014.
Convolutional
neural
networks for sentence classification.
In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural
Language Processing (EMNLP), pages
1746–1751, Doha, Qatar.
[Le and Mikolov2014]
Quoc
V.
Le
and
Tomas
Mikolov.
2014.
Distributed representations
of
sentences
and documents.
In arXiv
preprint
arXiv:1405.4053.
[Liu2010]
Bing Liu.
2010.
Sentiment analysis and
subjectivity.
In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural
Language
Processing.
Chapman & Hall/CRC,
Boca Raton,
USA, 2nd edition.
[Mikolov et al.2013]
Tomas Mikolov,
Ilya Sutskever,
Kai Chen, Greg S. Corrado, and Jeff Dean.
2013.
Distributed representations of words and phrases
and their compositionality.
In Advances in neural
information processing systems, pages 3111–3119.
[Nakov et al.to appear]
Preslav Nakov,
Alan Ritter,
Sara Rosenthal,
Veselin Stoyanov,
and Fabrizio
Sebastiani.
to appear.
SemEval-2016 Task 4:
Sen-
timent analysis in Twitter.
In Proceedings of
the
10th International
Workshop on Semantic Evalu-
ation (SemEval
2016), San Diego, USA.
[Pang and Lee2008]
Bo Pang and Lillian Lee.
2008.
Opinion mining and sentiment analysis.
Founda-
tions and trends in information retrieval, 2(1-2):1–
135.
[
ˇ
Reh˚uˇrek and Sojka2010]
Radim
ˇ
Reh˚uˇrek and Petr
Sojka.
2010.
Software
framework for
topic
modelling
with
large
corpora.
In
Proceed-
ings
of
the
LREC 2010
Workshop
on
New
Challenges
for
NLP Frameworks,
pages
45–50,
Valletta,
Malta.
ELRA.
http://is.muni.cz/
publication/884893/en.
[Severyn and Moschitti2015]
Aliaksei
Severyn
and
Alessandro Moschitti.
2015.
UNITN:
Training
deep convolutional neural network for Twitter sen-
timent
classification.
In Proceedings
of
the 9th
188
International
Workshop on Semantic Evaluation
(SemEval), pages 464–469, Denver, USA.
[Srivastava et al.2014]
Nitish
Srivastava,
Geoffrey
Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-
lan Salakhutdinov.
2014.
Dropout:
A sim-
ple way to prevent neural
networks from overfit-
ting.
The Journal
of Machine Learning Research,
15(1):1929–1958.
[Taddy2015]
Matt Taddy.
2015.
Document classifi-
cation by inversion of distributed language repre-
sentations.
In arXiv:1504.07295 [cs, stat].
[Zeiler2012]
Matthew D.
Zeiler.
2012.
ADADELTA:
An adaptive learning rate method.
arXiv preprint
arXiv:1212.5701.
189
