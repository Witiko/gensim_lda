Solving Cold-Start Problem in Large-scale Recommendation Engines:
A Deep Learning Approach
Jianbo Yuan
∗
,
Walid Shalaby
§
,
Mohammed Korayem
‡
,
David Lin
‡
,
Khalifeh AlJadda
‡
,
and Jiebo Luo
∗
∗
Department of Computer Science,
University of Rochester,
New York
Email: jyuan10,
jluo@cs.rochester.edu
§
Department of Computer Science,
University of North Carolina at Charlotte
Email: wshalaby@uncc.edu
‡
CareerBuilder,
Norcross,GA
Email: mohammed.korayem,
david.lin,
khalifeh.aljadda@careerbuilder.com
Abstract—Collaborative
Filtering (CF)
is
widely used in
large-scale recommendation engines because of
its efficiency,
accuracy and scalability.
However,
in practice,
the fact
that
recommendation engines based on CF require interactions be-
tween users and items before making recommendations,
make
it inappropriate for new items which haven’t been exposed to
the end users to interact with.
This is known as the cold-start
problem.
In this paper we introduce a novel
approach which
employs deep learning to tackle this problem in any CF based
recommendation engine.
One of
the most
important
features
of the proposed technique is the fact that it can be applied on
top of any existing CF based recommendation engine without
changing the CF core.
We successfully applied this technique
to overcome the item cold-start
problem in Careerbuilder’s
CF based recommendation engine. Our experiments show that
the proposed technique is very efficient
to resolve the cold-
start
problem while
maintaining high accuracy of
the
CF
recommendations.
Keywords-Deep Learning; Cold-Start; Recommendation Sys-
tem; Document Similarity; Job Search
I.
I
NTRODUCTION
Recommendation Systems (RSs)
utilize knowledge dis-
covery and data mining techniques in order to predict items
of
interest
to individual
users
and subsequently suggest
these items to them as recommendations [1],
[2].
Over the
years,
techniques and applications of RSs have evolved in
both academia and industry due to the exponential increase
in the
number
of
both on-line
information and on-line
users.
Such volumes
of
big data generated at
very high
velocities pose many challenges when it comes to developing
and deploying scalable accurate RSs.
Applications domains
of RSs include:
“e-government,
e-business,
e-commerce/e-
shopping,
e-library,
e-learning,
e-tourism,
e-resource ser-
vices and e-group activities” [2]. On the other hand, new RSs
deployment platforms emerged over years to include, besides
the classical
Web-based platform,
other
modern platforms
like mobile, TV, and radio [2]. Depending on the application
domain and the deployment platform,
dozens of techniques
and methods have been proposed to address cross-domain
and cross-platform scalability and quality challenges [1], [3],
[4].
Approaches for building RSs can be broadly clustered into
three main categories:
Content-Based (CB),
Collaborative
Filtering (CF),
and hybrid techniques.
CB techniques work
by measuring or predicting the similarity between profiles
of
the
items
(attributes/descriptions)
and profiles
of
the
users’
(attributes/descriptions of
past
preferred items)
[5],
[6].
Typically,
the similarity is
either
measured using a
suitable function (e.g., cosine similarity), or predicted using
statistical
or
machine
learning models
(e.g.,
regression).
CB systems are therefore suitable in domains where items’
descriptions or their metadata can be easily obtained (e.g.,
books,
news articles,
jobs,
TV shows,
etc.).
They are also
suitable when the lifetime of items is short and/or the change
in the item recommendation pool
is frequent
due to many
new items entering the pool over short periods.
These con-
straints limit
the number of ratings those items can receive
over their lifetime.
Many challenges arise when it comes to
CB systems.
For
example,
items’
metadata might
be very
limited or
does not
really contribute to generating user’s
interest.
Item description,
on the other
hand,
is typically
textual which makes the similarity scoring more challenging
due to language ambiguity raising the need for
semantic-
aware CB systems [7].
CF is perhaps the most
prominent
and successful
tech-
niques
of
modern recommendation systems.
Unlike
CB
methods which recommend items that are similar to what tar-
get user liked in the past,
CF methods leverage preferences
of
other
similar
users in order
to make recommendations
to the target
user
[4],
[5].
In other
words,
CF tries
to
predict good recommendations by analyzing the correlations
between all user behaviors rather than analyzing correlations
between items content.
CF is therefore suitable in domains
where obtaining meta-features or
descriptions of
items is
infeasible.
Another motivation for CF is to introduce more
diversity and serendipity to user’s experience by leveraging
the wisdom of
the crowd,
i.e.,
recommending items
that
arXiv:1611.05480v1 [cs.IR] 16 Nov 2016
the target
user might
not
have thought
about
or consumed
before but
rather
liked by other
similar
users.
One of
the
major challenges of CF based recommendation systems is
the cold-start problem which occurs when there is a lack of
information linking new users or items. In such cases, the RS
is unable to determine how those users or items are related
and is therefore unable to provide useful recommendations.
Hybrid RSs try to achieve better performance by combining
two or more techniques. The main theme of hybrid RSs is to
leverage the advantages of both CB and CF, and to alleviate
their shortcomings at the same time through hybridization.
In this work, we propose a deep learning based solution to
the cold-start problem. Our solution utilizes a state-of-the-art
deep learning document embedding algorithms (also known
as doc2vec) [8].
Our main contributions are as follows:
•
We propose a deep learning based matching algorithm
to solve cold-start
and sparsity problems in CF based
recommendation systems without major changes in the
existing system as shown in figure 1.
•
We
improve
the
performance
of
the
deep learning
matcher
by incorporating contextual
meta-data which
boost the accuracy significantly.
•
The proposed algorithm is capable of being extended
to solve similar
cold-start
problems in different
real
application scenarios such as relevancy and ranking in
search engines.
The remainder
of
the paper
is organized as follows:
in
Section II we will introduce the related work in recommen-
dation systems and the document
embedding;
Section III
concludes CF related techniques;
we discuss the problem
description and proposed algorithm in Section IV;
Section
V will
explain the experiments,
case study,
and discussion.
Finally we conclude in Section VI.
II.
R
ELATED
W
ORK
The main objective for
the Recommendation Systems
(RSs)
is providing a user
with content
he/she would like
by estimating the relevancy or the rating of these contents
based on the information about users and the items [1], [9].
The cold-start
problem is one of
the major
challenges in
RSs design and deployment.
Cold-start
occurs when either
new users or new items are introduced into the system.
In
this situation there would be no behavioral
data (ratings,
purchases, clicks, etc.) for CF to work properly. Addressing
cold-start is inevitable in modern RSs for two reasons [10]:
1)
the item and user
pools change on daily basis,
and 2)
CF is considered state-of-the-art recommendation technique
but
it
requires significant
behavioral
data in order to work
properly. Therefore, it is important to promote new items as
quickly as possible in order to establish links between them
and users to improve CF performance subsequently.
Another related problem to CF is the data sparsity which
appears due to insufficient
ratings per user and/or item in
Collabora've Filtering (CF) Core 
Pairing based on Deep 
Learning Matcher 
Applica'ons 
Figure 1: System architecture of CF based recommendation
engine with the proposed pairing layer to solve the cold-start
problem.
An application from the application’s layer sends
a recommendation request
to the CF core which analyzes
the interactions between users and items to generate rec-
ommendations.
Once those recommendations are generated
they are sent
to the pairing layer.
The pairing layer
will
check the list of recommended items and if any item among
the recommendation list
has been paired with a new item,
then the new item will be added to the recommendation list
which is passed to the application layer.
the rating matrix challenging the ability of
CF to predict
accurate user preferences and item similarities.
Several methods have been introduced to address the cold-
start and the data sparsity problems [10]–[14]. In this work,
we focus on the problem of
cold-start
in item-based CF.
Most
of
the proposed methods to address item cold-start
adopt
content-based approach;
they utilize the content
of
new items
in order
to identify similar
user
profiles
and
subsequently recommend these new items to them.
Saveski
and Mantrach [10] proposed Local
Collective Embeddings
(LCE)
as a solution for
item cold-start.
LCE utilizes de-
scriptions of the new items (i.e., the term-document matrix)
and project
it
collectively with the user-item rating matrix
into a common low-dimensional
space using non-negative
matrix factorization.
Soboroff
and Nicholas
[14]
utilized
Latent
Semantic
Indexing (LSI)
[15]
in order
to create
topical
representations of user profiles in the latent
space.
New items are then projected into the same latent
space
and compared to user
profiles
then recommended to the
most
similar
profiles.
Similar
to [14],
Schein et
al.
[11]
proposed an approach that
creates
a joint
distribution of
items and users through an aspect model in the latent space
by combining items’
content
and users’
preferences.
Our
approach to solve cold-start
problem relies on pairing the
new items with existing items that
have been exposed to
the users and gained enough ratings to be considered by
CF. Thus, our approach differs from prior ones which try to
utilize content-based techniques to pair the new items with
users by matching the content of these items with the content
of users’ profile.
III.
B
ACKGROUND
Collaborative
Filtering is
one
of
the
most
successful
techniques to building RSs due to their independence from
the content of items being recommended, which make them
easy to create and use across many application domains [4].
Typically,
CF methods utilize the user-item rating matrix
R ∈ R
|U |×|I|
which contains past ratings of items made by
users.
Rows in
R
represent
users
U = {u
1
, u
2
, ..., u
|U |
}
,
while columns represent
items
I
= {i
1
, i
2
, ..., i
|I|
}
.
Each
entry
r
ui
in
R
represents the rating user
u
gave to item
i
.
The role of the CF algorithm is to perform matrix completion
filling empty entries in
R
by analyzing existing entries.
The early generation of CF algorithms are the memory-
based
(a.k.a
neighborhood-based)
techniques
[16],
[17].
These techniques
work by measuring the similarities
or
correlations between either users (rows) or items (columns)
in the user-item rating matrix
R
. After finding these similar
neighbors,
recommendations are generated by choosing the
top-
K
items
similar
to a
given item in case
of
item-
based recommendations,
or
by aggregating the correlation
scores of items liked by similar users in case of user-based
recommendations [18]–[20].
The choice of the similarity or correlation metric has a
major contribution to the quality of recommendations [18].
Pearson-r correlation is a very popular metric used in CF
systems, and is used to estimate how well two variables are
linearly related.
Pearson-r correlation
corr
i,j
between two
items or two users
i, j
is computed as:
corr
i,j
=
P
w∈W
(r
wi
− ¯
r
i
)(r
wj
− ¯
r
j
)
p
P
w∈W
(r
wi
− ¯
r
i
)
2
p
P
w∈W
(r
wj
− ¯
r
j
)
2
(1)
where,
in case of
user-based recommendation,
w ∈ W
denotes items rated by both users
i, j
.
r
wi
&
r
wj
are ratings
of
item
w
by users
i, j
respectively.
¯
r
i
, ¯
r
j
are
average
ratings
of
users
i, j
respectively.
In case
of
item-based
recommendation,
w ∈ W
denotes users who rated items
i, j
.
r
wi
&
r
wj
are ratings of user
w
for items
i, j
respectively.
¯
r
i
, ¯
r
j
are average ratings of items
i, j
respectively.
Another
commonly used similarity metric is
the cosine similarity
which is computed as:
cos
i,j
=
~
R
i
.
~
R
j
||
~
R
i
|| ||
~
R
j
||
(2)
where
~
R
i
&
~
R
j
are the rating vectors
of
items/users
i, j
in the rating matrix
R
in case of item-based/user-based
recommendation respectively.
Another category of CF techniques are the model-based
approaches
which are more scalable than memory-based
techniques as they offline build a model
of
item-item or
user-user similarities and then use it at real-time to generate
recommendations. Several algorithms were proposed to gen-
erate such models including Bayesian networks [21]–[23],
clustering [24],
[25],
latent semantic analysis [26],
Singular
Value Decomposition (SVD) [27], Alternating Least Squares
(ALS) [28],
[29] regression models [30],
Markov Decision
Processes (MDPs) [31],
and others [32].
Another important aspect to understand this work is how
to represent
documents
in the
vector
space
to measure
similarity [33].
Lexical
features
are commonly used for
extracting vectors from documents including Bag-of-Words
(BoW),
n-grams (typically bigram and trigram),
and term
frequency-inverse document frequency (tf-idf ). Topic models
such as
Latent
Dirichlet
Allocation (LDA)
are also used
as
features
in document
classification problems
such as
sentiment
analysis and have shown promising results [34],
[35].
Moreover,
the application of deep learning to natural
language processing field has shown a great success.
Term Frequency-Inverse
Document
Frequency:
Com-
mon lexical features for document embedding include Bag-
of-Words, n-gram and tf-dif. Both Bag-of-Words and n-gram
model
draw much attention on frequent
words,
which may
not
be the best
way to measure the importance of a word
in a document.
On the other hand,
tf-idf
can be considered
as a weighted form of BoW for evaluating the importance
of a word to the document.
Let
tf (w; d)
be the number of
times word
w
appears in document
d
from a collection
D
,
idf (w; D)
indicates inverse document frequency of word
w
in set
D
,
then tf-idf
is defined in Equation 3 and 4.
tf − idf
= tf (w, d) × idf (w, D)
(3)
idf (w, D)
= log
N
1 + |{d ∈ D : w ∈ d}|
(4)
Latent Dirichlet Allocation: LDA is a probabilistic model
which learns
P (z|w)
, distribution of a latent topic variable
z
given a word
w
[34].
Compared with lexical features (BoW
and tf-idf ) mentioned above, representations learned by LDA
focus more on the semantic meanings of
each word,
and
have a feature space that is in low dimensions. Topic vectors
learned represent
the weights of words for each topic,
and
after
normalizing each word vector
from a sentence or
a
document, we obtain the vector of the sentence or document
for
all
topics and thus embed the target
document
into a
vector representation based on LDA model.
IV.
M
ETHODOLOGY
A.
Problem Description
In dynamic domains like job boards,
new items appear
frequently which makes CF not
feasible due to the cold-
start
problem.
However,
due to many shortcomings of the
CB recommendations, most domains still prefer CF over CB
due to the accuracy and diversity of results CF can provide
Figure 2:
(a) Framework of CF Based Recommendation System:
The only items to be considered are those which gained
some behavioral
data like ratings from users while all
the new items are not
considered until
they gain such behavioral
data.
(b) Framework of Proposed Pairing Algorithm to Solve the Cold Start
Problem:
Each new item without
behavioral
data is paired with existing item(s) that has behavioral data. Once the recommendations are retrieved by CF, each item with
behavioral data will pull its pair new item into the recommendation list.
which CB can not. That said, the cold-start problem continue
to impact substantial number of new items. Recommendation
engine is considered one of the major channels of engaging
users with items and continuing to re-engage them when they
leave a website via recommendation emails. Therefore, items
which have no chance to be recommended due to the cold-
start
problem,
will
lose that
major channel
of exposure to
end users. Thus, we propose a novel technique that helps CF
to be considered in dynamic domains with frequent presence
of new items by leveraging a deep learning matcher (DLM).
The DLM is used to pair each new item with an existing
item that has behavioral data (like ratings). This pairing will
allow the new items to be considered in CF even before any
users interact with these new items.
B.
Proposed Framework
In conventional
CF based recommendation engines (Fig-
ure 2-a) the only items to be considered for recommenda-
tions are those with behavioral data (i.e.
users interact with
those items by rating,
purchasing,
clicking...etc),
however,
all new items without such behavioral data can not be part of
the recommendations generated by CF. Our proposed system
which is depicted in Figure 2-b adds a new module which
can be thought
of
as a plug-in that
will
match each new
item
i
x
with an item that has behavioral data
i
y
, we call this
process pairing. Once this pairing is done, each pair
(i
x
, i
y
)
will be considered as one item,
so when item
i
y
is selected
for recommendation by CF,
item
i
x
(the new item with no
behavioral
data)
will
appear
with that
recommendation as
well. Therefore, the accuracy of the pairing is very important
since pairing irrelevant
items
will
introduce noise to the
recommendations.
Figure 3: Learning Word Vector and Document Vector.
C.
Document Embedding and Matching
Given the importance of accurate pairs (similar ones) for
the overall quality of the recommendations to be generated
by CF afterwards,
we tried the standard document
sim-
ilarity techniques
like Term Frequency-Inverse Document
Frequency (tf-idf )
and Latent
Dirichlet
Allocation (LDA)
but
the results were not
promising.
Therefore we built
a
Deep Learning Matcher (DLM) which outperforms all
the
other techniques significantly.
Our DLM was built utilizing
doc2vec which is considered the current state-of-the-art deep
learning algorithm for document
embedding and matching.
Contrary to lexical features,
the information extracted from
each word is distributed all
along a word window in dis-
tributed representations (as known as word2vec and doc2vec
feature) as shown in Figure 3 [8]. For a word vector learning,
given a sequence of
T
words
{w
1
, w
2
, ..., w
T
}
and a window
size
c
,
the objective function is as follows:
1
T
T −c
X
i=c
log p(w
i
|w
i−c
, ..., w
i+c
)
(5)
In order to maximize the objective function in Equation
5,
the probability of
w
i
is calculated based on the softmax
function shown in Equation 6 where the word vectors are
concatenated or
averaged for
predicting the next
word in
the content.
Similarly to learning the word vectors,
the
processing of learning the paragraph vector (i.e. the doc2vec
model) is maximizing the averaged log probability with the
softmax function by combining the word vectors with the
paragraph vector
p
i
in a concatenated or averaged fashion
as shown in Figure 3. For new documents input in the model,
the paragraph vector
is
learned by holding the softmax
parameters and gradient descending on the new vector entry.
p(w
i
|w
i−c
, ..., w
i+c
) =
e
y
w
i
P
j∈(1,...,T )
e
y
wj
(6)
D.
Contextual Features Enrichment
Simply using documents that
are in a large scale into
the
models
is
hardly good enough for
learning a
good
representation of
the documents [36].
doc2vec model
can
learn very good representations of
the documents seman-
tically.
However,
in some domains like job boards,
many
documents
can share major
overlapping content
such as
company or benefit descriptors,
while the important context
including the distinguished information such as requirements
or
qualifications
are overshadowed.
For
example,
in job
boards domain 90% of a job description may be dedicated
to describe the company and its values and culture,
while
only 10% describes the job requirements.
In such scenario,
doc2vec will
generate almost
same representation for
two
jobs posted by the same company,
however they are totally
different by the job requirements. To overcome this problem,
we enrich each document with contextual features including
the document
classification,
and location.
Additionally,
in
our domain since not all the parts of a document (job post-
ing) is equally important
to measure similarity,
we utilized
our in-house NLP document parser to extract the important
content such as job requirements and skills. These extracted
information is injected into the original
document
N
times
(we choose
N
to be 3 empirically) to guide doc2vec to a
better representation by improving the content distribution.
V.
E
XPERIMENT AND
R
ESULTS
To validate
and evaluate
the
proposed technique,
we
applied our
approach on top of
Careerbuilder’s CF-based
recommendation engine.
CareerBuilder operates one of the
largest
job boards in the world and has an extensive and
growing global presence, with millions of job postings, more
than 60 million actively-searchable resumes, over one billion
searchable documents, and more than a million searches per
hour.
In their recommendation engine,
Careerbuilder strives
to recommend the right
job to the right
person at
the right
time.
However,
the cold-start
problem is
very serious
at
Careerbuilder
given that
every day thousands of
new jobs
are posted and it
is important
for the employers who have
posted those jobs to start
receiving applications from job
seekers within a short period of time. Recommendation is a
major channel of job exposure, therefore new jobs will lose
a chance to be exposed through this important channel due
to the cold-start problem. In this section we will discuss the
different
experiments we run to evaluate solving the cold-
start problem using our technique. All experiments are done
in a machine with Intel(R) Xeon (R) E5-2667 series CPUs
(32 cores in total) and 264GB memory. The runtime of each
model
is also evaluated since our application scenario has
a requirement
of
running the whole workflow on a large
dataset (more than one million documents) on a daily basis.
Figure 4: Distribution of 24 General Job Classifications: x-
axis denotes the job category code and y-axis denotes the
proportion of jobs under each category.
A.
Job-to-Job (J2J) Matcher
All experiments used a sample of 1,147,725 classified jobs
from our dataset
[37].
The distribution of job classification
is
shown in Figure 4.
For
experiments
we use Gensim
[38]
to generate tf-idf
model
and train LDA and doc2vec
models.
For
the doc2vec model,
we train our
model
with
one iteration only due to speed limitation in order to run the
whole process on a daily basis,
and chose 100 dimensions
for the embedded vectors.
The reason is that,
according to
our
experiments
adding more dimensions
for
the vectors
barely improved the performance if any,
but
increased the
runtime notably.
We choose the number
of
job categories
which have at least 100 jobs included in our testing dataset
to be the number of topics used for generating LDA model,
so that
one topic is expected to indicate one job category
(java developer,
registered nurse,
etc).
Eventually we have
805 topics in total. We use the fine-grained categories instead
of the general ones since the number is small and can hardly
learn a very good representation of
the documents.
We
chose cosine similarity as the similarity metric to evaluate
Table I: Recall for Top 10-50 Most Similar Jobs.
Model
Top 10
Top 20
Top 30
Top 50
tf-idf
30.4%
37.3%
39.9%
44.8%
LDA
18.2%
21.4%
26.8%
30.2%
doc2vec
88.9%
95.0%
96.3%
97.7%
tf-idf
with contextual features
32.1%
41.2%
46.4%
49.5%
LDA with contextual features
19.9%
25.3%
30.1%
33.3%
similarity between documents with threshold 0.5 to indicate
if two documents are similar or not.
In this
section we test
the embedding algorithms
for
learning document
level
similarity,
among which the tf-idf
and LDA models
are set
as
baselines.
We evaluated the
performance of each model both with and without contextual
features enrichment. Since the contextual feature enrichment
is proposed to improve the performance for certain specific
circumstances,
we don’t
add it
in this experiments section.
Our objective is to evaluate the job-to-job matching perfor-
mance for our model
rather than to evaluate the results for
the whole recommendations,
i.e.,
to evaluate the similarity
we learned from different models.
Table II: Precision for Top 10 Most Similar Jobs.
Model
Precision
tf-idf
29.2%
LDA
17.1%
doc2vec
82.9%
tf-idf
with contextual features
30.3%
LDA with contextual features
18.4%
doc2vec with contextual
features
91.8%
We
started our
evaluation by selecting 100 randomly
selected jobs as test samples and generated the top 10 most
similar jobs for each test sample based on the doc2vec model
with contextual
features
to achieve the best
performance
and evaluated the result
manually as
good/bad matches.
The good matches are then considered as ground truth for
evaluating other models. The overall precision for this set-up
is 91.8%.
To evaluate the other models,
we use the similar
jobs which have been labeled as good matches as ground
truth and calculate the recall
value (namely,
among all
the
correct
matches,
how many of them have been returned as
matches by the other models) from top 10 to 50 most similar
jobs generated by the other models, as well as the precision
based on the top 10 most
similar jobs returned from each
model (namely, among the top 10 most similar jobs returned
from the other
models,
how many of
them are labeled as
correct
match in our evaluation).
The results are shown in
Table I and Table II.
According to the results shown in Table II,
our proposed
doc2vec
model
with contextual
features
yields
the
best
performance for job-to-job matching task. On the other hand,
Table III: Running Time for Different Model.
Model
Running Time (Minutes)
tf-idf
22
LDA
992
doc2vec
75
tf-idf
with contextual features
30
LDA with contextual features
1069
doc2vec with contextual features
89
the LDA model obtains the lowest. The LDA model learns a
good representation of the documents for classifying them
under different topics, however for job recommendation and
job-to-job pairing tasks it
does not
perform good enough.
Previous works have shown that tf-idf
model itself is capable
of
learning a good document
embedding for
documents
which are long enough,
and according to our results the tf-
idf
model outperforms the LDA model by about 10%. Since
the documents in our
dataset
are mainly job descriptions
with contextual features including job requirements, job title,
job classifications,
etc.,
which are not
expected to be long
enough for tf-idf
to learn a very good representation.
The
doc2vec outperforms the tf-idf
and LDA models significantly
and achieves the highest
precision of 91.8%.
By including
the contextual features we achieve a 9% gain in the precision
of
the top-10 most
similar
jobs generated by the doc2vec
model.
For LDA and tf-idf
the improvements are not
com-
parable because the two baseline models are not capable of
learning a good embedding for the documents and adding
more information does not help. However, on the other hand
the doc2vec model can learn a decent document embedding
semantically and by adding the extra information we are
able to obtain that significant improvement on performance.
Table I shows the recall value of different models from top
10 to 50 most similar jobs. Since the manually labeled results
based on doc2vec with contextual
features are established
as ground truth,
we did not calculate the recall value for it.
Based on our results the doc2vec model without contextual
features still performs better than the other models, followed
by the tf-idf
model
and then the LDA model.
The doc2vec
model
without
contextual
features is able to return about
90% of the correct job matches for top 10 most similar jobs
and almost all of the correct matches with the top 50, while
the tf-idf
model
reaches at
about
40% and the LDA model
Table IV: Case Study: Source Job (left),
Poor Pairing (middle) and Good Pairing (right).
Document Title:
HVAC Technician
XX Resort & Club
Banquet Houseperson - PM
HVAC Mechanic
Same Company
Values Context
Since being founded in 1919, Company XX has been a leader in the hospitality industry. Today, Company XX remains a
beacon of innovation,
quality,
and success.
This continued leadership is the result of our Team Members staying true to
our Vision, Mission, and Values. Specifically, we look for demonstration of these Values: H Hospitality - We’re passionate
about delivering exceptional guest experiences... In addition, we look for the demonstration of the following key attributes
in our Team Members: Living the Values Quality Productivity Dependability Customer Focus Teamwork Adaptability.
Same Company
Overview Context
What
will
it
be like to work for this Company XX Brand? What
began with the world’s most
iconic hotel
is now the
world’s most
iconic portfolio of hotels.
In exceptional
destinations around the globe,
XX Hotels & Resorts reflect
the
culture and history of their extraordinary locations,
as well
as the rich legacy of Company XX...
If you understand the
value of providing guests with an exceptional
environment
and personalized attention,
you may be just
the person we
are looking for to work as a Team Member with XX Hotels & Resorts.
Similar Benefits
Context
What
benefits will
I receive? Your benefits will
include a competitive starting salary and,
depending upon eligibility,
a
vacation or Paid Time Off (PTO) benefit. You will instantly have access to our unique benefits such as the Team Member
and Family Travel
Program,
which provides reduced hotel
room rates at
many of our hotels for you and your family,
plus discounts on products and services offered by Company XX Worldwide and its partners.
After 90 days you may
enroll
in Company XX Worldwide Health & Welfare benefit
plans,
depending on eligibility.
Company XX Worldwide
also offers eligible team members a 401K Savings Plan,
as well
as Employee Assistance and Educational
Assistance
Programs. We look forward to reviewing with you the specific benefits you would receive as a Company XX Worldwide
Team Member...
Differences:
The
XX Resort
& Club
is
seek-
ing an HVAC Technician who will
be responsible for maintaining the
physical
functionality and safety of
the hotels
heating,
ventilation and
air conditioning (HVAC) equipment
and machinery continuing effort to
deliver
outstanding
guest
service
and financial
profitability.
What will I be doing?
As an HVAC Mechanic,
you would
be responsible for maintaining the
physical
functionality and safety of
the hotels
heating,
ventilation and
air conditioning (HVAC) equipment
and machinery in the hotels
con-
tinuing effort
to deliver outstand-
ing guest service and financial prof-
itability.
Specifically...
As a Banquet
Set-Up Attendant,
you
would
be
responsible
set-
ting and cleaning banquet
facil-
ities for
functions in the hotel’s
continuing effort
to deliver
out-
standing guest
service and finan-
cial profitability. Specifically, you
would be responsible for perform-
ing
the
following
tasks
to
the
highest
standards:
Set
tables and
chairs
to meet
function specifi-
cations.
Clean meeting space in-
cluding,
but
not
limited to,
vac-
uuming, sweeping,
mopping,
pol-
ishing,
wiping areas and washing
walls before and after events...
An HVAC Mechanic with Company
XX Hotels
and Resorts
is
respon-
sible
for
maintaining the
physical
functionality and safety of the hotels
heating,
ventilation and air condi-
tioning (HVAC) equipment and ma-
chinery continuing effort to deliver
outstanding guest service and finan-
cial
profitability.
As an HVAC Mechanic,
you would
be responsible for maintaining the
physical
functionality and safety of
the hotels
heating,
ventilation and
air conditioning (HVAC) equipment
and machinery in the hotels
con-
tinuing effort
to deliver outstand-
ing guest service and financial prof-
itability.
Specifically...
reaches at about 30%. By adding the context we still obtain
an improvement on the recall values which is consistent with
the results on precision.
As introduced in Section IV,
we have thousands of new
jobs added to the system on a daily basis and about the same
number
of
jobs expiring.
Therefore it
is best
to train our
model and generate similar jobs for those jobs which suffer
from the cold-start
problem on a daily basis.
This would
require the runtime for the whole process including training
and inferring to be in an hourly-scale. The runtime for each
model is shown in Table III. We can conclude that by using
multi-process computing (all models are using 24 cores for
multi-process computing) we can run all
the models on a
daily basis except the LDA model.
contextual features adds
more text
contents and more computation in the process,
but
with multi-process computing the extra runtime is rea-
sonable compared with the significant
improvement
on the
performance.
The runtime for tf-idf
model
is the shortest,
while doc2vec model
with and without
contextual
features
is still comparable which can be completed within two hours.
For a daily based workflow it
is acceptable and we choose
to run it at midnight every day for our job recommendation
systems.
B.
Case Study
The job-to-job matcher
did admirably matching docu-
ments which are similar
to one another,
however,
we en-
countered challenges with documents comprised of approx-
imately 70% or higher of exact same content due to company
descriptors
like background,
benefits
and values.
As
the
example shown in Table IV, compared with the source job on
the left column, HVAC Technician was matched to Banquet
Houseperson in the middle with completely different
skill
sets and requirements.
This is due to the document
being
so similar
as
stated previously.
In this
example 25% of
the document
is actually different
which still
allows poor
recommendations which are highly similar
to prevail
and
Table V: Selected Parts of a User-to-Job Matching Example.
Resume
Job
Education
Master
of
Business Administration Degree
in Accounting.
Title
Senior Accountant - General Ledger(GL)
Certificate
Candidate for CPA (passed BEC and Regu-
lations).
Requirements
•
Must be a degreed accountant.
•
CPA and/or Master of Science in Accounting.
•
Minimum of 2-3 years general accounting experience
in a corporate division or
Big Four
environment
is
required.
•
Must
demonstrate proficiency in general
ledger
ac-
counting including preparing,
reading,
interpreting,
and analyzing financial statements.
•
...
Skills
•
Expert in handling full accounting cycle
operations with hands on experience in
receivables,
payable,
bank reconcilia-
tion,
tax planning and filing,
etc.
•
Successful project manager with exper-
tise to effectively budget
and forecast
resource needs,
plan and schedule time
and resources for meeting expectations.
•
...
Work Experience
•
Senior Accountant 12/2015-Present
•
Senior Accountant Consultant 11/2015-
2/2016
•
Accounting Consultant 7/2015-11/2015
•
Accounting Manager 5/2014-3/2015
•
Senior Accountant 7/2013-5/2014
•
...
Responsibilities
•
Maintain project
commitments
ensuring proper
ac-
counting treatment for all projects.
•
Compile and analyze financial
information to inter-
pret and communicate current and projected company
financial results to management.
•
Prepare and examine financial
statements and foot-
notes
as
assigned for
completeness,
accuracy and
conformance with relevant
accounting standards and
management reporting requirements.
•
...
be recommended.
To counter,
we enrich contextual features
which helped alleviate and push down similarity scores for
these edge cased documents.
Referring to the right column,
HVAC Mechanic was returned instead when we emphasized
on promoting the differences within the document.
In our
domain, the contextual features proved successful in filtering
out highly similar scoring documents with different roles and
functions.
VI.
C
ONCLUSIONS AND
F
UTURE
W
ORK
Collaborative Filtering is widely used in recommendation
systems
for
real
world applications,
however,
it
suffers
from the cold-start
problem where new entities cannot
be
recommended or
receive
recommendations
including the
users and items. To tackle the cold-start problem we build an
item-to-item deep learning matcher based on the document
similarities learned from the state-of-the-art
document
em-
bedding model doc2vec. The performance of document level
similarities learned by the doc2vec model
with contextual
features outperforms the baseline models significantly and
can run on a daily basis which fits the requirement
of our
practical
application.
Our approach can be integrated with
any existing CF-based recommendation engine with no need
to modify the CF core.
To proof the efficiency,
scalability,
and accuracy of the proposed technique we apply it on top
of Careerbuilder’s CF-based recommendation engine which
is
used to recommend jobs
to job seekers.
After
testing
this model on more than 1 million documents we prove its
efficiency in resolving the cold-start
problem in large scale
while maintaining high level
of accuracy.
We are working
on a multimodal
document
embedding model
for learning
user-to-user and user-to-job similarity whose initial
results
are very promising to solve the cold-start problem for user-
based CF as shown in Table V.
R
EFERENCES
[1]
Jes
´
us
Bobadilla,
Fernando
Ortega,
Antonio
Hernando,
and Abraham Guti
´
errez.
Recommender
systems
survey.
Knowledge-Based Systems,
46:109–132,
2013.
[2]
Jie Lu,
Dianshuang Wu,
Mingsong Mao,
Wei
Wang,
and
Guangquan Zhang.
Recommender
system application de-
velopments:
a survey.
Decision Support
Systems,
74:12–32,
2015.
[3]
Pasquale Lops,
Marco De Gemmis,
and Giovanni Semeraro.
Content-based recommender
systems:
State of
the art
and
trends.
In Recommender systems handbook,
pages 73–105.
Springer,
2011.
[4]
Xiaoyuan Su and Taghi M Khoshgoftaar. A survey of collabo-
rative filtering techniques.
Advances in artificial intelligence,
2009:4,
2009.
[5]
Charu C Aggarwal.
Content-based recommender systems.
In
Recommender Systems,
pages 139–166.
Springer,
2016.
[6]
Michael J Pazzani and Daniel Billsus.
Content-based recom-
mendation systems.
In The adaptive web,
pages 325–341.
Springer,
2007.
[7]
Marco de Gemmis, Pasquale Lops, Cataldo Musto, Fedelucio
Narducci, and Giovanni Semeraro.
Semantics-aware content-
based recommender
systems.
In Recommender
Systems
Handbook,
pages 119–159.
Springer,
2015.
[8]
Quoc V Le and Tomas Mikolov.
Distributed representations
of
sentences and documents.
In ICML,
volume 14,
pages
1188–1196,
2014.
[9]
Gediminas Adomavicius and Alexander Tuzhilin.
Toward the
next
generation of
recommender
systems:
A survey of
the
state-of-the-art and possible extensions.
Knowledge and Data
Engineering,
IEEE Transactions on,
17(6):734–749,
2005.
[10]
Martin Saveski
and Amin Mantrach.
Item cold-start
rec-
ommendations:
learning local
collective
embeddings.
In
Proceedings of
the 8th ACM Conference on Recommender
systems,
pages 89–96.
ACM,
2014.
[11]
Andrew I Schein,
Alexandrin Popescul,
Lyle H Ungar,
and
David M Pennock.
Methods and metrics for cold-start recom-
mendations.
In Proceedings of the 25th annual international
ACM SIGIR conference on Research and development
in
information retrieval,
pages 253–260.
ACM,
2002.
[12]
Guibing Guo.
Integrating trust
and similarity to ameliorate
the data sparsity and cold start
for
recommender
systems.
In Proceedings of the 7th ACM conference on Recommender
systems,
pages 451–454.
ACM,
2013.
[13]
Jes
´
uS Bobadilla,
Fernando Ortega,
Antonio Hernando,
and
Jes
´
uS Bernal.
A collaborative filtering approach to mitigate
the new user cold start problem.
Knowledge-Based Systems,
26:225–238,
2012.
[14]
Ian Soboroff and Charles Nicholas.
Combining content
and
collaboration in text
filtering.
In Proceedings of
the IJCAI,
volume 99,
pages 86–91.
Citeseer,
1999.
[15]
Scott
Deerwester,
Susan
T Dumais,
George
W Furnas,
Thomas K Landauer,
and Richard Harshman.
Indexing by
latent semantic analysis.
Journal of the American society for
information science,
41(6):391,
1990.
[16]
Paul
Resnick,
Neophytos
Iacovou,
Mitesh Suchak,
Peter
Bergstrom,
and John Riedl.
Grouplens: an open architecture
for collaborative filtering of netnews.
In Proceedings of
the
1994 ACM conference on Computer supported cooperative
work,
pages 175–186.
ACM,
1994.
[17]
Joseph A Konstan, Bradley N Miller, David Maltz, Jonathan L
Herlocker, Lee R Gordon, and John Riedl.
Grouplens: apply-
ing collaborative filtering to usenet news.
Communications of
the ACM,
40(3):77–87,
1997.
[18]
Badrul
Sarwar,
George Karypis,
Joseph Konstan,
and John
Riedl.
Item-based collaborative filtering recommendation al-
gorithms.
In Proceedings of the 10th international conference
on World Wide Web,
pages 285–295.
ACM,
2001.
[19]
Mukund Deshpande and George Karypis.
Item-based top-n
recommendation algorithms.
ACM Transactions on Informa-
tion Systems (TOIS),
22(1):143–177,
2004.
[20]
Greg Linden,
Brent
Smith,
and Jeremy York.
Amazon.
com
recommendations:
Item-to-item collaborative filtering.
IEEE
Internet computing,
7(1):76–80,
2003.
[21]
John S Breese, David Heckerman, and Carl Kadie.
Empirical
analysis of
predictive algorithms for
collaborative filtering.
In Proceedings of
the Fourteenth conference on Uncertainty
in artificial
intelligence,
pages
43–52.
Morgan Kaufmann
Publishers Inc.,
1998.
[22]
Koji Miyahara and Michael J Pazzani.
Collaborative filtering
with the simple bayesian classifier.
In Pacific Rim Interna-
tional
conference on artificial
intelligence,
pages 679–689.
Springer,
2000.
[23]
Xiaoyuan Su and Taghi M Khoshgoftaar.
Collaborative filter-
ing for multi-class data using belief nets algorithms.
In 2006
18th IEEE International
Conference on Tools with Artificial
Intelligence (ICTAI’06),
pages 497–504.
IEEE,
2006.
[24]
Lyle H Ungar
and Dean P Foster.
Clustering methods for
collaborative filtering.
In AAAI workshop on recommendation
systems,
volume 1,
pages 114–129,
1998.
[25]
Sonny Han Seng Chee,
Jiawei
Han,
and Ke Wang.
Rectree:
An efficient
collaborative filtering method.
In International
Conference on Data Warehousing and Knowledge Discovery,
pages 141–151.
Springer,
2001.
[26]
Thomas Hofmann.
Latent semantic models for collaborative
filtering.
ACM Transactions on Information Systems (TOIS),
22(1):89–115,
2004.
[27]
Daniel Billsus and Michael J Pazzani.
Learning collaborative
information filters.
In Icml,
volume 98,
pages 46–54,
1998.
[28]
Yunhong Zhou,
Dennis
Wilkinson,
Robert
Schreiber,
and
Rong Pan.
Large-scale parallel collaborative filtering for the
netflix prize.
In International
Conference on Algorithmic
Applications in Management, pages 337–348. Springer, 2008.
[29]
Yehuda Koren,
Robert
Bell,
Chris Volinsky,
et
al.
Matrix
factorization techniques for recommender systems. Computer,
42(8):30–37,
2009.
[30]
Slobodan Vucetic and Zoran Obradovic.
Collaborative fil-
tering using a regression-based approach.
Knowledge and
Information Systems,
7(1):1–22,
2005.
[31]
Guy Shani,
David Heckerman,
and Ronen I
Brafman.
An
mdp-based recommender system.
Journal of Machine Learn-
ing Research,
6(Sep):1265–1295,
2005.
[32]
Charu C Aggarwal.
Model-based collaborative filtering.
In
Recommender Systems,
pages 71–138.
Springer,
2016.
[33]
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
Word repre-
sentations: a simple and general method for semi-supervised
learning.
In Proceedings
of
the 48th annual
meeting of
the association for computational linguistics, pages 384–394.
Association for Computational Linguistics,
2010.
[34]
David M Blei,
Andrew Y Ng,
and Michael
I
Jordan.
La-
tent
dirichlet
allocation.
the Journal
of
machine Learning
research,
3:993–1022,
2003.
[35]
Andrew L Maas,
Raymond E Daly,
Peter
T Pham,
Dan
Huang,
Andrew Y Ng,
and Christopher
Potts.
Learning
word vectors for sentiment
analysis.
In Proceedings of
the
49th Annual
Meeting of
the Association for Computational
Linguistics: Human Language Technologies-Volume 1,
pages
142–150.
Association for Computational Linguistics,
2011.
[36]
Sunging Ahn,
Heeyoul
Choi,
Tanel
P
¨
arnamaa,
and Yoshua
Bengio.
A neural knowledge language model.
arXiv preprint
arXiv:1608.00318,
2016.
[37]
Mohammed Korayem,
Camilo Ortiz,
Khalifeh AlJadda,
and
Trey Grainger.
Query sense disambiguation leveraging large
scale user behavioral data.
In IEEE International Conference
on Big Data (Big Data 2015), pages 1230–1237. IEEE, 2015.
[38]
Radim
ˇ
Reh
˚
u
ˇ
rek and Petr
Sojka.
Software Framework for
Topic Modelling with Large Corpora.
In Proceedings
of
the
LREC 2010 Workshop on New Challenges
for
NLP
Frameworks, pages 45–50, Valletta, Malta, May 2010. ELRA.
http://is.muni.cz/publication/884893/en.
