A Domain-Independent Hybrid Approach for
Automatic Taxonomy Induction
Bushra Zafar
College of Electrical &
Mechanical Engineering,
National University of
Science and Technology
Islamabad,
Pakistan
bushra.zafar025@gmail.com
Usman Qamar
College of Electrical &
Mechanical Engineering,
National University of
Science and Technology
Islamabad,
Pakistan
usmanq@ceme.nust.edu.pk
Ayesha Imran
Department of Mathematical
Information Technology,
University of Jyv
¨
askyl
¨
a
Jyv
¨
askyl
¨
a,
Finland
ayimran@student.jyu.ﬁ
Abstract—Semantic
taxonomies
are
the
ﬂexible
way to or-
ganize,
navigate
and retrieve
information effectively.
Natural
Language Processing and Artiﬁcial Intelligence tasks are heavily
relied on these taxonomies.
This
paper
presents
a taxonomy
induction system that integrates two modules: word-embedding
and string inclusion.
We implement
a simple,
semi-supervised
and domain independent system based on Taxonomy Extraction
Evaluation (TExEval2) Task, SemEval 2016 . The task is divided
into two steps,
ﬁrst
is to identify hyponym-hypernym relations
and then to construct a taxonomy from a domain speciﬁc terms
lists. The system is trained over large general corpus. The system
learns vectors for phrases and utilizes word vectors with phrases
such as “known as”,
etc.
to generate possible hypernyms and
construct
taxonomy.
Three different
domains,
i.e.
environment,
food and science are considered for taxonomy induction.
The
constructed taxonomies
are
evaluated against
gold standard
taxonomies.
The proposed system achieved signiﬁcant results for
hyponym-hypernym identiﬁcation and taxonomy induction.
I.
I
NTRODUCTION
Semantics taxonomies are the ﬂexible way of
presenting
knowledge based information.
These taxonomies are exten-
sively used in thesaurus and ontology [1] [2].
In unstructured
text, different functional words are used for showing hyponym-
hypernym relations such as “is a”,
“is called”,
etc.
Among
them,
“is a” keyword is the most
common functional
word
that
is
used in construction of
semantic
taxonomies.
For
instance,
WordNet
[1]:
a lexical
database,
is based on “is a”
relation to well-organized senses.
For
example,
words such
as,
“dog” and “canine” are directly linked,
as “canine” is the
hypernym and “dog” is the hyponym in this relation.
These
semantics taxonomies and ontologies are the knowledge rich
source for multiple Natural Language Processing tasks such as
textual
entailment
[3].
However,
the existing taxonomies and
ontologies are limited in term of completeness and domain.
Manual
taxonomy construction is a time consuming and
knowledge incentive process.
In previous
study,
lexical
re-
sources are used to reﬁne and extend these taxonomies [2].
But
the scope of reﬁned and extended taxonomies are relied
heavily on the completeness of lexical
resources.
Recently,
a
lot of researches have been conducted to automate the process
of
taxonomy induction.
Although inducing taxonomy from
unstructured text is a great challenge in the research area.
In this paper,
we propose a two-modular,
semi-supervised
system which identiﬁes hyponym-hypernym relations and cre-
ates semantic taxonomy from a given set of words.
This way,
we can compare to the results of the SemEval
– Taxonomy
Extraction Evaluation (TExEval-2) workshop
1
.
II.
R
ELATED
W
ORK
The process of
taxonomy induction is divided into three
main
steps:
terms
extraction,
relation
discovery
and
tax-
onomy construction.
Previously,
comprehensive
study had
described various
approaches
tackling these challenges
[4].
The state-of-art approaches for automatic taxonomy induction
includes pattern-based approaches [5],
[6],
clustering based
approaches [7],
[8] and distributional approaches [9],
[10].
Pattern-based approaches [5] are simple and guarantee high
accuracy in ﬁnding instances of relations,
if the patterns are
wisely selected.
Patterns can be selected either manually [6]
or
automatically,
such as
automatic bootstrapping [5].
The
main ideas is to create a number of speciﬁc lexical-syntactic-
patterns.
These patterns are used to extract
relation from a
corpus. Hearst-style patterns [5] is a way to ﬁnd relations auto-
matically by performing bootstrapping. However, the problem
with pattern-based approaches is to cover diverse patterns from
a given corpus as these pattern have limited occurrence in a
given corpus.
Moreover,
large corpus ensures the accuracy of
pattern based approach [11],[6].
For
instance,
there is more
probability to ﬁnd patterns on the Web.
Clustering approaches based on similarity distance measure
score,
clusters terms iteratively.
The word context
is taken
as vectors,
the similarity distance is calculated between these
vectors.
The clustering approaches are capable of ﬁnding the
relations those not
explicitly available in the corpus.
Wong
et.
al,
proposed a cluster based methodology that depends on
ANT agent
[12].
First,
the crawler was used to construct
the
domain speciﬁc corpus then clustering method is applied on
this corpus.
However,
clustering approaches face a number of
challenges such as difﬁculty in labeling of clusters (non-leaf
nodes),
dependency on existing constructed features and on
1
http://alt.qcri.org/semeval2016/task13/
2016 17th International Conference on Parallel and Distributed Computing, Applications and Technologies
978-1-5090-5081-9/16 $31.00 © 2016 IEEE
DOI 10.1109/PDCAT.2016.84
372
hand-crafted designs also incorporate problems in clustering
approaches.
Recently,
distributional
approaches [9],
[10] have gained a
lot
of
popularity because of
the evolution of
deep learning
and neural
networks [13],
[14],
[15],[16].
The main concept
is semantic knowledge can be considered as a vector space,
where
each term is
depicted by a
single
point
and their
closeness represents the degree of
their
semantic similarity.
In vector space,
words are distributed across the context
in a
way that
similar words are grouped together or found nearer
to each other [17].
III.
M
ETHODOLOGY
In this section,
we present the simple,
semi-supervised and
domain independent
two-modular
framework that
combines
the strength of word embedding and the sub-string inclusion
methods similar to [18].
The system takes a list
of domain-
speciﬁc terms and general text corpus and then generate a well-
structured taxonomy.
It
includes three main steps.
First,
the
system crawl the general text corpus and generate the vectors
of each domain speciﬁc term and of phrases (is a, known as, is
called and type of). For vectors generation, the system utilizes
Word2vec [19].
Word2vec is
a neural
net
of
two-layer
that
convert
text
in numerical
form that
deep neural
network can understand.
These vectors
are used to ﬁnd out
the most
similar
word
vectors.
For
each word in the training and testing data set,
the neural
network can produce a vector which encodes the
semantic meaning of the word.
Note,
the system ignore the
term if the term is not present in the corpus. Then, the system
ﬁnd and extract
the hypernyms by using the algorithm based
on word-embeddings and sub-string inclusion methods. In the
last
step,
the system prune the duplicate hyponym-hypernym
relations from the generated taxonomy.
Below,
we provide an overview of proposed algorithm and
how we improve taxonomy by sub-string inclusion method.
A.
Identifying Hypernym with Transition Matrix
The distributional
methods generate word embedding that
preserve speciﬁc semantic and knowledge [20],
[15].
The
hyponym-hypernym pairs
also retain that
semantic similar-
ity property [21].
Such as
v(shrimp)-v(prawn)
≈
v(ﬁsh)-
v(goldﬁsh).
In example “v” represents vector.
The assumption is that hypernym can be identiﬁed by using
the supervised transition matrix
Φ
that projects all hyponyms
to their
hypernyms.
For
instance,
if
“cat” is a word,
it
can
translate to its
hypernym “animal”,
if
Φ
transition matrix
exits
between the vectors
of
these words.
Such as,
v(cat)
=
Φ
transition matrix * v(animal).
For
this
purpose,
two
methods
are
proposed [21]
i.e.
uniform linear
projection:
in which transition matrix
Φ
is
similar
for
all
the words
and the piece-wise linear
project:
in which different
term
clusters are generated.
For each cluster,
a separate transition
matrix
Φ
is learnt.
In both methods,
Φ
transition matrix is
domain dependent and highly dependent on the training of the
transition matrix.
To learn transition matrix
Φ
,
mean squared
error value of
|
(
Φ
)x-y
|
is reduced for all word pairs.
However,
instead of learning word embedding with super-
vised
Φ
,
a method is proposed [22] that
replace “is a”vector
with transition matrix
Φ
.
This method is simple but
do not
produce high quality results.
B.
Identifying a Hypernym with Function words Vectors
This module is based on previous work that
used word-
embedding model
for hyponym-hypernym identiﬁcation.
The
work utilized transition matrix that
identiﬁes hypernym from
the given hyponym.
Similar to
[22],
we used function word
i.e.
phrase “is a” instead of
supervised transition matrix to
make the module un-supervised but
we also consider
other
phrases that exhibits hyponym-hypernym relations such as “is
called”,
“known as” and “type of”.
This enables us to ﬁnd
more hyponym-hypernym relations from the text corpus.
For implementation, we used the latest Wikipedia dump as a
training data.
First,
we used WikiExtractor tool to preprocess
the Wikipedia dump and generate plain text
by eliminating
tags,
mark-up,
tables,
etc.
Then,
we produce domain-speciﬁc
corpus separately,
for each domain terms by extracting only
those documents that
contain the domain-speciﬁc terms.
We
used gensim library [23]
to train the word2vec model
[24],
over domain speciﬁc corpus.
Then,
we single tokenized all
phrases in order to generate
word-embeddings. We also tokenized the terms of the domain-
speciﬁc list
present
in the trained corpus.
For
each domain
speciﬁc term list,
we trained different
word2vec models.
We
multiply all
phrases vectors with the word vectors in order
to get most 10 similar word vectors.
For instance,
v(kiwis) *
v(is-a food )
≈
v(fruit) [22], we also used four phrases vectors
(mentioned above) to ﬁnd possible hypernyms.
C.
Sub-String Inclusion Module
A simple method of hypernym and hyponym identiﬁcation
is based on substring inclusion similar to [18] that
one term
contains other term such as “life science” is a “science”, “blue
berry” is a “berry”and so on. In English, hypernym are found
in the end of compound words.
For instance,
“environmental
chemistry” is a “chemistry”.
On contrast,
if preposition occur
between terms then ﬁrst term is considered as hypernym. Such
as in “noodle with tomato egg sauce”,
hypernym is “noodle”
not the “tomato egg sauce”. But we made little changes as we
only look for “of” and “with” prepositions.
For accuracy,we
apply the constraint
that
the length of
hypernym should be
greater than three and the length of hyponym should be greater
than two.
D.
Pruning of Hypernym-Hyponym Relations
To improve precision,
we includes only those hypernym-
hyponym relations from sub-string inclusion method that
are
not
found in constructed taxonomy.
Then we analyze the
taxonomy and remove the duplicate relations.
373
IV.
E
VALUATION
In this section,
we evaluate the proposed taxonomy induc-
tion system.
First,
we describe the benchmarks
which we
performed.
Then,
we present
our
results and comparison of
the proposed system’
taxonomies
with the other
systems’
taxonomies.
A.
Evaluation Method
For evaluation SemEvalTexEval-2 workshop
2
provided the
benchmarks to access the quality of the system taxonomies.
Each benchmark consists of a list of domain-speciﬁc terms and
gold standard taxonomies.
These domain-speciﬁc terms used
as input for the proposed system and gold standard taxonomies
are used to compare the system generated taxonomies.
The
main assumption is the generated taxonomies should closely
similar to the gold standard taxonomies as much as possible
without using these gold standard taxonomies and the sources
used to generate these gold standard taxonomies. These bench-
marks
belongs
to three
domain,
i.e.
environment,
science
and food.
For performance evaluation of system taxonomies
different measurements are calculated such as, precision, recall
and F-score.
“F = 2(P*R)/(P+R)”
(1)
where, F, P, R are F-score, Precision and Recall respectively.
Here the precision is
computing the number
of
hyponym-
hypernym relations common in both gold standard taxonomy
and system taxonomy over
the total
number
of
hypernym-
hyponym relations present in system taxonomy.
On the other
hand, the recall is based on the number of common hyponym-
hypernym relations
in both the system taxonomy and the
gold standard taxonomy over the total
number of hyponym-
hypernym relations available in the gold standard taxonomy.
Recall measure is evaluating the completeness of the generated
taxonomies. But precision and recall are inversely proportional
to each other if one value gets high,
others value decreases.
F-score is a way to achieve balance between two measures.
B.
Results
In this sub-section,
we present our system results and their
comparison with other systems.
Table I,
presents the system
evaluation score obtained by comparing against gold standard
taxonomies
across
3 different
domains.
Table
II,
presents
system comparison with other systems.
Our
system achieved the consistent
recall,
precision and
F-score across each domain.
Our
observation is,
the string
inclusion system can generate incorrect
results.
For instance,
“honey bunches” is a hypernym of “honey bunches of oats”
according to the sub-string inclusion system. However, “honey
bunches” is not the hypernym of “oats”and vice versa.
Thus,
we considered only those relations
in a taxonomy from a
sub-string inclusion system that
are not
present
in generated
taxonomy and are
not
opposing to existed relations
in a
taxonomy such as,
if
relation of
“animal
science” and “life
2
http://alt.qcri.org/semeval2016/task13/
science” is already present in a taxonomy where “life science”
is a hypernym of “animal science” then the relation of “animal
science” and “science”would not be added in a taxonomy. Al-
though, “science” is an indirect hypernym of “animal science”,
not the direct hypernym.
Additional
to
sub-string
inclusion
module,
our
system
mainly based on word-embedding approach trained over
a
large general corpus for hyponym-hypernym identiﬁcation and
arranging them in taxonomy.
Although,
our
system also identiﬁed additional
correct
hyponym-hypernym relations in a taxonomy that
are outside
of the Gold Standard taxonomy.
Thus,
we cannot
determine
the true effectiveness of our system.
In comparison with other systems of the TexEval workshop
2016,
we take the average of all domains for each measuring
parameter and presented in table II.
The systems participated
in the task are:
JUNLP,
TAXI,
NUIG-UNLP,
USAAR and
QASITT.
NUIG-UNLP system [25] is similar to our system
that
is based on word-embeddings.
NUIG-UNLP system is
based on vector-offset
of
generated word-embedding along
with sub-string inclusion method.
The workshop also imple-
mented sub-string inclusion method named as a baseline.
But
it only handled compound terms for hypernym identiﬁcation.
Others system includes JUNLP [26] which based on BabelNet
external
lexical
resource
and sub-string inclusion method
for hyponym-hypernym relations identiﬁcations. USAAR [27]
system is similar
to sub-string inclusion method named as
Hyponym Endocentricity.
TAXI system completely relied on
Hearst-patterns and a sub-string inclusion.
They used number
of
corpora both general
and domain speciﬁc such as Giga-
Word,
etc.
The QASSIT [28] system is pre-topological space
graph optimization techniques based on genetic algorithm and
lexical
patterns to identify hyponym-hypernym relations and
to construct taxonomy.
Table II
shows that
our
system outperformed the NUIG-
UNLP system that is similar to our technique. That’s why it’s
results are crucial for our system evaluation.
It also observed
that pattern-based systems outperformed other system, but did
not give high qualitative results.
V.
C
ONCLUSION AND
F
UTURE WORK
In this
paper,
we present
a semi-supervised and domain
independent
framework that
incorporates
word embedding
from a neural
net
for
the speciﬁc phrases and a sub-string
inclusion system for
hyponym-hypernym identiﬁcation and
taxonomy induction.
The proposed system is trained over
a
large general
corpus (Wikipedia dump)
in order
to identify
hyponym-hypernym relations between given domain speciﬁc
words. We evaluate our system by generating domain speciﬁc
taxonomies
against
gold standard taxonomies.
Our
system
signiﬁcantly performs against
current
system based on dis-
tributional semantics.
In future work,
we plan to incorporate results
of
Wiki-
Miner for hyponym-hypernym identiﬁcation by differentiating
between word similarity and word relativity. Moreover, we will
374
TABLE I
P
RECISION
, R
ECALL
, F-
SCORE OF THE
P
ROPOSED
S
YSTEM
A
GAINST THE
G
OLD
S
TANDARD
T
AXONOMY
Domain
Measure
Environment (Eurovoc)
Food (WordNet)
Food
Science (Eurovoc)
Science (WordNet)
Science
Average
Precision
0.2267
0.2112
0.2007
0.2569
0.2996
0.282
0.2462
Recall
0.2146
0.2857
0.242
0.2984
0.3016
0.3226
0.2775
F-score
0.2205
0.2492
0.2194
0.2761
0.3006
0.3009
0.2601
TABLE II
C
OMPARATIVE
A
UTOMATIC
E
VALUATION OF THE
P
ROPOSED
S
YSTEM WITH
O
THER
S
YSTEMS
Measure
Baseline
USAAR
TAXI
JUNLP
QASSIT
NUIG
Proposed
Avg.
Precision
0.5676
0.5677
0.3318
0.1474
0.1858
0.2200
0.2462
Avg.
Recall
0.2369
0.1916
0.3174
0.3009
0.263
0.1100
0.2775
Avg.
F-score
0.3330
0.259
0.3212
0.1961
0.2176
0.2028
0.2601
also use large domain speciﬁc corpora to identify hyponym-
hypernym relations more accurately.
A
CKNOWLEDGMENT
Bushra Zafar would like to thank NUST university for the
expert technical assistance.
Furthermore, it has to be mentioned that the implementation
of the software was greatly simpliﬁed by NLTK, Gensim, and
Apache Lucene.
R
EFERENCES
[1]
G. A. Miller, “Wordnet: a lexical database for english,” Communications
of the ACM,
vol.
38,
no.
11,
pp.
39–41,
1995.
[2]
F.
M.
Suchanek,
G.
Kasneci,
and G.
Weikum,
“Yago: A large ontology
from wikipedia and wordnet,” Web Semantics:
Science,
Services and
Agents on the World Wide Web,
vol.
6,
no.
3,
pp.
203–217,
2008.
[3]
M.
Geffet
and I.
Dagan,
“The distributional
inclusion hypotheses and
lexical entailment,” in Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics.
Association for Computational
Linguistics,
2005,
pp.
107–114.
[4]
W.
Wong,
W.
Liu,
and M.
Bennamoun,
“Ontology learning from text:
A look back and into the future,” ACM Computing Surveys (CSUR),
vol.
44,
no.
4,
p.
20,
2012.
[5]
M.
A.
Hearst,
“Automatic acquisition of
hyponyms
from large text
corpora,”
in Proceedings
of
the
14th conference
on Computational
linguistics-Volume 2.
Association for Computational Linguistics, 1992,
pp.
539–545.
[6]
Z.
Kozareva,
E.
Riloff,
and E.
H.
Hovy,
“Semantic class learning from
the web with hyponym pattern linkage graphs.” in ACL,
vol.
8,
2008,
pp.
1048–1056.
[7]
D.
Lin,
“Automatic
retrieval
and
clustering
of
similar
words,”
in
Proceedings
of
the 17th international
conference on Computational
linguistics-Volume 2.
Association for Computational Linguistics, 1998,
pp.
768–774.
[8]
P.
Pantel
and
D.
Ravichandran,
“Automatically
labeling
semantic
classes.” in HLT-NAACL,
vol.
3,
2004,
pp.
3–2.
[9]
L.
Van Der
Plas,
G.
Bouma,
and J.
Mur,
“Automatic acquisition of
lexico-semantic
knowledge
for
qa,”
in Proceedings
of
the
IJCNLP
workshop on Ontologies and Lexical Resources,
2005,
pp.
76–84.
[10]
A.
Lenci
and G.
Benotto,
“Identifying hypernyms
in distributional
semantic
spaces,”
in Proceedings
of
the
First
Joint
Conference
on
Lexical
and Computational
Semantics-Volume 1:
Proceedings
of
the
main conference and the shared task,
and Volume 2:
Proceedings of
the Sixth International Workshop on Semantic Evaluation.
Association
for Computational Linguistics,
2012,
pp.
75–79.
[11]
O.
Etzioni,
M.
Cafarella,
D.
Downey,
A.-M.
Popescu,
T.
Shaked,
S.
Soderland,
D.
S.
Weld,
and A.
Yates,
“Unsupervised named-entity
extraction from the web: An experimental study,” Artiﬁcial intelligence,
vol.
165,
no.
1,
pp.
91–134,
2005.
[12]
W.
Y.
Wong,
Learning lightweight ontologies from text across different
domains
using the web as
background knowledge.
University of
Western Australia,
2009.
[13]
T.
Mikolov and J.
Dean,
“Distributed representations
of
words
and
phrases
and their
compositionality,” Advances
in neural
information
processing systems,
2013.
[14]
J.
Pennington,
R.
Socher,
and C.
D.
Manning,
“Glove:
Global
vectors
for word representation.” in EMNLP,
vol.
14,
2014,
pp.
1532–43.
[15]
O.
Levy,
Y.
Goldberg,
and I.
Ramat-Gan,
“Linguistic regularities in
sparse and explicit
word representations.” in CoNLL,
2014,
pp.
171–
180.
[16]
N.
Shazeer,
R.
Doherty,
C.
Evans,
and C.
Waterson,
“Swivel:
Im-
proving
embeddings
by
noticing
what’s
missing,”
arXiv
preprint
arXiv:1602.02215,
2016.
[17]
J.
Mitchell
and M.
Lapata,
“Composition in distributional
models of
semantics,” Cognitive science,
vol.
34,
no.
8,
pp.
1388–1429,
2010.
[18]
E.
Lefever,
“Lt3:
a multi-modular
approach to automatic taxonomy
construction,” in 9th International
Workshop on Semantic Evaluation
(SemEval 2015).
Association for Computational Linguistics, 2015, pp.
944–948.
[19]
Y.
Goldberg and O.
Levy,
“word2vec
explained:
deriving mikolov
et
al.’s
negative-sampling word-embedding method,”
arXiv
preprint
arXiv:1402.3722,
2014.
[20]
T.
Mikolov,
W.-t.
Yih,
and G.
Zweig,
“Linguistic regularities in contin-
uous space word representations.” in HLT-NAACL
,
vol.
13,
2013,
pp.
746–751.
[21]
R. Fu, J. Guo, B. Qin, W. Che, H. Wang, and T. Liu, “Learning semantic
hierarchies via word embeddings.” in ACL (1),
2014,
pp.
1199–1209.
[22]
L. Tan, N. Ordan et al., “Usaar-chronos: Crawling the web for temporal
annotations,”
in Proceedings
of
the
9th International
Workshop on
Semantic Evaluation (SemEval 2015),
2015,
pp.
846–850.
[23]
R. Rehurek and P. Sojka, “Software framework for topic modelling with
large corpora,” in In Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks.
Citeseer,
2010.
[24]
T.
Mikolov,
K.
Chen,
G.
Corrado,
and J.
Dean,
“Efﬁcient estimation of
word representations in vector space,” arXiv preprint arXiv:1301.3781,
2013.
[25]
J.
Pocostales,
“Nuig-unlp at
semeval-2016 task 13:
A simple word
embedding-based approach for
taxonomy extraction,” Proceedings of
SemEval,
pp.
1298–1302,
2016.
[26]
P. Maitra, D. Das, and I. Kolkata, “Junlp at semeval-2016 task 13: A lan-
guage independent
approach for hypernym identiﬁcation,” Proceedings
of SemEval,
pp.
1310–1314,
2016.
[27]
L.
Tan,
F.
Bond,
and J.
van Genabith,
“Usaar
at
semeval-2016 task
13: Hyponym endocentricity,” Proceedings of SemEval, pp. 1303–1309,
2016.
[28]
G.
Cleuziou and J.
G.
Moreno,
“Qassit
at
semeval-2016 task 13:
On
the integration of semantic vectors in pretopological
spaces for lexical
taxonomy acquisition,” Proceedings of SemEval,
pp.
1315–1319,
2016.
375
