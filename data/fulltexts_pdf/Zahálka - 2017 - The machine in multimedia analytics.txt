The Machine in Multimedia Analytics
Jan Zahálka
Cover design: Morris Franken
Copyright © 2017 by Jan Zahálka.
All rights reserved. No part of this publication may be reproduced or transmitted in
any form or by any means, electronic or mechanical, including photocopy, recording,
or any information storage and retrieval system, without permission from the author.
ISBN 978-80-7568-037-2
The Machine in Multimedia Analytics
ACADEMISCH PROEFSCHRIFT
ter verkrijging van de graad van doctor
aan de Universiteit van Amsterdam
op gezag van de Rector Magnificus
prof. dr. ir. K. I. J. Maex
ten overstaan van een door het college voor promoties
ingestelde commissie,
in het openbaar te verdedigen in de Agnietenkapel
op donderdag 6 juli 2017, te 10:00 uur
door
Jan Zahálka
geboren te Praag, Tsjechoslowakije
Promotiecommissie
Promotor:
Prof. dr. M. Worring
University of Amsterdam
Co-promotor:
Dr. S. Rudinac
University of Amsterdam
Overige leden:
Prof. dr. D. A. Keim
University of Konstanz
Prof. dr. J. J. van Wijk
Eindhoven University of Technology
Prof. dr. M. de Rijke
University of Amsterdam
Prof. dr. Z. J. M. H. Geradts
University of Amsterdam
Dr. C. G. M. Snoek
University of Amsterdam
Faculteit der Natuurwetenschappen, Wiskunde en Informatica
This work was conducted in the Intelligent Sensory
Information Systems group of the Informatics Institute,
University of Amsterdam.
This work is part of the research programme Sort-It-
Out: Visual Analytics for Multimedia Collections with
project number 12540, which is (partly) financed by
the Netherlands Organisation for Scientific Research
(NWO),
NWO Domain Applied and Engineering
Sciences (TTW, previously Technology Foundation
STW).
Advanced School for Computing and Imaging
This work was carried out in the ASCI graduate school.
ASCI dissertation series number 372.
The printing of this thesis was supported by the Co
van Ledden Hulsebosch Center, Amsterdam Center for
Forensic Science and Medicine.
CONT E NT S
1
INTRODUCTION
7
2
A MULTIMEDIA ANALYTICS
MODEL
11
2.1
Introduction
12
2.2
Human perception of multimedia
13
2.2.1
Representation and learning
14
2.2.2
Multimedia visualization
15
2.3
Machine perception of multimedia
19
2.3.1
Representation and learning
19
2.3.2
Interaction
23
2.4
Multimedia analytics
25
2.4.1
Model
25
2.4.2
Pioneer systems
31
2.4.3
Research agenda
33
2.5
Conclusion
34
3
INSTANTIATING THE MODEL
35
3.1
Introduction
36
3.2
Related Work
38
3.2.1
Unimodal approaches
39
3.2.2
Multimodal approaches
40
3.3
Data Collection
41
3.4
Topical Analysis of Users and Venues
42
3.4.1
Feature Extraction
43
3.4.2
Semantic User and Venue Topics
44
3.5
Interactive Venue Recommendation
44
3.6
Experimental Setup
47
3.6.1
Data
47
3.6.2
Evaluated Algorithms
48
3.6.3
Experimental Procedure
49
3.7
Experimental Results
50
3.7.1
General Performance Evaluation
50
3.7.2
Analysis of User and Venue Topic Configurations
51
3.7.3
Modality Analysis
52
3.7.4
Nature of Generated Recommendations
53
3.8
Conclusion
54
4
EVALUATING ANALYTIC QUALITY
55
4.1
Introduction
56
4.2
Related work
58
4.3
Method
60
4.3.1
Evaluating analytic tasks
60
4.3.2
Artificial actors
62
4.3.3
Evaluation pipeline
65
5
6
Contents
4.3.4
Evaluation measures
67
4.3.5
Interpretation
69
4.4
Showcase
71
4.5
Conclusion
75
5
SCALING UP MULTIMEDIA ANALYTICS
77
5.1
Introduction
78
5.2
Related Work
81
5.3
Data Representation
82
5.3.1
Feature Selection
83
5.3.2
Compression and Decompression
84
5.3.3
The Ratio-64 Representation
87
5.4
Interactive Learning
88
5.5
Experimental Setup
90
5.6
Results and Discussion
95
5.7
Conclusion
99
6
CONCLUSION
101
6.1
Summary
101
6.2
Reflections and Future Work
103
Bibliography
105
1
I NT RODUC T ION
“Life without knowledge is death in disguise.” Indeed, building up and maintaining
knowledge is one of the main human endeavours. From early childhood to old age
we gain and revise our knowledge both at macro scale over the years and at micro
scale on a continuous basis.
Knowledge plays a role in virtually all aspects of life,
from survival (“where do I find water in this land?”) through daily routine (“what
is the best way to get to work?”)
to fundamental understanding of the universe.
Knowledge starts with observing raw, unstructured, heterogeneous data constantly
flowing from the world around us.
Relevant subsets of data are stored in digital
datasets for further use.
On their own,
datasets do little more than occupy disk
space.
Humans can use stored data in order to gain information by selecting those
pieces of data that carry meaning. Information is a central resource that constitutes
the building blocks of knowledge.
Knowledge comes about when information is
further processed and/or organized by a cognitive observer. This means involving
a human,
who either processes the data directly,
or uses sophisticated automatic
techniques reflecting the state of the art in the respective domain. In contrast, data
and information can exist on their own without someone observing them. The very
definitions of data, information, and knowledge hint that in the modern age machines
assist us humans in our endeavours related to knowledge gain. The interplay between
humans and machines in the knowledge gain process is one of the core concepts
investigated in this thesis.
The modern era has revolutionized the way people gain and organize knowl-
edge.
The change does not lie so much in the knowledge gain process itself, but
rather in the drastic growth of the amount of information flowing our way. In the
prehistoric era, an individual’s knowledge archive comprised a few pictures, usually
crude anatomical studies, painted on the wall of a cave. Over time, this has evolved
into libraries with a large number of written works. The Internet era has digitized
these works and more,
making them accessible on demand at all
times.
Finally,
the smartphone has ushered an era of user-contributed multimedia content:
vast
amounts of non-curated, unstructured content comprising heterogeneous, multi-
modal
information channels with images and videos.
That we move away from
written, curated works towards increasingly archiving our knowledge in sensory
7
8
INTRODUCTION
information contributed by the members of the community might give an impression
that we are in a sense back to the drawings on the cave walls. Except in our era, we
are not limited to the crude anatomical studies in our tribe’s cave.
We can access
overwhelming amounts of caves in seconds, at any time, in search for the information.
To gain knowledge in our era is to process high-bandwidth, multimodal information
contained in very large-scale collections ever increasing in size.
The word insight is often used interchangeably with knowledge.
From the
computer science point of view, we distinguish the two as follows. Knowledge is an
objective understanding that can be formalized in structures such as the Semantic Web
or ontologies. Insight, on the other hand, is individual; it is the user’s understanding
of the analyzed domain based on her expert knowledge and the assessment of the
information.
Insight gain involves putting the information into the context of
previous experience and/or expertise, and as such is an incremental process.
In this
thesis, we focus on insight, i.e., the individual user’s understanding.
Given the individual nature of insight, the user must process the data herself,
which involves interacting with them. Thus, when designing any computer science
approach striving to support insight gain, we must bear the interaction with the user
in mind. Gaining insight is an intricate process requiring an interplay of a number
of factors. While the term insight is intuitively understandable to many people, it is
difficult to define precisely. However, there are a number of attributes defined by
North that need to be taken into account [
123
]. First, insight requires the utilization
of large amounts of data, not only a handful of items. Second, the process takes time,
one does not gain insight by glancing over the data. Third, it is subjective and not
exact. Fourth, it requires connecting the relevant data to the user’s already established
knowledge, i.e., context.
Finally, serendipity plays a big role, as pieces of insight
can manifest in unexpected places. Such as the opening quote of the thesis: it does
not come from a great philosopher, but from the lyrics of a dance song by the band
Scooter. For the machine to succeed in assisting the user in insight gain, the often
contrasting aspects of insight must be taken into account.
One of the computer science fields tackling this problem is multimedia analytics,
defined as the combination of the interactive visualizations and interfaces of visual
analytics, and automatic methods for analyzing multimedia content from disciplines
such as machine learning, computer vision, or information retrieval [
27
]. Multimedia
analytics strives to unlock the potential
of large-scale multimedia collections as
knowledge sources in scientific and commercial applications: the user gains further
insight in her respective domain, and her formalized insight contributes to knowledge
increase in that domain. Multimedia analytics is a relatively new field: the term itself
was coined in 2010 [
27
]. As such, it provides ample opportunities for opening new
research frontiers.
INTRODUCTION
9
As the title suggests, in this thesis we focus on the machine side of multimedia
analytics. To that end, this thesis revolves around one central research question:
How can we enable the machine to assist the user in insight gain, thus
advancing the knowledge gain objective of multimedia analytics?
We present four works on the topic, each answering a sub-question contribut-
ing to the overall answer to our main research question.
Multimedia analytics by its definition requires a delicate interplay of techniques
from its constituent fields.
There have been successful niche pioneer multimedia
analytics systems since the 1990s. However, these works did not result in a general
theoretical multimedia analytics model. This leads to the first sub-question:
How to connect the theories of the constituent disciplines of multimedia analytics into a
single model?
In Chapter 2, we survey research in the fields related to multimedia analytics
and synthesize a general multimedia analytics model based on the survey, answering
the first sub-question.
The model paves the way for what multimedia analytics is expected to deliver:
useful, practical systems for a wide array of knowledge domains.
To validate the
model, at least one successful instantiation is necessary. This necessity inspires the
second sub-question:
How can we instantiate the multimedia analytics model in an applied domain such that the
resulting tool is competitive with the respective state of the art?
To study this question, we have chosen venue recommendation, i.e., suggesting
places of interest to travellers, as the testing ground for our model. There are two
reasons for selecting this domain. First, venue recommendation is highly personal,
with clear distinction between knowledge and insight: objectively popular venues
are not always a good recommendation for a user, as everyone has their own distinct
taste.
Second, the user base is broad and data are readily available on social media
platforms. We have succeeded in demonstrating the usability of the model by winning
the ACM Multimedia Grand Challenge 2014 with New Yorker Melange, a venue
recommendation system harnessing social media to discover users with similar taste
to the interacting user. City Melange, a general interactive venue recommendation
method formalizing the New Yorker Melange,
is presented in Chapter 3.
The
experimental results demonstrate that City Melange is competitive with the state of
the art.
10
INTRODUCTION
While we were able to instantiate the model
successfully,
it is difficult to
measure scientific progress in any field,
including multimedia analytics,
without
proper evaluation.
Evaluating multimedia analytics is an incredibly difficult task
due to its interdisciplinary and interactive nature. For complex evaluation, we must
evaluate both the visualization and the underlying machine algorithms. In addition,
given that insight is a complex phenomenon with many different aspects, the user and
user behaviour must be taken into account. Insight-based evaluation is researched in
a number of works [
123
,
124
,
171
], chiefly from the interface and visualization side.
The machine side is addressed by the third sub-question:
How to evaluate automatic multimedia analysis algorithms in a multimedia analytics
context?
The state of the art has two dominant approaches: user studies, which chiefly
look at the evaluated systems through the prism of the interface, and benchmarks,
which are fully automatic. The Analytic Quality evaluation framework proposed in
Chapter 4 provides a middle ground, allowing the designers of multimedia analytics
approaches to conduct fully automatic evaluation sessions emulating user behaviour,
allowing them to gauge how well
their proposed approach fares in an analytics
context.
In order for multimedia analytics to be a true enabler of knowledge gain in
our increasingly digital times, we must address the problem of the ever-increasing
scale of the datasets we face. As the scale grows, it is increasingly difficult to process,
annotate,
and categorize data by humans.
In turn,
this sets back the currently
dominant supervised approaches to multimedia, which require labelled data to train
models analyzing multimedia data.
The scale of collections thus directly impacts
the methodology for large-scale multimedia analysis and analytics.
The fourth
sub-question is thus related to scale:
How to scale automatic multimedia analysis algorithms to large-scale collections such that
they produce relevant suggestions in interactive time?
To that end, Chapter 5 presents Blackthorn, a novel large-scale interactive
multimodal learning framework that enables interactive analysis of collections with
up to 100 million items. This interactive learning approach operates on unsupervised
collections with few user-annotated examples.
By not requiring collection-wide
annotations and still providing relevant results in interactive time, Blackthorn offers
a possibility of deploying multimedia analytics on large-scale data, alleviating the
scale-related limitations to analytics.
The combination of the answers to the research questions yields a complete
framework enabling the machine to interactively assist the user in multimedia ana-
lytics on even very large-scale collections.
2
A M U LT I M E DI A A N A LY T IC S M ODE L
Published as Zahálka and Worring:
Towards Interactive, Intelligent, and Integrated Multimedia
Analytics
.
In proceedings of the IEEE Conference on Visual
Analytics Science and Technology
(VAST), pages 3–12, 2014.
11
12
A MULTIMEDIA ANALYTICS
MODEL
2.1
INTRODUCTION
Recent years marked a rapid growth of importance and volume of multimedia data.
An increasing number of scientific fields, such as physics, biology, and astronomy,
utilize scientific imagery to advance the state of the art [
83
]. Modern medical science
also relies increasingly on multimedia datasets,
which greatly assist physicians in
diagnosis.
In the non-scientific world,
the advent of smartphones made devices
with good multimedia recording capabilities ubiquitous, resulting in the general
public contributing large amounts of social media shared on immensely popular sites
like Flickr or Facebook.
Such social media bring resources for social sciences and
media companies.
The abundance of public multimedia data also provides police,
intelligence services, and forensics with a major information source for investigation
of felonies like child abuse or terrorism. As shown by these examples, multimedia
datasets provide a wealth of resources and tremendous potential for knowledge gain
in a wide spectrum of application areas. Being able to gain insight into multimedia
datasets is thus of paramount importance. However, with current multimedia col-
lections easily comprising millions of images and months of video, it is no longer
feasible to have multimedia datasets analyzed by humans only. Sophisticated systems
to assist the human analyst in assessing multimedia datasets are needed, and despite
the increasing importance of multimedia in our society, few such systems exist.
Multimedia analytics
,
an emerging field combining visual
analytics and
multimedia analysis, focuses on creating systems for large-scale multimedia analysis
[
27
].
Visual analytics, the science of analytical reasoning facilitated by interactive
visual interfaces [
165
], has been successfully applied in diverse fields since its inception
in 2005.
Multimedia analysis, the other component of multimedia analytics, is an
umbrella term for many different automatic multimedia analysis techniques. In this
chapter, we focus on visual multimedia collections (images/videos) with associated
data sources like text annotations and metadata, making the fields of computer vision,
image retrieval, and video retrieval our focus prism [
154
][
157
]. Multimedia analytics
aims to guide the analyst to deep insight. They aim to combine the analyst’s natural
expertise in analyzing multimedia information (humans master this skill
shortly
after birth) with the memory and processing power of the machines,
which are
able to process contemporary large-scale collections.
True to the visual analytics
spirit embodied by the visual
analytics process diagram by Keim et al.
[
82
][
83
],
this integration needs to be
interactive
. Multimedia analytics also need to be able
to utilize the heterogeneous data sources within a collection, combining the visual
content with text annotations and string/numeric metadata. The aims of multimedia
analytics are ambitious, and the integration of relevant techniques to fulfill these aims
is far from trivial.
The difficulty of fulfilling the multimedia analystics ambitions stems from
multimedia collections being quite a specific data source.
While humans perceive
2.2 HUMAN PERCEPTION OF MULTIMEDIA
13
multimedia chiefly through the semantic properties of the visual content, machines
need a mathematical representation, which does not provide comparable semantic
richness and is unintuitive for a human. The heterogeneity of data sources within a
multimedia collection also provides a challenge, both for the visualization and the
model. Moreover, any one multimedia data instance has a much higher information
bandwidth and also size than an instance in a classic dataset. This results in a larger
computational load on the machine, and techniques related to multimedia analytics
need to be carefully examined before adaptation.
In this chapter, we provide a comprehensive overview of related state-of-the-
art techniques and theory:
in Section 2.2,
we focus on the human perception of
multimedia; Section 2.3 reviews machine processing of multimedia data. To this end,
we processed the relevant literature in the following four steps:
1.
Exhaustive search on articles since 2003 in leading journals and conference
proceedings Ñ thousands of references
2.
Filtering abstracts and titles based on topical relevance, i.e., papers concern-
ing multimedia analytics, visual analytics theory, multimedia visualization or
multimedia analysis Ñ „800 references
3.
Topical-relevance filtering based on the key sections of the content
Ñ „370
references available online [196]
4.
Final filtering based on topical relevance of the complete content and on the
impact of the article in the respective community (measured by Google Scholar
citations) Ñ „100 references in this chapter
In Section 2.4, we synthesize the relevant techniques into a general multimedia
analytics model, which is, to the best of our knowledge, still largely missing in the
literature. Section 2.5 concludes the chapter.
2.2
HUMAN PERCEPTION OF MULTIMEDIA
The human’s excellent capability to analyze multimedia heavily revolves around
being able to see the multimedia items in question.
Hence, this section is focused
on visualization techniques and analytic interfaces.
Issues related to the human
processing of multimedia data are outlined in Section 2.2.1. The text focuses on issues
related to data representation and the technical aspects of multimedia visualization.
Other important issues, whose thorough description is beyond the scope of this work,
also exist: examples include the cognitive aspects of image understanding [
12
] and
relevance judgment [
55
] by the brain, quality of the data [
52
], or performance issues.
Multimedia browsers, the interactive interfaces used to access multimedia collections,
are surveyed in Section 2.2.2.
14
A MULTIMEDIA ANALYTICS
MODEL
MULTIMEDIA COLLECTION
. . .
. . .
item
item
item
C
A
M
F
S
C
A
M
F
C
A
M
F
S
Statistics
human
(dis)similarity
human
(dis)similarity
Figure 2.1:
Representing multimedia: a multimedia collection comprises individual
items, which are composed of content (C), annotations (A), and metadata
(M). Thick arrows depict data transformations into derived values (white
text, black background): features (F), (dis)similarity (S) and statistics.
2.2.1
Representation and learning
When working with multimedia data, we are initially provided with a
multimedia
collection
,
structurally depicted in Figure 2.1.
Multimedia collections comprise
individual raw multimedia
items
, i.e., single images and/or video clips. Individual
items consist of three elements — content, annotations, and metadata.
The visual
information present in the item and conveyed by it is the item’s
content
.
Those
individual
pieces of visual
information carrying an objective semantic meaning,
for example “cat” or “dog,” are called
semantic concepts
.
Annotations
are text
descriptors such as labels, captions, or tags assigned to individual items by a human
which relate to the content in an objective or subjective manner.
Metadata
are
string or numeric descriptors related to the technical parameters of the item, such as
Exif information or GPS localization.
Humans are excellent multimedia analysts by nature, trained to process high-
bandwidth visual information through the visual cortex since their early days. Hu-
mans can extract semantic information directly from raw multimedia. Then, they
synthesize it into concepts and similarity measures with complexity ranging from
purely visual-based through visually-semantic (i.e., representing semantics grounded
in visual characteristics) to completely abstract [
185
]. This entire process is remark-
ably fast.
The reasoning about the data and its structure is heavily context- and
2.2 HUMAN PERCEPTION OF MULTIMEDIA
15
intent-based. To illustrate, consider a toy image collection containing a picture of a
British phone booth, a picture of Little Red Riding Hood, and a picture of Queen
Elizabeth II. The notion of similarity (and thus structure) can be based on the “amount
of red colour,” “person,” and “United Kingdom” characteristics, corresponding to
a visual-based, visually-semantic, and abstract similarity notion, respectively. Each
conveys different structure and none of them is inherently wrong.
This learning
and reasoning process shows that humans do not perceive multimedia content in a
mathematical manner. Mathematical summaries and statistics are thus less prominent
than in the case of classic visual analytics approaches. Indeed, Santini and Jain show
that modelling multimedia similarities mathematically is rather treacherous [
147
].
However,
they are still
useful
for annotations,
metadata,
and simple visual
statis-
tics like the amount of a certain colour. The main limitation of humans is limited
cognitive capacity [
54
], barring any attempt at brute-force analysis of a large-scale
collection containing million images or more. Analytic interfaces thus need to be
semantically navigable to support the human’s reasoning process.
Since a human needs to see the items in question to derive information from
them,
the most natural visualization paradigm is directly displaying the items or
their respective thumbnails on the screen. Using primitives and conventional charts
for visual content is in principle possible, but quite unorthodox: the visual content,
a powerful
information channel,
is lost,
diminishing the ability to discriminate
between content classes [
46
].
Direct visualization has large demands for screen
space.
The human needs to see the content,
so each thumbnail
needs to be big
enough and occlusion should be minimized.
As many items as possible should be
displayed on the screen. Analytic interfaces thus need to be
screen-space-efficient
,
minimizing unused screen space.
Annotations and metadata might also provide a
valuable information gain.
They should be visualized in an
integrated manner
within an analytic interface.
2.2.2
Multimedia visualization
In the current big data era, even the most extravagantly large screens cannot contain
entire large-scale collections, and even if they could, the displayed data exceeds the
human’s cognitive limit.
Interactivity
is thus crucial for any analytic interface. The
classic approach towards visualizing multimedia is the
multimedia browser
, a prime
example of a casual information visualization tool [
135
]. Multimedia browsers are
used daily by a large number of computer users to examine multimedia collections.
Examples include Internet image search interfaces like Google Images or Bing Images
and social sites like Flickr or Instagram. Based on the conducted survey, we grouped
the state of the art into five groups based on the visual paradigm used.
While this
grouping is necessarily non-exhaustive, it helps identifying the suitability of state-of-
the-art multimedia browsers for different tasks. The main multimedia visualization
16
A MULTIMEDIA ANALYTICS
MODEL
Visualization
Efficiency
Navigability
Heterogeneous
Basic grid
+
´
´
Similarity space
´
++
´
Similarity-based
+
+
´
Spreadsheet
˚
˚
++
Thread
´
+
+
Table 2.1:
Summary of analytic capabilities of multimedia visualizations: screen space
efficiency
,
semantic
navigability
,
and integration of
heterogeneous
information channels, i.e., content, annotations and metadata. The values
range from
´
(poor) through
+
(good) to
++
(excellent),
˚
denotes a
value dependent on the task at hand (explained in the text).
techniques discussed in this section are conceptually depicted in Figure 2.2, their
analytic aspects are summarized in Table 2.1.
The most prevailing multimedia collection visualization is a two-dimensional
grid
of thumbnails.
The grid is navigated by scrolling,
usually one-dimensional
and vertical. The user interacts with the thumbnails, usually magnifying the image
and revealing annotations and/or metadata in a side panel. A grid is quite efficient
in utilizing screen space:
except for a side panel (if present), all of it is devoted to
displaying individual
items with zero overlap.
A grid browser may also feature
sorting, filtering and/or hierarchical display. In a basic grid, these interactions rely
only on non-semantic attributes like file name or time.
This makes the semantic
exploration capability of a basic grid difficult. This problem is tackled by approaches
which order the items in the grid according to a similarity measure. These approaches
are discussed below as “similarity-based.” Overall, a grid is certainly a very familiar,
screen-space efficient, and usable interface. Its strong suit are tasks where viewing
the full content of the item is imperative: high screen-space efficiency ensures high
visibility of the thumbnails, and the user can typically magnify the item of interest
and inspect it.
One of the issues are the lack of integration of annotations and
metadata, which are displayed separately in the side panel. Also, in the case of the
basic, non-similarity based grid, semantic navigation of large collections is limited.
A basic grid is thus unsuitable for tasks where the overall structure of the collection
is of importance.
Another approach is the
similarity space
browser.
The items’
position on
the screen is determined by a projection from the high-dimensional feature space
to the 2-D screen space.
The projection should preserve the similarity and the
resulting structure present in the feature space as accurately as possible [
120
][
141
].
Examples of similarity space browsers are numerous, including the browser by Liu et
al. [
107
], the semantic image browser by Yang et al. [
191
], the news video databases
browser by Luo et al. [
110
], or the Flickr summarization browser by Fan et al. [
45
].
2.2 HUMAN PERCEPTION OF MULTIMEDIA
17
Similarity space browsers convey a notion of structure, making them excellently
navigable. Scalability and screen space efficiency are an issue, however: some parts
of the screen space are empty, and thus wasted; some parts may be cluttered with
overlapping thumbnails. Yang et al. tackled this problem using miniature thumbnails
[
191
]. These display the distribution of colour in the collection quite well, but lack
the expressiveness to convey other visual
content.
Fan et al.
use representative
images to represent a semantic similarity neighbourhood [
45
].
Annotations and
metadata are typically displayed in a side panel, making the integration rather poor.
However, in some systems, annotations and metadata are used as a basis for the entire
similarity structure, for example in Chronosphere by Worring et al. [
186
]. Overall,
similarity space browsers are useful as a semantics-conveying paradigm due to them
directly showing data structure. This makes them an excellent choice for applications
focused on uncovering the structure of the collection.
However, the screen space
and scalability issues make it rather difficult to inspect individual items of interest.
Similarity-based
approaches utilize a space-filling view (typically a grid or a
treemap) where the items are arranged based on a similarity measure. Rodden et al.
showed that arranging similar images together in the grid indeed helps to determine
the clusters in the collection, but also somewhat hampers serendipitous discovery,
due to interesting images standing out less [
141
]. Bederson’s PhotoMesa adapts the
treemap algorithm to establish similarity [
9
].
PhotoMesa’s interface is zoomable,
which along with the grouping provides overview of the collection and its structure.
Zavesky et al.
map image features of interest into a 2D abstraction layer based on
similarity and then snap the images to a grid, forming visual islands [
201
].
Visual
islands can be further used for guided navigation:
items of interest can be used as
probes to rearranging the visual islands. Quadrianto et al. introduce a multiple-tier
semantic hierarchy [
136
], with the grid in each successive tier showing representative
images for an increasingly detailed neighbourhood of interest.
The approach by
Brivio et al. utilizes Voronoi diagrams to fill the display, with the focus item placed in
the center and other items being represented on the screen based on their distance to
the focus with respect to an ordering of the collection [
19
]. Overall, similarity-based
approaches combine screen space efficiency with an enhanced notion of structure
through visually contiguous regions. This makes them easily navigable and suitable
for both structure overview and inspection of individual items.
Annotations and
metadata are typically still relegated to a side panel, lacking true integration.
In the
spreadsheet
paradigm, rows and columns represent different dimen-
sions of annotations and metadata, and cells display individual items. The first browser
based on the spreadsheet paradigm is PhotoSpread by Kandel et al. [
81
] MediaTable
by de Rooij et al. visualizes each item in one row, with its content, annotations, and
metadata forming individual columns [
36
]. Multimedia Pivot Tables by Worring
et al.
allow the analyst to define the rows and columns of the table herself [
187
].
Navigability and screen space efficiency are dependent on the role of annotations
18
A MULTIMEDIA ANALYTICS
MODEL
Basic grid
Similarity space
Similarity-based
Spreadsheet
Thread-based
Figure 2.2: Multimedia visualizations (conceptual depiction).
and metadata in the task at hand.
If those are missing or their role is marginal,
then a spreadsheet browser is a poor choice, since only few columns will be filled
and informative. When annotations and metadata are crucial, however, the spread-
sheet provides a very screen-space-efficient way to explore the collection.
Other
approaches centered on annotations and metadata are driven by specific types of
metadata, especially geographic location. Examples include the browser summarizing
photographs based on their location by Jaffe et al. [
69
] or building a 3D scene of the
location from the corresponding photos, as conceived by Szeliski et al.
[
163
] Out
of all the groups mentioned in this chapter, spreadsheets emphasize and integrate
annotations and metadata the best, making it a great choice for tasks with heavy
involvement of annotations and metadata. Moreover, the individual items are easily
inspectable. The spreadsheet paradigm, however, falls short when the focus of the
task is purely or mostly visual content.
Another approach is the
thread
browser, where the collection can be navigated
along threads,
i.e.,
sequences of items based on a certain criterion.
Originally,
this paradigm was used by de Rooij et al.
for navigating large video collections
[
33
], where threads were temporal, semantic or based on annotations and metadata
ordering. In a thread browser, the interface displays the focus item at the center of
the screen and a number of threads relevant to the focus. The user can then navigate
along a thread of choice,
shifting the focus to the item along the chosen thread.
The user is thus able to navigate the collection without requiring her to overview
large number of items at once or search manually.
Multiple browsers implement
this paradigm, the difference lies in the number of threads displayed at once:
the
CrossBrowser displays two, the ForkBrowser five [
34
], the RotorBrowser up to eight
[
33
].
The thread browser’s design is centered around semantic navigation, which
makes it excellent for tasks where inspecting individual items based on semantic
2.3 MACHINE PERCEPTION OF MULTIMEDIA
19
dimensions is imperative. Annotation- and metadata-based threads also integrate the
heterogeneous data channels seamlessly. Screen-space efficiency is the main issue of
the thread browser: only a limited number of threads can be displayed in order not
to overwhelm the user [
33
], therefore a part of the screen is empty. This makes the
thread browser lacking in cases where an overview of the structure of the collection
is needed.
There is a great variety of multimedia visualizations, with our text covering the
main approaches and being by no means exhaustive. From the analytic perspective,
there is no strongly dominant solution, each is strong in different areas and suitable
for different tasks. The quest for visualizations and metaphors suitable for multimedia
analytics thus remains open.
2.3
MACHINE PERCEPTION OF MULTIMEDIA
Unlike humans, machines have an excellent capability to process large-scale data,
thanks to their large memory and processing power.
This makes them excellent
assistants to the human analyst, who struggles when faced with large collections.
In this section, techniques enabling the machine to provide such assistance are re-
viewed: Section 2.3.1 focuses on machine representations of multimedia and machine
learning in multimedia analysis, chiefly focusing on feature extraction, supervised
and unsupervised learning [
56
], and ranking. Interactivity in machine learning is
surveyed in Section 2.3.2.
2.3.1
Representation and learning
The workflow of machine multimedia analysis algorithms is depicted in Figure 2.3.
Machines need to extract an explicit and mathematical intermediate representation
of the data in order to extract semantics.
Building the representation starts with
features
, i.e., numeric values derived from pixels in a single item mimicking the low-
level representation used by the human’s perceptual system. There are many kinds
of features, each being a different abstraction from pixel data.
Typically, multiple
features per item are computed, resulting in a
feature vector
representing that item.
Feature vectors are in turn aggregated into a
feature representation
of the entire
collection. Annotations can be treated either as string data and represented directly,
or converted to text features or conceptual representations if semantics extraction is
needed. Metadata are machine-readable per se and require no conversion. Extracting
semantics from feature representations is then performed by machine learning.
The semantics extraction process faces numerous challenges. The main one
is the
semantic gap
defined by Smeulders et al.,
i.e.,
the disproportion between
20
A MULTIMEDIA ANALYTICS
MODEL
the information extractable from a multimedia item’s content by a human and
the information extractable from the feature representation of the same item by a
machine [
154
].
Simply put, machines are essential to multimedia analytics due to
their capability to handle large multimedia collection much better than a human, but
their understanding of multimedia is worse than a human’s. In addition, not only is
the human representation difficult for machines, but also vice versa. The human’s
semantic, non-numeric perception of multimedia renders the vast majority of features
unintuitive.
Since most features carry no meaning to a human, the usefulness of
modelling the data using basic feature statistics is limited.
All in all, the semantic
gap remains a major research challenge prohibiting easy high-level extraction of
semantics from multimedia data.
In the current state of the art, there are essentially two pipelines for semantic
multimedia analysis:
explicit feature extraction followed by classification
, and
deep learning
. The former is the more classic one. The most used features are local,
each of them corresponding to a certain region within the image. Their advantage is
a number of invariances, for example positional invariance, i.e., a concept is detected
irregardless of its position in an item. The dominant one is the scale-invariant feature
transform (SIFT) by Lowe [
109
]. The existing variations of SIFT, each focused on a
different visual aspect, are surveyed by Van de Sande et al.
[
169
].
A faster option,
achieving similar properties to SIFT, are the speeded-up robust features (SURF) by
Bay et al. [
8
]. GIST by Oliva and Torralba is an example of a global feature, which
computes scene characteristics directly from the data [
126
]. Feature representations
involve grouping similar feature vectors together (typically using quantization and/or
histograms). The dominant representations in the state of the art are the histogram of
oriented gradients (HOG) by Dalal and Triggs [31], bag of visual words (BoW) by
Sivic and Zisserman [
153
], Fisher vectors by Perronnin et al. [
131
], and the vector of
locally aggregated descriptors (VLAD) by Jégou et al. [
74
]. Feature representations
can be further enhanced by incorporating spatial information, using spatial pyramids
by Lazebnik et al. [
92
], part-based models by Felzenszwalb et al. [
48
] or codemaps
by Li et al.
[
102
].
Hashing,
surveyed by Zhang and Rui [
202
],
is often used in
order to reduce dimensionality.
The second step of the pipeline, classification, is
in almost all cases carried out by a support vector machine (SVM) by Cortes and
Vapnik [
29
], although other classifiers such as nearest neighbour [
16
] or random
forests [
44
] have also been used. The second pipeline, deep learning, revolves around
using deep neural nets, which extract features and semantics using one model. In the
visual multimedia domain, convolutional neural networks (CNN) by LeCun and
Bengio are used [
93
]. While the original idea of a convolutional neural network can
be traced back to the 80s, efficient algorithms for training have not existed until fairly
recently: the first efficient algorithm for a deep net was conceived in 2006 by Hinton
et al. [
60
]. Since then, deep nets have successfuly been used both in a narrow domain
as shown by Tang et al.
in their face recognition experiments [
164
], and a broad
2.3 MACHINE PERCEPTION OF MULTIMEDIA
21
Knowledge
Visualization
Data
Multimedia
collection
Feature
vectors
Feature
representation
Statistics
Model
Supervised learning
Unsupervised learning
Ranking
add data
new features
different learning paradigm
different model type
Figure 2.3:
Workflow of machine-centered approaches related to the visual analytics
process diagram by Keim et al. [
82
][
83
]. Elements of the visual analytics
process not utilized in machine processing are greyed out.
domain such as the ImageNet classification performed by Krizhevsky et al.
[
86
]
The ImageNet paper is considered a breakthrough in computer vision, establishing
deep learning as the currently dominant approach. Both pipelines remain viable and
actively researched though, and overall, the quality of semantics extraction in the
state of the art has been steadily rising in recent years [156].
The typical case of supervised learning in semantics extraction involves classic
classification and using only one information channel of the data (typically visual).
In multimedia analysis, more and more effort is being made to make the task and the
data more flexible. Discovery of new classes and transfer of knowledge between them
is the domain of zero-shot learning, which is predominantly realized by attribute
learning,
i.e.,
representing each class by a set of attributes [
88
].
New classes are
characterized by attributes learned from previously available classes.
The three
heterogeneous data sources (content, annotations, metadata) are widely utilized for
learning in the video domain [
157
].
In the image domain,
surprisingly enough,
comparatively little attention has been given to this phenomenon,
even though
annotations and metadata have been shown to improve the quality of the machine
22
A MULTIMEDIA ANALYTICS
MODEL
learning model in several studies [
21
][
23
][
101
][
184
][
189
]. This situation is changing
due to the advent of social media, which provide vast annotated collections at the cost
of high noise.
Learning to annotate, i.e., assign annotations based on the content,
has received much attention in the recent years. Annotation as a task is essentially a
special case of classification, and the main approaches exploit this [
22
][
49
][
180
][
183
].
The classic annotation approaches rely on training on reliable expert annotations,
whose reliability is not a given in the social media era. The emerging field of social
image retrieval caters for the low quality and high noise of social tag annotations
by incorporating tag relevance learning [
99
].
To summarize, supervised learning
is very well-studied in the multimedia domain, with an array of well-performing
approaches with increasingly flexible and heterogeneous models.
Unsupervised learning in the multimedia domain, useful for structuring the
collection, is wholly dependent on the feature representation and similarity measure
used. Since neither the representation nor the similarity measure is canonical, clusters
of multimedia items convey a structure of the data, rather than the structure of the
data, as shown on the “Queen-Little Red Riding Hood-phone booth” example in
Section 2.2.1.
Technically,
unsupervised learning algorithms do not differ from
those used on conventional datasets, since once the collection is converted to features,
an unsupervised algorithm does not distinguish whether it is operating on feature
representation or on a conventional dataset [
32
]. Mainstream algorithms thus include
k
-means [
108
], the EM algorithm [
38
], and hierarchical clustering, reflecting the
application invariance of unsupervised learning.
Ranking is crucially important for semantic search, with search results being
a reranking of the collection based on relevance to the search query.
There are
two dominant approaches towards semantic search: text-based search and content-
based search.
Text-based search
relies on performing text search on annotations,
while
content-based search
performs the search based on the semantics extracted
from visual content [
202
]. Text-based search is the more mature field, but depends
heavily on accuracy and relevance of annotations, a handicap in the current social
tagging era. Content-based search, on the other hand, suffers from the semantic gap.
Semantic search in multimedia collections does not have a canonical query scheme.
Dominant query methods, surveyed by Snoek and Worring [157], comprise query
by keyword (matching the query against annotations/metadata), query by example
(the user provides an example item and wants similar results), and query by concept
(results are based on relevance to the specified concept). A comprehensive survey of
ranking methods in semantic search has been provided by Mei et al. [
113
]. Similarly
to supervised learning, ranking has also received much research attention, with the
respective state of the art providing respectable performance.
2.3 MACHINE PERCEPTION OF MULTIMEDIA
23
2.3.2
Interaction
Interactive machine learning techniques operate in a semi-supervised setting where
labels exist only for a small fraction of the otherwise unlabeled dataset [
25
].
This
is precisely the situation we face in multimedia analytics.
The flow of interactive
machine learning is depicted in Figure 2.4. The quality of the results of an interactive
machine learning technique is determined by three factors:
relevance,
speed of
convergence, and speed of response.
Relevance of items in each round and speed
of convergence are competing principles.
The trade-off between them is known
in the literature as the
precision-recall trade-off
[
47
].
Increasing
precision
, i.e.,
the proportion of relevant items in each round, harms
recall
, the proportion of the
relevant items in the whole dataset returned during all interaction rounds, and vice
versa.
Each system thus needs to select the position on the precision-recall curve
carefully. Search favours precision, since the highest number of relevant items in the
early round(s) is imperative; while thorough exploration leans towards recall, since
we need to see as many relevant items as possible in a small number of rounds. The
speed of response is a hard constraint:
interactivity imposes the time between the
user input and the system response not exceeding the order of seconds. Relevance
of results,
speed of convergence,
and response are all key to interactive machine
learning employed in multimedia analytics.
The precision-recall trade-off is most reflected in the query strategy of interac-
tive machine learners, i.e., the selection of items whose relevance is to be judged by
the user. The original, and to date still used interactive machine learning paradigm
is
relevance feedback
, surveyed by Zhou and Huang [
208
].
Relevance feedback
approaches maximize precision, showing the items the learner is most certain about
being relevant.
Presenting those items means a positive effect on the user, but lit-
tle gain for the learner, since it is already certain about them.
Another approach,
currently the dominant one, is
active learning
, which queries items the learner is
least certain about, i.e., those with the biggest information gain.
Query strategies
in active learning are extensively surveyed by Settles [
150
]. Maximizing the infor-
mation gain for the learner results in better recall and results overall, but requires a
patient, cooperative user. The queried items that are difficult for the learner might
also be difficult for the user. This phenomenon is called variable labelling cost, with
several works devoted to predicting the cost [
175
] and conducting active learning
on a labelling budget [
176
]. For multimedia analytics, both relevance feedback and
active learning have their merit: we need to convince the analyst that the machine is
learning by showing more relevant items each round, but we also need to maximize
the information gain per interaction to retrieve as many relevant items in total as
possible.
The technical execution of interactive machine learning boils down to two
key aspects: the interactions and the algorithms. The most dominant interaction is
24
A MULTIMEDIA ANALYTICS
MODEL
Results
Model
Data
Knowledge
features
mark
relevant
update
results
initial
results
final
results
Figure 2.4:
Interactive machine learning workflow,
incorporated into the visual
analytics process diagram by Keim et al. [
82
][
83
]. Elements of the visual
analytics process not utilized in interactive machine learning are greyed
out.
marking relevant items, possibly with degrees of certainty or explicit marking of
irrelevant items [
208
]. A newer trend is relative feedback, where the analyst states
what attributes the queried item lacks in comparison to the relevant one, enabling
pruning items which are more lacking in the particular attribute than the queried
item. Examples of approaches successfully using this paradigm include WhittleSearch
by Kovashka and Grauman [
85
] and the weighted online attribute learner by Biswas
and Parikh [
14
]. State-of-the-art active learning approaches are thus shifting from
simpler relevance indications to more complex, informative ones. Algorithm-wise,
the three dominant approaches in active learning, as surveyed and explained in detail
by Huang et al. [
65
], are active SVM by Tong and Chang [
168
], biased discriminant
analysis (BDA) by Zhou and Huang [
207
], and rank-based approaches surveyed by
Mei et al. in their broader ranking survey [
113
]. Approaches with a more flexible
model also exist.
Weak class labels have been considered by Mitra et al.
in their
probabilistic active SVM [
117
], as well as in the conditional active learning setting
by Li and Sethi [
97
].
Evolving classes and new class discovery are considered for
example by the binary feedback framework by Joshi et al.
[
78
], and are surveyed
by Settles [
150
].
Overall,
interactive machine learning provides a wide array of
2.4 MULTIMEDIA ANALYTICS
25
techniques with an increasingly elaborate interaction structure, showing promise
with respect to incorporation into multimedia analytics.
2.4
MULTIMEDIA ANALYTICS
Now that the individual human-centered and machine-centered components and
techniques are reviewed, the multimedia analytics model can be proposed. This is
done in Section 2.4.1. Section 2.4.2 reviews pioneer multimedia analytics systems
in view of the desired capabilities.
Section 2.4.3 presents a research agenda for
multimedia analytics.
2.4.1
Model
The previous sections structurally revised the preliminaries of a multimedia analyt-
ics system,
with emphasis on interactivity,
joint utilization of the heterogeneous
data sources in the collection (content,
annotations,
and metadata) and semantic
navigability.
The strong need for semantic navigability and the related semantic
gap phenomenon are especially important considerations and the main difference
between multimedia analytics and the closely-related classic visual analytics.
The proposed multimedia analytics model, which we explain in this section, is
depicted in Figure 2.5 and divided into four tiers. The first and lowest tier,
methods
,
represents atomic techniques of visualization,
interaction and data analysis in a
multimedia analytics setting. The second tier,
intents
, represents combinations of
methods which express individual intentions of the analyst. The third tier,
procedure
,
corresponds to a high-level model of activities and intents taking the analyst from
the beginning to the completion of her objective.
Objectives
, the fourth and top
tier, are the master goals of the analyst.
To illustrate, consider a medical scientist
using a multimedia analytics system on a collection of medical scans. Her objective is
to find the incidence of cancer in the population. The procedure to take the analyst
to her objective involves exploring the scans, searching for symptoms of cancer and
determining the distribution of cancer within the population.
Each of the steps
taken in the procedure exhibits a certain intent, e.g., “sort the patients based on liver
abnormality.” This intent is accomplished for example by the following methods:
computing the ranking, visualizing thumbnails of the scans in a grid ordered by
the ranking,
the analyst panning over the grid,
and magnifying the thumbnails.
The four-tiered model thus provides a structured overview of all cornerstones of
multimedia analytics.
Let us first examine the objectives. The main, high-level multimedia analytics
objective is to guide the analyst through large and complex multimedia collections to
26
A MULTIMEDIA ANALYTICS
MODEL
OBJECTIVES
. . .
Gain insight
[82][165]
„
Make sense
[133]
„
Complete
high-level task
. . .
PROCEDURE
PROCESS (Keim et al. [82][83])
Data
Visualization
Model
Knowledge
TASK MODEL
Exploration
Search
Categorization
INTENTS
VISUALIZATION (Pike et al. [132])
Depict
Differentiate
Identify
Show outliers
Compare
INTERACTION (Yi et al. [193])
Select
Explore
Reconfigure
Encode
Filter
Connect
Abstract/Elaborate
METHODS
STRUCTURAL (Amar et al. [6])
Retrieve value
Cluster
Characterize distribution
Determine range
Find anomalies
Correlate
Compute derived value
Find extremum
VISUALIZATION (Pike et al. [132])
. . .
Charts
Grid
. . .
INTERACTION (Pike et al. [132])
. . .
Brush
Pan/Zoom
. . .
Figure 2.5:
The four-tiered multimedia analytics model.
The tiers progress from
low-level
methods (bottom) to high-level
objectives (top).
Bold
text
indicates components critical for multimedia analytics tasks, orange text
denotes components desired to operate on content semantics.
knowledge. This knowledge ranges from abstract to particular. Abstract knowledge
means that the analyst knows something about the data she did not before, understands
the data, or grasps their structure. Abstract knowledge gain models are well known
in visual analytics literature.
Pirolli and Card coined the term sensemaking [
133
].
Insight is maybe the most used term for abstract knowledge gain, appearing both
in information visualization [
123
][
194
] and visual analytics [
82
][
83
][
165
]. Insight
is a notoriously fickle term evading the boundaries of precise definition.
Rather,
North enumerates the characteristics of insight: insight is complex, deep, qualitative,
unexpected,
and relevant [
123
].
Particular knowledge gain means completing a
complex,
high-level task,
like the medical scientist determining the incidence of
cancer. Gaining particular knowledge involves gaining abstract knowledge: in any
domain, the analyst still needs to explore and understand the data. Hence, from the
2.4 MULTIMEDIA ANALYTICS
27
model point of view, we consider the different terms for knowledge gain largely
equivalent.
In the rest of the chapter, we will use the term
insight
to indicate the
main objective.
Several works develop the methods and intents as incorporated in the model.
Their grouping in the model depicted in Figure 2.5 is inspired by the work of Pike
et al.
[
132
].
For analytic purposes, it is highly desirable that they operate on the
semantics of the content.
Thus, they need to be realized by employing machine
learning techniques, and as such are affected by the semantic gap.
Structural meth-
ods
, focusing on uncovering the structure of the data, correspond to the interactions
by Amar et al.
[
6
].
Their usefulness in the multimedia domain is limited:
since
the human perception of multimedia is non-numeric, retrieve value, compute de-
rived value, find extremum, determine range, and characterize distribution are all
meaningless in their purely numeric sense.
Find anomalies, cluster, and correlate
are viable, but the analyst needs to see the items and perform them in a semantic
manner.
Visualization methods
(particular visualization techniques like charts or
grids) and
interaction methods
(atomic interactions like panning or zooming),
both adapted from the work of Pike et al. [
132
] are in their technical sense unaffected
by the semantic gap.
Interaction intents
, corresponding to the 7 interactions by
Yi et al.
[
193
],
represent individual steps towards insight.
Four are impacted by
the semantic gap:
explore, reconfigure, filter, and abstract/elaborate.
Indeed, the
analyst might want to navigate further based on a semantic dimension (exploration,
“show me other dogs of this colour”); sort based on a semantic concept (reconfigure,
“sort dresses from least formal to most formal”); filter semantically (“show me only
pictures of people hiking”); and visualize item subsets based on concept hierarchies, if
present (abstract/elaborate, “show me all animals
Ñ
all mammals
Ñ
all foxes”). The
usefulness of “encode” in the visual domain is limited due to the items being displayed
directly. Finally, the
visualization intents
, conceived by Pike et al. [
132
], represent
visualization capabilities leading to insight. In the multimedia domain, “differentiate,”
“show outliers,” and “compare” are expected to be semantic. All intents and methods
affected by the semantic gap are precisely those that are actually related to the analysis
of the collection as a data resource.
Indeed,
if we select only those methods and
intents unaffected by the semantic gap, we end up with the capabilities of a generic
multimedia browser as discussed in Section 2.2.2.
These are certainly useful
for
browsing and visualizing data,
but might not be very suitable for the analysis of
large-scale multimedia resources. This emphasizes the need to maintain a semantic
model of the data to facilitate semantic methods and intents.
Now that the high-level objectives and the lower-level methods and intents are
in place, the model of the procedure actually bringing the analyst to her objectives
using the methods and intents needs to be discussed. In visual analytics, the process of
attaining insight in visual analytics has been modelled by Keim et al. [
82
][
83
]. This
process, which takes the analyst from data processing through iteratively updated
28
A MULTIMEDIA ANALYTICS
MODEL
Exploration
Search
Browsing
Structuring
Summarization
Finding relevant
items
Finding “needles
in the haystack”
Ranking
Figure 2.6: Exploration-search axis with example tasks.
visualization and model
to knowledge,
is also highly relevant in the multimedia
analytics domain and it is highly desirable its flow gets fully adapted.
Its
iterative
flow captures the nature of building insight, which takes time, non-trivial interactions,
and reasoning.
The visual analytics process also advocates tight integration of the
visualization with the model, a notion equally important for multimedia analytics due
to the emphasis on semantics. Bearing in mind the nature of multimedia analytics
objectives, there are two basic approaches towards attaining them [111]:
•
Exploration
, applicable when the analyst is faced with a collection she does
not know much about beforehand, and wants to discover what is inside and/or
how the data are structured. An exploratory session typically takes time and
involves a very dynamic model of the data, continuously refined as the analyst
iteratively gains knowledge.
•
Search
, applicable when the analyst has a clear idea what she is looking for and
queries the system for items relevant to certain attributes. A search session is
then a sequence of query-response pairs, and the analyst expects fast response.
The data model is static, since the analyst knows exactly what she is looking
for, and communicated to the system through a query.
A typical
multimedia analytics task has elements of both.
For example,
a
forensics expert might not only want to judge if the multimedia content of a suspect’s
seized computer contains incriminating content (search), but also determine the full
extent of the suspect’s illegal activities (exploration). Based on this observation, we
model multimedia analytics tasks as lying on an
exploration-search axis
as depicted
in Figure 2.6. During the analytic session, the proportion of exploration and search
in a task at hand changes over time. Typically, the task is more exploration-oriented
in the beginning, as the analyst does not know much about the collection yet, and
progress more towards search when the analyst already has a good grasp of the
collection’s content and knows what to look for.
The exploration-search axis is
an umbrella model
for most multimedia analytics tasks.
The depiction of some
archetypal tasks in the multimedia domain is included in Figure 2.6. In order to help
the analyst achieve insight, multimedia analytics systems need to enable the user to
alternate between exploration and search.
Even though exploration and search have different, sometimes antagonistic
properties, a multimedia analytics model needs to incorporate exploration and search
2.4 MULTIMEDIA ANALYTICS
29
integrally. Otherwise, the analyst cannot alternate between the two and the system
breaks down into disjoint exploratory and search components. The analyst perceives
multimedia content through attributes which are chiefly semantic, optionally also
involving annotation and metadata and the related statistics.
Pure search is then
simply filtering or reranking based on those attributes.
When building a mental
model of the data, the analyst structures her interest into attribute aggregates, e.g.,
“hiking in the Alps” (combining the “person” and “outdoors” semantic concepts
with geo coordinates matching the Alps region). The attributes are categorical; the
aggregations of the attributes into meaningful concepts, if we disregard fringe cases
where the semantics of the content play no role, are thus
categories
. The analyst then
assigns, often implicitly, its labels to individual items. This applies, for example, to
the nowadays typical tasks where we select images based on some notion of relevance
or interest,
like picking out nice photos from the collection shot on vacation or
searching for pictures of a favourite celebrity. Even though we might not explicitly
assign labels, by picking relevant images we actually categorize them as relevant,
while the non-picked ones are effectively categorized as irrelevant.
Exploration is
then an instance of categorization with few dynamically evolving categories, while
search could be instantiated as categorization with fixed categories corresponding to
the degree of relevance of items to the search query.
Indeed, the vast majority of
multimedia analysis techniques concerning semantic tasks is a variation on supervised
learning, and classification (i.e., categorization) in particular. Categorization is thus
a useful umbrella task for the exploration-search axis, and thus, by proxy, able to
accomodate most multimedia analytics tasks:
•
Categorization
— the task of assigning individual items into categories from
a category model.
The category model
is defined by the analyst based on
attribute aggregates.
As discussed extensively in the previous text and shown by the majority of the
model components involving semantics in Figure 2.5, categorization in multimedia
analytics is heavily revolving around semantics.
In addition,
visual
multimedia
categorization is much less dependent on statistics (semantic statistics are unintuitive
and hard to obtain) and has much lower tolerance for error from the users (since
the individual errors are spotted instantly) than categorization on other types of
multimedia collections. Hence, it is doubly imperative that the learned categorical
model
very closely follows the mental
model
of the analyst.
There is,
however,
a difference between analytic categorization as a model of human reasoning and
categorization or classification in the classic statistics and machine learning sense.
The human’s notion of categories is quite flexible: she can adapt new information into
her knowledge schema and conversely, adapt the knowledge schema to fit the newly
acquired information [
54
]. Categorization in the machine world, i.e., classification,
is in its classic form much more rigid: the class schema is defined beforehand and it
cannot be changed without retraining the model from scratch. The meaning and
30
A MULTIMEDIA ANALYTICS
MODEL
System
capabilities
Semantic
gap
Pragmatic
gap
Limited
Intermediate
Advanced
Non-semantic model,
only metadata and
basic visual character-
istics (e.g., color)
Model uses objective
semantic concepts
(e.g., “person”)
Model uses high-level,
complex semantics
(e.g., “this patient
has cancer”)
Non-adaptive, rigid
model
(classic classification)
Model adaptive to
interactions,
categories static
(= fixed classes)
Model fully adapts
to user intent,
dynamic categories
Figure 2.7:
Semantic and pragmatic gaps and their effect on multimedia analytics
models.
parameters of categorization thus depend on whether it is performed by a human or
a machine. In linguistics, the phenomenon of the meaning of words being dependent
on context is studied in pragmatics [
115
].
Hence,
we call
this phenomenon the
pragmatic gap:
•
Pragmatic gap
— the gap between the parameters of a categorization task
as performed by the human and the parameters of a categorization task as
performed by the machine.
In other words, the pragmatic gap is related to the adaptability of the model
as the analyst progresses towards insight.
To support the multimedia analyst, the
machine models need to be able to mimic her flexible mental model as closely as
possible. That involves at least three aspects:
•
New categories on the fly
— For example, when a forensics expert who has
initially worked with “child abuse” and “harmless” categories encounters evi-
dence of terrorism, she adapts her cognitive model to include a new “terrorism”
category. The machine should be able to do the same.
•
Non-exclusive categories
— If
one category is “people,” and another is
“Rome,” then the analyst should not be forced to choose where to put a photo
of a couple in front of Fontana di Trevi.
•
Dynamic category semantics
— Suppose an analyst is looking for evidence
of arms trafficking and his “suspicious” category contains firearms.
Further
exploration reveals photos involving explosives,
so they are added into the
model of the “suspicious” category. Later, the firearms photos are deemed no
longer suspicious and removed from the model of the category (e.g., due to
the suspect’s firearm being properly licensed), which now concerns explosives
only. The machine’s category model should follow all these steps.
2.4 MULTIMEDIA ANALYTICS
31
Limited
Intermediate
Advanced
Limited
Inter-
mediate
Advanced
I-SI [182]
Newdle [192]
sVisit [112]
Informedia [58]
Canopy [20]
Similiarity
browser
[121]
INA browser [174]
MediaTable [35][36]
semantic gap
pragmatic gap
Figure 2.8:
Comparison of pioneer multimedia analytics systems based on their po-
sition with respect to the semantic and pragmatic gaps (axis ticks cor-
respond to system capabilities described in Figure 2.7).
Orange
points
represent systems whose model involves heterogeneous data (visual con-
tent, annotations, metadata),
black
points correspond to systems without
a heterogeneous model.
The pragmatic gap is orthogonal to the semantic gap: the former’s concern is
the adaptability of the model, the latter is related to the semantic expressiveness of the
model. Figure 2.7 highlights the impact of the gaps on multimedia analytics models.
Bridging the gaps will foster better overall “understanding” between the machine
and the human, allowing the machine to create models more closely resembling the
analyst’s actual model. Bridging the gaps in the context of exploration and search is
thus one of the core challenges in multimedia analytics.
2.4.2
Pioneer systems
To the best of our knowledge, no multimedia analytics system in existence can handle
both the semantic and the pragmatic gaps fully. In this section, we review pioneer
multimedia analytics systems that have so far paved the way towards narrowing the
gaps.
Figure 2.8 depicts the discussed systems in the semantic gap-pragmatic gap
space, mapping the current state of the art in multimedia analytics.
One of the first pioneers is the Informedia system used for semantic navigation
of the eponymous online digital library, conceived by Hauptmann and Smith in 1995
[
57
]. Being continuously updated since then, Informedia has received a relevance
32
A MULTIMEDIA ANALYTICS
MODEL
feedback component in 2008 [
58
] (albeit one refining search results,
rather than
maintaining an adaptive model).
Another early pioneer system is the similarity
manipulation browser by Nguyen et al. [
121
]. This approach employs a similarity
space browser through which the user directly manipulates the similarity space, with
the machine recomputing the used similarity and rearranging the items based on
the interactions.
The last example of an early adopter is the multimedia browser
developed for the French Audiovisual Institute (INA) by Viaud et al.
[
174
] This
approach combines a similarity space browser, visual summary techniques, and active
learning for interactive exploration of the French TV archives.
The field of multimedia analytics has been defined in 2010 by Chinchor et
al. [
27
] and since then, the first systems bearing the multimedia analytics label have
started appearing. One group of approaches, including Newdle by Yang et al. [
192
]
and I-SI by Wang et al. [
182
], targets news and social media, bringing interactive
exploration of topic trends in news archives and on social networks.
MediaTable
by de Rooij et al. [
36
] facilitates categorization of large image collections using the
spreadsheet visual metaphor, making it the first approach in multimedia analytics
devoted to categorization. MediaTable has been extended with the active buckets
framework maintaining an adaptive model of the data [
35
].
Meghdadi and Irani
adapted the multimedia analytics spirit to the surveillance domain:
their sViSIT
system allows security experts to search for objects of interest within long video
segments and track their trajectory [
112
]. Canopy by Burtner et al. [
20
] has a strong
focus on integrating content with annotations and metadata, involving the three
data sources both in learning and visualization. However, the machine model is not
adaptive. These examples show that the field is steadily growing and that multimedia
analytics systems are increasingly capable to fill their respective niches. There is no
perfect solution which covers all the aspects yet, opening exciting opportunities for
multimedia analytics research.
2.4 MULTIMEDIA ANALYTICS
33
2.4.3
Research agenda
The previous section concluded that so far, no perfect solution covering both gaps
exists. In order to advance multimedia analytics, numerous research questions need
to be answered.
In this section,
we propose several key research questions based
on the insight gained from the survey process, establishing a multimedia analytics
research agenda:
1.
Multimedia visualizations and interfaces
a)
Are the existing multimedia visualizations and interfaces described in
Section 2.2.2 suitable for categorization as defined in this chapter? If not,
how can such an interface be created?
b)
How can the heterogeneous data in multimedia collections be presented
in a truly integrated manner?
c)
What is the best way to present large-scale (
ą
1M items) collections to
the user?
d)
How can multimedia analytics systems be evaluated?
2.
Semantic gap
a)
How can increasingly higher-level semantics be extracted from raw mul-
timedia?
b)
Can high-level semantics be extracted in a manner not prohibiting an
interactive multimedia analytics experience?
c)
How can the heterogeneous data in multimedia collections be leveraged
to improve the semantic quality of the model?
3.
Pragmatic gap
a)
How can the performance and learning speed of active learning be im-
proved, especially in view of the large scale of the data?
b)
Do the currently used interactions with the model described in Section
2.3.2 have sufficient information bandwidth for the model to improve?
How can they be improved?
c)
What is the most efficient way to introduce new categories on the fly? Is
it attribute-based zero-shot learning [88], or is there a better way?
d)
What is the best way to introduce non-exclusive categories?
Is there
a better way than training
n
1-vs-all classifiers as done for example in
MediaTable [35]?
e)
What is the best way to model and introduce fully dynamic, human-like
categories whose semantics and boundaries evolve over time?
34
A MULTIMEDIA ANALYTICS
MODEL
Visualization
Model
Knowledge
. . .
Category 1
people
61 items
. . .
Category 2
nature
93 items
. . .
Data
MM collection
Content
Annotations
Metadata
Features
Statistics
Similarity
interactive
model update
semantic nav.
directions
interact/navigate interface
add data
new features
model user intent
learn semantics
adapt categories
Figure 2.9:
The proposed multimedia analytics process, expanding upon the diagram
by Keim et al. [82][83].
2.5
CONCLUSION
The mission of multimedia analytics,
facilitating understanding and insight into
large-scale multimedia, is very important and ambitious.
In this chapter, we have
surveyed a large body of work related to multimedia analytics and proposed a novel
general
multimedia analytics model.
This model
brings two major benefits:
it
provides a structured overview of all levels of multimedia analytics and establishes
a clear agenda for the future of multimedia analytics.
Both benefits are based on
the extensive survey,
therefore grounded in the established scientific theory and
bearing in mind the possibilities and limitations of the state-of-the-art techniques in
the related fields.
This chapter thus paves the way towards interactive, intelligent,
and integrated multimedia analytics systems of the future, schematically depicted in
Figure 2.9. We believe that those systems will play an increasingly important role in
our increasingly digital and multimedia society.
3
I N S TA NT I AT I NG T H E M ODE L
Published as Zahálka et al.:
Interactive Multimodal Learning for Venue Recommendation
. IEEE
Transactions on Multimedia (TMM), 17 (12), pages 2235–2244, December 2015.
35
36
INSTANTIATING THE MODEL
3.1
INTRODUCTION
Venue recommendation — suggesting places of interest (venues) to visit to a user
based on her preferences — is both an actively researched problem and a domain of
systems with millions of users. An increasing number of travellers and locals alike
prefer personalized, authentic local experience to tours organized by a third party. To
this end, they visit online travel recommender systems such as Foursquare, Yelp, or
TripAdvisor. Underlying state-of-the-art algorithms, originating in the research field
of recommender systems, keep increasing their accuracy when predicting user-venue
matches based on the user profile and/or search queries.
They are thus successful
when a user is searching for specific venues. However, how do they fare when the
user wants to explore a new city guided by the broad spectrum of her interest without
formulating them as an explicit query?
The classic recommender paradigm has a number of innate shortcomings with
respect to exploration.
Typically, classic recommenders depend either on a user-
item matrix that matches users to venues, or on the overall popularity of individual
venues.
Both variants suffer from the cold start problem:
they require an active
user base before the recommendations start to make sense. Also, they often tend to
gravitate towards mainstream venues visited by many users.
However, local steak
afficionados do not necessarily go exclusively to the most visited steakhouses, but
also to restaurants off the beaten track.
Moreover, understanding user preference
towards a particular venue and the aspects that make two venues similar for the user
is a non-trivial task. For example, some users prefer a certain restaurant for the food,
some for the ambience, and some perhaps for the garden terrace.
Aspects such as
aesthetics, ambience, or coziness are beyond the scope of the binary assignment of
the venue to the user in the user-item matrix and are hard to capture without content
analysis. Thus, the applicability of the classic recommender systems paradigm with
respect to exploring a new city might be limited.
In this chapter, we present City Melange, an interactive multimedia content-
based venue explorer.
The pipeline of City Melange is conceptually depicted in
Figure 3.1. The first step involves collecting a cross-platform multimedia dataset of
venues and social media users.
In the second step, this dataset is used to construct
a number of semantic topics for each venue and social
media user by clustering
on state-of-the-art visual
(ConvNet) and text (LDA) features.
These topics are
then used in the third step:
the interactive city exploration session.
City Melange
allows the interacting user to iteratively build her user preference profile and get
highly personalized recommendations regardless of previous user activity, with each
interactive step taking seconds at most.
City Melange is built on three cornerstones:
interactivity,
multimodality,
and utilization of cross-platform data.
Using interactive machine learning to build
3.1 INTRODUCTION
37
the interacting user’s profile and match related social media users and their venues
in real time tackles three important issues: the cold start problem, personalization,
and user friendliness.
The cold start issue is avoided by learning user preferences
on the fly from interactions.
By quick iterative updates, City Melange is able to
provide increasingly personalized suggestions. Direct interaction with the images
of the venues visited by the social media users and generating relevant suggestions
based on these interactions can be viewed as more seamless and user-friendly than a
series of query-response pairs using, for example, Foursquare or Yelp alone. Taking
a multimodal
approach has a number of advantages.
Firstly,
venue-related social
multimedia content is abundant and readily downloadable. Secondly, we can exploit
the synergy between individual modalities, painting a diverse picture of the users’
and venues’
underlying topics.
In addition,
sometimes one modality covers the
weaknesses of others. For example, no text can surpass a photograph in conveying
the atmosphere of the Sistine Chapel.
The cross-platform nature of City Melange
further diversifies the captured user and venue topics, since different platforms often
accomodate different users with different interests. With its focus on content analysis,
City Melange avoids the difficult problem of matching users between platforms.
Combining these three aspects allows us to tailor our approach to better suit city
exploration.
The main contribution of City Melange is the novel interactive and responsive
venue exploration pipeline, which combines the three aforementioned cornerstones:
•
Tight cross-platform data integration.
•
Multimedia content analysis of user and venue data.
•
Interactive machine learning allowing the users to build their profile and get
personalized recommendations in seconds.
The contribution of City Melange is extensively evaluated in this chapter.
City
Melange addresses three recommender challenges identified in a recent survey by Shi
et al. [
151
]: social recommendation, cross-domain recommendation, and interaction
and recommendation.
This chapter is organized as follows. In Section 3.2 we reflect on the related
work. In Section 3.3 we describe our data collection procedure. Section 3.4 explains
the construction of user and venue semantic topics. Section 3.5 describes our pro-
posed interactive venue exploration method. The experimental setup evaluating the
proposed approach is described in Section 3.6. The experimental results are discussed
in Section 3.7. Section 3.8 provides concluding remarks.
38
INSTANTIATING THE MODEL
Section 3.3
Data
collection
. . .
Section 3.4
User & venue
topical analysis
ConvNet
LDA
k
-means
Section 3.5
Interactive
city exploration
SVM
Modality fusion
Max-matching
Figure 3.1: Overview of the City Melange pipeline.
3.2
RELATED WORK
Probably the most intuitive approach to venue recommendation, which is still com-
monly used in commercial travel recommenders,
is popularity ranking.
Despite
appearing to be a competitive baseline [
30
,
71
,
152
,
160
], it has many downsides, such
as the absence of personalisation and the failure to satisfy the needs of non-mainstream
users.
To address some of those challenges, a number of effective collaborative fil-
tering approaches have been proposed,
which rely on the analysis of individual
preferences encoded in the user-item matrix. For example, Rendle et al. propose a
generic Bayesian approach optimized for producing a personalized ranking [
139
].
Two other well-known collaborative filtering approaches target producing recom-
mendations when user preferences are only provided implicitly, for example when
it is unclear which purchased products or visited locations a particular user disliked
[30, 128].
Recent studies (e.g., [
3
] and [
151
]) show the benefits of utilizing information
beyond the user-item matrix. This inspired a number of more sophisticated location
recommendation and exploration applications using community-contributed content
[
26
,
84
,
96
,
125
,
129
,
152
]. In those papers, data sparsity has been shown to negatively
affect the effectiveness of those traditional location-based recommender systems which
are based on the collaborative filtering methods utilizing the user-item rating matrix
[
96
].
With regard to the data types in the collection used for modelling the user
preferences and overcoming the problem of data sparsity, the existing approaches can
be roughly split into two categories based on their use of modalities. We consider the
term modality in the multimedia sense to mean various media types, such as visual,
3.2 RELATED WORK
39
audio, or text. Unimodal approaches, summarized in Section 3.2.1, make use of at most
one modality, possibly in conjuntion with other data types, such as human mobility
patterns, social (friendship) relations or the textual or categorical venue description.
Multimodal approaches, outlined in Section 3.2.2, encompass approaches that use at
least two multimedia modalities jointly. City Melange, which utilizes the visual and
text modalities, falls into the multimodal category. As “units” of recommendation, the
presented works use either tourist destinations (in particular, cities), venues, landmarks,
or geographic locations in general. Although some of these units are similar or have a
sub/superset relation, for consistency with the original works, we opt to keep the
terminology adopted by the authors.
3.2.1
Unimodal approaches
Shi et al.
address the problem of non-trivial
landmark recommendation using a
collaborative filtering approach with matrix factorization [
152
]. To compensate for
the sparsity of the user-venue matrix, the authors extract information about venue
categories from Wikipedia and use it to compute similarities between the venues at a
topical level. Liu et al. propose a framework for point of interest (POI) recommenda-
tion based on Foursquare check-ins publicly shared via Twitter [
105
]. The approach
jointly utilizes information about regional venue popularity, user mobility patterns
and an LDA model aggregated from the textual venue descriptions for profiling the
users and POIs.
Similarly, Lian et al.
propose an approach which analyses human
mobility records on the Jiepang location-based social
network (LBSN) for POI
recommendation [
103
]. To cope with an extreme sparsity of user-venue matrix, the
approach utilizes a weighted matrix factorization, which is further augmented with
the user activity area vectors and influence area vectors of the POIs. In another work,
Kurashima et al. model geographic location of venues and user mobility patterns on
the Tabelog LBSN for estimating topics of interest to the users and identifying the
areas they are frequenting, such as home and work place [
87
]. The approach further
uses textual annotations associated with venue-related Flickr images to describe such
discovered topics. Starting from the assumption that users’ preferences are influenced
by both item location and its context captured in item tags and category informa-
tion, Yin et al.
propose a recommender system based on location-context-aware
topical modelling [
195
].
The approach presented by Noulas et al.
jointly utilizes
information about social ties and history of previously visited venues, gathered from
location-based social networking sites to predict which previously unvisited venues a
user will likely visit [
125
]. Information about social relations is also utilized in the
personalised venue recommendation framework proposed by Zhao et al. [
205
]. The
framework unifies the analysis of visiting behaviour and social relations of Foursquare
users with the analysis of venue similarities computed based on user contributed
annotations (i.e., Foursquare tips). Liu et al. jointly analyse content of the images and
40
INSTANTIATING THE MODEL
their associated geo coordinates to build 3D models of urban scenes [
106
].
Based
on the experiments performed on a collection of street view and Flickr images, the
approach was proven effective in various location-based services, such as accurate
mobile visual localisation and rendezvous point finding. In a recent work, Quercia
et al.
make use of crowdsourcing for annotating locations in a city as beautiful,
quiet, or happy and then learn to generate the routes that are both pleasant and short
[
137
]. The importance of contextual information for producing more relevant and
personalized recommendations has also recently been recognized by some of the
major commercial travel recommender systems.
In 2014 TripAdvisor introduced
the Just for You feature to replace the default popularity hotel ranking [
2
].
When
producing new hotel recommendations, the feature incorporates user preferences
specified by applying various filters related to, e.g., hotel price, location, and style
along with the users’ textual feedback about their travel experiences. Similarly, Expe-
dia has introduced several new features making use of user preference patterns in
regard to various parameters, such as travel time and location, for improving hotel
and flight recommendation [
51
]. Although these approaches succeed in significantly
improving recommendation performance, they do not exploit valuable information
about user preferences and venue properties encoded in the visual modality.
3.2.2
Multimodal approaches
To compensate for the weaknesses of the approaches presented in Section 3.2.1, an-
other group of approaches deploys a multimodal analysis for venue recommendation
or exploration. For example, Pang et al. mine user-generated travelogues comprising
textual descriptions of tourist destinations from the TravelPod and IgoUgo platforms
to discover location-specific tags [
129
]. Those tags are used for selecting relevant and
representative Flickr images to visualize a particular tourist destination. Kofler et al.
conceived a personalised system for off-the-beaten track location recommendation
[
84
]. The system analyses geo-referenced Flickr images captured by the users during
their previous travels to recommend off-the-beaten track locations within the destina-
tion city matching their interest. Popescu et al. present an approach which analyses a
collection of Flickr images for discovery of tourist routes within a city and recommen-
dation of new ones [
134
]. To a similar end, the personalised travel recommendation
system proposed by Cheng et al.
automatically analyses community-contributed
images for detecting user attributes such as gender, age and race [
26
]. The attributes
are used for recommending an optimal route for a particular user demographic, e.g.,
family travellers.
Rudinac et al.
propose an approach for discovering and summa-
rizing aspects of a geographic area around a location recommended to a user [
142
].
The approach jointly analyses image content and the user-generated annotations
as well as the information about users and their social network. Fang et al. analyse
Google Street View panoramas and Flickr images to detect geo-informative (i.e.,
3.3 DATA COLLECTION
41
discriminative and representative) attributes for location recognition and exploration
[
46
].
Co-occurrence between images and their associated annotations is utilized
for giving those geo-informative attributes a semantic interpretation. Finally, Zhao
et al.
propose a multimodal
approach for detecting overlapping communities in
Foursquare mobile social networking service [
204
]. Although not directly focusing
on recommendation, we mention this work here because of the use of a common data
source and the utilization of multimodal information for discovering like-minded
users.
What sets us apart from abovementioned related work, which focuses mostly on
“static” recommendations, is an interactive approach to venue recommendation and
city exploration in which the user preferences are learned online. Additionally, while
in most related work the units of recommendation are the geographic entities, e.g.,
venues or landmarks, in our approach the venues are recommended indirectly, as a
part of the like-minded users’ profiles, which puts a strong accent on the user. Finally,
to gain a better insight into user preferences and venue properties,
we integrate
multimedia data from diverse content-sharing and location-based social networking
platforms.
3.3
DATA COLLECTION
There exists a vast amount of publicly available multimedia data related to venues
across the cities in the world.
Thus,
for any major city,
it should be possible to
construct a meaningful, reasonably-sized dataset covering a diversity of aspects of
the city. In this section, we describe a method for constructing such a representative
dataset.
The first step involves obtaining the list of venues in the city.
To ensure
coverage, this is done by querying online venue data sources (further denoted as V)
for all venues within the city limits (denoted as
G
C
). There is a number of such venue
sources with an API, such as Foursquare or Yelp. To ensure full coverage of the city,
a number of queries
Q
v
(
¨
)
is submitted to each venue source
v
P
V, each pertaining
to a certain geo coordinate rectangle
g
P
G
C
. The resulting venue list
V
C
is then a
union of all query results pertaining to individual venue sources and geo rectangles:
V
C
=
ď
vPV
g
P
G
C
Q
v
(
g
)
(1)
Once the list of venues is established, we can query online multimedia sources
(M) for multimedia content related to each venue
v
P
V
C
.
We are interested in
results matching both the venue name (
N
v
) and geo coordinates (
G
v
). We enforce
exact name matches, since partial matches can easily increase noise. For instance, if
42
INSTANTIATING THE MODEL
looking for “Exampleville Bakery” in the Exampleville neighbourhood, using partial
matching will include additional items not related to the bakery, because the name
and geo coordinates of the neighbourhood match. Geo coordinates are allowed to
be within a certain radius
r
, since the technology capturing geo coordinates and/or
manual user geo labels might be imprecise.
It is,
however,
desirable to keep
r
as
low as possible to keep the signal-to-noise ratio as favourable as possible. The set of
multimedia content
M
C
for city
C
and any modality is again a union of the results
returned for all individual queries:
M
C
=
ď
mPM
v
P
V
C
Q
m
(
N
v
^ |
G
m
,
G
v
| ă
r
)
(2)
The richness of information in
M
C
is determined by the multimedia sources
M. In City Melange, we utilize the following modalities: the visual content
V
(i.e.,
the images), the associated text content
T
(e.g., descriptions, tags, and comments),
and user information
U
(at least the identity of the user who uploaded the item). The
user information in
U
is further combined with the venue list
V
C
to construct the
user-venue matrix
U
V
whose rows correspond to users
u
P
U
and columns to venues
v
P
V
C
.
U
V
u,v
=
1
if user
u
has uploaded content related to venue
v
and 0 otherwise.
The resulting City Melange dataset covers the multimedia content itself (
V
,
T
), user
information (
U
), the venues in the city (
V
C
), and the user-venue mapping in the
form of the
U
V
matrix.
The City Melange data collection method is general.
For any major city,
the list of venues
V
C
can be easily obtained from locally prevalent venue platforms
such as Foursquare,
Yelp,
or Yellow Pages regardless of the user activity in the
city. Given the plethora of globally popular social platforms with APIs usable as a
multimedia resource, we believe that a sufficiently rich dataset can be obtained for
any desired major city. The rather strict query strategy targetting both name and
geo coordinates of the venue keeps the noise levels low and the associated multimedia
items tend to be highly relevant.
The City Melange data collection method thus
provides a generalizable framework instantiable in popular cities around the world.
3.4
TOPICAL ANALYSIS
OF USERS
AND VENUES
This section discusses our approach to constructing semantic topics from the collected
data (
V
,
T
, U, V
C
, U
V
) described in Section 3.3. The topic construction pipeline is
conceptually depicted in Figure 3.2. Section 3.4.1 describes the feature extraction
process, Section 3.4.2 describes the construction of the topical representation of users
and venues within City Melange.
3.4 TOPICAL ANALYSIS
OF USERS
AND VENUES
43
Content
V
Images
T
Tags
Comments
. . .
Features
V
F
ConvNet
T
F
LDA
Clustering
V
C
Venues
U
Users
Processed data
V
V
T
Visual venue
topics
V
U
T
Visual user
topics
T
V
T
Text venue
topics
T
U
T
Text user
topics
U
V
User-venue
matrix
Figure 3.2: The City Melange data processing pipeline.
3.4.1
Feature Extraction
Venue recommendation is a highly semantic problem. Indeed, an actor’s venue pref-
erences are often based on high-level semantic concepts such as ambiance, coziness,
or sensory pleasure such as the aesthetics of a building, the acoustics of a concert hall,
or the taste of food. To introduce the semantic dimension to the recommendations,
extracting semantic features from the raw multimedia content is required.
In the visual domain, we use a deep convolutional neural network as conceived
by Krizhevsky et al. [
86
], trained on the ImageNet Large Scale Visual Recognition
Challenge (ILSVRC) 2012 dataset of 1000 semantic concepts [
39
]. Each individual
image is represented by the respective output of the deep net. For City Melange, we
opted to use the final output of the net, i.e., the 1000 concept scores, as the visual
features. The concept dictionary captures broad semantics, including concepts such
as
cinema
,
confectionery
, or
park bench
.
This allows human interpretation of
the features and ensures that we stay on as high semantic level as possible, making
our visual feature choice consistent with our text feature choice motivated below.
Deep net features,
apart from being the state of the art with regard to semantic
descriptiveness, are very compact. This is crucial for the interactive performance of
City Melange. The visual feature representation (
V
F
) is the collection of the features
of all individual images in
V
.
In the text domain, we tokenize the text in
T
using the Natural Language
Toolkit (NLTK) [
13
] and remove all markup elements as dictated by the format
of the obtained text data in
T
.
Then, we remove common stopwords and words
appearing only once. To index the text in a persistent manner, we use the Gensim
44
INSTANTIATING THE MODEL
framework [
177
] designed for topic modeling of large corpora. After computing the
bag of words representation for each document, we perform online Latent Dirichlet
Allocation (LDA) [
61
] to identify latent topics associated with the images.
We
propose using 100 latent topics, which is enough for the topics to be descriptive and
discriminative and small enough to ensure that City Melange remains interactive.
Two examples of New York topics are
{building
,
state
,
empire
,
top
,
manhattan
,
view
,
rock
,
newyork
,
nyc}
and
{local
,
tavern
,
street
,
market
,
avenue
,
bank
,
new
,
union
,
farmer
,
village}
. The text feature representation (
T
F
) is a collection
of latent topic annotations for each item in
T
.
3.4.2
Semantic User and Venue Topics
Processing millions of multimedia items places a non-trivial computational load on
the machine,
thus prohibiting interactivity.
Hence,
a compact representation of
individual venues and users is needed. We conjecture that each venue and user has a
number of topics of interest within the respective set of images. For example, images
of a restaurant might depict the food,
the interior,
or groups of people enjoying
their drink. A user’s images, while potentially diverse and numerous, will typically
share topics corresponding to the user’s main interests.
Determining a reasonable
number of topics to represent users and venues provides not only a more compact
representation, but also a basis for matching them.
To obtain the topics of interest, we cluster all items associated with a venue
using
k
-means clustering. The clustering is performed independently on the visual
(
V
F
) and text (
T
F
) features. The cluster centroids represent the individual venue visual
(
V
V
T
) and text (
T
V
T
) topics.
Similarly, we employ
k
-means clustering on all items
uploaded by a particular user and select the resulting centroids to topically represent
user interests in the visual (
V
U
T
) and text (
T
U
T
) domains. In the original New Yorker
Melange system, we opted for
k
=
5
for both users and venues. In this chapter, we
provide an extensive experimental evaluation of various
k
configurations in Section
3.7.2. The topics are constructed on top of the semantic features, reduce the dataset
size, and at the same time establish a common feature space for users and venues in
each modality.
3.5
INTERACTIVE VENUE RECOMMENDATION
This section discusses our approach to providing interactive, multimodal, and content-
driven topical venue recommendations to the interacting user (henceforth named
actor to avoid confusion with the social media users in the dataset). That is, how to
harness the semantic topics extracted as discussed in Section 3.4 to provide suggestions
3.5 INTERACTIVE VENUE RECOMMENDATION
45
V
V
T
,
T
V
T
Venue topics
V
U
T
,
T
U
T
User topics
Grid
Rel.
venues
User ranking
V
+
T
,
T
+
T
Positives
V
´
T
,
T
´
T
Negatives
Linear
SVM
U
Users
User
ranking
U
S
Suggested
users
Rand.
sample
Venue ranking
Venue
ranking
U
V
User-venue
matrix
V
S
Suggested
venues
Map
(
U
S
, V
S
)
Suggestions
Relevance
indication
Figure 3.3:
City Melange venue recommendation. The transition from grid to map
interface takes seconds, making City Melange a truly interactive method.
(
U
S
, V
S
)
, i.e., the set of topically relevant users (
U
S
) and their visited venues ranked
by relevance (
V
S
). The interactive exploration pipeline of City Melange, conceptually
depicted in Figure 3.3, can be decomposed into four steps described in this section.
Processing the user interactions (Step 2–4) only takes a few seconds on a standard PC,
which makes City Melange a responsive and truly interactive venue recommendation
method.
Step 1: Initial actor interest indication (Grid)
: The actor is first presented
with a set of images arranged in a grid interface. These images represent individual
venues. The actor selects the relevant venues of interest by clicking on the images.
The visual and text topics associated with these venues are used to initialize the sets
of positive training examples (one per modality):
V
+
T
for visual,
T
+
T
for text (cf.
Section 3.4.2). The sets of negative examples denoted
V
´
T
,
T
´
T
for the visual and text
modalities respectively, are initially empty.
Step 2: User ranking
:
To identify users similar to the actor, we train two
linear SVM models:
one for the visual modality, one for the text modality.
Each
model is trained on the respective set of positive and negative examples. If the size of
the set of negatives for any modality is too small, in our approach
|
V
´
T
| ă
2
|
V
+
T
|
or
|
T
´
T
| ă
2
|
T
+
T
|
, it is supplemented with random samples from the collection of user
topics (
V
U
T
,
T
U
T
) to ensure good positives-negatives balance. The relevance score
s
M
u
of user
u
P
U
for content modality
C
P t
V
,
T
u
is then the maximum of SVM scores
σ
(
¨
)
per user topic
t
u
P
C
u
T
:
s
C
u
=
max
t
u
P
C
u
T
σ
(
t
u
)
(3)
46
INSTANTIATING THE MODEL
In other words, each user is as relevant as her most relevant topic. This “max-
matching” strategy is motivated by the exploration purpose of City Melange: if any
of the actor’s and the user’s topics match,
they highly probably share an interest,
and thus said user’s venues become potentially relevant. The overall user relevance
ranking is determined by aggregating the score rankings across modalities using
Borda count [
40
] as follows. For each modality, we sort the users by
s
C
u
in descending
order.
The highest ranking user gets
|
U
|
points, the second-highest gets
|
U
| ´
1
points and so on.
The final
number of points per user is the sum of points per
modality.
The final ranking is obtained simply by sorting the users by their final
number of points in a descending order. The top 5 ranked users are selected as the
users suggested to the actor (
U
S
).
Step 3: Venue selection
:
The next step involves ranking the venues of the
users in
U
S
based on the relevance to the actor’s topics.
This ranking eventually
determines the order in which the venues will be displayed to the actor. This enables
the actor to steer her exploration towards relevant venues even though she does not
provide relevance on venues directly. The relevance score
s
C
v
of venue
v
P
U
V
u,1..
|
U
|
for content modality
C
P t
V
,
T
u
is the minimal cosine distance
dist
cos
(
¨
)
between
the topics of
v
(
C
v
T
) and user topics
C
u
T
of the given suggested user
u
P
U
S
:
s
C
v
=
min
t
v
P
C
v
T
t
u
P
C
u
T
dist
cos
(
t
v
, t
u
)
(4)
These scores are again aggregated using Borda count, yielding
V
S
, the list of
venues ranked by relevance for each suggested user
u
P
U
S
. The system will display
up to five venues per user, with the remaining venues available on demand so as not
to overwhelm the actor.
Step 4:
Actor interaction (Map)
:
The suggested users and their venues
(
U
S
, V
S
) are displayed within the map interface depicted in Figure 3.4. Each venue is
represented by a thumbnail placed on the map at the venue’s coordinates. The border
of each thumbnail is color-coded corresponding to the visiting user(s).
The users
themselves are anonymized. The anonymization does not prohibit the manifestation
of individual user experiences, but ensures the users’ privacy. Clicking on a venue
image thumbnail enables the actor to inspect the image and basic information about
the venue, such as name, venue type, and link to the venue’s website. The actor can
indicate which user(s) are relevant. In that case, the user topics of the relevant users
are added to the sets of visual (
V
+
T
) and text (
T
+
T
) positives. Similarly, the user topics
of users marked as non-relevant are added to the sets of negatives (
V
´
T
and
T
´
T
). The
topics of all displayed users (five in the current interface) and venues (up to five per
each user) are removed from the collection of user centroids, and will not show up in
subsequent iterations. After these adjustments to the SVM training data, the system
proceeds with step 2.
3.6 EXPERIMENTAL SETUP
47
Figure 3.4:
A screenshot with user recommendations generated by New Yorker
Melange, a system instantiating City Melange [198].
3.6
EXPERIMENTAL SETUP
In this section, we describe the experimental setup. The aim of the experiments is to
compare City Melange to the existing approaches with respect to recommendation
relevance. Section 3.6.1 presents the datasets used in our experiments. The baseline
algorithms and the configurations of the Melange algorithms are described in Section
3.6.2. Section 3.6.3 explains the evaluation techniques used to judge the performance
of the algorithms.
3.6.1
Data
For the experimental evaluation, we deploy City Melange on two very different
metropolises: New York and Amsterdam. New York is well-known to be a bastion
of large-scale social
multimedia data.
The main challenge is thus to be able to
intelligently process the abundance of data in an interactive,
responsive manner.
Amsterdam, while being one of world’s major tourist destinations, is essentially a small
city. Venue-related social networks like Foursquare are much less used than in New
York. As a result, the main challenge lies in finding meaningful recommendations in
the comparatively smaller-scale and sparse data.
48
INSTANTIATING THE MODEL
We have used Foursquare as a source of venue information and images, Flickr
as a multimedia data source for both cities, and Picasa as a multimedia data source
for New York.
Picasa data is missing in the Amsterdam dataset, as the Picasa API
did not allow for querying both the venue name and the geo location at the time
of the Amsterdam dataset construction.
As visual features (
V
F
) (cf.
Section 3.4.1),
we use the 1000 ILSVRC concept detector scores. The text features (
T
F
) were con-
structed from the Flickr (
title
,
description
,
tags
) and, in the case of New York, also
Picasa (
title
,
summary
,
description
,
keywords
,
albumtitle
,
albumdesc
,
snippet
)
text components of the individual items. We use 100 latent topics to ensure reasonable
granularity.
The New York dataset, also used in our original New Yorker Melange applica-
tion [
198
], contains information about 7,246 verified venues, and the corresponding
multimedia collection contains 1,072,181 items (429,921 images from Foursquare,
243,292 images with associated text from Flickr, 398,968 images with associated text
from Picasa). The Amsterdam dataset contains information about 693 verified venues
and a collection of 55,990 items (37,788 images from Foursquare, 18,202 images
with associated text from Flickr).
3.6.2
Evaluated Algorithms
As baselines, we use three approaches common in the field of recommender systems.
Two are based on collaborative filtering of the user-venue matrix
U
V
through
matrix factorization: Weighted regularized matrix factorization by Hu et al.
[
62
]
(denoted
wrmf
) and Bayesian personalized ranking matrix factorization (denoted
bprmf
) [
139
].
For the evaluation of both collaborative filtering approaches,
we
use the MyMediaLite implementation [
50
].
The third baseline used is popularity
ranking (
poprank
), which has been shown to perform very well in numerous cases
[
30
,
71
,
152
,
160
]. In this approach, the venues are ranked by the number of visits,
and in each round the algorithm recommends the top ranked venues which have not
been seen yet. We extract the number of visits from Foursquare venue information.
This baseline represents the vox populi as presented by popular venue recommenders
like Foursquare or Yelp.
Our approach described in Section 3.5 is experimentally evaluated using a
number of configurations. To evaluate the contribution of individual modalities to the
overall results, we consider a variant of City Melange using only the visual modality
(
melange_vis
) or only the text modality (
melange_txt
). The configuration using
both modalities jointly is denoted
melange_mm
.
Further,
we vary the number of
topics per user and venue. This is necessary due to the unsupervised nature of the
k
-means algorithm used to obtain the topics as described in Section 3.4. The number
of topics per user (
T
u
) and venue (
T
v
) are varied independently. In our experiments,
T
u
, T
v
P t
1, 5, 10
u.
3.6 EXPERIMENTAL SETUP
49
3.6.3
Experimental Procedure
For evaluation, we apply a protocol inspired by the common practices in evaluation
of recommender systems. For each city, we select 100 random users to be used as
artificial actors.
For each artificial actor, we use 4-fold cross-validation as follows:
In each fold, 25% of the venues visited by the user are withheld as test data.
The
remaining venues are indicated as relevant (cf. Section 3.5, step 1). The artificial actor
then goes through 10 interaction rounds (cf. Section 3.5, steps 2–4). Realistically, we
expect the user to go through only a couple of rounds, so the first interaction rounds
are the most important. In each interaction round of the
melange
algorithms, the
artificial actor gives relevance judgment on 5 suggested users with up to 5 suggested
venues each as follows:
users with previously unseen relevant venues are marked
as relevant, users with no relevant venues are marked as non-relevant. Users with
relevant venues that have all been seen before are not marked at all. The baselines
recommend venues directly.
The actor receives up to 25 suggested venues per
interaction round. In the case of the collaborative filtering baselines,
wrmf
and
bprmf
,
all relevant venues are added to the actor profile and used further to produce the next
rounds of recommendations. The
poprank
algorithm does not need any relevance
indication, as it simply recommends the most popular venues without taking the
actor preferences into account.
We measure the quality of city exploration along three dimensions: type appro-
priateness of the suggested venues, time efficiency, and personalization with respect
to mainstreamness. For the first dimension, we measure two characteristics: venue
type precision and venue type recall.
Venue type of each venue corresponds to the
Foursquare categories assigned to it. We use 621 fine-grained Foursquare categories,
such as “concert hall,” “Japanese restaurant,” or “skate park.” The venue type infor-
mation is a variable withheld for evaluation: none of the evaluated algorithms uses
the venue type information to provide suggestions. For city exploration evaluation,
we deem a suggested venue accurate as long as it matches any of the venue types
associated with the venues indicated as relevant in the first step (cf. Section 3.5, step
1). In other words, if the actor initially indicates interest in karaoke bars and cheese
shops, any suggested venue of any of the two types is an accurate suggestion. Venue
type precision is the ratio of the number of accurate suggestions to the total number of
suggestions in each interaction round. Venue type recall at interaction round
n
is the
percentage of recalled actor venue types up until and including interaction round
n
. As the time efficiency measure, we report length of one interaction round in seconds
(further denoted simply as time), which enables us to judge the interactive quality and
responsiveness of the evaluated algorithms. All reported values are averaged across all
folds and all 100 artificial actors.
The last evaluated dimension is the mainstreamness of the recommendations.
A mainstream user should receive mainstream recommendations, which is easy to
50
INSTANTIATING THE MODEL
achieve by popularity-based approaches given that there are enough votes.
How-
ever, afficionados with specialized taste should receive recommendations tailored
accordingly. To this end, we evaluate the nature of the recommendations in light
of them being mainstream or off the beaten track. To rank the venues in each city
based on “mainstreamness,” we use the number of Foursquare checkins,
i.e.,
the
same information used for the
poprank
method.
We split the venues into 4 bins
based on their position in the popularity ranking (top 25% mainstream, top 25–50%
mainstream, top 25–50% off-the-beaten-track, top 25% off-the-beaten-track). The
ground truth bins are constructed from the true distribution of user venue visits
encoded in the user-item matrix
U
V
. Then, for each evaluated method, we take all
the venue recommendations, and construct the mainstreamness distribution of all
recommendations in the same manner as in the case of ground truth.
Algorithms
that are able to match the mainstreamness distribution of the user activity within a
given city are likely to perform well in overall mainstreamness personalization.
3.7
EXPERIMENTAL RESULTS
In the experiments, we aim to answer the following research questions:
A.
Is City Melange effective in producing recommendations relevant to the inter-
acting user?
B.
How does the performance depend on the assumed number of user and venue
topics?
C.
What is the contribution of each modality to the overall performance of City
Melange?
D.
Does our system manage to recommend the off-the-beaten track venues as well?
3.7.1
General Performance Evaluation
The venue type precision, venue type recall, and time measures for both cities are
depicted in Figure 3.5. In the case of New York,
melange_mm
maintains the highest
venue type recall
throughout all
interaction rounds.
It is also the top performer
with respect to venue type precision,
only narrowly losing to
wrmf
in the first
interaction round (0.006 difference in precision). However, the
wrmf
baseline is quite
slow: it takes roughly 7 seconds per interaction round. Our approach,
melange_mm
,
takes 2-2.5 seconds per interaction round, which makes it a much more responsive
approach. The Amsterdam dataset is small enough for all approaches to be deemed
truly interactive, but it brings the challenge of lack of data: low venue coverage of the
city, low user activity, and low amount of data per user/venue. On the Amsterdam
data,
the
melange
algorithms outperform the baselines on venue type precision.
3.7 EXPERIMENTAL RESULTS
51
1
2
3
4
5
6
7
8
9
10
Interaction round
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Venue type precision
NY - Venue type precision
1
2
3
4
5
6
7
8
9
10
Interaction round
0.0
0.2
0.4
0.6
0.8
1.0
Venue type recall
NY - Venue type recall
1
2
3
4
5
6
7
8
9
10
Interaction round
0
1
2
3
4
5
6
7
8
Time (seconds)
NY - Time
1
2
3
4
5
6
7
8
9
10
Interaction round
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Venue type precision
AMS - Venue type precision
1
2
3
4
5
6
7
8
9
10
Interaction round
0.0
0.2
0.4
0.6
0.8
1.0
Venue type recall
AMS - Venue type recall
1
2
3
4
5
6
7
8
9
10
Interaction round
0
1
2
3
4
5
6
7
8
Time (seconds)
AMS - Time
poprank
melange_vis
bprmf
melange_txt
wrmf
melange_mm
Figure 3.5:
Performance comparison of
the
melange
configurations
with the
baselines.
Curiously,
poprank
is the top performer with respect to venue type recall, followed
closely by
melange_mm
.
Apparently,
in a dataset with only 693 venues,
poprank
covers a more complete set of venue types associated with the users than in the
case of the New York dataset with thousands of venues. However,
poprank
’s venue
type precision is not a top performer.
Overall, the results confirm that utilizing a
content-based approach towards city exploration is beneficial. Except for the case
of venue type recall on the Amsterdam dataset,
melange
provides top performance
while staying responsive and interactive.
3.7.2
Analysis of User and Venue Topic Configurations
As discussed in Section 3.5, we assume that each user and venue are associated with
a certain, relatively low number of topics that we refer to as the user preferences
and venue properties. To investigate how this assumption influences performance of
our system, in Figure 3.6 we report the obtained venue type precision and recall for
a varying number of user (
T
u
) and venue (
T
v
) topics, averaged over 10 interaction
rounds.
The experiments show that there is a tradeoff between venue type precision
and recall with respect to the number of user topics. The lower the number of user
52
INSTANTIATING THE MODEL
topics, the higher the precision and lower the recall. We believe that this is due to
the user matching strategy (a user is as relevant as her most relevant topic) and to
the fact that all the topics of the relevant user are added to the relevant topic profile
upon relevance indication.
Higher number of topics per user allows
melange
to
facilitate broader exploration, recalling higher number of relevant venue types at the
cost of precision.
Conversely, with
T
u
=
1
, the user’s one topic chiefly represents
her dominant interest, so the diversity of venue types and thus also the venue type
recall are lowered.
At the same time,
the algorithm performs a narrower search,
matching the venues indicated as relevant (cf. Section 3.5, step 1) to the users with the
corresponding dominant interest, which increases precision. Regarding the number
of venue topics,
T
v
=
1
generally lowers both venue type precision and recall.
T
v
=
5
seems to be slightly preferrable for higher recall, and
T
v
=
10
tends to yield higher
precision. Bearing in mind the user topic tradeoff, we have selected
T
u
=
5, T
v
=
10
,
which is the highest-precision
melange_mm
approach with
T
u
‰
1
, as the general
recommended configuration. This configuration is the one shown in Figure 3.5 and
discussed in Sections 3.7.1 and 3.7.4. Changing the number of topics per user allows
us to tune melange towards higher precision or recall as appropriate.
3.7.3
Modality Analysis
We further investigate the merit of the multimedia-centricity of our approach in
contrast to utilizing single-modality approaches.
As shown in Figure 3.6,
with
the right topical configuration,
melange_mm
is the best performer with respect to
both venue type precision and recall.
Figure 3.5 shows the performance of the
general recommended
melange
configuration. Venue type recall is always boosted by
multimodality, with
melange_mm
scoring higher than the single-modality
melange
configuration since interaction round 1 on both datasets, remaining higher up until
very late interaction rounds (up to 50) which go beyond the 10 rounds discussed in
this section. This indicates that multimodality brings additional information gain,
rather than increasing performance by virtue of training 2 models per interaction
round only. In addition, multimodality does not harm interactivity: the cost of fusing
the modalities is minimal and
melange_mm
is shown to be responsive. Taking into
account venue type precision, the chosen fusion approach (Borda count) does not
seem to be universally beneficial. It does improve precision on New York data, but on
Amsterdam data, both
melange_vis
and
melange_txt
outperform
melange_mm
for
most of the interactive session, despite
melange_mm
providing the highest precision in
the first interaction round. The fusion approach could thus be improved. However,
despite this shortcoming,
multimodality is both efficient and effective overall:
it
generally improves the performance given the right choice of topical configuration
and does not prohibit interactivity and responsiveness.
3.7 EXPERIMENTAL RESULTS
53
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Venue type precision
1 510 1 510
T
u
T
v
Venue type precision
melange_mm
melange_txt
melange_vis
wrmf
bprmf
poprank
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Venue type recall
1 510 1 510
T
u
T
v
Venue type recall
Figure 3.6:
Venue type precision and recall comparison of the baselines with
melange
topical configurations, averaged over 10 interaction rounds. The rectan-
gles next to each bar encode the number of topics: in the left rectangle,
the used number of user topics (
T
u
) is highlighted; similarly, the right
rectangle encodes the number of venue topics (
T
v
).
3.7.4
Nature of Generated Recommendations
The difference between the true user-venue distribution of our data and the distribu-
tion of the recommendations up until interaction round 10 is shown in Figure 3.7.
For this evaluation, we only used the New York data. Due to the low user activity on
venue-related social media in Amsterdam and the resulting sparse user-venue matrix,
we do not want to draw general conclusions about the mainstreamness of venues in
Amsterdam. Figure 3.7 shows that the suggestions provided by
melange_mm
match
the true distribution reasonably well.
The
wrmf
baseline, curiously, appears to be
“not mainstream enough,” providing less suggestions from the top 25% mainstream
venues bin than desired,
while providing too many from the “top 25-50% bin.”
The second collaborative filtering baseline,
bprmf
,
tends to be more mainstream
than desired, providing less off-the-beaten-track recommendation than it should.
54
INSTANTIATING THE MODEL
Figure 3.7:
Mainstreamness comparison between the ground truth user-venue New
York data distribution and the venue recommendation distributions of
the evaluated methods. For each recommendation method
R
, we show
the difference between the probability density functions of the respective
venue recommendations (
f
R
(
V
S
)
) and the ground truth (
f
G
(
U
V
)
).
The popularity-based baseline,
poprank
, is by definition mainstream. Considering
mainstreamness of the suggested venues, melange matches the true distribution the
best. Combined with its good performance with respect to venue type precision and
recall,
melange
is more likely to produce off-the-beaten-track recommendations for
the afficionados and popular venues to the mainstream tourist than the competing
approaches.
3.8
CONCLUSION
We have presented City Melange, an interactive and multimodal venue explorer
that connects travellers with venues visited by social
network users exhibiting a
similar taste. Experiments show that our multimodal approach performs better than
both collaborative filtering and popular-vote-based recommenders with respect
to city exploration.
The interactive nature of the method allows the actor to get
personalized recommendations within seconds.
The experiments further suggest
that City Melange is capable of recommending both on- and off-the-beaten-track
locations.
This is due to the recommendations being based on topical
relevance
ranking: mainstream topics will rank higher for mainstream users, specialized topics
will rank higher for afficionados, as long as the actor provides consistent feedback.
The two instantiations for New York and Amsterdam demonstrate that City Melange
is capable of interactive, responsive performance on a large metropolis, while also
being capable to guide the interacting users’ exploration meaningfully when data is
rather scarce. City Melange is thus shown to be a well-performing, personalized, and
information-rich venue explorer with the potential of being deployed world-wide.
4
E VA L UAT I NG A N A LY T IC QUA L IT Y
Published as Zahálka et al.:
Analytic Quality:
Evaluation of Performance and Insight in Mul-
timedia Collection Analysis
.
In proceedings of the ACM Multimedia Conference (MM),
pages
231–240, 2015.
55
56
EVALUATING ANALYTIC QUALITY
4.1
INTRODUCTION
The abundance of multimedia collections has given data analysts in diverse fields a
rich new resource. For instance, multimedia collections provide forensic evidence
in cases of child abuse or terrorism.
In many scientific fields, such as physics, new
discoveries are made and validated using various observations and simulations, which
are decidedly multimedia data. In the business domain, news media companies often
find entire stories to cover in the content of social media platforms. In arts, multimedia
applications are instrumental in navigating and exploring cultural heritage collections.
In most data analytics use cases, timeliness of the results is a critical factor as insight in
the data has to lead to an action as fast as possible. For instance, a forensics expert has
48 hours to decide whether a suspect should be detained further. Likewise, a media
expert looking for stories connected to a globally-important event like the Charlie
Hebdo attack cannot wait weeks before publishing them.
In such cases, decisions
are based on millions of multimedia data items and thus difficult. In short, analysts
from diverse fields of expertise need to gain understanding of increasingly large and
complex multimedia collections, and they need to gain this understanding fast. So
what is needed to support the analytics process for multimedia collections?
To support multimedia analytics tasks, sophisticated underlying multimedia
analysis tools and techniques are needed.
A solid basis of multimedia analysis tool
has been presented within the multimedia community, including algorithms like
active SVM [
168
] or tools like Caffe [
75
]. In recent years, such multimedia analysis
methods have reached the state of being able to truly support insightful multimedia
analytics: the features are short enough to allow smooth interaction and sufficiently
descriptive to allow the algorithms to operate on high semantic levels with high
accuracy. In combination with hardware developments, time is ripe for multimedia
analytics. However, which multimedia analysis techniques are the best and how to
optimize or develop them further for multimedia analytics?
To answer this question we should carefully consider the goal of data analytics:
gaining understanding about the data. This understanding, or insight, as it is called
in the fields of information visualization and visual analytics [
165
], is complex and
requires interplay of a number of factors. Insight builds up on itself and over time,
requires all or most data at hand and is often serendipitous [
123
]. Interaction is thus
crucial: the analyst needs to navigate the collection through intelligent interactions in
order to gradually build insight and not to get overwhelmed by the scale of the data.
In the case of multimedia data, analytic tasks involve a combination of exploration
and search, and in order to build insight, the analyst needs to be able to organically
move back and forth between the two [
200
]. Hence, it is imperative that techniques
behind multimedia analytics support both exploration and search, as well as take into
account the various aspects of insight.
4.1 INTRODUCTION
57
Benchmark
tasks
Batch
algorithms
Relevance
ranking
Analytic
tasks
Artificial
actors
Performance and
insight over time
User
tasks
Human
experts
User experience
and effectiveness
Analytic quality (
AQ
)
Figure 4.1:
The novelty of analytic quality (
AQ
), the framework proposed in this
chapter.
Evaluation is a necessary part of designing any good-quality system.
The
abovementioned works, while providing a theoretical basis for insight, do not provide
any means of actually evaluating it. In the multimedia community, we are heavily
relying on a number of benchmarks and datasets enabling evaluation of the individual
methods. Examples include MediaEval [
90
], TRECVID [
127
], MSR-Bing IRC [
63
],
or visual sentiment ontology [
18
]. These benchmarks are instrumental in establishing,
comparing, and improving the quality of the analysis. Most benchmarks focus on
the relevance of the results. Indeed, higher relevance improves analytics in general.
However, the underlying user model is only implicit and very simple:
the notion
of relevance is fixed, users process the top ranked results only, and the time for the
analysis is unlimited. Do the benchmarks paint the complete picture with respect to
the analytic process in which experts are interacting with the data to gain insight?
One way to look at the support of the process of gaining insight is the large
body of multimedia research on human-centered interaction and computing [
70
].
The bulk of the evaluation in this human-centered field is done through user studies,
where groups of real users large enough to yield statistically significant results typically
assess two well-defined conditions through detailed interviews or questionnaires. This
methodology allows for gauging user experience and effectiveness of the evaluated
method with regard to the user tasks. From the perspective of multimedia analytics
systems design, however, the space of options and possible design paths for future
systems is vast and not all these paths can be explored by full user studies. Human-
computer interaction studies are appropriate when close to a final design, but different
mechanisms are needed in earlier stages.
In order to be truly able to design and assess the analytic potential of multimedia
analysis techniques, a new evaluation paradigm is needed. It should not only evaluate
the relevance of the returned results, but also the insight gain and the time needed to
acquire it. In this chapter, we therefore propose analytic quality (
AQ
), a time-based
58
EVALUATING ANALYTIC QUALITY
evaluation framework which evaluates the system performance and at the same time
estimates the user insight.
AQ
uses a novel artificial user model, which simulates
the behaviour of an analyst striving to gain data understanding.
As illustrated in
Figure 4.1,
AQ
brings the worlds of relevance-based benchmarks and user studies
closer together.
Hence,
AQ
is a significant first step towards evaluation covering
both performance and insight aspects of multimedia analytics.
The rest of the chapter is organized as follows.
Section 4.2 summarizes the
related work.
In Section 4.3,
the
AQ
framework is developed along with all
its
constituent components. Section 4.4 showcases
AQ
evaluation. Section 4.5 concludes
the chapter.
4.2
RELATED WORK
This section summarizes the related work on evaluation in the multimedia community
and the related fields such as computer vision or information retrieval. Namely, we
investigate the relevance-based benchmarks, evaluation of interactivity, and existing
work on evaluation paradigms going beyond relevance alone.
The dominant paradigm are relevance-based benchmarks. Evaluation sessions
are usually not timed and fully automatic with no user involvement. The dominant
evaluation metrics are the classic and well-known accuracy, precision, recall, and
average precision.
These gained traction in the information retrieval benchmarks
such as TREC [
37
] and are now also widely used in computer vision benchmarks, for
example ILSVRC [
145
], Pascal VOC [
43
], or TRECVID [
127
]. Some MediaEval
tracks also involve these metrics [
42
].
For a good reason:
the main reason for
employing a multimedia analysis algorithm is to get results relevant to our intent.
Gaining insight rarely boils down to just going over a list of retrieved results.
A visual analytics study by North et al. shows that insight-based evaluation provides
richer feedback, and thus much more valuable lessons to the method designers, than
the feedback obtained from benchmark-based evaluation [
124
]. The human cognitive
model of visual analytics by Green et al. sheds light on user insight by establishing
that a human analyst keeps a categorical model with up to
7
˘
2
categories when
reasoning about new data[
54
].
The carrier of analyst’s multidimensional intent is
interaction,
extensively investigated by Pike et al.
[
132
].
Interactivity is actively
researched also by the multimedia community. The key techniques are summarized
in a survey by Thomée and Lew [
167
].
Considering evaluation,
interactivity is
explicitly considered for example by the Video Browser Showdown, a competition
between user-centered video search engines [
148
]. Extending the visual analytics
and interaction theory, Zahálka and Worring model multimedia analytics insight
as a set of categories of relevance defined by the analyst herself [
200
].
However,
[
200
] presents only a theoretical model and does not provide any means of actually
4.2 RELATED WORK
59
evaluating system performance and multimedia analytic quality.
All in all, while
relevance rightfully takes a spotlight with regard to insight, there are more aspects
that contribute to the user’s understanding of the data.
The multimedia-related fields are well-aware of the imperfections of relevance
alone. Indeed, the gap between relevance metrics and user preferences was confirmed
by Sanderson et al.
[
146
].
Numerous metrics expand the classic binary relevance
paradigm, mostly originating in the field of information retrieval.
The notion of
graded relevance is embodied in discounted cumulative gain (DCG) by Järvelin and
Kekäläinen [
72
].
The expected reciprocal rank (ERR) of Chapelle et al.
extends
graded relevance to include a simple user model, measuring retrieval quality as the
inverse expected effort by the user to satisfy her information need [
24
]. Smucker and
Clarke introduce the crucially important notion of time to IR effectiveness measures,
making the notion of information gain dependent on time the user needs to reach
the item, rather than its position in the list alone [
155
]. By modelling the correlation
between the position of the item in the list and the time the user needs to reach
it, this work is one of the first ones to consider time explicitly.
These metrics are
already being used in text-based benchmarks, including a number of TREC tracks,
for example the federated Web search track [
37
]. Research on evaluation reaching
beyond simple benchmark relevance is thus gaining momentum in recent years.
Multimedia analytics tasks consist not only of search, but also of exploration.
Hence, to truly evaluate the analytic capabilities of multimedia analysis methods, the
exploration component has to be taken into account.
In the multimedia domain,
exploration is strongly tied to summarization: the analyst gains understanding about
the structure of the collection by seeing relevant, representative, and diverse results.
The metrics evaluating the quality of summarization again originate in the field of
information retrieval, the most prominent examples including BLEU by Papineni et
al. [
130
], ROUGE by Lin [
104
], Meteor by Lavie and Agarwal [
91
], and pyramid
score by Nenkova et al.
[
119
].
CIDEr by Vedantam et al.
automatically measures
image description consensus [
173
].
Some of those metrics have been adapted by
the multimedia community.
VERT by Li and Merialdo is an extension of BLEU
and ROUGE [
100
], while Rudinac et al. have extended the pyramid score to visual
summaries [
144
]. The increasing importance of diversity is reflected in a number
of benchmarks, such as ImageCLEF 2009 [95] or MediaEval Diverse Social Images
[67]. The latter resulted in Div400, a diversity benchmark dataset [68].
Overall, the research on evaluation metrics for multimedia is active. There are
metrics covering aspects of either search or exploration. Yet multimedia analytics
is an intricate interplay of both.
The main drawback we perceive in most current
paradigms is the absence of interaction. Currently, each evaluated method analyzes
the entire collection using a relevance indication (e.g., a query or class annotation)
and returns a list of results, the entirety of which is used to compute the respective
relevance metric. In addition, the notion of relevance is static from the beginning to
60
EVALUATING ANALYTIC QUALITY
Exploration
Search
Browsing
Structuring
Summarization
Finding relevant
items
Finding “needles
in the haystack”
Ranking
Figure 4.2:
The exploration-search axis with example multimedia analytics (sub)tasks
[200].
the end. Time is rarely considered, and few approaches take into account the time
the analyst needs to invest to reach a particular result. These shortcomings, combined
with the fact that gaining analytical insight is an open-ended task involving both
exploration and search in all its phases, is the motivation for our evaluation method
developed further in the chapter.
4.3
METHOD
This section presents the
AQ
evaluation framework.
Section 4.3.1 discusses the
analytic task model and its implications for
AQ
. Section 4.3.2 describes the artificial
actors,
i.e.,
the user model
used by
AQ
.
Section 4.3.3 outlines the details of the
AQ
evaluation process.
Section 4.3.4 presents the evaluation measures used by
AQ
.
Finally, Section 4.3.5 provides guidelines for interpreting the results of
AQ
evaluation.
4.3.1
Evaluating analytic tasks
Our evaluation method,
AQ
, is built on the notion of the exploration-search axis
[
200
], illustrated in Figure 4.2. A user analyzing a collection builds insight over time,
i.e., her needs, intent, and the notion of relevance are changing over the course of
the analysis. Moreover, the user will be tilting back and forth between exploration
and search as the insight builds up. To complete her analytic task, the user will be
interacting with the collection, undertaking (sub)tasks based on the current notion of
relevance. Examples of these (sub)tasks are depicted on the exploration-search axis
in Figure 4.2. Analytic categorization, i.e., the task of assigning individual items into
categories defined by the analyst, is the umbrella task for the exploration-search axis
task model [
200
]. These categories can be completely different than any categories
associated with the dataset itself, such as class labels or annotations. An example of
such a category could be for example “suspicious activity” in forensic research. This
category can encode multiple aspects in multiple modalities: e.g., presence of firearms
in the image, text description inciting terrorism, or geo location corresponding to
terrorist training camps.
In order to support the evaluation of analytic tasks, the
4.3 METHOD
61
artifical actors in the
AQ
user model need to be able to create analytic categories of
relevance, further denoted simply “categories.”
To define categories, an artificial actor needs to “make sense” of the content
of the individual items in the analyzed collection. For the individual artificial actors
to be able to do that, the items need to be annotated with content annotations, e.g.,
“this image contains a person, a car, and a house”, or “the topics in the text are politics
and USA.” However, in the analytic context, we cannot expect the collection to be
annotated. Hence, we need to collect these annotations ourselves.
To this end, we employ a so called arbiter, i.e., a black box producing content
annotations for each item in the analyzed collection. An arbiter needs to be:
•
Consistent. Inputting the same item twice yields identical annotations.
•
Semantic. A human can interpret the annotations and judge their presence in
the item.
•
Autonomous. The arbiter is self-contained, providing the annotations without
any involvement of the evaluated methods.
•
Mostly accurate. The error rate of the arbiter annotation assignment is as low as
possible.
The consistency condition prevents randomness and ensures repeatability of
the results.
The semantic condition is necessary for
AQ
to simulate actual human
behaviour. It would be technically possible for artificial actors to operate on low-level
machine features,
but given that humans rarely think in terms of these features,
such approach would hardly measure analytic quality.
The autonomous condition
ensures fairness:
an artificial actor explicitly using the same data model as one of
the evaluated methods would have an innate edge over the other evaluated methods.
The requirement for autonomy does not ensure full independence of the annotations.
For example, it may occur that a visual arbiter’s concept dictionary overlaps with
that of one (or more) evaluated methods. Given that the arbiter is autonomous, the
underlying features might be correlated, but are not identical. Bearing in mind that
this correlation is hard to measure, we conjecture that having non-identical data
representations is enough to treat the method’s data model as autonomous from the
arbiter data model for the purposes of evaluation. The fourth condition requires that
the arbiter annotations have to be accurate enough. This condition is also hard to assert.
However, we conjecture that the level of the state of the art in visual and text analysis
algorithms is high enough to provide meaningful annotations. This has been shown
by a number of applications, for example in interactive venue recommendation [
198
].
The arbiter annotation process itself has two steps. The first one is selecting
the arbiter. There is an abundance of excellent tools allowing fast and meaningful
content annotation. In the visual domain, Caffe by Jia et al. can be used to obtain
concepts from a convolutional deep network trained on another dataset [
75
]. In the
62
EVALUATING ANALYTIC QUALITY
text domain, a solid option for content annotation is extracting the LDA topics using
the Gensim framework [
177
]. The second step involves crisp assignment of content
annotations to individual items. If the arbiter produces scores for each annotation,
these need to be thresholded to determine annotations present on each item.
An
example of such crisp assignment is assigning those concept annotations reaching
at least 80% of the maximum confidence score for each item.
Once each item is
associated with arbiter annotations, the candidate analytic categories for the actors
can be generated.
The candidate categories (
C
c
) are created from the arbiter annotations as follows.
Let
A
i
=
t
a
1
, a
2
,
¨ ¨ ¨
, a
|
A
i
|
u
be the set of visual,
text,
and metadata annotations
associated with item
i
in the analyzed multimedia collection
I
. Each category
C
P
C
c
is composed of the arbiter annotations associated with the category (
A
C
) and the set
of all items belonging to the category (denoted
I
C
):
C
=
t
A
C
, I
C
u
(5)
The categories of each item
i
P
I
correspond to all subsets of
A
i
except the
empty set. Indeed, if we take for instance an item
i
with
A
i
=
tblood
,
firearmsu
,
we want to associate it not only with the category
tblood
,
firearmsu
, but also the
category
tbloodu
and the category
tfirearmsu
. Let
P(A
i
)
denote the power set of
A
i
and
A
I
=
Ť
i
P
I
P(A
i
)
.
C
c
is then:
C
c
=
ď
A
C
P
A
I
z
∅
t
A
C
,
t
i
|
i
P
I
^
A
C
Ď
A
i
uu
(6)
If necessary,
C
c
can be further pruned to contain only those categories
C
P
C
c
for which
|
I
C
|
is above a certain threshold. All actors will draw their categories from
C
c
. In the further text, random draw from
C
c
is a shorthand for uniformly drawing
an item from the set and removing it.
The removal is done to foster the diversity
of the individual artificial actors. The resulting set of candidate categories
C
c
is the
source of analytic categories for the artificial actors described in the next section.
4.3.2
Artificial actors
The
AQ
user model operates with artificial actors, i.e., computer agents interacting
with the evaluated method and simulating user behaviour. The artificial actors are
built on three cornerstones: analytic categories, changing notion of relevance, and
limited time. The analytic categories are obtained from the candidate categories set
(
C
c
) defined in Section 4.3.1. The actors change their categories of relevance over
time, modelling the dynamic nature of insight. The limited time adresses the real-life
need for timely analysis.
The conjunction of these factors addresses the aspects of
4.3 METHOD
63
insight by North [
123
].
Providing arbiter annotations of multiple modalities and
defining analytic categories as compounds of these annotations simulates complexity,
since the actors are using all
of the data channels or at least most of them.
The
artificial actor model allows for both incremental evolution of the categories (depth)
and abrupt category changes (serendipity). Relevance is embodied in the very mode of
interaction, where each actor guides the evaluation session based on what items are
relevant to the current category definitions. In this section, we describe the four-step
probabilistic process of generating the artificial actors.
Step 1:
Initial actor categories
.
In the first step,
the set of categories of
relevance (
C
a
) is established for each actor
a
P
A
.
As mentioned in Section 4.2, a
study by Green et al.
indicates that a human analyst can operate with up to
7
˘
2
categories of relevance [
54
]. Each artificial actor’s number of initial categories is thus
an integer uniformly drawn from the
[
1, 9
]
interval. Each of the initial categories is
in turn uniformly drawn from
C
c
.
Step 2: Number of insight changes
.
Once the initial
C
a
is established, we
need to determine the number of insight changes, or breakpoints, the actor will make
throughout the session (denoted as
n
B
). The maximum number of these breakpoints
(
n
max
B
) is a parameter to be chosen based on the domain of expertise: for example, a
casual user browsing celebrity photos will typically have a shorter attention span and
focus than a medical scientist analyzing a medical dataset, resulting in a higher
n
max
B
.
In our method, we account for different kinds of users within the same domain of
expertise. To this end, we propose a simple user behaviour model. A portion of the
actors is single-minded, with a clear purpose from start to end, i.e.,
n
max
B
=
n
B
=
0
.
The rest is volatile and have their
n
B
determined in a probabilistic manner. We treat
a breakpoint as a rarely occurring event, and hence draw
n
B
from an exponential
distribution with
λ
=
ln 10
n
max
B
. This distribution ensures that increasingly large values
of
n
B
are drawn with decreasing probability (a number of volatile actors will actually
be single-minded with
n
B
=
0
). The value of
λ
ensures that 90% of the actors will
have their
n
B
ă
n
max
B
. The remaining 10% accounts for the unpredictability of user
behaviour; we cannot assume a crisp upper bound for
n
B
in any domain.
Step 3:
Insight change times
.
For each breakpoint
b
P t
b
1
, . . . , b
n
B
u
,
we
need to determine the time when it occurs (
t
b
). The value of
t
b
is again probabilistic:
since insight is serendipitous, occuring unexpectedly [
123
], it is not fully predictable.
However, insight is also deep and it takes time to build it [
123
].
Thus, given
n
B
,
we can expect the session to be divided in
n
B
+
1
segments of roughly equal length.
Let
t
eq
seg
=
t
s
n
B
+
1
denote the length of each segment assuming the breakpoints are
equidistant.
For the
i
-th breakpoint
b
i
,
we draw
t
b
from a normal
distribution
N
i
(
µ
i
,
σ
)
, setting
µ
i
=
i
¨
t
eq
seg
and
σ
=
1
6
t
eq
seg
. The value of
µ
i
expresses the centering
of individual
breakpoints on equidistant time ticks.
The value of
σ
is motivated
by the desire to leave sufficient time between breakpoints to build up insight (the
64
EVALUATING ANALYTIC QUALITY
“deep” characteristic). To assert sufficient segment length, we want the majority of
the
t
b
draws to fall within
1
2
t
eq
seg
seconds of the mean.
Using the three sigma rule,
setting
3
σ
=
1
2
t
eq
seg
ensures that 99.7% of the drawn values fall within the desired
time interval.
Step 4: Insight change actions
. The last aspect of the actor’s insight scenario
to be determined is the type of the insight change and the associated action. These
actions have to account for both depth and serendipity of building insight:
the
changes can range from incremental to abrupt [
123
].
Thus,
in our actor model,
we account for 6 distinct insight change events with equal probabilities for each
breakpoint:
Action 1: Add category. If
|
C
a
| ă
9
, a category is randomly drawn from
C
c
and
added to the
C
a
. Otherwise, “replace category” is performed.
Action 2:
Remove category.
If
|
C
a
| ą
1
, a category is uniformly drawn from
C
a
and removed from the set. Otherwise, “replace category” is performed, with the
removal step being enforced.
Action 3:
Replace category.
Performs “remove category” followed by “add
category.”
Action 4: Expand category.
Replaces category
C
P
C
a
with a category whose
associated annotations are a superset of those associated with
C
.
For example, the
{
dog
} category will get replaced by the {
dog
,
house
} category.
C
sup
is established
as the set of “annotation superset” categories:
C
sup
=
t
C
k
P
C
c
| D
C
P
C
a
:
A
C
Ă
A
C
k
u
(7)
Then, a category is uniformly drawn from
C
sup
, replacing that category in
C
a
whose
annotation superset it corresponds to.
If there are multiple such categories in
C
a
,
one is selected randomly with uniform probability. If
C
sup
= ∅
, “add category” is
performed instead.
Action 5: Reduce category — Replaces category
C
P
C
a
with a category whose
associated annotations are a subset of those associated with
C
.
For example,
the
{
forest
,
river
} category will get replaced by the {
forest
} category. The reduction
process is an analogy to the expand process:
C
sub
is established as the set of “annotation
subset” categories:
C
sub
=
t
C
k
P
C
c
| D
C
P
C
a
:
A
C
Ą
A
C
k
u
(8)
Then, a category is uniformly drawn from
C
sub
to replace a category whose annota-
tion subset it corresponds to.
If there are multiple candidates for replacement, the
reduced category is again uniformly drawn. If
C
sub
= ∅
, remove category is performed
instead.
4.3 METHOD
65
Action 6: Change category — Replaces category
C
P
C
a
with a category
C
i
P
C
c
whose associated annotations are of the same size and contain at least one annotation
associated with category
C
.
For example,
the {
parrot
,
rainforest
} category
will
get replaced by the {
parrot
,
savanna
} category.
C
ch
,
the set of candidate
replacements, is formally defined as:
C
ch
=
"
C
k
P
C
c
| D
C
P
C
a
:
|
A
C
|
=
|
A
C
k
|
A
C
X
A
C
k
‰
∅
*
(9)
Once
C
ch
is established, an existing category
C
P
C
a
is replaced. If
C
ch
= ∅
, replace
category is performed instead.
The artificial actors capture all key aspects of the multimedia analytics process.
They operate with multiple categories of relevance, which are not defined by the
evaluated method, but externally, using arbiter annotations. This simulates the real
user’s ability to define the categories herself. Moreover, the artificial actors address
all enumerable characteristics of insight as defined by North [
123
].
The artificial
actor model thus provides a user model conforming to multimedia analytics task and
interaction theory.
4.3.3
Evaluation pipeline
The pipeline of
AQ
is depicted in Figure 4.3.
In the preparation phase, we need
to generate candidate categories of relevance (cf.
Section 4.3.1) and construct the
artificial actors (cf. Section 4.3.2). Each method is evaluated using the same session
time (
t
s
) and the same set of actors (
A
,
the number of actors to be generated is
denoted
n
A
). The value of
t
s
depends on the domain and/or purpose of the evaluated
method. For instance, the sessions in a system for casual exploration of social network
content will typically be shorter than forensic investigations, warranting a smaller
t
s
.
Considering
n
A
, we posit the default
n
A
=
100
. This value is a trade-off between
having enough actors to capture nuances in the insight gaining process and offset
the stochastic generation process on the one hand, and not having too many so that
the evaluation time stays reasonable.
The third
AQ
parameter to be considered is
the time it takes an actor to process one item (
t
a
1
).
This simulates the time cost of
processing individual items by a human analyst, which is certainly higher than if the
machine would be allowed to iterate over results uninhibited. Indeed, even in rapid
serial visual presentation (RSVP), users have been shown to only be able to perform
basic processing (e.g., “is this an orange?”) on 5–15 images per second, as shown
by Van der Corput and Van Wijk [
170
]. To the best of our knowledge, there is no
decisive study on the “correct” value of
t
a
1
. For
AQ
, however, the main implication
is that
t
a
1
remains the same across all methods.
For simplicity, we posit the default
t
a
1
=
1 s
. If necessary for the domain of expertise,
n
A
and
t
a
1
can be both treated as a
66
EVALUATING ANALYTIC QUALITY
Data
Analytic
task
Arbiter annotation
Artificial actor
generation
Evaluated
method
Evaluation
session
Suggest items
Indicate relevance
Time up?
Evaluation
measures
Interpretation
Record
[NO]
[YES]
Figure 4.3: The
AQ
evaluation method pipeline.
free parameter. Once the actors are generated and the session time is established, the
evaluation itself can start.
Each evaluation session is first divided into segments, i.e., time periods between
the insight changes by the actors when the actor’s categories of relevance are constant.
A sub-session is ran for each segment, and each involves two parallel execution threads:
•
Actor thread executes the artificial actor’s interaction with the evaluated method.
•
Observer thread records the progression of the actor.
Each session starts with the first query by the artificial actor and runs unin-
terrupted until
t
s
is reached.
The time the evaluated method takes to produce the
results is thus included in the session time.
The actor thread starts with the evaluated method suggesting items to the
actor. Then, the actor indicates which items are relevant according to the current
categories of relevance.
An item is marked as relevant if it belongs to at least one
category of relevance, and as not relevant otherwise. After processing each item, the
actor sleeps for
t
a
1
seconds before proceeding further to simulate the time needed
by a real user to process the item.
Once the actor has given a complete relevance
indication over all items suggested by a method, a time check is performed. If the
elapsed time is greater or equal to
t
s
, the actor thread stops and records the set of all
4.3 METHOD
67
items (
p
I
(
t
)
) and the set of all relevant items (
p
I
r
(
t
)
) seen by the actor in the segment.
Otherwise, the thread goes back to the first step, asking for new suggestions from
the evaluated method.
The observer thread records the progression of the actor thread at certain
equidistant time points further called ticks. The length of each tick,
t
T
, depends on
t
s
,
and as such is also dependent on the domain and/or purpose of the evaluated method.
At each tick
t
, the observer thread snapshots two sets: the set of all items seen by the
user (
p
I
(
t
)
), and the set of all relevant items seen by the user (
p
I
r
(
t
)
).
Note that the
ticks start counting at the start of the session, rather than the start of each segment.
The recorded values per ticks and segments are used to compute the results of
AQ
.
Once all sessions are complete, the
AQ
measures are computed. The measures,
described in Section 4.3.4, take into account relevance, as well as other characteristics
of multimedia analytic quality,
namely the speed of the method,
the diversity of
the suggested items, and how well is the analyst able to estimate the percentage of
relevant items in the collection. Finally, in Section 4.3.5, we explain how to interpret
the results.
4.3.4
Evaluation measures
In this section, we describe the evaluation measures collected by
AQ
. Since
AQ
is a
time-based evaluation method, each of the measures is a function of time. This allows
for unbiased test of the analytic capabilities of the evaluated methods:
measuring
performance by recall
steps does not reflect the computational
efficiency of the
evaluated method.
AQ
provides a clear overview of the methods’ performance with
respect to both relevance and efficiency.
The natural starting point is judging the relevance of the items suggested to
the user by the evaluated method.
For this, we use the classic metrics which have
been the cornerstone of multimedia analysis evaluation for years: recall and precision.
Let
I
r
be the set of relevant items in the collection and
p
I
r
(
t
)
the set of relevant items
seen by the actor up until time
t
.
I
denotes the entire collection, and
p
I
(
t
)
corresponds
to the set of items seen by the actor up until time
t
. RECALL (
R
(
t
)
) is defined as:
R
(
t
) =
|
p
I
r
(
t
)
|
|
I
r
|
(10)
PRECISION (
P
(
t
)
) is:
P
(
t
) =
|
p
I
r
(
t
)
|
|
p
I
(
t
)
|
(11)
68
EVALUATING ANALYTIC QUALITY
These two metrics have time and again proved their mettle with respect to
judging the relevance of the results. Another analytic consideration is the diversity of
the results, as already considered by the diversity benchmarks and datasets mentioned
in Section 4.2. An exploring analyst will want to see as many different kinds of items
as possible. In general, it is difficult to evaluate diversity. Fortunately, for
AQ
, we
can utilize the arbiter annotations.
Let
A
denote the set of all arbiter annotations,
and
p
A(
t
)
the set of annotations encountered in the seen items up until time
t
. The
measure of
DIVERSITY (
D
(
t
)
) of results shown to the actor is then defined as:
D
(
t
) =
|
p
A(
t
)
|
|
A
|
(12)
A notion which is rarely taken into account by benchmarks is the evaluated
method’s speed with respect to producing the results, despite efficiency and respon-
siveness being a crucial requirement for many multimedia analytics systems. Since the
evaluation sessions in
AQ
run uninterrupted, the time the evaluated method takes to
produce results occupies an important portion of
t
s
and as such should be evaluated.
Let
|
p
I
(
t
)
|
max
=
t
t
t
a
1
u
denote the maximum number of items that the actor could
have processed by time
t
, given the time to process 1 item (
t
a
1
).
The THROUGHPUT
(
T
(
t
)
) measure is then the ratio of the true number of items seen by the actor to the
theoretical maximum:
T
(
t
) =
|
p
I
(
t
)
|
|
p
I
(
t
)
|
max
(13)
The last important concern that we consider in this chapter is the ability to
estimate the ratio of relevant items per category to the size of the entire collection.
This ratio has been shown to greatly affect the perception of precision-recall results
[
66
], so it is imperative that
AQ
reports it. Giving the user a correct impression about
the ratio is also important for analytic purposes.
For example, consider a medical
scientist wanting to establish the percentage of patients with cancer.
An analytic
tool that she uses to explore the collection of body scans should be able to give her a
correct estimate throughout the exploration. Let
RS
C
=
|
I
C
|
|
I
|
denote the size of the set
of the items in category
C
P
C
a
relative to the entire collection
I
and
x
RS
C
(
t
) =
|
p
I
C
(
t
)
|
|
p
I
(
t
)
|
denote the relative size estimate based on what the actor has seen up until time
t
.
Using these values, we can enumerate how much the relative size estimate over- and
underestimates the true value by computing
x
RS
C
(
t
)
RS
C
and
RS
C
x
RS
C
(
t
)
, respectively. For the
purposes of our measure, we treat over- and underestimation equally. The RELEVANCE
PERCENTAGE ESTIMATE (
RPE
(
t
)
) measure for actor
a
is then the minimum of these
two values, averaged over all relevance categories of the actor:
4.3 METHOD
69
RPE
(
t
) =
1
|
C
a
|
ÿ
C
P
C
a
min
x
RP
C
(
t
)
RP
C
,
RP
C
x
RP
C
(
t
)
!
(14)
For each of those measures, a value per each actor needs to be determined. Let
s
X
a
denote the final value of measure
X
for actor
a
.
T
and
D
are computed for the
entire session at once by design, and hence
s
T
a
=
T
(
t
s
)
and
s
D
a
=
D
(
t
s
)
. For
R
,
P
,
and
RPE
, the value for actor
a
is the average over all segments
seg
in the actor’s set
of segments (denoted
S
a
):
s
X
a
=
1
|
S
a
|
ÿ
seg
P
S
a
X
(
t
seg
)
,
X
P t
R, P, RPE
u
(15)
Let
s
X
denote the final value of measure
X
. The final value for each measure
X
P t
R, P, D, T, RPE
u is then the average of values per actor:
s
X
=
1
n
A
ÿ
a
P
A
s
X
a
,
X
P t
R, P, D, T, RPE
u
(16)
The analytic quality measure (
aq
m
) of the evaluated method
m
is then the
vector of the individual final values:
aq
m
= (
s
R,
s
P,
s
D,
s
T,
Ę
RPE
)
(17)
4.3.5
Interpretation
Rather than being a single metric,
AQ
is a collection of metrics. Each of them captures
a different aspect of the analytics process and can be easily interpreted. Apart from the
classic presentation of the results in a table, an especially strong tool for interpreting
results is the time plot, plotting time on the
x
axis and the individual measures on
the
y
axis. For the time plots, individual values are computed per observer time tick,
showing the development of the measures with respect to time with fine detail. The
time frame provides a fair analytic comparison of individual methods by showing the
naked truth about what the analyst sees and can work with at time
t
. The traditional
recall-step-based plots, on the other hand, can easily disguise shortcomings with
respect to key analytic aspects.
For example throughput:
as long as the method
provides more relevant results, it is always shown to be the best, even if it is unusably
slow. This can result in a warped view of the performance of the individual methods.
Comparing two sets of measures to assess ranking of multiple evaluated meth-
ods might be difficult. To tackle this problem, we propose
AQ
svm
, which learns the
70
EVALUATING ANALYTIC QUALITY
key aspects of the interplay between individual
AQ
measures. Let
aq
a
m
denote the
vector of
AQ
measures corresponding to method
m
and actor
a
:
aq
a
m
= (
s
R
a
,
s
P
a
,
s
D
a
,
s
T
a
,
Ę
RPE
a
)
(18)
Computing
AQ
svm
m
for method
m
then involves the following steps:
1.
Collect
AQ
a
rand
, the set containing the
AQ
vectors for all actors
a
P
A
obtained
by a random baseline, which always suggest a uniform random sample from
the collection (without repetition):
AQ
a
rand
=
ď
a
P
A
aq
a
rand
(19)
2.
Collect
AQ
a
M
, the set containing the
AQ
vectors for all actors
a
P
A
for all
evaluated methods
m
P
M
:
AQ
a
M
=
ď
m
P
M
a
P
A
aq
a
m
(20)
3.
Train a linear SVM model using the vectors in
AQ
a
M
as positive training data
and the vectors in
AQ
a
rand
as negative training data.
4.
For each method
m
,
AQ
svm
m
is equal to the SVM score assigned to
aq
m
by the
model trained in the previous step.
The intuition behind
AQ
svm
is as follows. The linear SVM model fits a sep-
arating hyperplane into the space of
AQ
measures such that the margin between
positives (the actual evaluated methods) and the random baseline is maximal.
We
want our evaluated methods to be as far from this separating hyperplane (and thus,
from the random baseline), as possible.
Hence, comparing two methods, the one
with higher
AQ
svm
wins. The difference between the
AQ
svm
values of the random
baseline and the evaluated methods also indicate the benefit of using the methods
over a simple random baseline:
the higher the difference, the better.
AQ
svm
thus
provides easy, powerful, and non-parametric aggregation of the
AQ
measures for
easy comparison of methods.
Overall, using
AQ
measures has a number of distinct advantages. They provide
rich information beyond simple relevance. Taken into account separately, they shed
light on the evaluated methods’ performance with respect to distinct key analytic
aspects. Time plot analysis provides a fair comparison of methods, and it is easy to
compare how the individual methods fare in different stages of the analytic session.
AQ
svm
provides a simple way to aggregate
AQ
measures for the purpose of ranking of
individual methods. Moreover, individual
AQ
measures can be selected or discarded
based on their importance to a specific purpose.
This applies to all interpretation
techniques presented in this section. Combining the time-based nature of the
AQ
measures with the insight-centered artificial actor model employed to obtain them
yields a much broader and richer perspective of the evaluated methods’ capabilities
than the classic benchmark paradigm and shedding light on the potential insight
gain by a real user.
4.4 SHOWCASE
71
Table 4.1: Showcase
AQ
results.
Method
R
P
T
D
RPE
random
0.037
0.014
0.999
0.443
0.405
vis
0.263
0.134
0.532
0.218
0.105
txt
0.186
0.070
0.977
0.397
0.271
mm
0.203
0.138
0.465
0.239
0.108
Table 4.2:
AQ
svm
results for search (precision and recall) and exploration (precision
and diversity).
Method
AQ
svm
, search
AQ
svm
, exploration
random
-0.11
0.12
vis
1.73
0.85
txt
1.02
0.45
mm
1.39
0.87
4.4
SHOWCASE
In this section, we demonstrate the capabilities of
AQ
on an example analytic task.
Note that
AQ
itself is independent of this showcase example, both in terms of task
definition and the evaluated methods.
For the showcase task,
an urban planner
employed by the municipality of Amsterdam has downloaded a dataset of 20,000
Flickr images with their associated text related to Amsterdam. Her task is to quickly
assess neighbourhood decay: deserted and/or poorly maintained buildings, graffiti,
waste, loiterers etc. She wants to discover which neighbourhoods are problematic
and what the problems are.
The analytic session starts with the analyst providing
three relevant examples of a multimedia item capturing a problematic aspect, i.e.,
with a query by example.
Then, the session proceeds with the analyst going over
the multimedia items provided by the analytic system, exploring the collection and
interacting with the system throughout the session, steering the flow of the task by
her current notion of relevance.
In our showcase, we want to see whether it is more beneficial for the analyst
to use a system guiding her using a simple interactive learning algorithm, or if she is
better off just selecting items at random. As designers, we want to see the build-up
of quick insight, setting
t
s
to 15 minutes.
For the other two parameters, number
of actors (
n
A
) and time to process 1 item (
t
a
1
), we use the defaults, i.e.,
n
A
=
100
,
t
a
1
=
1 s
. To evaluate the methods, we employ
AQ
. We are interested in the overall
picture
AQ
paints. Since neighbourhood decay is a wide semantic notion, we will
be looking at the performance of the individual methods with respect to using a
broad image concept/topic dictionary. More specifically, we want to assess individual
72
EVALUATING ANALYTIC QUALITY
methods’ capabilities with respect to both exploration and search, as the analyst in
our case will need both: exploring the city and seeing diverse neighbourhoods, but
also being able to search for specific signs of neighbourhood decay matching her
relevance criteria.
The data processing step involved collecting the arbiter annotations and the
visual and text features to be used for the interactive learning method. Since we are
interested in the methods’ performance on varying images, we opt for arbiter models
that have been shown to work well on natural images and general text corpora. As the
arbiter visual annotations, we used the 1000 ImageNet visual concepts provided by
the Inception convolutional deep net conceived by Szegedy et al. [
162
], the winning
entry of ILSRVC 2014. As the arbiter text annotations, we employed latent Dirichlet
allocation to extract 100 latent topics using the Gensim framework [
177
].
As the
visual features for the interactive learning method, we use 15k concepts provided
by a custom Inception network retrained to provide annotations for all ImageNet
classes with more than 200 examples. The text features for the interactive learning
method are 300 latent topics again extracted using Gensim [177].
The random baseline (further denoted as
random
) simply returns a uniform
random sample of the collection. The interactive learning method we use is a simple
linear SVM model trained on the visual modality only (further denoted as
vis
), a
linear SVM trained on the text modality only (
txt
); or two linear SVM models, one
per modality, whose results are fused using Borda count (
mm
). The interactive learning
method is seeded by three relevant examples provided by the user before the start
of the session. After each relevance indication by the user, the interactive learning
algorithm updates the positives and negatives, retrains the model accordingly, and
provides its item suggestions based on the retrained model.
All methods return 5
results at a time for the user to process. Each item can be seen up to one time, i.e.,
the user sees no repeated suggestions.
The
AQ
results are summarized in Table 4.1. If we would look at the results
through the optics of the classic relevance-based paradigm, the clear winner is
vis
,
winning over all competitors significantly in recall while being a very close second
to
mm
on precision. All intelligent learning approaches dominate the
random
baseline
on precision and recall. However, a broader look at all the components of
AQ
reveals
that there are analytic components like diversity and relevance percentage estimation
where
random
ranks first. These seem to be correlated to throughput, emphasizing
that important elements of analytic quality can be dependent on the performance
of the algorithm.
In addition,
random
, due to it’s fairness in selecting suggestions,
covers a wider range of different kinds of items in the collection. These aspects of
multimedia analytics are neither shown nor enumerated by the classic evaluation
approaches.
To judge the exploration and search capabilities of individual methods, we
obtain the respective
AQ
svm
scores. We aggregate precision and recall for the search
4.4 SHOWCASE
73
AQ
svm
, following the classic relevance-based paradigm. For the exploration
AQ
svm
,
we use precision and diversity.
The
AQ
svm
scores are summarized in Table 4.2.
AQ
svm
confirms the intuition with respect to search,
which is purely relevance-
based:
vis
comes out strongly on top,
mm
second,
txt
third.
The
random
baseline
is scored far from the intelligent learning methods, confirming the big gap with
respect to relevance. In the exploration case,
mm
pulls slightly ahead of
vis
, which
exactly reflects the situation when looking at
D
and
P
individually in Table 4.1. The
gap between
random
and the winner is much smaller than in the case of search. This
reflects that the
random
baseline is actually very strong in one of the characteristics
and it is not as easy to discriminate
random
from the intelligent learning methods as
in the case of search.
AQ
svm
is thus shown to provide a meaningful, comprehensive
purpose-based ranking of methods.
The time plot of individual
AQ
metrics (Figure 4.4) gives additional insights
with respect to the development of the metrics over time. Let
t
=
150 s
be roughly
the boundary of the early stage of exploration.
Drawing this boundary, as shown
in Figure 4.4,
allows easy comparison of individual methods in the early vs.
the
late stage. In the precision plot, we notice that
mm
is strongly dominant in the early
stage.
After that point, the precision of
mm
and
vis
remains similar until the end
of the session.
This can be interpreted as
mm
being very strong in the “pure query
response” phase when the notion of relevance is constant.
At the same time, it is
lackluster in adapting to the insight changes, since it experiences the sharpest drop out
of all intelligent learning methods. This gives a suggestion for improvement: since
the model works well with a small number of positives/negatives and much worse
with more, introducing decay to keep only the last few relevance indications might
improve the performance. The recall plot confirms the modality synergy of the
mm
algorithm in the early stage and shows that
mm
is being significantly dragged down
by the underperforming
txt
method in the later stages. The diversity plot reveals
that the solid
txt
performance is not reflected in the
mm
method at all. The recall and
diversity analyses give yet another suggestion: since the text modality is beneficial
only initially,
switching it off in the
mm
algorithm after a certain time might be
considered to increase performance. As a side effect, this would increase throughput,
which is also desirable.
Overall,
the time plot analysis reveals key strengths and
weaknesses of individual algorithms, providing design insight into what to improve
and what to exploit.
Compared with the classic evaluation taking into account only precision and
recall
or derivations thereof (mAP,
F-measure etc.),
AQ
offers a much broader
overview.
In our case,
it was able to characterize the four evaluated algorithms,
pinpointing which techniques are more suitable for exploration and which for search.
Moreover, the
AQ
analysis highlighted particular aspects to focus on in the case of
mm
method, which can be used to further improve its performance. This would not
have been possible by just comparing individual values across evaluated methods,
74
EVALUATING ANALYTIC QUALITY
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
Diversity
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Recall
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
Precision
random
vis
txt
mm
0
100
200
300
400
500
600
700
800
900
Time (seconds)
0.0
0.2
0.4
0.6
0.8
1.0
Throughput
Figure 4.4:
AQ
time plots with
t
=
150 s
marking the boundary of the early stage
of the session.
as is the case in the classic benchmark paradigm.
AQ
thus not only reveals which
method is the best, but, unlike the classic evaluation approaches, also gives insights
into the why.
4.5 CONCLUSION
75
4.5
CONCLUSION
In this chapter, we presented
AQ
(analytic quality), a novel paradigm for multimedia
analysis method design and evaluation. It significantly expands on the classic relevance
evaluation paradigm by adding a number of real-life analytic considerations to the
table, namely time, interactivity, and user insight.
The time-based nature of
AQ
provides both the method designers and the end users with a picture about the analytic
capabilities in a realistic time-limited setting, removing the strong assumption that the
analyst will go over all the results. Including interactivity and evolving insight into
the user model mimicks real user behaviour. This, in conjunction with using multiple
evaluation metrics, each capturing a different aspect of the method, provides a much
more detailed feedback and conclusions for further design improvement than just
comparing two values of mean average precision. Another distinct advantage is the
modularity of
AQ
. Metrics of interest can be selected for different method purposes,
and new ones can be easily collected from the artificial actor sessions on demand.
Moving multimedia analysis evaluation towards realistic analytic settings involving
end users is certainly desirable.
AQ
is a leap forward on this path, bringing new
challenges to the multimedia community which the computer vision and machine
learning communities cannot yet address.
5
S C A L I NG U P M U LT I M E DI A A N A LY T I C S
Under revision at IEEE Transactions on Multimedia as Zahálka et al.:
Blackthorn:
Large-Scale
Interactive Multimodal Learning.
77
78
SCALING UP MULTIMEDIA ANALYTICS
5.1
INTRODUCTION
Multimedia collections have become sources of knowledge in a large and ever-growing
number of scientific and applied domains. A large research effort is being devoted
to making the knowledge in large-scale collections more available. The dominant
approach revolves around search. Indeed, the search engine is currently the main-
stream interface of interaction with multimedia collections.
The search-centered
retrieval approaches utilize state-of-the-art models, often based on machine learning,
to construct a semantic index of the data and offer interactivity by query-response
pairs. This is suitable for cases when the user has a clear information need and is able
to formulate it as a precise query. However, what if the analyst wants to explore the
collection, looking for the question to ask? What if she wants to structure or catego-
rize the data herself, and not according to the structure given by the search engine’s
index? Such cases require a long process of meaningful interaction with the collection
in order to iteratively build the knowledge according to custom analyst-driven data
categorization.
To offer broad analytic capabilities, multimedia systems should support inter-
active, open-ended tasks where the objective is the analyst’s knowledge gain.
In
her interactive session, the analyst usually alternates between exploration and search,
structuring the data. The analyst’s knowledge gain is thus captured in a categorization
of the collection. Notably, for the categorization to capture insight, it must be defined
by the analyst herself, and not beforehand by the system [
200
]. To truly be able to
support various interactive solutions for tasks enhancing the analyst’s knowledge
gain, we set a number of requirements.
The first group relates to performance and
comprises three main requirements:
•
Interactivity. The user must receive suggested results within seconds at most.
•
Scalability.
The user should be able to interact with even very large-scale
(Web-scale) collections.
•
Availability. The interactions of one user must be handled by only one work-
station.
The interactivity requirement enables iterative knowledge gain: by interacting
with a responsive interface, the user can build up her knowledge step by step based
on the flow of relevant data from the system. Non-responsive interfaces, on the other
hand, often result in the user abandoning the analysis altogether.
The scalability
requirement is especially important in light of the fact that as collections grow to
Web scale, obtaining collection-wide human annotations required by supervised
approaches becomes infeasible. Semi-supervised, user-driven approaches that scale
well do not suffer from the lack of annotations and thus have a potential for large-scale
collection analysis.
The interactivity and scalability requirements combined have
5.1 INTRODUCTION
79
an implication for computational efficiency: for interactivity to scale properly, the
computations must complete in
O
(
N
)
time in the worst case (where
N
is the size of
the collection). Indeed, given that large-scale collections comprise millions to billions
of items,
it is difficult to conceive an approach scaling superlinearly with
N
that
completes in interactive time. The availability requirement is set to make interactive
knowledge gain approaches accessible.
Indeed, it is expensive and wasteful to tie
entire computational clusters or distributed systems down in order to complete an
interactive session. In this work, we consider the availability requirement satisfied
when the method operates on a single contemporary high-end workstation (16 cores,
64 GB RAM).
The second group of requirements relates to the information relevance for the
user and again consists of three requirements:
•
Relevance. The data items suggested by the machine must be relevant to the
user’s request.
•
Comprehensibility.
The machine features must have a semantic meaning that
the user can understand.
•
Adaptivity. The user must not be required to follow a data structure or hierarchy
defined by the system.
The relevance requirement is, unsurprisingly, a necessity for the knowledge
gain: if the user does not receive relevant data, she cannot build up any knowledge,
deems the method useless,
and abandons the analysis.
The comprehensibility re-
quirement relates to the fact that the objective of user knowledge gain cannot be
accomplished without the user understanding the data she interacts with.
Under-
standing the features the machine uses for the analysis enables the user to understand
the interaction process as a whole, in turn enabling the user to input the invaluable
context and her expertise into the process.
The adaptivity requirement addresses
the need for the user to be able to structure the collection herself.
Structuring the
collection is an integral part of user knowledge gain [
123
,
200
]. Requiring the user
to follow a precomputed structure pigeonholes her into a certain way of thinking,
which is undesirable.
Interactive multimodal learning is a good fit for supporting tasks ranging from
open-ended exploration to precise search through analytic categorization, which
has a machine learning counterpart in classification [
200
].
Therefore, interactive
classification adapting to the analyst’s categorization takes the spotlight. Interactive
learning sessions,
using techniques such as relevance feedback or active learning,
elicit training data labels from the user directly and train the model based on them
[
64
].
The user is able to steer the analysis directly through selecting relevant and
non-relevant items based on her expertise.
The iterative nature aligns with the
iterative nature of knowledge build-up. While the interactive learning paradigm is
80
SCALING UP MULTIMEDIA ANALYTICS
valid, the classic relevance feedback and active learning approach [
64
], designed for
much smaller collections than those we face nowadays, must be revised to fit current
needs and meet the six requirements.
Concerning scale, currently the major challenge in the multimedia community
is the Yahoo Flickr Creative Commons 100M (YFCC100M) collection, comprising
over 12 TB of multimedia data [
166
]. Its sheer size places a heavy load on computa-
tional resources and impedes interactivity. In addition, as collection sizes grow, the
discriminativeness of feature vectors decays: more items will have (nearly) identical
feature vectors, making them indistinguishable from each other [
4
,
59
,
140
].
The
body of work on the analysis of the dataset is steadily increasing [
76
,
80
]. YFCC100M
has become the central dataset in a number of benchmarks, e.g., the MediaEval Plac-
ing Task [
89
,
28
], and the topic of workshops such as MMCommons [
10
]. Most of
the work to date, however, employs entire computational clusters for the analysis,
violating the availability requirement.
To the best of our knowledge, no work currently meets all six requirements.
How to support interactive solutions for the wide variety of tasks related to knowledge
gain in multimedia collections satisfying the six requirements is the research question
addressed in this chapter. Therefore, we develop Blackthorn, an efficient interactive
multimodal learning approach for large-scale collections originally introduced in
our previous work [
197
]. Blackthorn, conceptually depicted in Figure 5.1, brings
the following contributions enabling it to fulfill all six requirements outlined above:
1.
A compression method reducing the size of semantic representations by up
to order of hundreds while preserving most of the semantic information and
reducing the number of parameters to minimum.
2.
Incorporation of feature selection methods improving the information preser-
vation capabilities of the compressed representation.
3.
Revision of the interactive learning process for the large-scale case: building up
on the compressed representation and incorporationg good practices, the com-
putational complexity is reduced such that large-scale analysis (
„
100 million
multimedia items) completes in seconds.
The rest of this chapter is organized as follows. Section 5.2 summarizes related
work. Section 5.3 presents the data compression method. Section 5.4 describes the
adaptation and optimizations to the interactive learning process. Section 5.5 outlines
the protocol of the experimental evaluation of our method, while Section 5.6 discusses
the experimental results. Section 5.7 concludes the chapter.
5.2 RELATED WORK
81
Data
Images
Text
Semantic
representation
Compressed
representation
Indicate relevance
Linear
SVM
Relevant
items
End
session
Extract features
Compress
Train
Score
items
Present items
Figure 5.1:
Interactive learning with Blackthorn.
The components of interactive
learning innovated by Blackthorn are marked orange.
5.2
RELATED WORK
In response to the rapid growth of digital collections, a number of approaches fa-
cilitating more efficient search and exploration have been proposed. Informedia is
an early example of a system enabling retrieval, summarization and organisation of
video collections [
179
]. In the years that followed, interactive learning (e.g., relevance
feedback, learning new concepts on the fly) has been proven invaluable for improving
multimedia information retrieval [
64
].
The research efforts of the community on
interactive search and browsing were concentrated around benchmark initiatives
such as VideOlympics [
158
] or the Video Browser Showdown [
149
]. However, the
collections considered in these benchmarks are much smaller than the one considered
in this chapter.
Most of the state-of-the-art approaches employ entire computa-
tional clusters to facilitate interactive or near-interactive analysis of the data. This is
true not only for indexing-based approaches such as the content-based indexing of
YFCC100M [
76
], but also for analytics approaches such as active buckets [
35
]. The
algorithmic efficiency component of these approaches boils down to reducing the
number of items for which the similarity is computed, reducing the dimensionality
of data representation, or simplifying the similarity computation.
Recently, a large body of work has focused on the scalability of image retrieval.
Early work demonstrated that exact
k
-nn search techniques severely suffer from the
curse of dimensionality and that their performance degrades rapidly as soon as the
dimensionality of the vectors becomes high enough [
11
].
Around the turn of the
millennium, the emphasis therefore turned to approximate methods for trading off
speed against accuracy. For example, several recent works from the multimedia and
computer vision communities rely on embedding visual feature vectors in binary
space to compress the data and reduce the distance computation time [
17
,
53
,
74
,
98
,
82
SCALING UP MULTIMEDIA ANALYTICS
122
,
181
,
203
]. Depending on whether the distances between a query and items in the
collection are computed directly in binary space or a real-valued intermediate space,
these approaches can be categorised as symmetric or asymmetric. Each category has
its own merits: performing distance computation directly in binary space is generally
faster, but it sacrifices search accuracy.
LSH is a well-known scalar-based indexing technique [
7
];
it has been suc-
cessfully applied to many contexts (e.g., see [
138
,
161
]).
The NV-tree is another
scalar-based method, which has been shown to significantly outperform LSH [
94
];
the NV-tree only requires one eighth of of the processing time of LSH,
while
only storing about 6 bytes per vector.
Product quantization is a family of vector-
based quantization schemes,
which decompose the high-dimensional
space into
low-dimensional subspaces that are indexed independently,
thus producing very
compact code-words [
73
,
79
,
190
]. Another approach applies Map-Reduce to index
up to 30 billion high-dimensional descriptors in a distributed setting [
118
]. This is
a simple, yet scalable vector-based quantization method which neither requires a
conversion to binary space nor a dissection of the high-dimensional descriptors.
While these approaches for scaling up image retrieval were proven effective in
k
-nearest neighbour search, their applicability in analytic tasks remains limited for
at least two reasons. First, state-of-the-art
k
-NN search approaches such as binary
hashes or product quantization are not suited for classification, which is an essential
component of most analytic platforms. Moreover, in many analytic tasks search and
exploration are alternately performed, which requires updating, summarizing and
re-partitioning of the collection based on user interactions [
200
]. Such operations
require preservation of the original vectors or significant portion of information
contained in them.
The existing approaches do not focus on analytics, but rather
establishing an efficient search structure over the dataset.
Novel
approaches are
needed for building such representations and deploying them efficiently in analytic
tasks.
5.3
DATA REPRESENTATION
The first step in any analysis of a multimedia collection is constructing the data
representation.
In the interactive setting in general
and multimedia analytics in
particular,
it is highly desirable that the user can understand the representation.
Therefore, this chapter focuses on semantic representations that assign semantic labels
to the data in the visual and text modality, for example visual concepts [
145
] or LDA
topics for text [
15
]. Additional information channels, such as geo coordinates, time,
or metadata are semantic per se and can be represented without compression.
The main problem we face with a multimedia collection with 100 million
items is the sheer size of the representation. Assuming 1000 visual concepts, 100 text
5.3 DATA REPRESENTATION
83
topics, and 8 B as the size of one floating-point number, representing a dataset of
100 million items requires roughly 880 GB of RAM, which is prohibitive. This size
must be drastically reduced for the method to fulfill the scalability and availability
requirements. Moreover, the comprehensibility and adaptivity requirements must be
borne in mind. In this section, we describe a data compression method that greatly
reduces the representation size and satisfies the requirements with only modest
information loss.
Fortunately, state-of-the-art semantic label representations of multimedia data
are generally effectively sparse, i.e., most of the concept and topic scores are zero or
negligible. This allows the utilization of sparse representation practices [
188
]. Let
x
denote a feature vector of length
n
f
that represents one multimedia item in a particular
modality. When using a sparse representation, instead of recording all
n
f
features,
we represent each item
X
by a tuple
(
S, I
)
comprising
S
=
t@
x
i
P
x
|
x
i
ą
0
u
, the
set of non-zero feature scores, and
I
=
t@
i
P
[
1, n
f
]
|
x
i
ą
0
u
, the feature indices
corresponding to the scores in
S
. In this section, we describe how to further reduce
the size of
X
by feature selection (Section 5.3.1) and how to compress the selected
features into a compact, efficient representation (sections 5.3.2 and 5.3.3).
We note that sparse representations using semantic labels are not the only ones
used by the state of the art in multimedia analysis and related disciplines.
This is
especially prominent in computer vision: dense representations that represent each
item by the output of an intermediate layer in a convolutional neural network are
widely used. While we design Blackthorn for sparse representations, in this chapter
we also conduct experiments on Blackthorn using dense visual features for the sake
of completeness of the analysis.
5.3.1
Feature Selection
In the general large-scale collection case, the data volume reduction through sparse
encoding alone (i.e., encoding
x
as
(
S, I
)
) is insufficient.
Therefore, to provide a
truly scalable compressed data representation, we must select only a small number
of features per modality to represent each item. Formally, this comprises selecting
t
f
features according to some feature ordering (denoted
O
=
t
o
1
, o
2
, . . . , o
n
f
u
). The
process produces a reduced set of feature scores
S
1
=
t
x
o
1
, x
o
2
, . . . , x
o
t
f
u
and the
corresponding feature IDs
I
1
=
t
o
1
, o
2
, . . . , o
t
f
u
for each item in each modality.
The adaptivity requirement results in the need for an unsupervised feature selection
approach. Given the size of target collections, the approach has to come at minimal
computational cost. In this section, we describe the three feature selection approaches
of Blackthorn.
The first one is the
top-feature
selection:
for each item in each modality,
the top
t
f
features sorted by value are taken to represent the item.
This is a very
straightforward method aligned with the established practices [188].
84
SCALING UP MULTIMEDIA ANALYTICS
The following two feature selection approaches aim to address the value im-
balance across different features: some features may tend to score consistently higher
than others,
but at the same time they might not be the descriptive ones.
In the
visual domain, this has been identified as an issue associated with semantic concepts
[
5
,
143
]. In this work, we combat this value imbalance by considering feature values
not only within one item, but also relative to the other values of that feature in the
dataset. The
thresholded
selection for each feature
f
first determines the significance
threshold for each feature defined as
θ
f
=
µ
f
+
σ
f
, where
µ
f
and
σ
f
are the feature
mean and standard deviation across all items, respectively. Then, for each item
X
,
we consider only those features
x
f
that have high values relative to the other items,
i.e., those which surpass their respective significance threshold (
x
f
ě θ
f
). The item
is represented by the top
t
f
features that pass the threshold, sorted by feature value.
The third method is the
TF-IDF
selection, inspired by the eponymous time-
tested information retrieval method, which has also been successfully deployed in
concept-based video representation [
143
]. For each feature
x
f
of item
X
, the TF-IDF
score
tfidf
(
x
f
)
is described in Equation 21 (
x
is the feature vector,
J
¨
K
is the Iverson
bracket, and
N
is the number of items in the collection
C
):
tfidf
x
f

=
x
f
¨
log

1
+
N
ř
x
P
C
J
x
f
ą µ
f
+
σ
f
K

(21)
The TF portion corresponds to
x
f
, the value of the feature itself.
The IDF
portion determines the feature’s rarity,
representing the ratio of the items in the
collection where the feature is strongly present (using the same significance threshold
as in the thresholded selection):
the lower the ratio,
the more discriminative the
feature is. The features in each item are sorted by
tfidf
(
x
f
)
in descending order, and
as before, the top
t
f
features sorted by value are selected to represent the item. The
TF-IDF selection takes into account both the feature value itself and the distribution
of the feature values for each feature.
5.3.2
Compression and Decompression
After the feature selection is completed and the top
t
f
features per item are selected,
the data must be compressed. This section describes the compression method used by
Blackthorn, as well as the decompression allowing the reconstruction of the features
as dictated by the second requirement for interactive learning representations.
Sparse encoding means that for each of the
t
f
features, we must encode the
feature ID and the feature value. The memory required for representing the top
t
f
features in each modality depends on three parameters: the value of
t
f
, the number of
bits required to encode the feature ID (denoted
b
i
), and the number of bits required
5.3 DATA REPRESENTATION
85
F
i
1
s
1
b
i
bits
b
v,1
bits
I
i
2
. . .
i
t
f
b
i
bits
b
i
bits
R
s
2
s
1
. . .
s
t
f
s
t
f
´
1
b
v
bits
b
v
bits
Figure 5.2:
The ratio representation, encoding the top-valued feature (
F
), feature
IDs (
I
), and ratios between subsequent features (
R
).
to represent the feature value (
b
v
). The value of
b
i
in turn depends on
n
f
, the total
number of features in the original feature set:
b
i
=
r
log
2
n
f
s
. The value of
b
v
depends
on the decimal precision
p
required for the recorded value:
b
v
=
r
p
¨
log
2
10
s
. Note
that this assumes feature values between 0 and 1, which can easily be achieved by
normalization.
The straightforward way to sparsely encode features is to directly encode
the feature value in one bit field and the feature ID in another.
Indeed, this is the
paradigm used in the first version of Blackthorn [
197
]. Whilst this poses no problem
for encoding the feature IDs, the selection of
p
is difficult, which raises an important
issue with respect to accuracy and compactness. When
p
is too low, many features
fall below the decimal precision of
p
and are encoded as zeros, increasing the loss of
information and wasting storage space. When
p
is too high, the information drop is
alleviated, but the compactness of the representation suffers. Dynamic
p
for different
items impairs random access to the features per item. Therefore, a different solution
is needed.
To alleviate the decimal precision problem, we introduce the
ratio represen-
tation
, schematically depicted in Figure 5.2. The ratio representation uses three bit
fields.
The first field,
F
, encodes the feature identifier and value of the strongest
feature directly, as the value of the strongest feature is the starting point of the ratio
representation.
The decimal precision of the strongest feature’s encoding and the
corresponding bit cost are henceforth denoted
p
1
and
b
v,1
, respectively.
For each
successive feature, the
I
bitfield encodes the feature identifiers, and the
R
bitfield
encodes the ratio between the
i
-th feature score (
2
ď
i
ď
n
f
) and the preceding
(
i
´
1
)-th feature score.
Thus,
the
p
parameter does not determine the maximal
decimal precision of the encoded features anymore, but rather the lowest possible
ratio, or the maximal allowed value “drop” between any two features. Note that this
makes the
p
parameter much less constraining, as given that the value drop between
86
SCALING UP MULTIMEDIA ANALYTICS
the features is less than
p
orders,
we are now able to encode feature scores with
arbitrary decimal precision.
The compression operates as follows.
Let
b
v,1
denote the number of bits
required to encode the top feature score in the
F
bitfield and
p
1
the decimal precision
pertaining to the encoding of the top feature. Further, let
[
¨
]
denote the rounding
operation,
_
the binary OR operation,
and
!
the left bit shift operation.
The
construction of the
F
,
I
, and
R
bit fields is described by Equations 22, 23, and 24,
respectively.
F
=
[
10
p
1
¨
s
1
]
_
(
i
1
!
b
v,1
)
(22)
I
=
t
f
ł
k
=
2
(
i
k
!
(
k
´
2
)
¨
b
i
)
(23)
R =
t
f
ł
k
=
2

10
p
¨
s
k
s
k
´
1

!
(
k
´
2
)
¨
b
v

(24)
Most of the original feature vector can be reconstructed by decompressing
F
,
I
,
and
R
, with the features outside the top
t
f
set to 0. Let
^
denote the binary AND
operation and
"
the right aritmetic bit shift operation. The feature ID decompression
for the
k
-th feature is defined in Equation 25 (
k
=
1
) and Equation 26 (
2
ď
k
ď
t
f
).
The feature score decompression for the
k
-th feature is defined in Equation 27 (
k
=
1
)
and Equation 28 (
2
ď
k
ď
t
f
).
δ
(
i
1
)
= F
"
b
v,1
(25)
δ
(
i
k
)
=
(
I
"
((
k
´
2
)
¨
b
i
))
^

2
b
i
´
1

(26)
δ
(
s
1
)
= F
^
2
b
v,1
(27)
δ
(
s
k
)
=
δ
(
s
1
)
k
´
2
ź
j
=
0
(
R
"
j
¨
b
v
)
^
2
b
v
´
1

10
p
(28)
Note that
δ
(
s
k
)
does not allow for equal-time random access to the
k
-th feature
score. This constitutes the computational price for the ratio representation. However,
since the compressed representation is designed to be decompressed sequentially to
obtain the entire feature vector, this is not an issue: we can simply keep the “running
feature score” in one extra variable whilst decompressing. This very minor memory
demand is more than outweighed by the advantage of arbitrary decimal precision
brought by the ratio representation.
5.3 DATA REPRESENTATION
87
ι
F
V
64 bits
i
V
1
s
V
1
F
T
64 bits
i
T
1
s
T
1
I
V
i
V
2
. . .
i
V
φ
+
1
.
.
.
i
V
ιφ´φ
+
2
. . .
i
V
ιφ
+
1
64 bits
R
V
s
V
2
s
V
1
. . .
s
V
φ
+
1
s
V
φ
.
.
.
s
V
ιφ´φ
+
2
s
V
ιφ´φ
+
1
. . .
s
V
ιφ
+
1
s
V
ιφ
64 bits
I
T
i
T
2
. . .
i
T
φ
+
1
.
.
.
i
T
ιφ´ι
+
2
. . .
i
T
ιφ
+
1
64 bits
R
T
s
T
2
s
T
1
. . .
s
T
φ
+
1
s
T
φ
.
.
.
s
T
ιφ´φ
+
2
s
T
ιφ´φ
+
1
. . .
s
T
ι¨φ
+
1
s
T
ιφ
64 bits
Figure 5.3:
The Ratio-64 representation encoding the visual (
V
) and text (
T
) features.
For each modality,
F
encodes the first feature value and ID.
I
and
R
are
each a collection of
ι
64-bit integers. In the
i
-th slot,
I
encodes the
i
-th
feature ID, while
R
encodes the ratio between the
i
-th and the
i
´
1
-th
feature value.
5.3.3
The Ratio-64 Representation
Currently, most machines are 64-bit, making it desirable to match the actual size
of the
F
,
I
and
R
bitfields as defined by Equations 22,
23 and 24,
respectively.
Aligning the bitfields with the 64-bit architecture boils down to choosing
t
f
,
p
1
, and
p
accordingly. The selection must be made bearing in mind
n
f
, the total number of
features, and the corresponding
b
i
.
Encoding
F
in one 64-bit integer is more than sufficient, because
b
v,1
=
64
´
b
i
leaves more than enough bits for high
p
1
:
for example, in the case of feature sets
with
n
f
ď
16384
,
b
i
=
14
, which makes
b
v,1
=
50
, allowing the encoding of the top
feature score with the decimal precision of 15. Encoding
I
is straightforward: each
feature ID requires exactly
b
i
bits, making the total cost of encoding the feature IDs
in
I
equal to
t
f
¨
b
i
. Encoding
R
requires checking the decimal precision required
to encode the minimal ratio between any two features in the dataset and setting
b
v
accordingly. Encoding
R
in
ι
64-bit integers boils down to encoding
φ
=
t
64
b
v
u
feature score ratios per integer.
I
is encoded analogously.
The memory-efficient
Ratio-64 representation
, depicted in Figure 5.3, only
requires
2
ι
+
1
64-bit integers per item and modality.
Table 5.1 compares the
uncompressed representation with the Ratio-64 representation.
With
ι
=
1
,
the
representation is 183.5x smaller than the uncompressed representation and allows
fitting 1.25 billion items into 60 GB of memory. The number of preserved features
depends on the values of
b
i
,
and
b
v
,
which is intrinsically tied to the data itself.
Assuming the space-generous case of
b
i
=
b
v
=
14
(allowing for
p
=
4
and
n
f
up to
88
SCALING UP MULTIMEDIA ANALYTICS
1 item
100M
60 GB
Uncompressed
8.81 kB
880.8 GB
6.8M items
Ratio-64, ι
=
1
48 B
4.8 GB
1250M items
Ratio-64, ι
=
5
176 B
17.6 GB
340.9M items
Ratio-64, ι
=
10
336 B
33.6 GB
178.5M items
Ratio-64, ι
=
15
496 B
49.6 GB
120.9M items
Table 5.1:
Comparison of the memory cost of Ratio-64 and the uncompressed rep-
resentation: the memory cost of 1 item, the memory cost of 100 million
items, and how many items fit into 60 GB.
16,384),
φ
is then equal to 4, enabling the preservation of tens of features per item.
The ratio paradigm ensures that arbitrarily low values can be preserved. Note that the
features are selected per item, not for the entire dataset. Thus, no features get discarded
en bloc based on dataset-wide statistics. The value of
ι
is subject to experimentation in
this work. With its design, the Ratio-64 representation aligns with the requirements
for large-scale interactive multimodal learning set in the introduction.
5.4
INTERACTIVE LEARNING
The compressed representation is a necessary condition for interactive analytics
on large scale multimedia collections, but not a sufficient one.
To facilitate large-
scale interactive multimedia analytics, the interactive learning process itself must be
optimized. In this section, we describe the performance optimizations considered in
this chapter.
An interactive multimodal learning session consists of a number of interaction
rounds, each with 3 steps:
1.
An interactive classifier
C
I
(per modality) is trained on a set of examples provided
by the user.
2.
Using
C
I
, the unlabelled items in the collection are scored.
3.
The top
r
results are returned to the user.
Using late modality fusion,
this
requires to fuse the rankings across modalities to obtain the final top
r
.
The process requires an interactive classifier, i.e., one that can be trained during
each interaction round without violating the requirements outlined in the introduc-
tion, especially interactivity, relevance, and scalability. As a result, not all classifiers
are suitable. For instance, deep nets, which are very much embedded in the state of
the art in multimedia analysis, computer vision, and information retrieval, are out of
the question: they generaly take too long to train on large datasets and require more
than a few judgments provided by the user.
5.4 INTERACTIVE LEARNING
89
A user typically produces tens of examples per interaction round at most. As a
consequence, the set of training examples does not scale with the size of the collection
(
N
). We consider two variants of training. The first one is training on the original
data, loading the uncompressed features for the training samples on demand. This
approach utilizes the full information contained in the features, but random access
to the file containing the uncompressed features (as they cannot be kept in RAM)
may scale sub-optimally with collection size.
The second variant is training on
the decompressed Ratio-64 features.
This provides faster access, but the training
algorithm has access to incomplete feature information due to the information loss
incurred by the Ratio-64 representation. The effect of using the decompressed data
or original, uncompressed data is subject to experimentation in this work.
The time cost of steps 2 and 3 increases dramatically as the collection grows
larger.
Compression and efficient implementation do not alleviate this problem
on their own.
To tackle this problem,
Blackthorn considers two optimizations
of the interactive learning process:
an adaptation of the classifier scoring method
exploiting the structure of the compressed dataset, and modality fusion with reduced
computational overhead. Both follow the established good practices in the multimedia
community.
Adapting the scoring method to suit the compressed Ratio-64 representation
means scoring the items directly in the compressed space without expanding each
of the vectors into the original vector space.
This is straightforward for classifiers
that employ a dot product of a weight vector and the feature vector to score an
item, such as SVM, perceptron, or linear discriminant analysis. For such classifiers,
the dot product computation can be sped up by iterating only over those feature
coordinates actually recorded in the Ratio-64 representation of the item, instead of
all coordinates of the original feature space. Expanding the compressed vector into
the original feature space yields no benefit, since all non-recorded features are 0 and
iterating over them has no effect on the value of the dot product. To demonstrate the
speed-up, consider for instance the linear SVM and its scoring function (Equation 29,
w
is the SVM weight vector and
b
the bias):
σ
(
I
) =
w
T
x
+
b
(29)
Assuming equal computational cost of addition and multiplication for the sake
of simplicity, this costs
2n
f
+
1
arithmetic operations per item and modality. Given
a semantic representation with 1000 visual concepts and 100 text topics, this amounts
to 2001 operations for the visual modality and 201 operations for the text modality.
Using the Ratio-64 representation, only the
φ ¨ ι
encoded features carry any value.
Iterating over those features only yields the scoring function in Ratio-64-compressed
space (σ
c
) defined in Equation 30:
90
SCALING UP MULTIMEDIA ANALYTICS
σ
c
(
I
)
=
t
f
ÿ
k
=
1

w
δ
(
i
k
)
¨ δ
(
s
k
)

+
b
(30)
The feature ID decompressions cost 2 mathematical operations each (cf. Equa-
tion 26), except the first one, which costs 1 operation (cf. Equation 25). The feature
score decompressions cost 4 operations each (cf. Equation 28), except the first one,
which costs 1 operation (cf. Equation 27). On top of the decompression, the score
computation comprises
t
f
+
1
additions and
t
f
multiplications.
This amounts to
8t
f
´
3
mathematical operations per item and modality. Since
t
f
!
n
f
,
σ
c
is faster
than
σ
by up to orders of magnitude.
For example, in the case of
ι
=
1,
φ
=
6
, a
visual concept dictionary of 1000 concepts, and text topic dictionary of 100 topics:
σ
c
performs 37.7x fewer operations in the visual modality and 3.7x fewer operations
in the text modality, while producing exactly the same result.
The second aspect requiring attention is late modality fusion, which has been
long established to perform better than early fusion [
159
]. With a straightforward
approach, this requires
O
(
N log N
)
time, which is higher than the
O
(
N
)
complexity
of scoring. To reduce the total complexity to
O
(
N
)
, we follow the protocol of the
top-
r
list rank aggregation [
41
] in order to obtain the top
r
relevant results.
For
each modality,
the top-scoring
ν
(with
r
ď ν !
n
) results in that modality are
nominated to the final ranking by iterating over the data once and checking whether
each respective item qualifies for the top-
ν
(
O
(
N
)
time). The multimodal ranking
is obtained by performing rank aggregations on the nominated items (
O
(
ν
log
ν
)
time).
Because
ν !
N
, modality fusion is essentially computed in sublinear time.
Optimized modality fusion in combination with efficient scoring ensures that all
stages of interactive learning complete in
O
(
N
)
time.
5.5
EXPERIMENTAL SETUP
In the experimental
evaluation of Blackthorn,
we aim to answer the following
questions:
1.
What is the overall speed-up of Blackthorn compared to state-of-the-art meth-
ods in the interactive learning setting?
2.
How does Blackthorn compare to the state of the art regarding the ability to
produce relevant results?
3.
Which combination of Blackthorn’s parameters yields the best performance?
4.
Does Blackthorn work with dense feature representation as well?
5.
How does Blackthorn scale compared to state-of-the-art methods in the inter-
active learning setting?
5.5 EXPERIMENTAL SETUP
91
Positives
Negatives
Suggestions
Figure 5.4:
Example items involved in the first interaction round performed on the
YFCC100M dataset by the actor interested in items from Prague.
In
[
197
], none of the suggestions were regarded as relevant for the evaluation,
despite the semantically similar content.
In order to evaluate the individual optimizations/parameters proposed in this
chapter and their effect on Blackthorn’s performance, the experiments are conducted
on a number of variants of Blackthorn.
The Blackthorn variants based on the
optimization used are named
bt_#FEATSEL_#TRAIN
.
#FEATSEL
is a label
for the
feature selection used (cf.
Section 5.3.1),
which can have three values:
topft
for
the top-feature selection,
thres
for the thresholded selection,
and
tfidf
for the
TF-IDF selection.
#TRAIN
is a label for the feature variant used for the training of
the interactive learning classifier (cf. Section 5.4), which can have two values:
ucmp
whenever the original, uncompressed features were used for the training and
comp
whenever the compressed features were used. Further, we vary the
ι
(cf. Section 5.3.3)
and
ν
(cf.
Section 5.4) parameters:
ι
=
t
1, 5, 10, 15
u
,
ν
=
t
100, 500, 1000
u
.
The
φ
parameter has been set to
φ
=
6
based on the nature of the data in accordance with
the practice outlined in Section 5.3.3, i. e., checking the decimal precision required
to encode
R
.
In the experiments, we compare Blackthorn with two approaches reflecting
the state of the art. The first one is the standard relevance feedback approach [
64
]
(henceforth denoted
rf_standard
), which is still being used in the interactive learn-
ing scenario to this day.
Given that Blackthorn is essentially optimized relevance
feedback,
sacrificing the accuracy of the feature representation for speed,
this al-
lows us to evaluate how favourable Blackthorn’s accuracy-speed trade-off is.
The
second approach uses product quantization [
73
,
79
,
190
] to compress the features.
Product quantization is the most applicable state-of-the-art technique in the con-
text of interactive learning, despite the fact that interactive learning cannot make
use of its efficient search structure.
Following [
73
], we split the feature vectors in
m
subvectors, replacing each with the respective cluster ID resulting from a sub-
quantizer partitioning the feature space into
k
clusters.
We opted for
m
=
12
and
k
=
1024
.
This choice is motivated by the good practices outlined in [
73
] (large
92
SCALING UP MULTIMEDIA ANALYTICS
k
and small
m
is better than the other way round).
In particular,
m
=
12
ensures
rf_pq
is on par with the efficient configurations on Blackthorn compression-wise,
and
k
=
1024
is reasonable regarding both compression quality (cf. [
73
], Fig. 1) and
computational speed. As the quantization algorithm we use a randomized quantizer
that establishes
k
random samples from the collection as centroids and assigns each
item to the closest centroid. In other words, we perform 1-step random-initialized
k
-means, establishing a Voronoi partition of the feature space.
The choice of this
simple algorithm is dictated by the dimensionality of quantizing a large-scale dataset,
which is both time- and memory-consuming. The resulting representation is then
used in a standard relevance feedback setting.
This approach is further denoted
rf_pq
. Comparing Blackthorn with
rf_pq
pits Blackthorn’s Ratio-64 representation
against a compressed representation widely used in state-of-the-art
k
-NN search.
Note that the features obtained by product quantization violate the comprehensibility
requirement.
Also note that the experiments are meant to gauge the efficiency of
product quantization in the context of interactive learning only,
not to evaluate
product quantization as a whole.
As the instantiation of the interactive classifier
C
I
,
we use linear SVM [
29
]
for all three approaches. The main reason is that linear SVM still remains the most
widely adopted interactive classifier, appearing in a number of recent state-of-the-art
works, e.g., [
85
,
116
]. As discussed in Section 5.4, deep nets, which are currently the
mainstream classifier in multimedia analysis and especially computer vision, take too
long to train and are thus unsuitable in the interactive setting.
All approaches are
implemented in C and use the VLFeat implementation of the SVM [172].
Interactive learning in the multimedia analytics context is notoriously difficult
to evaluate, due to the difficult benchmarking of insight-driven analysis.
Indeed,
every user proceeds to different insights through different means, which is difficult to
capture in objective benchmarks. Since this chapter focuses on the learning process
in general, and not on a particular interactive system, a user study is not an option,
as it is very difficult to isolate the effect of using an improved learning method from
the effect of the interface on the user.
Therefore,
the evaluation of Blackthorn’s
efficiency is inspired by the Analytic Quality paradigm [
199
], employing a large
number of automated computer agents (or actors) simulating user behaviour. These
actors simulate the actions of a human user in the classic relevance feedback scenario:
each round manually selecting relevant and non-relevant items and submitting the
selection to the relevance feedback algorithm, which returns the items deemed most
relevant for the next round of the user’s consideration.
Since there are no annotated datasets suitable for multimedia analytics eval-
uation, let alone ones on the order of hundreds of millions of items, we resort to a
custom evaluation task performed on YFCC100M. The evaluation task is to retrieve
items pertaining to a large city in an interactive session based solely on visual and
text content. This task is inspired by the MediaEval Placing Task [
89
,
28
], one of the
5.5 EXPERIMENTAL SETUP
93
main multimedia benchmark challenges revolving around the YFCC100M dataset.
However,
note that the task is quite difficult.
Its only purpose is to compare the
general interactive learning approaches with each other. In this chapter, we do not
strive to outperform the state-of-the-art localization approaches built for the purpose
of the Placing task.
We define the actors that shall interact with the evaluated algorithms as follows.
First, we sort world cities in descending order by the number of associated items in
the 2016 test dataset of the Placing task. Each actor then corresponds to one city in
the top 50, and its set of relevant items is defined as all items within 1000 kilometers
of the city centre,
corresponding to the largest radius considered in the Placing
task. This large radius has been chosen to counter the problem encountered in the
initial iteration of Blackthorn [
197
] and illustrated in Figure 5.4, where the methods
were penalized for returning reasonable, semantically similar suggestions that did not
come from exactly the same city, but from a culturally and architecturally close city.
For each actor, 50 sessions are conducted. The initial relevance indication in each
comprises 100 uniform random samples from the actor’s relevant items as positives
and 200 uniform random samples from the collection as negatives. In each interaction
round, the evaluated algorithm retrains the model and suggests 25 items to the actor,
i. e.,
r
=
25
for all experiments. Those suggestions that are contained in the actor’s
relevant item set are added to the set of positives for the next interaction round. The
set of negatives for the next round contains 100 uniform random samples from the
collection.
Two datasets were used for the evaluation:
the Placing 2016 test set and
YFCC100M. With 1.5 million items, the Placing test set is rich enough to provide
a challenge for the algorithms and reasonably sized to allow for comparison of
rf_standard
with the Blackthorn variants. We experiment on the Placing test set to
answer the first four experimental questions. The second dataset, YFCC100M, is used
to gauge the scalability of Blackthorn to truly large datasets. Note that YFCC100M is
too big to be handled by
rf_standard
. A persistent approach with the data stored on
disk is not an option, since the amount of I/O operations would cripple any attempt
at interactivity.
To provide a broader, general overview of the algorithms’ performance, two
different semantic representations were used in each of the two modalities for both
datasets. In the visual modality, we extracted the 1000 ILSVRC ImageNet concepts
[
145
] and the 365 Places2 concepts [
206
], both using a GoogLeNet convolutional
neural network [
162
]. In addition, we also experiment on the dense 1024-dimensional
features obtained as the output from the average pooling layer of each respective
GoogLeNet.
This allows us to showcase Blackthorn on dense features which are
commonly used in the computer vision community.
However,
we consider the
dense features auxiliary and focus on the sparse concept features in greater detail.
The reason for this is that the average-pooling features violate the comprehensibility
94
SCALING UP MULTIMEDIA ANALYTICS
requirement posited in the introduction: they are meaningless to the interacting user.
In the text modality, we extracted 100 LDA topics directly from the YFCC100M
corpus, as well as 100 topics obtained by applying the LDA model constructed on
the Wikipedia corpus [
1
].
The LDA extraction was conducted using the Gensim
framework [178].
Bearing in mind the selected feature representations, we instantiate the Ratio-
64 parameters (
b
i
, b
v
, b
v,1
, t
f
,
φ
,
ι
, cf. Section 3.3) as follows. We set these parameters
equally for both modalities for the sake of simplicity. The highest number of features
(
n
f
) is 1024, corresponding to
b
i
=
10
. We set
b
v
=
10
as well, as this is sufficient for
encoding all ratios in all the datasets. The
b
v,1
parameter is set to 54, which allows
for a very generous encoding precision and uses all 64 bits of the top-valued feature
(
F
) bitfield. Given the
b
v
, φ
=
6
and thus, for each modality,
t
f
=
6
ι
+
1
.
To gauge the performance of the evaluated algorithms, we report four evalua-
tion measures reflecting the requirements of an analyst:
•
Time per interaction round
•
Precision at interaction round
•
Recall after 10 interaction rounds
•
Recall over time
Time per interaction round is the alpha and omega of interactivity, not merely
a measure of computational efficiency or convenience for the user. Indeed, for an
algorithm to be interactive, the time per interaction round cannot exceed a couple of
seconds, as dictated by the interactivity requirement. The time per interaction round
encompasses all three steps defined in Section 5.4. Precision at interaction round reflects
the need for the algorithm to produce relevant items in the individual interaction
rounds to engage the analyst. Indeed, if the algorithm goes through long dry spells
with respect to relevance, the analyst quickly loses confidence in its analytic capability,
again increasing the likelihood of the user ending the session prematurely. Recall after
10 interaction rounds reflects the cumulative information gain after the early stage of
the analysis. However, this does not reflect the time needed to arrive at that particular
information gain. Recall over time captures both timely analysis and user insight gain.
Indeed, the more relevant items the analyst sees and the earlier she sees them, the
better. We report the algorithmic recall over time, not taking into account the time
required by the user to process the results. This is due to the user processing time
being highly volatile and influenced by factors beyond the scope of this work, such
as the interface of the system implementing the algorithm.
5.6 RESULTS
AND DISCUSSION
95
IMAGENET (CONCEPTS), YFCC100M
precision
recall
time
rf_standard
0.176
6.62
¨
10
´
4
4.72
s
rf_pq (ν
=
100
)
0.106
1.61
¨
10
´
4
0.03
s
bt_tfidf_ucmp (ι
=
1,
ν
=
1000
)
0.191
6.17
¨
10
´
4
0.09
s
bt_tfidf_comp (ι
=
5,
ν
=
1000
)
0.180
6.19
¨
10
´
4
0.14
s
bt_thres_comp (ι
=
1,
ν
=
100
)
0.154
4.67
¨
10
´
4
0.07
s
IMAGENET (CONCEPTS), WIKIPEDIA
precision
recall
time
rf_standard
0.214
7.98
¨
10
´
4
4.65
s
rf_pq (ν
=
100
)
0.105
1.61
¨
10
´
4
0.03
s
bt_tfidf_comp (ι
=
1,
ν
=
1000
)
0.200
7.44
¨
10
´
4
0.09
s
bt_tfidf_ucmp (ι
=
1,
ν
=
100
)
0.186
7.09
¨
10
´
4
0.06
s
PLACES365 (CONCEPTS), YFCC100M
precision
recall
time
rf_standard
0.184
7.48
¨
10
´
4
3.07
s
rf_pq (ν
=
1000
)
0.104
1.68
¨
10
´
4
0.05
s
bt_tfidf_ucmp (ι
=
10,
ν
=
1000
)
0.193
6.62
¨
10
´
4
0.17
s
bt_thres_ucmp (ι
=
5,
ν
=
1000
)
0.192
6.67
¨
10
´
4
0.10
s
bt_thres_comp (ι
=
1,
ν
=
100
)
0.154
4.69
¨
10
´
4
0.05
s
PLACES365 (CONCEPTS), WIKIPEDIA
precision
recall
time
rf_standard
0.211
8.07
¨
10
´
4
3.07
s
rf_pq (ν
=
1000
)
0.104
1.69
¨
10
´
4
0.05
s
bt_tfidf_ucmp (ι
=
1,
ν
=
500
)
0.202
7.75
¨
10
´
4
0.05
s
bt_thres_ucmp (ι
=
5,
ν
=
1000
)
0.195
7.90
¨
10
´
4
0.10
s
bt_thres_ucmp (ι
=
1,
ν
=
100
)
0.194
7.67
¨
10
´
4
0.04
s
Table 5.2:
Precision, recall at 10 interaction rounds, and time per interaction round
on the placing test dataset, using the sparse concept scores as visual features.
5.6
RESULTS
AND DISCUSSION
Table 5.2 summarizes the precision, recall after 10 interaction rounds, and time per
interaction round using sparse visual features, i.e., the visual concept scores. For the
sake of brevity, we report up to five results per combination of features:
rf_standard
,
the
rf_pq
configuration with the highest precision/recall, the Blackthorn configura-
tion with highest precision, the Blackthorn configuration with highest recall (if it
is different from the previous one), and finally the fastest Blackthorn configuration.
The results clearly reveal that speed and efficiency are Blackthorn’s forte. The fastest
configurations of Blackthorn consistently complete one interaction round in under a
tenth of a second, which is on par with the slightly faster
rf_pq
, and yields a massive
speedup of 61.4–77.5 compared to
rf_standard
’s barely interactive 3–4.5 seconds.
Blackthorn’s speed-up answers the first experimental question quite convincingly.
Regarding the second experimental question, Table 5.2 shows that relevance-
wise, Blackthorn is on par or sometimes better than rf_standard and much better
than
rf_pq
in all cases. This means that the information loss incurred by the Ratio-64
representation is very modest in the worst case and even turned into an information
gain in the best case, making the information loss-speed trade-off quite favourable.
Out of all four feature set combinations, Blackthorn preserves at least 93% of the preci-
sion achieved by
rf_standard
and increases the precision to 108% of
rf_standard
’s
96
SCALING UP MULTIMEDIA ANALYTICS
0
50
100
150
200
250
300
Time (seconds)
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
Recall
rf standard
rf pq
(
ν = 500
)
bt tfidf comp
(
ν = 1000, ι = 1
)
Figure 5.5:
Recall
over
time,
Placing test
dataset,
ImageNet
visual
features,
YFCC100M LDA topics.
in the best case. Regarding recall at 10 interaction rounds, Blackthorn achieves 89–
98% of that achieved by
rf_standard
. However, when recall over time is taken into
account, Figure 5.5 clearly showcases that Blackthorn dominates both
rf_standard
and
rf_pq
already from the early interaction rounds. Note that all highest-scoring
Blackthorn configurations with respect to precision and recall use either TF-IDF or
thresholded feature selection, validating the feature selection approaches presented in
Section 5.3.1. Thus, the answer to the second experimental question is that Black-
thorn is able to provide relevant results comparable or better than the best baseline,
incurring minimal information loss further alleviated by feature selection.
Figure 5.6 plots the effect of the choice of parameters on Blackthorn’s perfor-
mance. Each bar corresponds to the average of all results achieved by configurations
on the given feature combinations with the given parameter (varying the others).
Remarkably, the
ι
parameter has very little impact on precision and recall, and higher
ι
does not appear to translate to better semantic descriptiveness. This is in line with
the human perception of visual and text information: only a few concepts describe
an image;
only a few topics describe the associated text.
Given that higher
ι
di-
rectly translates into increased time per interaction round, it is overall advisable to
choose low values of
ι
. Since the Ratio-64 representation allows easy selection of
φ
minimizing encoding errors, the encoding problems related to the previously free
decimal precision parameter [
197
] are not an issue. Regarding
ν
, higher
ν
tends to
lead to higher recall and mostly also to higher precision (certainly when going from
100 to 500). Similarly to the case of
ι
, higher
ν
leads to higher time per interaction
round. Thus,
ν
can be viewed as a trade-off parameter. Regarding feature selection,
thresholded selection slightly outperforms top-feature selection on precision and
5.6 RESULTS
AND DISCUSSION
97
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
0.25
0.30
ι
— Precision
ι = 1
ι = 5
ι = 10
ι = 15
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0
1
2
3
4
5
6
7
ι
— Recall
×10
−4
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
0.25
ι
— Time/1 round (s)
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
0.25
0.30
ν
— Precision
ν = 100
ν = 500
ν = 1000
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0
1
2
3
4
5
6
7
ν
— Recall
×10
−4
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
ν
— Time/1 round (s)
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Feat. sel. — Precision
topft
thres
tfidf
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0
1
2
3
4
5
6
7
Feat. sel. — Recall
×10
−4
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
0.25
Feat. sel. — Time/1 round (s)
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Train. feat. — Precision
comp
ucmp
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0
1
2
3
4
5
6
7
Train. feat. — Recall
×10
−4
Imagenet
YFCC100M
Imagenet
Wiki
Places365
YFCC100M
Places365
Wiki
0.00
0.05
0.10
0.15
0.20
Train. feat. — Time/1 round (s)
Figure 5.6:
The effect of parameter choice on Blackthorn results (Placing test dataset).
recall, while being itself outperformed by TF-IDF selection. Thresholded selection is
noticeably faster than the others. This is due to it introducing additional zeros in the
features and as a result encoding less information in total. Generally, it is advisable to
pick TF-IDF selection for maximizing precision/recall, and thresholded selection for
maximal speed. Regarding the feature variant used for SVM training, using the un-
compressed features tends to have an edge over the compressed features with respect
to precision and time per interaction round. Overall, while some parameters have
higher impact than others, the parameter choices allow customization of Blackthorn
and design choices for the speed-accuracy trade-off axis.
Table 5.3 summarizes the results of experiments with the dense visual features,
i.e., the outputs of the average-pooling layer of each respective deep net. Blackthorn
results are on par with
rf_standard
,
despite the Ratio-64 representation design
heavily favouring sparse features:
Blackthorn achieves 87–96% of
rf_standard
’s
precision and 85–94% of the recall, whilst maintaining the massive speed-up. Sim-
ilarly to the case of sparse visual features,
rf_pq
is shown to be fast,
but inferior
98
SCALING UP MULTIMEDIA ANALYTICS
IMAGENET (AP), YFCC100M
precision
recall
time
rf_standard
0.191
7.29
¨
10
´
4
4.34
s
rf_pq (ν
=
1000
)
0.106
1.69
¨
10
´
4
0.05
s
bt_thres_comp (ι
=
5,
ν
=
1000
)
0.175
6.87
¨
10
´
4
0.13
s
IMAGENET (AP), WIKIPEDIA
precision
recall
time
rf_standard
0.208
9.37
¨
10
´
4
4.15
s
rf_pq (ν
=
1000
)
0.106
1.72
¨
10
´
4
0.05
s
bt_thres_comp (ι
=
5,
ν
=
1000
)
0.199
8.56
¨
10
´
4
0.13
s
PLACES365 (AP), YFCC100M
precision
recall
time
rf_standard
0.172
5.90
¨
10
´
4
4.44
s
rf_pq (ν
=
1000
)
0.103
1.64
¨
10
´
4
0.05
s
bt_thres_comp (ι
=
5,
ν
=
1000
)
0.150
5.12
¨
10
´
4
0.13
s
PLACES365 (AP), WIKIPEDIA
precision
recall
time
rf_standard
0.195
8.24
¨
10
´
4
4.35
s
rf_pq (ν
=
1000
)
0.103
1.64
¨
10
´
4
0.05
s
bt_thres_comp (ι
=
5,
ν
=
1000
)
0.174
7.02
¨
10
´
4
0.13
s
Table 5.3:
Precision, recall at 10 interaction rounds, and time per interaction round
on the placing test dataset, using the dense average-pooling (AP) visual
features.
with regard to relevance in comparison with both Blackthorn and
rf_standard
.
Interestingly enough, the best Blackthorn results on each combination of features in
the dense case are achieved by the same parameter configuration that suits the dense
features well.
Indeed,
ι
=
5
accounts for the necessity of recording more features
in the dense case compared to the sparse case favouring
ι
=
1
.
The thresholded
feature selection is the one emphasizing features with exceptional values the most,
which is valuable when we need to discard a large number of feature values per
item. Overall, whilst Blackthorn does not bring information gain when using dense
visual features,
we believe the answer to the fourth experimental question is that
even though Blackthorn was not designed for dense features, it performs adequately
on them with respect to relevance while maintaining the speed-up brought by the
efficient Ratio-64 representation.
Regarding scalability, Blackthorn succeeds with respect to interactivity: on
the entire YFCC100M dataset,
the
ι
=
1
configurations take
between 0.8 and
1.1 seconds
per interaction round.
The performance with respect to relevance is
adequate.
The precision at interaction round ranges from 0.09 to 0.39.
Notably,
the higher values in this range surpass those on the smaller Placing test dataset. We
have two explanations for this. Firstly, the Placing test dataset is specifically designed
to offer a challenge in determining places around the world,
unlike the general
YFCC100M dataset. Secondly, the precision numbers report the performance within
the 25 suggestions each round, i.e., the absolute top of the ranking corresponding
to very early precision.
In this case, using the entire YFCC100M dataset offers a
larger pool of relevant candidates, which in the top-25 case appears to outweigh
5.7 CONCLUSION
99
the proportionally increased size of the set of negatives.
Indeed, one actor in the
100M setting corresponds to roughly 4.5 million relevant items on average, which
is three times the size of the entire Placing test dataset — and we still need only
the top 25, because the user’s capacity to process items interactively does not scale
with the dataset.
The recall
after 10 interaction rounds ranges from
7
¨
10
´
6
to
4
¨
10
´
5
.
Note that the apparent small
values are caused by the sheer size of the
actors. The parameter influence on the results on the YFCC100M dataset mirrors
the one on the smaller dataset shown in Figure 5.6,
with one notable exception:
training on the original
uncompressed features (the
ucmp
configuration) suffers
from crippling feature file random access times, which increase the length of one
interaction round to tens of seconds. Thus, in the large-scale case, the model should
be trained on compressed features at all times.
Blackthorn is much stronger than
rf_pq
on the large dataset:
whilst
rf_pq
is slightly faster than Blackthorn at 0.7
seconds per interaction round,
the precision and recall
values of Blackthorn are
much higher:
rf_pq
reaches precision of 0.04–0.05 and recall of
2
¨
10
´
6
–
3
¨
10
´
6
.
These scalability results complement those reported in the previous iteration of
Blackthorn [
197
].
In this work, we have increased the tolerance for the distance
from city center,
focusing on generally semantically similar results,
whereas the
previous results without the increased tolerance correspond to a more specialized,
“needle-in-the-haystack” approach. Overall, Blackthorn is shown to be scalable, as it
is able to produce relevant results in truly interactive time on large-scale data.
5.7
CONCLUSION
In this chapter, we have presented Blackthorn, an efficient interactive multimodal
learning chapterwork which supports full interactive-learning-based analysis of large-
scale collections with up to 100M multimedia items.
The Ratio-64 compression
method of Blackthorn is shown to massively reduce the size of multimodal features.
In addition, it not only preserves most of the information contained in the original
features, but combined with feature selection at times yields better interactive learning
performance than the original features.
Blackthorn yields a massive speed-up in
comparison to the competing relevance feedback algorithms.
The experiments
further show that Blackthorn is suitable for the analysis of Web-scale datasets. It is
able to learn on the fly from the user-provided training samples, and one interaction
round on a collection with 100 million items takes roughly a second.
Its high
efficiency and low resource cost would also support multi-category exploration with
proactive suggestions by the system.
The analysis can be performed on a single
standard high-end workstation with 64 GB RAM and 16 CPU cores. In order to
foster further research, the Blackthorn software package has been made available
as an open-source tool.
In conclusion, Blackthorn is a step forward towards fully
harnessing the wealth of information contained in large-scale multimedia collections.
6
C ONC LU SION
6.1
SUMMARY
This thesis investigates the role of the machine in multimedia analytics, a discipline that
combines visual analytics with multimedia analysis algorithms in order to unlock the
potential of multimedia collections as sources of knowledge in scientific and applied
domains [
27
]. Specifically, the central research question of this thesis is how to enable
the machine to assist the user in the knowledge gain objective of multimedia analytics.
The thesis presents four works contributing towards the answer to this question.
Chapter 2 focuses on how to connect the theories of the constituent fields of
multimedia analytics into a single multimedia analytics model. The size and importance
of visual multimedia collections grew rapidly over the last years, creating a need
for sophisticated multimedia analytics systems enabling large-scale, interactive, and
insightful analysis.
These systems need to integrate the human’s natural expertise
in analyzing multimedia with the machine’s ability to process large-scale data. The
chapter starts off with a comprehensive overview of representation, learning, and
interaction techniques from both the human’s and the machine’s point of view. To this
end, hundreds of references from the related disciplines (visual analytics, information
visualization, computer vision, multimedia information retrieval) have been surveyed.
Based on the survey, a novel general multimedia analytics model is synthesized. The
model features semantic navigation of the collection as its key aspect and multimedia
analytics tasks are placed on the exploration-search axis.
The axis is composed of
both exploration and search in a certain proportion which changes as the analyst
progresses towards insight.
Categorization is proposed as a suitable umbrella task
realizing the exploration-search axis in the model. Finally, the pragmatic gap, defined
as the difference between the tight machine categorization model and the flexible
human categorization model
is identified as a fundamental
multimedia analytics
topic. Thus, this work structures interactions and techniques from visual analytics
and multimedia analysis into a unified model, as well as providing a task model and
challenges for multimedia analytics.
101
102
CONCLUSION
Chapter 3 provides an instantiation of the multimedia analytics model in the domain
of venue recommendation. This chapter proposes, City Melange, an interactive and
multimodal content-based venue explorer, that allows gaining insight into venues in
a particular city. Our framework matches the interacting user to the users of social
media platforms exhibiting similar taste.
The data collection integrates location-
based social networks such as Foursquare with general multimedia sharing platforms
such as Flickr or Picasa.
In City Melange, the user interacts with a set of images
and thus implicitly with the underlying semantics.
The semantic information is
captured through convolutional deep net features in the visual domain and latent
topics extracted using Latent Dirichlet Allocation in the text domain.
These are
further clustered to provide representative user and venue topics.
A linear SVM
model learns the interacting user’s preferences and determines similar users.
The
experiments show that our content-based approach outperforms the user-activity-
based and popular vote baselines even in the early phases of interaction, while also
being able to recommend mainstream venues to mainstream users and off-the-beaten-
track venues to afficionados. City Melange is shown to be a well-performing venue
exploration approach, successfully instantiating the multimedia analytics model in
a domain with highly personal insight resulting from a wide variety of tastes and
preferences of individual travellers.
Chapter 4 addresses the question of how to evaluate multimedia analysis algo-
rithms in a multimedia analytics context.
A novel paradigm for the evaluation of
machine analysis methods for multimedia analytics, analytic quality (
AQ
), is proposed.
AQ
complements the existing evaluation methods based on either machine-driven
benchmarks or user studies. It includes the notion of user insight gain and the time
needed to acquire it, both critical aspects of large-scale analysis of multimedia collec-
tions. To incorporate insight,
AQ
introduces a novel user model. In this model, each
simulated user, or artificial actor, builds its insight over time, at any time operating
with multiple categories of relevance.
The methods employ the actors in timed
sessions to evaluate their analytic capabilities in an objective manner. The artificial
actors interact with each method and steer the course by indicating relevant items
throughout the session based on a pre-defined ground truth.
AQ
measures not only
precision and recall, but also throughput, diversity of the results, and the accuracy
of estimating the percentage of relevant items in the collection. The resulting mea-
sures are shown to provide a wide picture of analytic capabilities of the evaluated
methods and enumerate how their strengths differ for different purposes. The
AQ
time plots provide design suggestions for improving the evaluated methods.
AQ
is
demonstrated to be more insightful than the classic benchmark evaluation paradigm
both in terms of method comparison and suggestions for further design.
Finally,
Chapter 5 focuses on the problem of scaling automatic multimedia
analysis algorithms to large-scale collections such that they are both relevant and
6.2 REFLECTIONS
AND FUTURE WORK
103
interactive. To that end, Chapter 5 presents Blackthorn, an efficient interactive multi-
modal learning approach facilitating analysis of large-scale multimedia collections of
up to 100 million items on a single high-end workstation. To reach this performance,
Blackthorn features efficient data compression, feature selection, and optimizations to
the interactive learning process. The Ratio-64 data representation introduced in this
work only costs tens of bytes per item, yet it preserves most of the visual and textual
semantic information with good accuracy. The optimized interactive learning model
scores the Ratio-64-compressed data directly, greatly reducing the computational
requirements. The experiments show that Blackthorn is up to 77.5x faster than the
conventional relevance feedback alternative. Blackthorn is also shown to outperform
the baseline with respect to the relevance of results. It vastly outperforms the baseline
on recall over time, which was presented as an important analytic measure in the
AQ
evaluation framework (Chapter 4), and reaches up to 108% of its precision. On
the full YFCC100M dataset, Blackthorn performs one complete interaction round
in roughly one second, thus opening multimedia collections comprising up to 100
million items to fully interactive learning-based analysis.
6.2
REFLECTIONS
AND FUTURE WORK
Given that the field of multimedia analytics is still in its early years, we view this
thesis as early work on the topic of the machine’s role in multimedia analytics. We
have made the following three distinct contributions to the topic:
•
A theoretical multimedia analytics model, connecting visual analytics theory
with multimedia analysis algorithms, presented in Chapter 2.
We have also
successfully instantiated the model in the domain of venue recommendation,
validating the model empirically (Chapter 3).
•
An evaluation framework allowing the designers of automatic multimedia analysis
algorithms to judge the quality of their approach in the multimedia analytics
context, presented in Chapter 4.
•
An efficient large-scale interactive learning framework that can be directly used
in a relevance feedback or active learning scenario in multimedia analytics
systems, deployable on even the largest collections researched by the multimedia
community (Chapter 5).
Bearing in mind these contributions,
we believe this thesis constitutes the
first complete framework for machine algorithms in multimedia analytics: we can
now deploy efficient, highly responsive, and intelligent machine analysis algorithms
in multimedia analytics, we can tackle even very large scale datasets, and we can
evaluate whether we are doing it right.
The topic of the machine in multimedia analytics, however, is far from con-
cluded. In Section 2.4.3 (published in a 2014 paper [
200
]), we present a multimedia
104
CONCLUSION
analytics research agenda comprising 12 questions. In this thesis, we provide direct
answers to two research questions in this agenda: how to evaluate multimedia an-
alytics systems (1d) and how can the performance and speed of active learning be
scaled up (3a). Further, we have contributed towards answering question (2c), how
can the heterogeneous data in multimedia collections be leveraged to improve the
semantic quality of the model. The multimedia, computer vision, and information
retrieval communities have provided for a tremendous progress with respect to the
quality of semantics extracted from multimedia content, contributing to research on
the semantic gap. Many questions are still open, and we believe the agenda remains
valid, as the open questions contain necessary steps to truly unlocking the potential
of multimedia analytics.
Out of the many open challenges for the machine in multimedia analytics,
we consider the pragmatic gap and the information bandwidth of interactions the
biggest and most important ones for future work. The pragmatic gap, defined and
discussed in Chapter 2, is the gap between the properties of a categorization task as
performed by the human and the properties of a categorization task as performed by
the machine. To close the gap, multimedia analytics models must become increasingly
flexible, moving from mere classifiers to adaptive models that are able to follow the
analyst’s train of thought.
Bridging the pragmatic gap will
require much work
in both constituent disciplines of multimedia analytics,
i.e.,
visual
analytics and
multimedia analysis.
The reward,
however,
is accordingly high:
versatile,
truly
intelligent multimedia analytics tools.
The problem of the information bandwidth of interactions is evident from the
hourglass-like structure of multimedia analytics: various interactive interfaces and
multimedia analysis tools that are connected by a very narrow interaction dictionary,
which most of the time boils down to the user only explicitly marking which images
are relevant and which are not. This is both tedious and slow, as the user essentially
needs to go over the collection sequentially,
with the machine component only
determining the order.
We have investigated this topic in two works beyond the
scope of this thesis. Firstly, we have analyzed semantic interactions in the context of
efficient querying of multimedia stored in database systems, which is a step towards
scalable multimedia analytics [
77
]. Secondly, we have introduced a wide palette of
sematic interactions in multimedia pivot tables, a multimedia analytics tool bringing
the familiar spreadsheet and pivot table metaphors to image collections [
187
]. Further
developments in interactions that carry information about many items at the same
time would improve the speed, flow, and feel of multimedia analytics as a whole.
Overall, while the road of multimedia analytics is still wide open, as is the role of
the machine in the journey on it, we believe this thesis significantly contributes to the
first kilometers. We hope that the comprehensive multimedia framework presented
in this thesis empowers further research advancing the machine’s capabilities in
assisting the user with insight gain, in turn also advancing multimedia analytics.
B I B L I OG R A P H Y
[1]
English wikipedia dump.
https://dumps.wikimedia.org/enwiki/
.
Downloaded
on 03-12-2015.
[2]
What is the just for you feature?
https://www.tripadvisorsupport.com/hc/
en-us/articles/203825386-What-is-the-Just-for-You-feature-
, Accessed
in February 2015.
[3]
G. Adomavicius and A. Tuzhilin.
Context-aware recommender systems.
In F. Ricci,
L. Rokach, B. Shapira, and P. B. Kantor, editors, Recommender Systems Handbook, pages
217–253. Springer US, 2011.
[4]
C. C. Aggarwal, A. Hinneburg, and D. A. Keim.
On the Surprising Behavior of Distance
Metrics in High Dimensional Space, pages 420–434.
Springer Berlin Heidelberg, Berlin,
Heidelberg, 2001.
[5]
R. Aly, A. Doherty, D. Hiemstra, and A. Smeaton.
Beyond Shot Retrieval: Searching
for Broadcast News Items Using Language Models of Concepts, pages 241–252.
Springer
Berlin Heidelberg, Berlin, Heidelberg, 2010.
[6]
R.
Amar,
J.
Eagan,
and J.
Stasko.
Low-level
components of analytic activity in
information visualization.
In INFOVIS, pages 111–117, 2005.
[7]
A. Andoni and P. Indyk.
Near-optimal hashing algorithms for approximate nearest
neighbor in high dimensions.
Commun.
ACM, 51(1):117–122, 2008.
[8]
H. Bay, A. Ess, T. Tuytelaars, and L. V. Gool.
SURF: Speeded-up robust features.
Computer Vision and Image Understanding, 110(3):346–359, 2008.
[9]
B. B. Bederson.
PhotoMesa:
A zoomable image browser using quantum treemaps
and bubblemaps.
In ACM Symposium on User Interface Software and Technology (UIST),
pages 71–80, 2001.
[10]
J. Bernd, D. Borth, C. Carrano, J. Choi, B. Elizalde, G. Friedland, L. Gottlieb, K. Ni,
R. Pearce, D. Poland, K. Ashraf, D. A. Shamma, and B. Thomee.
Kickstarting the
Commons: The YFCC100M and the YLI corpora.
MMCommons, pages 1–6, 2015.
[11]
K. S. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft.
When is ‘nearest neighbor’
meaningful? In International Conference on Database Theory (ICDT), 1999.
[12]
I. Biederman. Recognition-by-components: A theory of human image understanding.
Psychological Review, 97(2):115–147, 1987.
[13]
S. Bird, E. Klein, and E. Loper.
Natural language processing with Python.
" O’Reilly
Media, Inc.", 2009.
[14]
A. Biswas and D. Parikh.
Simultaneous active learning of classifiers & attributes via
relative feedback.
In CVPR, pages 644–651, 2013.
[15]
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 3:993–1022,
2003.
105
106
Bibliography
[16]
O. Boiman, E. Shechtman, and M. Irani.
In defense of nearest-neighbor based image
classification.
In CVPR, 2008.
[17]
S. Bondugula, V. Manjunatha, L. S. Davis, and D. Doermann.
Shoe: Sibling hashing
with output embeddings.
In ACM MM, pages 823–826, 2015.
[18]
D. Borth, R. Ji, T. Chen, T. Breuel, and S.-F. Chang.
Large-scale visual sentiment
ontology and detectors using adjective noun pairs.
In ACM MM, pages 223–232,
2013.
[19]
P. Brivio, M. Tarini, and P. Cignoni.
Browsing large image datasets through Voronoi
diagrams.
TVCG, 16(6):1261–1270, 2010.
[20]
R. Burtner, S. Bohn, and D. Payne.
Interactive visual comparison of multimedia data
through type-specific views.
In SPIE Visualisation and Data Analysis (VDA), 2013.
[21]
L. Cao, J. Luo, H. Kautz, and T. S. Huang.
Annotating collections of photos using
hierarchical event and scene models.
In CVPR, 2008.
[22]
G. Carneiro, A. B. Chan, P. J. Moreno, and N. Vasconcelos.
Supervised learning of
semantic classes for image annotation and retrieval.
TPAMI, 29(3):394–410, 2007.
[23]
P. Chandrika and C. V. Jawahar.
Multi modal semantic indexing for image retrieval.
In CIVR, pages 342–349, 2010.
[24]
O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.
Expected reciprocal rank for
graded relevance.
In ACM CIKM, pages 621–630, 2009.
[25]
O. Chapelle, B. Schölkopf, and A. Zien, editors.
Semi-Supervised Learning.
MIT Press,
2006.
[26]
A.-J. Cheng, Y.-Y. Chen, Y.-T. Huang, W. H. Hsu, and H.-Y. M. Liao. Personalized
travel recommendation by mining people attributes from community-contributed
photos.
In ACM MM, pages 83–92, 2011.
[27]
N. A. Chinchor, J. J. Thomas, P. C. Wong, M. G. Christel, and W. Ribarsky.
Multi-
media analysis + visual analytics = multimedia analytics.
TCGA, 30(5):52–60, 2010.
[28]
J. Choi, C. Hauff, O. V. Laere, and B. Thomee.
The Placing task at MediaEval 2015.
MediaEval, 2015.
[29]
C. Cortes and V. Vapnik.
Support-vector networks.
Machine Learning, 20(3):273–297,
1995.
[30]
P. Cremonesi, Y. Koren, and R. Turrin.
Performance of recommender algorithms on
top-n recommendation tasks.
In ACM Conference on Recommender Systems (RecSys),
pages 39–46, 2010.
[31]
N. Dalal and B. Triggs.
Histograms of oriented gradients for human detection.
In
CVPR, pages 886–893, 2005.
[32]
R. Datta, D. Joshi, J. Li, and J. Z. Wang.
Image retrieval: Ideas, influences, and trends
of the new age.
ACM Computing Surveys, 40(2):1–60, 2008.
[33]
O. de Rooij and M. Worring. Browsing video along multiple threads. TMM, 12(2):121–
130, 2010.
[34]
O. de Rooij and M. Worring.
Efficient targeted search using a focus and context
video browser.
ACM TOMCCAP, 8(4):51, 2012.
Bibliography
107
[35]
O. de Rooij and M. Worring.
Active bucket categorization for high recall video
retrieval.
TMM, 15(4):898–907, 2013.
[36]
O. de Rooij, M. Worring, and J. J. van Wijk.
MediaTable: Interactive categorization
of multimedia collections.
TCGA, 30(5):42–51, 2010.
[37]
T. Demeester, D. Trieschnigg, D. Nguyen, K. Zhou, and D. Hiemstra.
Overview of
the TREC 2014 federated Web search track.
In TREC, 2014.
[38]
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete
data via the EM algorithm.
J Royal Statistical Society,
Ser.
B, 39(1):1–38, 1977.
[39]
J.
Deng,
A.
Berg,
S.
Satheesh,
H.
Su,
A.
Khosla,
and F.-F.
Li.
Large scale visual
recognition challenge 2012.
www.image-net.org/challenges/LSVRC/2012.
[40]
C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the
web.
In International Conference on World Wide Web (WWW), pages 613–622, 2001.
[41]
C. Dwork, R. Kumar, M. Naor, and D. Sivakumar.
Rank aggregation methods for
the web.
In WWW, pages 613–622, 2001.
[42]
M.
Eskevich,
R.
Aly,
R.
Ordelman,
S.
Chen,
and G.
J.
F.
Jones.
The search and
hyperlinking task at MediaEval 2013.
In MediaEval, 2013.
[43]
M. Everingham,
L. V. Gool,
C. K. I. Williams,
J. Winn,
and A. Zisserman.
The
PASCAL Visual Object Classes (VOC) challenge.
Int J Comp Vis, 88(2):303–338, 2010.
[44]
R.
Ewerth,
K.
Ballafkir,
M.
Mühling,
D.
Seiler,
and B.
Freisleben.
Long-term
incremental web-supervised learning of visual concepts via random savannas.
TMM,
14(4):1008–1020, 2012.
[45]
J. Fan, Y. Gao, H. Luo, D. A. Keim, and Z. Li.
A novel approach to enable semantic
and visual image summarization for exploratory image search.
In ACM MIR, pages
358–365, 2008.
[46]
H. Fang, G. K. Tam, R. Borgo, A. J. Aubrey, P. W. Grant, P. L. Rosin, C. Wallraven,
D.
Cunningham,
D.
Marshall,
and M.
Chen.
Visualizing natural image statistics.
TVCG, 19(7):1228–1241, 2013.
[47]
T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters, 27(8):861–874,
2005.
[48]
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan.
Object detection
with discriminatively trained part-based models.
TPAMI, 32(9):1627–1645, 2010.
[49]
S. L. Feng, R. Manmatha, and V. Lavrenko.
Multiple Bernoulli relevance models for
image and video annotation.
In CVPR, pages II–1002–II–1009, 2004.
[50]
Z. Gantner, S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme.
MyMediaLite: A
free recommender system library. In ACM Conference on Recommender Systems (RecSys),
2011.
[51]
S.
Gavin.
Tapping big data to make air booking easier.
http://viewfinder.
expedia.com/features/tapping-big-data-to-make-air-booking-easier
,
Accessed in February 2015.
[52]
G.
Ghinea and J.
P.
Thomas.
Quality of perception:
User quality of service in
multimedia presentations.
TMM, 7(4):786–789, 2005.
108
Bibliography
[53]
A. Gordo, F. Perronnin, Y. Gong, and S. Lazebnik.
Asymmetric distances for binary
embeddings.
IEEE TPAMI, 36(1):33–47, 2014.
[54]
T. M. Green, W. Ribarsky, and B. Fisher.
Building and applying a human cognition
model for visual analytics.
SAGE InfoVis, 8(1):1–13, 2009.
[55]
T. L. Griffiths, M. Steyvers, and A. Firl.
Google and the mind:
Predicting fluency
with PageRank.
Psychological Science, 18(12):1069–1076, 2007.
[56]
T. Hastie, R. Tibshirani, and J. Friedman.
The Elements of Statistical Learning: Data
Mining,
Inference,
and Prediction.
Springer, 2nd edition, 2009.
[57]
A. G. Hauptmann and M. A. Smith.
Text, speech and vision for video segmentation:
The Informedia project.
In AAAI Symp Compu Mod for Int Lang and Vis, 1995.
[58]
A. G. Hauptmann, J. J. Wang, W. H. Lin, J. Yang, and M. Christel.
Efficient search:
The Informedia video retrieval system.
In ACM CIVR, pages 543–544, 2008.
[59]
A. Hinneburg, C. C. Aggarwal, and D. A. Keim.
What is the nearest neighbor in
high dimensional spaces? In International Conference on Very Large Data Bases (VLDB),
pages 506–515, 2000.
[60]
G. E. Hinton, S. Osindero, and Y. W. Teh.
A fast learning algorithm for deep belief
nets.
Neural Computation, 18(7):1527–1554, 2006.
[61]
M. Hoffman, F. R. Bach, and D. M. Blei. Online learning for latent dirichlet allocation.
In Advances in neural information processing systems (NIPS), pages 856–864, 2010.
[62]
Y.
Hu,
Y.
Koren,
and C.
Volinsky.
Collaborative filtering for implicit feedback
datasets.
In IEEE International Conference on Data Mining (ICDM), pages 263–272, Dec
2008.
[63]
X.-S. Hua, Y. Ming, and J. Li.
Mining knowledge from clicks:
MSR-Bing image
retrieval challenge.
In IEEE ICMEW, pages 1–4, 2014.
[64]
T. Huang, C. Dagli, S. Rajaram, E. Chang, M. Mandel, G. E. Poliner, and D. Ellis.
Active learning for interactive multimedia retrieval.
Proc.
IEEE, 96(4):648–667, 2008.
[65]
T. S. Huang,
C. K. Dagli,
S. Rajaram,
E. Y. Chang,
M. I. Mandel,
G. E. Poliner,
and D. P. W. Ellis.
Active learning for interactive multimedia retrieval.
Proc IEEE,
96(4):648–667, 2008.
[66]
D. P. Huijsmans and N. Sebe.
How to complete performance graphs in content-based
image retrieval: Add generality and normalize scope.
IEEE TPAMI, 27(2), Feb. 2005.
[67]
B. Ionescu, M. Menéndez, H. Müller, and A. Popescu. Retrieving diverse social images
at MediaEval 2013: Objectives, dataset, and evaluation.
In MediaEval, 2013.
[68]
B. Ionescu, A.-L. Radu, M. Menéndez, H. Müller, A. Popescu, and B. Loni.
Div400:
A social image retrieval result diversification dataset.
In ACM MMSys, pages 29–34,
2014.
[69]
A. Jaffe, M. Naaman, T. Tassa, and M. Davis.
Generating summaries and visualization
for large collections of geo-referenced photographs.
In ACM MIR, pages 89–98, 2006.
[70]
A. Jaimes, N. Sebe, and D. Gatica-Perez.
Human-centered computing: A multimedia
perspective.
In ACM MM, pages 855–864, 2006.
Bibliography
109
[71]
D. Jannach, L. Lerche, F. Gedikli, and G. Bonnin.
What recommenders recommend
– an analysis of accuracy,
popularity,
and sales diversity effects.
In S.
Carberry,
S. Weibelzahl, A. Micarelli, and G. Semeraro, editors, User Modeling,
Adaptation,
and
Personalization, volume 7899 of Lecture Notes in Computer Science, pages 25–37. Springer
Berlin Heidelberg, 2013.
[72]
K. Järvelin and J. Kekäläinen.
Cumulated gain-based evaluation of IR techniques.
ACM TOIS, 20(4):422–446, Oct. 2002.
[73]
H.
Jégou,
M.
Douze,
and C.
Schmid.
Product quantization for nearest neighbor
search.
IEEE TPAMI, 33(1):117–128, 2011.
[74]
H. Jégou, F. Perronnin, M. Douze, J. Sánchez, P. Pérez, and C. Schmid.
Aggregating
local image descriptors into compact codes.
IEEE TPAMI, 34(9):1704–1716, 2012.
[75]
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama,
and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM
MM, pages 675–678, 2014.
[76]
L. Jiang, S.-I. Yu, D. Meng, Y. Yang, T. Mitamura, and A. G. Hauptmann.
Fast and
accurate content-based semantic search in 100M internet videos.
In ACM MM, pages
49–58, 2015.
[77]
B. T. Jónsson, M. Worring, J. Zahálka, S. Rudinac, and L. Amsaleg.
Ten research
questions for scalable multimedia analytics. In Lecture Notes in Computer Science, volume
9517, pages 290–302. Springer, 2016.
[78]
A. J. Joshi, F. Porikli, and N. P. Papanikolopoulos. Scalable active learning for multiclass
image classification.
TPAMI, 34(11):2259–2273, 2012.
[79]
Y. S. Kalantidis and Y. Avrithis.
Locally optimized product quantization for approxi-
mate nearest neighbor search.
In CVPR, 2014.
[80]
S. Kalkowski, C. Schulze, A. Dengel, and D. Borth. Real-time analysis and visualization
of the YFCC100M dataset.
In MMCommons, pages 25–30, 2015.
[81]
S. Kandel, E. Abelson, H. Garcia-Molina, A. Paepcke, and M. Theobald. PhotoSpread:
A spreadsheet for managing photos.
In ACM CHI, pages 1749–1758, 2008.
[82]
D.
A.
Keim,
J.
Kohlhammer,
G.
Ellis,
and F.
Mansmann,
editors.
Mastering The
Information Age - Solving Problems with Visual Analytics.
Eurographics, 2010.
[83]
D.
A.
Keim,
F.
Mansmann,
J.
Schneidewind,
J.
Thomas,
and H.
Ziegler.
Visual
analytics: Scope and challenges.
In Visual Data Mining: Theory,
Techniques and Tools
for Visual Analytics, pages 76–90. Springer, 2008.
[84]
C. Kofler, L. Caballero, M. Menendez, V. Occhialini, and M. Larson.
Near2me: An
authentic and personalized social media-based recommender for travel destinations.
In ACM SIGMM International Workshop on Social Media (WSM), pages 47–52, 2011.
[85]
A. Kovashka, D. Parikh, and K. Grauman.
Whittlesearch: Interactive image search
with relative attribute feedback.
International J ournal of Computer Vision, 115(2):185–
210, 2015.
[86]
A. Krizhevsky,
I. Sutskever,
and G. E. Hinton.
Imagenet classification with deep
convolutional neural networks. In F. Pereira, C. Burges, L. Bottou, and K. Weinberger,
editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. 2012.
110
Bibliography
[87]
T. Kurashima, T. Iwata, T. Hoshide, N. Takaya, and K. Fujimura.
Geo topic model:
Joint modeling of user’s activity area and interests for location recommendation.
In
ACM International Conference on Web Search and Data Mining (WSDM), pages 375–384,
2013.
[88]
C. H. Lampert,
H. Nickish,
and S. Harmeling.
Learning to detect unseen object
classes by between-class attribute transfer.
In CVPR, pages 951–958, 2009.
[89]
M.
Larson,
M.
Soleymani,
P.
Serdyukov,
S.
Rudinac,
C.
Wartena,
V.
Murdock,
G. Friedland, R. Ordelman, and G. J. F. Jones.
Automatic tagging and geotagging
in video collections and communities.
In ACM International Conference on Multimedia
Retrieval (ICMR), pages 51:1–51:8, 2011.
[90]
M. A. Larson, B. Ionescu, X. Anguera, M. Eskevich, P. Korshunov, M. Schedl, M. So-
leymani, G. Petkos, R. F. E. Sutcliffe, J. Choi, and G. J. F. Jones, editors.
MediaEval
Workshop, 2014.
[91]
A. Lavie and A. Agarwal.
Meteor: An automatic metric for mt evaluation with high
levels of correlation with human judgments.
In StatMT, pages 228–231.
[92]
S.
Lazebnik,
C.
Schmid,
and J.
Ponce.
Beyond bags of features:
Spatial
pyramid
matching for recognizing natural scene categories.
In CVPR, pages 2169–2178, 2006.
[93]
Y. LeCun and Y. Bengio.
Convolutional networks for images, speech and time series.
In M. A.
Arbib,
editor,
The Handbook of Brain Theory and Neural Networks,
pages
255–258. The MIT Press, 1995.
[94]
H. Lejsek, B. T. Jónsson, and L. Amsaleg.
NV-Tree: nearest neighbors at the billion
scale.
In ICMR, 2011.
[95]
M.
Lestari Paramita,
M.
Sanderson,
and P.
Clough.
Diversity in photo retrieval:
Overview of the ImageCLEF photo task 2009.
In Multilingual
Information Access
Evaluation II.
Multimedia Experiments, pages 45–59. 2010.
[96]
J. J. Levandoski, M. Sarwat, A. Eldawy, and M. F. Mokbel.
Lars: A location-aware
recommender system.
In IEEE International Conference on Data Engineering (ICDE),
pages 450–461, 2012.
[97]
M. Li and I. K. Sethi.
Confidence-based active learning.
TPAMI, 28(8):1251–1261,
2006.
[98]
P. Li, M. Wang, J. Cheng, C. Xu, and H. Lu.
Spectral hashing with semantically
consistent graph for image indexing.
IEEE TMM, 15(1):141–152, 2013.
[99]
X. Li, C. G. M. Snoek, and M. Worring.
Learning social tag relevance by neighbor
voting.
TMM, 11(7):1–14, 2009.
[100]
X. Li, C. G. M. Snoek, and M. Worring.
Unsupervised multi-feature tag relevance
learning for social image retrieval.
In ACM International Conference on Image and Video
Retrieval, 2010.
[101]
X.
Li,
C.
G.
M.
Snoek,
M.
Worring,
and A.
W.
M.
Smeulders.
Fusing concept
detection and geo context for visual search.
In ACM ICMR, 2012.
[102]
Z.
Li,
E.
Gavves,
K.
E.
A.
van de Sande,
C.
G.
M.
Snoek,
and A.
W.
M.
Smeul-
ders.
Codemaps: Segment, classify and search objects locally.
In IEEE International
Conference on Computer Vision, pages 2136–2143, 2010.
Bibliography
111
[103]
D. Lian, C. Zhao, X. Xie, G. Sun, E. Chen, and Y. Rui.
Geomf: Joint geographical
modeling and matrix factorization for point-of-interest recommendation.
In Proceed-
ings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD ’14, pages 831–840, New York, NY, USA, 2014. ACM.
[104]
C.-Y. Lin.
ROUGE: A package for automatic evaluation of summaries.
In Association
for Computational Linguistics (ACL) Workshop, pages 74–81, 2004.
[105]
B. Liu, Y. Fu, Z. Yao, and H. Xiong.
Learning geographical preferences for point-of-
interest recommendation.
In ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 1043–1051, 2013.
[106]
H.
Liu,
T.
Mei,
J.
Luo,
H.
Li,
and S.
Li.
Finding perfect rendezvous on the go:
Accurate mobile visual localization and its applications to routing. In ACM MM, pages
9–18, 2012.
[107]
H. Liu, X. Xie, X. Tang, Z. W. Li, and W. Y. Ma.
Effective browsing of web image
search results.
In ACM MIR, pages 84–90, 2004.
[108]
S. P. Lloyd.
Least squares quantization in PCM.
IEEE Transactions on Information
Theory (TIT), 28(2):129–137, 1982.
[109]
D. G. Lowe.
Object recognition from local scale-invariant features.
In ICCV, pages
1150–1157, 1999.
[110]
H. Luo, J. Fan, J. Yang, W. Ribarsky, and S. Satoh.
Analyzing large-scale news video
databases to support knowledge visualization and intuitive retrieval.
In VAST, pages
107–114, 2007.
[111]
G. Marchionini.
Exploratory search: From finding to understanding.
Comm ACM,
49(6):41–46, 2006.
[112]
A. H. Meghdadi and P. Irani.
Interactive exploration of surveillance video through
action shot summarization and trajectory visualization.
TVCG, 19(12):2119–2128,
2013.
[113]
T. Mei, Y. Rui, S. Li, and Q. Tian.
Multimedia search reranking: A literature survey.
ACM Computing Surveys, 46(3), 2014.
[114]
P. Mettes, J. C. van Gemert, S. Cappallo, T. Mensink, and C. G. M. Snoek.
Bag-of-
fragments: Selecting and encoding video fragments for event detection and recounting.
In ACM ICMR, pages 427–434, 2015.
[115]
J. L. Mey.
Pragmatics: An Introduction.
Oxford: Blackwell, 2nd edition, 2001.
[116]
I. Mironică, B. Ionescu, J. Uijlings, and N. Sebe. Fisher kernel temporal variation-based
relevance feedback for video retrieval. Computer Vision and Image Understanding, 143:38
– 51, 2016.
Inference and Learning of Graphical Models
Theory and Applications in
Computer Vision and Image Analysis.
[117]
P. Mitra, C. A. Murthy, and S. K. Pal.
A probabilistic active support vector learning
algorithm.
TPAMI, 26(3):413–418, 2004.
[118]
D. Moise, D. Shestakov, G. Gudmundsson, and L. Amsaleg.
Indexing and searching
100m images with map-reduce.
In ICMR, pages 17–24, 2013.
112
Bibliography
[119]
A. Nenkova, R. Passonneau, and K. McKeown.
The pyramid method: Incorporating
human content selection variation in summarization evaluation.
ACM TSLP, 4(2),
May 2007.
[120]
G. P. Nguyen and M. Worring.
Interactive access to large image collections using
similarity-based visualization.
J Vis Lang and Comp, 19(2):203–224, 2008.
[121]
G. P. Nguyen, M. Worring, and A. W. M. Smeulders.
Interactive search by direct
manipulation of dissimilarity space.
TMM, 9(7):1404–1415, 2007.
[122]
M. Norouzi, A. Punjani, and D. Fleet. Fast search in hamming space with multi-index
hashing.
In CVPR, pages 3108–3115, 2012.
[123]
C. North.
Towards measuring visualization insight.
TCGA, 26(3):6–9, 2006.
[124]
C. North, P. Saraiya, and K. Duca.
A comparison of benchmark task and insight
evaluation methods for information visualization.
InfoVis, 10(3):162–181, July 2011.
[125]
A.
Noulas,
S.
Scellato,
N.
Lathia,
and C.
Mascolo.
A random walk around the
city: New venue recommendation in location-based social networks.
In ASE/IEEE
International Conference on Social Computing and ASE/IEEE International Conference on
Privacy,
Security,
Risk and Trust (SOCIALCOM-PASSAT), pages 144–153, 2012.
[126]
A. Oliva and A. Torralba.
Modeling the shape of the scene: A holistic representation
of the spatial envelope.
Int J Comp Vis, 42(3):145–175, 2001.
[127]
P. Over, A. F. Smeaton, and G. Awad.
The TRECVid 2008 BBC Rushes summariza-
tion evaluation.
In ACM TRECVid Video Summarization Workshop, ACM MM, pages
1–20, 2008.
[128]
R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose, M. Scholz, and Q. Yang.
One-class
collaborative filtering.
In IEEE International Conference on Data Mining (ICDM), pages
502–511, Dec 2008.
[129]
Y.
Pang,
Q.
Hao,
Y.
Yuan,
T.
Hu,
R.
Cai,
and L.
Zhang.
Summarizing tourist
destinations by mining user-generated travelogues and photos.
Computer Vision and
Image Understanding, 115(3):352 – 363, 2011.
Special issue on Feature-Oriented Image
and Video Computing for Extracting Contexts and Semantics.
[130]
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
BLEU: A method for automatic
evaluation of machine translation.
In ACL, pages 311–318, 2002.
[131]
F. Perronnin, Y. Liu, J. Sánchez, and H. Poirier.
Large-scale image retrieval with
compressed Fisher vectors.
In CVPR, pages 3384–3391, 2010.
[132]
W. A. Pike, J. Stasko, R. Chang, and T. A. O’Connell.
The science of interaction.
SAGE InfoVis, 8(4):263–274, 2009.
[133]
P.
Pirolli and S.
Card.
The sensemaking process and leverage points for analyst
technology as identified through cognitive task analysis.
In Int Conf Intel Analysis,
2005.
[134]
A. Popescu, G. Grefenstette, and P.-A. Moëllic.
Mining tourist information from
user-supplied collections.
In ACM CIKM, pages 1713–1716, 2009.
[135]
Z. Pousman, J. T. Stasko, and M. Mateas. Casual information visualization: Depictions
of data in everyday life.
TVCG, 13(6):1145–1152, 2007.
Bibliography
113
[136]
N. Quadrianto, K. Kersting, T. Tuytelaars, and W. L. Buntine.
Beyond 2D-grids: A
dependence maximization view on image browsing.
In ACM MIR, pages 339–348,
2010.
[137]
D. Quercia, R. Schifanella, and L. M. Aiello.
The shortest path to happiness:
Rec-
ommending beautiful, quiet, and happy routes in the city.
In ACM Conference on
Hypertext and Social Media (HT), pages 116–125, 2014.
[138]
V. Rao, P. Jain, and C. Jawahar.
Diverse yet efficient retrieval using locality sensitive
hashing.
In ACM ICMR, pages 189–196, 2016.
[139]
S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme.
BPR: Bayesian
personalized ranking from implicit feedback.
In Conference on Uncertainty in Artificial
Intelligence (UAI), pages 452–461, 2009.
[140]
B. Richard.
Adaptive control processes: A guided tour, 1961.
[141]
K. Rodden, W. Basalaj, D. Sinclair, and K. Wood.
Does organisation by similarity
assist image browsing? In ACM CHI, pages 190–197, 2001.
[142]
S. Rudinac, A. Hanjalic, and M. Larson.
Generating visual summaries of geographic
areas using community-contributed images.
IEEE TMM, 15(4):921–932, June 2013.
[143]
S.
Rudinac,
M.
Larson,
and A.
Hanjalic.
Leveraging visual
concepts and query
performance prediction for semantic-theme-based video retrieval. Int J MIR, 1(4):263–
280, 2012.
[144]
S. Rudinac, M. Larson, and A. Hanjalic.
Learning crowdsourced user preferences for
visual summarization of image collections.
IEEE TMM, 15(6), Oct. 2013.
[145]
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,
A.
Khosla,
M.
Bernstein,
A.
C.
Berg,
and L.
Fei-Fei.
Imagenet large scale visual
recognition challenge.
IJCV, 115(3):211–252, 2015.
[146]
M. Sanderson, M. L. Paramita, P. Clough, and E. Kanoulas.
Do user preferences and
evaluation measures line up? In ACM SIGIR, pages 555–562, 2010.
[147]
S. Santini and R. Jain.
Similarity measures.
TPAMI, 21(9):871–883, 1999.
[148]
K. Schoeffmann.
A user-centric media retrieval competition:
The video browser
showdown 2012-2014.
IEEE Multimedia, 21(4):8–13, 2014.
[149]
K. Schoeffmann.
A user-centric media retrieval competition:
The video browser
showdown 2012-2014.
IEEE MM, 21(4):8–13, 2014.
[150]
B. Settles.
Active learning literature survey.
Computer Sciences Technical Report
1648, University of Wisconsin–Madison, 2009.
[151]
Y. Shi,
M. Larson,
and A. Hanjalic.
Collaborative filtering beyond the user-item
matrix:
A survey of the state of the art and future challenges.
ACM Comput.
Surv.,
47(1):3:1–3:45, May 2014.
[152]
Y. Shi, P. Serdyukov, A. Hanjalic, and M. Larson.
Nontrivial landmark recommenda-
tion using geotagged photos.
ACM Trans.
Intell.
Syst.
Technol., 4(3):47:1–47:27, July
2013.
[153]
J. Sivic and A. Zisserman.
Video Google: A text retrieval approach to object matching
in videos.
In ICCV, volume 2, pages 1470–1477, 2003.
114
Bibliography
[154]
A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain.
Content-based
image retrieval at the end of the early years.
TPAMI, 22(12):1349–1380, 2000.
[155]
M. D. Smucker and C. L. A. Clarke.
Time-based calibration of effectiveness measures.
In ACM SIGIR, pages 95–104, 2012.
[156]
C. G. M. Snoek and A. W. M. Smeulders.
Visual-concept search solved? Computer,
43(6):76–78, 2010.
[157]
C. G. M. Snoek and M. Worring.
Concept-based video retrieval.
In Foundations and
Trends in Information Retrieval, 2009.
[158]
C. G. M. Snoek, M. Worring, O. d. Rooij, K. E. A. van de Sande, R. Yan, and A. G.
Hauptmann.
VideOlympics:
Real-time evaluation of multimedia retrieval systems.
IEEE MultiMedia, 15(1):86–91, 2008.
[159]
C. G. M. Snoek, M. Worring, and A. W. M. Smeulders.
Early versus late fusion in
semantic video analysis.
In ACM MM, pages 399–402, 2005.
[160]
H. Steck.
Item popularity and recommendation accuracy.
In ACM Conference on
Recommender Systems (RecSys), pages 125–132, 2011.
[161]
N.
Sundaram,
A.
Turmukhametova,
N.
Satish,
T.
Mostak,
P.
Indyk,
S.
Madden,
and P.
Dubey.
Streaming similarity search over one billion tweets using parallel
locality-sensitive hashing.
VLDB, 6(14):1930–1941, 2013.
[162]
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van-
houcke, and A. Rabinovich.
Going deeper with convolutions.
CVPR, 2015.
[163]
R. Szeliski, N. Snavely, and S. M. Seitz.
Navigating the worldwide community of
photos.
ACM TOMCCAP, 9(1s):47:1–47:4, 2013.
[164]
Y.
Tang,
R.
Salakhutdinov,
and G.
E.
Hinton.
Robust Boltzmann machines for
recognition and denoising.
In CVPR, pages 2264–2271, 2012.
[165]
J. J. Thomas and K. A. Cook.
Illuminating the Path:
The Research and Development
Agenda for Visual Analytics.
Computer Society, 2005.
[166]
B. Thomée, B. Elizalde, D. A. Shamma, K. Ni, G. Friedland, D. Poland, D. Borth,
and L.-J. Li.
YFCC100M: The new data in multimedia research.
Commun.
ACM,
59(2):64–73, 2016.
[167]
B. Thomée and M. S. Lew.
Interactive search in image retrieval: a survey.
Int J MIR,
1(2):71–86, 2012.
[168]
S. Tong and E. Chang.
Support vector machine active learning for image retrieval.
In ACM MM, pages 107–118, 2001.
[169]
K. E. A. van de Sande, T. Gevers, and C. G. M. Snoek.
Evaluating color descriptors
for object and scene recognition.
TPAMI, 32(9):1582–1596, 2010.
[170]
P. van der Corput and J. J. van Wijk.
Effects of presentation mode and pace control
on performance in image classification.
IEEE TVCG, 20(12):2301–2309, Dec. 2014.
[171]
J. J. van Wijk.
The value of visualization.
In IEEE VIS, pages 79–86, 2005.
[172]
A. Vedaldi and B. Fulkerson.
VLFeat:
An open and portable library of computer
vision algorithms.
http://www.vlfeat.org/, 2008.
Bibliography
115
[173]
R. Vedantam, C. L. Zitnick, and D. Parikh.
CIDEr: Consensus-based image descrip-
tion evaluation.
arXiv:1411.5726, 2015.
[174]
M. L. Viaud, J. Thièvre, H. Goëau, A. Saulnier, and O. Buisson.
Interactive compo-
nents for visual exploration of multimedia archives.
In ACM CIVR, pages 609–616,
2008.
[175]
S. Vijayanarasimhan and K. Grauman.
What’s it going to cost you?: Predicting effort
vs. informativeness for multi-label image annotations.
In CVPR, pages 2262–2269,
2009.
[176]
S. Vijayanarasimhan, P. Jain, and K. Grauman. Far-sighted active learning on a budget
for image and video recognition.
In CVPR, pages 3035–3042, 2010.
[177]
R. Řehůřek and P. Sojka.
Software framework for topic modelling with large corpora.
In LREC Workshop on New Challenges for NLP Frameworks, pages 45–50, May 2010.
[178]
R. Řehůřek and P. Sojka.
Software framework for topic modelling with large corpora.
In LREC, pages 45–50, 2010.
[179]
H. D. Wactlar, T. Kanade, M. A. Smith, and S. M. Stevens. Intelligent access to digital
video: Informedia project.
Computer, 29(5):46–52, 1996.
[180]
C. Wang, D. Blei, and L. Fei-Fei.
Simultaneous image classification and annotation.
In CVPR, pages 1903–1910, 2009.
[181]
J. Wang, H. T. Shen, S. Yan, N. Yu, S. Li, and J. Wang.
Optimized distances for
binary code ranking.
In ACM MM, pages 517–526, 2014.
[182]
X. Wang, W. Dou, Z. Ma, J. Villalobos, Y. Chen, T. Kraft, and W. Ribarsky.
I-SI:
Scalable architecture for analyzing latent topical-level information from social media
data.
Comp Graph For, 31(3):1275–1284, 2012.
[183]
X. J. Wang, L. Zhang, F. Jing, and W. Y. Ma.
AnnoSearch: Image auto-annotation
by search.
In CVPR, pages 1483–1490, 2006.
[184]
R. C. F. Wong and C. H. C. Leung.
Automatic semantic annotation of real-world
web images.
TPAMI, 30(11):1933–1944, 2008.
[185]
M. Worring.
Easy categorization of large image collections by automatic analysis and
information visualization.
In Class & Vis Int UDC Sem, pages 235–242, 2013.
[186]
M. Worring, A. Engl, and C. Smeria. A multimedia analytics framework for browsing
image collections in digital forensics.
In ACM MM, pages 289–298, 2012.
[187]
M. Worring, D. Koelma, and J. Zahálka.
Multimedia pivot tables for multimedia
analytics on image collections.
To appear in IEEE Transactions on Multimedia, 2016.
[188]
J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan.
Sparse representation
for computer vision and pattern recognition.
Proceedings of the IEEE, 98(6):1031–1044,
2010.
[189]
Y. Wu,
E. Y. Chang,
and B. L. Tseng.
Multimodal metadata fusion using causal
strength.
In ACM MM, pages 872–881, 2005.
[190]
E. S. Xioufis, S. Papadopoulos, Y. Kompatsiaris, G. Tsoumakas, and I. P. Vlahavas.
A comprehensive study over VLAD and product quantization in large-scale image
retrieval.
IEEE TMM, 16(6), 2014.
116
Bibliography
[191]
J.
Yang,
J.
Fan,
D.
Hubball,
Y.
Gao,
H.
Luo,
and W.
Ribarsky.
Semantic image
browser: Bridging information visualization with automated intelligent image analysis.
In VAST, pages 191–198, 2006.
[192]
J. Yang, D. Luo, and Y. Liu.
Newdle: Interactive visual exploration of large online
news collections.
TCGA, 30(5):32–41, 2010.
[193]
J. S. Yi, Y. Kang, J. T. Stasko, and J. A. Jacko.
Toward a deeper understanding of the
role of interaction in information visualization.
TVCG, 13(6):1224–1231, 2007.
[194]
J. S. Yi, Y. Kang, J. T. Stasko, and J. A. Jacko.
Understanding and characterizing
insights:
How do people gain insights using information visualization?
In ACM
BELIV 2008, 2008.
[195]
H. Yin, Y. Sun, B. Cui, Z. Hu, and L. Chen.
Lcars: A location-content-aware recom-
mender system.
In ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD), pages 221–229, 2013.
[196]
J.
Zahálka.
Multimedia analytics article library.
http://staff.fnwi.uva.nl/j.
zahalka/maal.html, 2014.
[197]
J. Zahálka, S. Rudinac, B. T. Jónsson, D. C. Koelma, and M. Worring.
Interactive
multimodal learning on 100 million images.
In ACM ICMR, pages 333–337, 2016.
[198]
J. Zahálka, S. Rudinac, and M. Worring.
New Yorker Melange: Interactive brew of
personalized venue recommendations.
In ACM MM, pages 205–208, 2014.
[199]
J. Zahálka, S. Rudinac, and M. Worring.
Analytic quality: Evaluation of performance
and insight in multimedia collection analysis.
In ACM MM, pages 231–240, 2015.
[200]
J. Zahálka and M. Worring.
Towards interactive, intelligent, and integrated multime-
dia analytics.
In IEEE VAST, pages 3–12, 2014.
[201]
E. Zavesky, S. F. Chang, and C. C. Yang.
Visual islands: Intuitive browsing of visual
search results.
In ACM CIVR, pages 617–626, 2008.
[202]
L. Zhang and Y. Rui.
Image search — from thousands to billions in 20 years.
ACM
TOMCCAP, 9(1s), 2013.
[203]
L. Zhang, Y. Zhang, J. Tang, X. Gu, J. Li, and Q. Tian. Topology preserving hashing
for similarity search.
In ACM MM, pages 123–132, 2013.
[204]
Y.-L. Zhao, Q. Chen, S. Yan, T.-S. Chua, and D. Zhang.
Detecting profilable and
overlapping communities with user-generated multimedia contents in lbsns.
ACM
TOMCCAP, 10(1):3:1–3:22, Dec. 2013.
[205]
Y.-L. Zhao, L. Nie, X. Wang, and T.-S. Chua.
Personalized recommendations of
locally interesting venues to tourists via cross-region community matching.
ACM
TIST, 5(3):50:1–50:26, July 2014.
[206]
B.
Zhou,
A.
Khosla,
A.
Lapedriza,
A.
Torralba,
and A.
Oliva.
Places:
An image
database for deep scene understanding.
arXiv, 2016.
[207]
X. S. Zhou and T. S. Huang.
Small sample learning during multimedia retrieval using
BiasMap.
In CVPR, pages I–11–I–17, 2001.
[208]
X. S. Zhou and T. S. Huang. Relevance feedback in image retrieval: A comprehensive
review.
Multimedia Systems, 8(6):536–544, 2003.
SA M E N VAT T I NG
DE MACHINE IN MULTIMEDIA ANALYTICS
In dit proefschrift onderzoeken wij de rol van de machine in multimedia analytics, een discipline
dat visual
analytics combineert met multimedia analyse algoritmes om de potentie van
multimedia collecties te openen als bronnen van kennis in wetenschappelijke en toegepaste
domeinen [
27
].
De centrale onderzoeksvraag van dit proefschrift is hoe de machine te
gebruiken als hulpmiddel voor de gebruiker om kennis op te doen in multimedia analyse.
Dit proefschrift presenteert vier werken die helpen om deze vraag te beantwoorden.
Hoofdstuk 2 richt zich op de theorie van gecombineerde velden van multimedia
analyse in een uniform multimedia analyse model te plaatsen. De grootte en het belang van
visuele multimedia collecties zijn snel gegroeid in de laatste jaren, wat resulteert in een nood
voor slimme multimedia analyse systemen die grootschalige, interactieve, en behulpzame
analyse doen. Deze systemen moeten de natuurlijke expertise van mensen om multimedia te
analyseren integreren met de mogelijkheid van machines om grote hoeveelheden data te
verwerken. Het hoofdstuk begint met een uitgebreid overzicht van de representatie, leren, en
interactie technieken van zowel het oogpunt van de mens als machine. Honderden referenties
van vergelijkbare disciplines (zoals visual analytics, informatie visualisatie, beeldverwerking,
en multimedia informatie herwinning) zijn onderzocht.
Op basis van dit onderzoek is
een nieuw generiek multimedia analyse model samengevat.
Het model bevat onderdelen
als semantische navigatie van de collectie als het belangrijkste onderdeel
en multimedia
analyse taken zijn geplaatst op een exploratie-zoek as.
Deze as gaat van exploratie naar
zoek in bepaalde proporties, waarbij de proportie verandert als de analist zich naar inzicht
werkt. Categorisatie is voorgesteld als een geschikte kapstok taak op deze as. Verder is het
pragmatische gat – het verschil tussen het vaste categorisatie model van de machine en de
flexibele categorisatie model van de mens – geïdentificeerd als een fundamenteel multimedia
analyse onderwerp.
Samengevat structureert dit werk de interacties en technieken van
visual analytics en multimedia analyse in een verenigd model, en wordt een taakmodel en
uitdagingen gegeven.
Hoofdstuk 3 stelt een instantie van het multimedia analyse model voor, in het domein
van plaats aanbeveling. Dit hoofdstuk stelt City Melange voor, een interactieve en multimodale
plaatszoeker, die inzicht geeft in de plaatsen in een specifieke stad. Ons framewerk klikt de
interactieve gebruiker met de gebruikers van sociale media platforms met dezelfde smaak.
De data collectie integreert sociale netwerken met plaatinformatie zoals Foursquare met
generieke multimedia deelplatformen zoals Flickr en Picasa.
In City Melange werkt de
gebruiker met een aantal plaatjes en daarmee impliciet met de onderliggende semantiek.
De semantische informatie wordt uit attributen van convolutionele diepe netwerken in het
visuele domein gehaald en uit onderwerpen van tekst informatie gehaald met Latent Dirichlet
Allocation.
Deze worden samengevoegd om relevante onderwerpen van gebruikers en
plaatsen te kunnen geven. Een lineair classificatiemodel leert de voorkeuren van de gebruiker
117
118
Samenvatting
en bepaalt wie vergelijkbare gebruikers zijn. Experimenten laten zien dat onze aanpak beter
werkt dan aanpakken die gebruik maken van populariteit van plaatsen en van aanpakken die
kijken naar gebruiker activiteiten. Verder is onze aanpak in staat om gebruikelijke plaatsen
voor te stellen aan standaard gebruikers en specifieke plaatsen voor te stellen aan liefhebbers.
City Melange laat zien dat het goed werkt als plaatse aanbeveling aanpak, door succesvol
het multimedia analyse model toe te passen in een domein dat zeer specifieke persoonlijke
inzichten bevat, wat een gevolg is van een grote afwisseling in smaak een voorkeuren van
individuele reizigers.
Hoofdstuk 4 behandelt hoe multimedia analyse algoritmes geëvalueerd moeten worden.
Een nieuwe paradigma voor de evaluatie voor multimedia analyse methoden,
genaamd
analytic quality (
AQ
), wordt voorgesteld.
AQ
breidt huidige evaluatiemodellen op basis van
machine-gedreven evaluaties of gebruiker studies uit. De notie van van gebruikersinzicht
wordt toegevoegd en ook tijd die nodig is om dat inzicht te verkrijgen; beiden zijn belangrijke
aspecten in de grootschalige analyse van multimedia collecties. Om inzicht te verkrijgen stelt
AQ
een nieuw gebruikersmodel voor. In dit model bouwt een gesimuleerde gebruiker (ook
een artificial actor genaamd) zijn inzicht over tijd door op elk moment met meerdere relevantie
categorieën te werken. De methoden gebruiken de gesimuleerde gebruikers in getimede
sessie om de analytische mogelijkheden van de methoden objectief te kunnen meten.
De
gesimuleerde gebruikers werken met elke methode en sturen de methoden bij door aan te
geven wat relevante voorbeelden zijn tijdens de sessie op basis van een vooraf gedefinieerde
waarheden.
AQ
meet niet alleen de precisie en de recall, maar ook doorstroom, diversiteit
van de resultaten, en de precisie van het meten van de ratio van relevante voorbeelden in de
collectie. Deze metingen laten een breed beeld zien van de analytische capaciteiten van de
methoden die worden geëvalueerd en laten zien hoe de sterke aspecten van de methoden
verschillen met andere aanpakken. De tijdplots van
AQ
geven suggesties voor het verbeteren
van de geëvalueerde methoden.
AQ
laat zien dat het meer inzicht geeft dan standaard
evaluaties zowel op het gebied van het vergelijken van methoden als het geven van suggesties
voor verbeteringen.
Tenslotte concentreert Hoofdstuk 5 zich op het probleem van het schalen van automa-
tische multimedia analyse algoritmes naar grootschalige collecties zodat ze zowel relevant
en interactief blijven. Daarom stelt Hoofdstuk 5 Blackthorn voor, een efficiënt interactief
en multimodaal leeraanpak dat de analyse van grootschalige multimedia collecties (tot 100
miljoen voorbeelden) faciliteert op een enkel sterk werkstation. Om deze werking te verkrij-
gen bevat Blackthorn effectieve data compressie, selectie van attributen, en optimalisaties in
het interactieve leerproces. De Ratio-64 datarepresentatie die in dit werk wordt voorgesteld
kost slechts tientallen bytes per voorbeeld, maar behoudt het meeste van de visuele en tekstuele
semantische informatie met goede precisie.
Het geöptimaliseerde interactieve leermodel
werkt direct op de Ratio-64 representatie, wat de computationele benodigdheden sterk doet
afnemen. De experimenten laten zijn dat Blackthorn tot 77.5x sneller is dan de standaard
relevantie feedback aanpak. Experimenten laten ook zien dat Blackthorn beter werkt dan
de standaard aanpak om de relevantie van voorbeelden te bepalen.
Het verbetert sterk de
standaard aanpak op grond van recall over tijd, een aspect dat als een belangrijk meetpunt
geldt in het
AQ
evaluatie protocol van Hoofdstuk 4 en bereikt tot 108% van de precisie van
de standaard aanpak. Op de volledige YFCC100M dataset verwerkt Blackthorn een gehele
interactieronde in ongeveer een second, wat openingen biedt voor volledige interactieve
interacties met multimedia collecties tot 100 miljoen voorbeelden.
A RT IC L E AUT HOR S H I P A N D AUT HOR ROL E S
Chapter 2
:
Towards Interactive,
Intelligent,
and Integrated Multimedia Analytics,
in
proceedings of the IEEE Conference on Visual Analytics Science and Technology
(VAST), pages 3–12, 2014.
•
Jan Zahálka: All aspects of the paper.
•
Marcel Worring: Scientific insights, supervision.
Chapter 3
: Interactive Multimodal Learning for Venue Recommendation, IEEE Transac-
tions on Multimedia (TMM), 17 (12), pages 2235–2244, December 2015.
•
Jan Zahálka: All aspects of the paper.
•
Stevan Rudinac: Scientific insights, related work, extraction of the LDA topics
•
Marcel Worring: Scientific insights, supervision.
Chapter 4
:
Analytic Quality:
Evaluation of Performance and Insight
in Multimedia
Collection Analysis, in proceedings of the ACM Multimedia Conference (MM), pages
231–240, 2015.
•
Jan Zahálka: All aspects of the paper.
•
Stevan Rudinac: Scientific insights, related work.
•
Marcel Worring: Scientific insights, supervision.
Chapter 5
: Blackthorn: Large-Scale Interactive Multimodal Learning, under revision at
IEEE Transactions of Multimedia (TMM).
•
Jan Zahálka: All aspects of the paper.
•
Stevan Rudinac: Scientific insights, related work.
•
Björn Þór Jónsson: Scientific insights, related work.
•
Dennis C.
Koelma:
Technical
and performance-related insights,
technical
support.
•
Marcel Worring: Scientific insights, supervision.
119
ACKNOWLEDGMENTS
First, I would like to thank Marcel, my supervisor.
Marcel, I am deeply thankful for the
opportunity to work with you. I genuinely enjoyed your supervision and our collaboration
as a whole.
I find it remarkable that many core concepts of our research apply directly
to the manner of our collaboration.
It was indeed based on a truly interactive dialogue
and led to many insights. You were always encouraging, never pushing. You were always
involved in our research down to details most professors don’t care about, but you were never
micromanaging. You were always caring, promoting work-life balance even in the heat of a
deadline and making my life so much easier to pursue my personal endeavours. You were
always listening and receptive to new ideas. You were always down-to-earth, sharing your
wisdom through poignant arguments based on cold, hard facts, never relying on your status
as the senior party in our dialogues. I am fully aware that I use many universal quantifiers in
this acknowledgment, and I am equally aware that it is neither scientific nor Dutch to use
too many big words. But the collaboration was just that great. Thank you very much once
again.
Next, I would like to thank Stevan and Svetlana. There are few people in this world
that have as big hearts as you two.
You were always there for me whenever I could use
some emotional support, regardless of your own individual struggles at the time. I will never
forget that. Stevo, thanks for the friendship and great moments such as Once Upon a Time
in Key West, going ’round in Badhuis, or pretty much any discussion about our favourite
topics. I am proud to have you as my co-promotor, and thanks for the immense scientific
contributions to our work. Sorry that you were almost always the one doing the related work,
but you’re just that good with remembering who wrote what (including their complete
CV). Your contribution, however, goes way beyond that, and I will always count especially
the 2014 conceptual discussions about New Yorker Melange (and the subsequent victory)
among my finest hours. Svetle, thanks for the friendship from the very early stages of my
doctoral quest. Your warm, friendly presence has made every day in the office so much more
enjoyable. I also greatly enjoyed the squash sessions, and I still think that your “Aleksandra
Makedonska” squash battlecry is one of the fiercest in the history of mankind.
Now’s the time to thank the collaborators and co-authors.
First of all,
Dennis.
It
goes without saying that without the computational infrastructure excellently maintained by
you, the majority of our ideas would not be executable with such ease. But this is just the
base of the pyramid. From the first weeks, I have realized you are a very efficient man. At
first, mostly from the zero overhead with regards to your lunch menu selection, which is
one of the few true constants in this world. Being an epicurean, this efficiency I wouldn’t
enjoy. However, the efficiency regarding actually applying the science such that our methods
are sleek and optimized is a topic very dear to me.
In that regard, I consider you a rock
star. Thanks for the contributions, especially to Blackthorn. Björn, I am very glad that you
have selected our group for your sabbatical. Much like Marcel, Stevan, and Dennis, you too
are a great, down-to-earth, friendly, and knowledgeable guy.
The collaboration and the
non-work-related chats have all been very enjoyable. Thank you for your contribution both
121
122
Acknowledgments
to the science behind Blackthorn and to the friendly and constructive atmosphere in our
team, both in and out of the office.
While acknowledging the collaborators, I must not forget the colleagues from the
Eindhoven University of Technology: Jack and Paul. Multimedia analytics is all about the
collaboration between the visualization and the machine algorithms. I wish that the state-of-
the-art interactions between the two will one day be as insightful and smooth as between
the visualization (TU/e) and machine analysis (UvA) parts of our common project.
Jack,
thank you very much for the insightful remarks and inputs you had towards our research.
Paul, thanks for the wonderful discussions about the conceptual direction of our respective
research paths.
Now, Pascal and Spencer. You guys are some of the least sentimental people I know,
so I would like to keep my thanks to you non-sappy. Let me just say that it was great having
you around the office and I think we had a lot of fun. Pascal, [
114
] (and thanks for translating
the summary). Spencer, this sentence acknowledges you personally.
Anyone living in Amsterdam knows how difficult the question of accomodation is,
especially on a doctoral candidate budget. Often, you have to cohabitate with weirdos that
sometimes make the experience too colourful to bear. Not me, though. Morris, thanks for
being not only a wonderful neighbour and essentially flatmate, but also a great friend.
I
will always cherish the neighbour teas, Overwatch sessions and nice chats in general. Also,
thank you for designing the thesis cover.
Šimon,
thank you very much for introducing
me to Amsterdam and putting me up when the need for a shelter was the most dire.
For
the wonderful discussions that are an integral part of the time I was just embarking on my
doctoral path. About Nebel, Rychlé džípy, Curb Your Enthusiasm, Bushido feat. Karel Gott,
Drive and many more.
Being a doctoral candidate means many work Saturdays and Sundays. Empty building,
no queues at the coffee machine, smelly air due to the lack of air conditioning, eerie melodies
played by someone on the piano on the fourth floor... There were two hard-working people
that I often met in the office these days that made the experience so much nicer: Silvia and
Lou. Silvia, it would be easy to just thank you for being my “sister in darkness,” but there’s
more than just dark humour. I mean, what’s not to enjoy about interactions with a person
that likes Terry Pratchett and sushi? Lou, thank you for the wonderful discussions about the
Chinese culture and life in general.
Also, your dedication towards completing your own
doctorate was certainly a bright example for me.
I would also like to acknowledge the UvA people I had the pleasure to interact with
over the years. Thanks, Arnold, Cees, Theo, Jan, Thomas, Kandan, Hamdi, Sezer, Fares, Ran,
Zhenyang, Amir, Masoud, Mihir, Jörn, Agni, Stratis, and Yash. An honourable mention goes
to two sweet ladies not affiliated with our research directly, but contributing to an overall
positive atmosphere through their smiles and warm attitude: Virginie and the short-haired
cashier lady from the canteen downstairs.
I thank the committee members for taking the time to read my thesis and honouring
my defense with their presence and feedback. I would also like to thank Filip and Jirka from
the Czech Technical University in Prague for helping me embark on the exciting path of
machine learning and its applications.
My stay in the Netherlands was made so much nicer by my Dutch friends that have
provided a warm, cordial atmosphere and made me feel I am truly welcome in the country.
Acknowledgments
123
Big thanks to the van Wouw family:
Stefan, Mariëlle, Richard, and Angèle.
I always felt
welcome at your wonderful home in Oude Wetering. Thank you, Remon, Kevin, Robert,
Sandra, and Penelope, for the awesome conversations and activities we have done together.
Now I am coming home with my acknowledgments,
to the Czech Republic and
Slovakia. I would like to thank my closest friends: Ruda, Zoran, and Cohen. I am honored I
could share the stories about my doctoral endeavour with you. Thanks for supporting me
at all times, and the wonderful laughs we shared together are too many to count. Thanks
to Čiky,
Péťa,
Jarda,
Martin,
Ďuri,
Marek,
Jany,
Kaučo,
and all
my other great friends
from Prague, Vlkov, and Krivá for all the fun that has helped tremendously with the R ’n R
necessary for accumulating the strength for another push for a paper (admittedly, at times
achieved through hard memory resets).
In Slavic languages, the point to be emphasized is placed at the end of the sentence.
I would like to use this technique for this acknowledgment text and truly emphasize my
gratitude to my amazing family. I thank my wonderful parents, Zuzana and Michal, for the
unconditional love, support, and wisdom they have provided through all my years. You are
the greatest people in this world and the best parents I could’ve hoped for. I have received
much love, support, and wisdom also from my broader family: grandparents Květa and Jan,
aunt Martina, uncle Libor, cousins Te-e and Aňouš. Thank you very much, your support
means the world to me.
Last, but not least, I would like to thank my beloved Anička for
all her love, warmth, and listening. In Czech fairy tales, the Honza character goes abroad,
learns valuable lessons, defeats a dragon, gets a wonderful girl, and gloriously returns home.
The fairy tale came true for me in full, and I love you all very much.
I would like to dedicate this thesis to the memory of my paternal grandparents, Jitka
and Jan. Even though you are no longer with us, I am very much with you in spirit. I will
always remember you with fond memories and follow the example you set in life.
Věrný
zůstanu.
Title: The Machine in Multimedia Analytics
Author: Jan Zahálka
Publishing house: powerprint, Praha
Print: powerprint s.r.o., Brandejsovo nám. 1219/1, Praha 6 - Suchdol, 165 00
Year of publication: 2017
ISBN 978-80-7568-037-2
