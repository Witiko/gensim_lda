Targeted Advertising Based on Browsing History
Yong Zhang
a,∗
, Hongming Zhou
b,
, Nganmeng Tan
b
, Saeed Bagheri
b
, Meng Joo Er
a
a
School
of Electrical
and Electronic Engineering, Nanyang Technological
University, Singapore
b
GroupM, Singapore
Abstract
Audience interest, demography, purchase behavior and other possible classifications are ex-
tremely important factors to be carefully studied in a targeting campaign.
This information
can help advertisers and publishers deliver advertisements to the right audience group.
How-
ever, it is not easy to collect such information, especially for the online audience with whom
we have limited interaction and minimum deterministic knowledge.
In this paper,
we pro-
pose a predictive framework that can estimate online audience demographic attributes based
on their browsing histories.
Under the proposed framework,
first,
we retrieve the content
of
the websites visited by audience,
and represent the content as website feature vectors;
second, we aggregate the vectors of websites that audience have visited and arrive at feature
vectors representing the users;
finally,
the support vector machine is exploited to predict
the audience demographic attributes.
The key to achieving good prediction performance is
preparing representative features of
the audience.
Word Embedding,
a widely used tech-
nique in natural language processing tasks, together with term frequency-inverse document
frequency weighting scheme is used in the proposed method.
This new representation ap-
proach is unsupervised and very easy to implement.
The experimental results demonstrate
that the new audience feature representation method is more powerful than existing baseline
methods, leading to a great improvement in prediction accuracy.
Keywords:
Demographic Prediction, Targeted Advertising, Information Retrieval,
Browsing History, Machine Learning, Word Embedding
1.
Introduction
Nowadays people spend a great amount of
time online doing all
kinds of
things,
like
reading news,
playing games,
shopping,
etc.
This consumer behavior results in advertisers
putting great efforts and investment in online advertising.
However,
advertising audiences
or web users,
in this case,
seldom click on online ads,
which significantly decreases the
∗
Corresponding author.
The work is done when Yong Zhang worked as an intern in GroupM.
Email
addresses:
yzhang067@e.ntu.edu.sg (Yong Zhang), hongming.zhou@groupm.com (Hongming
Zhou), nganmeng.tan@groupm.com (Nganmeng Tan), saeed.bagheri@groupm.com (Saeed Bagheri),
emjer@ntu.edu.sg (Meng Joo Er)
Preprint submitted to Elsevier
November 15, 2017
arXiv:1711.04498v1 [cs.IR] 13 Nov 2017
Figure 1:
General view of predicting an audience’s demographics based on browsing history
effectiveness of
the promotion.
Targeted advertising is a practice delivering personalized
ads relevant to users as opposed to pushing same ads to all.
Personalization for customer
targeting across different channels and devices have become a great challenge and seen a
huge increase in R&D expense by firms seeking marketing efficiency [17].
According to
studies of TM advertising, targeted advertising gains 115% more business traffic a year and
the targeted consumers have higher brand awareness than random viewers [12].
Targeted
advertising is based on information of individual users like geographical location, behavioral
variables,
and demographic attributes.
In this paper,
we are interested in demographic
attributes (e.g.,
gender,
age,
income,
etc.)
of
website audience as they play a key role in
providing personalized services.
The data used in this paper abides by the standards under
U.S. and EU privacy laws and is blind to personally identifiable information (PII), including
names, email addresses, phone numbers, financial information, etc.
Traditional methods for determining demographic attributes are mainly based on panel
data.
The demographic tendency of websites is estimated statistically by known demographic
information of
panel
members who have visited those sites.
This approach is reliable for
websites visited by a large number of panel members but may lead to a bias for those sites not
covered by a sufficient number of panel members.
Besides, panel data may be difficult and
expensive to obtain.
Machine learning approaches construct the relationship between website
audience’s demographics and their online features (e.g.,
web content,
web category,
search
keywords,
etc.)
by building a model
on a subset of
audiences with known demographics.
Then the model
can be used to predict other web users’
demographic attributes.
Various
other online data has been used to build prediction models,
among which social
network
data is a popular choice.
Facebook and twitter profiles have been proven to be very effective
in determining the users’
personality and demographics [1,
10,
11,
28,
35,
36].
However,
these models require to have access to extensive information about people’s social network,
which may step into the sensitive area of privacy.
In this paper, we also take advantage of machine learning approach to predict audiences’
demographic attributes.
The proposed method constructs a model to build the relationship
between users’ browsing history and their demographic attributes.
The general view of the
method is depicted in Figure 1.
The user’s identification information can be easily hashed
and obfuscated,
resulting being PII-compliant.
The various information contained in the
browsing history has been employed to do the prediction job in literature.
The authors of
[2] used web-category information, users’ browsing time, browser type and other information
to predict personal attributes.
In [6], the authors employed more features by including visit
frequency, visit duration, web ID, etc.
Search keywords entered by users were employed to
2
do the prediction job in [14, 24].
However, such information is usually not available for most
users,
resulting in a weak generalization of
the trained models.
The most robust method
may be content-based methods [12, 15].
The web content is always available as long as the
website is still there.
The proposed model in this paper is also purely based on web content.
The only browsing history information needed in our method is hashed users ids together
with their browsed domain-level URLs.
Under the proposed framework, firstly we have built a web crawler to retrieve the content
of the websites visited by audiences,
and represent the content as website feature vectors.
Exhaustive experiments have been done to determine the crawling rule.
Secondly,
we ag-
gregate the vectors of
websites that audiences have visited and arrive at feature vectors
representing the audiences.
Feature representations of
audiences are the most important
step of
the entire framework because existing literature demonstrates that even a simple
classifier can achieve good performance as long as it takes in good features as input.
At last,
a classification model
is exploited to predict the audiences’
demographic attributes.
The
support vector machine [5] is used as the top classifier in our method because it outperforms
other baseline classifiers.
Our new model proposes an innovative method to obtain feature
vectors representing audiences.
The new feature representation method takes advantage of
word embedding technique and is easy-to-implement and effective.
Word embedding is a technique representing words with low-dimensional vectors.
Word
embedding has drawn great attention in recent years because it can capture both semantic
and syntactic information.
In the vector space,
words with similar semantics lie close to
each other, for example, vec(American) is closer to vec(France) than to vec(Bread).
Vector
representations of
words can even preserve the semantic relationship.
Word embedding
turns out to be quite effective in many natural
language processing applications such as
parsing [32], tagging [13], name entity recognition [4], document summarization [37, 39, 40],
sentiment analysis [7,
38]
and machine translation [41].
With advances in deep learning
techniques, word vector representation has become a common practice for natural language
processing tasks.
Website feature representation in this paper is the same as document representation in
natural language processing community.
Traditional research works in document represen-
tation without using word embedding usually employ the bag-of-words model
which uses
occurrence frequency of
words as features.
They face the problem of
high dimensionality
and sparsity and losing word order of sentences [3, 23].
To address these issues, most recent
works exploit deep learning methods together with word embedding technique and lever-
age neural networks to construct non-linear interactions between words so as to capture the
word order information.
The usually used deep learning models include convolutional neural
network [7, 16, 18, 20], recurrent neural networks [8, 30], recursive neural networks [32, 33],
paragraph vector [19], etc.
The problems with deep learning models are that they are very
sophisticated, have a lot of parameters to tune, and are time-consuming to train.
Considering the specific characteristics of the task in this paper, we propose a simple but
effective model
to represent website features.
As the crawled content of websites is consti-
tuted by fragmented information,
the word order information may not be very significant.
The simplest document representation method based on word embedding is taking the sum
3
or average of
all
the vectors of
words contained in the document if
the word order is not
important.
Our approach is developed based on this simple idea but incorporates a term
weighting function based on term frequency–inverse document frequency (tf-idf) formula.
With the tf-idf weighting scheme, the document representation vector not only leverages on
word co-occurrence probabilities but also takes into consideration the information about the
general distribution of terms appearing in different documents.
The idea is inspired by [25],
in which the authors adopted variants of original tf-idf to represent documents for sentiment
analysis and achieved significant improvement in classification accuracy.
Instead of
using
the tf-idf vectors to represent documents,
our method takes them as weights of word vec-
tors
1
.
The proposed method has both the power of word embedding capturing semantic and
syntactic information as well as, the statistical nature of tf-idf.
This intuitive representation
approach is unsupervised,
requiring no human annotations.
It is computationally efficient
compared with the neural network language models.
Experimental results demonstrate that
the new method outperforms the baseline document representation methods and achieves
even better performance than sophisticated deep learning methods.
The main contribution
of this paper is summarized as follows:
• A simple but effective unsupervised document representation method is proposed for
targeted advertising.
The method takes advantage of pre-trained word embedding and
variants of tf-idf weighting scheme.
• Exhaustive experiments have been done to determine an internal
protocol
to crawl
web content.
• Two benchmark datasets are created for demographic prediction task.
This paper is organized as follows:
Section 2 gives a brief
review of
related works.
In
Section 3,
the proposed methodology is described in details.
The datasets are introduced
in Section 4.
Section 5 demonstrates the prediction performance of the proposed method.
Conclusions are drawn in Section 6.
2.
Related Works
Many researchers have explored to predict audiences’
demographics based on browsing
behavior.
Baglioni et al.
[2] predicted gender of users by analyzing users’ navigational be-
havior.
They also analyzed the hierarchical ontology design of URLs visited and employed
other click-through data (e.g.,
timestamps,
browser type,
etc.)
as features.
The decision
tree was used to predict the attribute but only achieved slightly better accuracy than the
random guess.
Similarly,
the authors of
[6]
used more click-through data (e.g.
click fre-
quency, browsing duration, etc.) to build features.
They used the random forest to predict
demographic attributes including gender, age, occupation, and education.
The performance
can hardly be named satisfactory as well.
Jones et al.
[14] exploited audiences’ query logs
1
Word embedding and word vector appear alternatively in this paper.
They have the same meaning.
4
to build feature vectors and used support vector machine (SVM) to predict gender and age.
They achieved very good prediction performance on both attributes.
In [24],
the authors
used both search keywords and accessed web-pages as input features, and mapped them to a
reduced vector space using latent semantic analysis (LSA). Then the reduced vectors are fed
into a neural network-based model to predict attributes of unknown audiences.
A common
characteristic of all the aforementioned methods is that they all used audience-specific data.
However,
such information is usually not available for most audiences.
Therefore,
these
models may not be generalizable.
Goel et al.
[9] used web domains themselves as features
to predict five demographic attributes.
Each user is represented as a sparse vector whose
elements are binary features (e.g.,
1 for visited and 0 for not visited).
The SVM classifier
was trained on a huge panel data and achieved good prediction performance.
However, due
to the large number of
web domains not included the model’s prediction capability as it
scales by is doubtful.
The literature so far demonstrates that content-based methods may
be the most robust and effective method among all the approaches.
In [12], Hu et al.
used
both content-based features and web-category-based features to complement each other,
but only achieved a slight performance improvement compared with using content-based
features alone.
They first predicted the demographic tendency of web-pages and then used
a Bayesian network to derive representation vectors for audiences.
In [15], only web content
is used and the prediction task is completed by regression-based approaches.
The web con-
tent in the above two papers is represented by bag-of-words model and standard tf-idf term
weighting scheme respectively.
The new method proposed in this paper is also only based
on website content.
3.
Methodology
In our framework, we first represent the websites browsed by an audience as vectors and
then aggregate the website vectors to derive a vector representing the audience.
At last,
a
supervised classification model is used to predict the audience’s demographic attributes.
The
flowchart of our framework is depicted in Figure 2.
The details of the proposed methodology
are discussed in the remainder of
this section.
The website representation approach is
introduced in Section 3.2.
Three aggregation methods are discussed in Section 3.3.
The
classifier is introduced in 3.4.
3.1.
Problem Formulation
Before introducing the proposed methodology, we define the problem in this section.
A
web user’s browsing history is a set of
websites he or she has visited.
Suppose there are
totally M users U = {u
1
, u
2
, ..., u
i
, ..., u
M
} and N websites S = {s
1
, ..., s
j
, ..., s
N
}.
Then
the browsing data of all the users can be represented by an adjacency matrix R, where the
element r
ij
denotes the weight from user u
i
to website s
j
.
The weight is deemed as the visit
frequency of user u
i
on website s
j
.
Given the demographic attributes Y = {y
1
, y
2
, ..., y
M
} of
some users are known, the problem is to train a general model (M : U ∼ Y ) on the known
users and use the trained model to predict demographic attributes of unknown users.
5
Aggregation
Website 1
Website 2
Website N
Browsing
Histories
Unknown
User
Site vector 1
Website
Vectors
Site vector 2
Site vector N
User vector
Classification
Model
Gender
Age
Occupation
User
Demography
Classification
Figure 2:
The flow chart that explains the general methodology
3.2.
Website Representation
The first step of our methodology is to represent websites browsed by web users as site
vectors.
Only content-based features are used in our model.
In this paper,
we propose a
new method by taking a powerful technique named word embedding.
Variants of the tf-idf
weighting scheme are employed as weights of word vectors to help improve the representation
power.
3.2.1.
Word Embedding
There are M websites which need to be represented as vectors in our problem setting.
Each website s
j
contains a set of
words.
Recent research results have demonstrated that
word vectors are more effective in representing words than traditional bag-of-words method.
A word can be represented by a dense vector as follows:
v = Lx
(1)
where x ∈ R
V
is a one-hot vector where the position that the word appears is one while the
other positions are zeros, L ∈ R
d×V
is a word representation matrix, in which the ith column
is the vector representation of the ith word in the vocabulary, and V is the vocabulary size.
Word vectors are mostly learned through neural language models [3, 22, 23, 27, 34].
In
neural
language models,
each word is represented by a dense vector and can be predicted
by or used to predict its context representations.
However, we can easily adopt off-the-shelf
word embedding matrices without spending time and efforts to train our own word vectors.
Previous research demonstrated some pre-trained word vectors are universal representations
for various tasks and can make good use of semantic and grammatical associations of words.
Word2vec [23] and GloVe [27] are two most widely used pre-trained word embedding matri-
ces.
Previous research results demonstrate that different performance may result from using
either matrix on different tasks, but with slight differences.
In this paper, we use word2vec
2
embedding.
The word2vec vectors are trained on 100 billion words from the Google News
2
https://code.google.com/p/word2vec
6
Table 1:
Variants of tf weight
Weighting scheme
TF
default (d)
f
t,d
binary (b)
(
1,
f
t,d
> 0
0,
otherwise
logarithm (l)
1 + log(f
t,d
)
augmented (a)
0.5 + 0.5 ·
f
t,d
max{f
t
0
,d
:t
0
∈d}
Table 2:
Variants of idf weight
Weighting scheme
IDF
unary (u)
1
default (d)
log(
N
n
t
)
smoothed (s)
1 + log(
N
1+n
t
)
by using the Skip-gram method and maximizing the average log probability of all the words
as follows:
1
T
T
X
t=1
X
−c≤j≤c,j6=0
logp(w
t+j
|w
t
)
(2)
p(w
O
|w
I
) =
exp(v
0T
w
O
v
w
I
)
P
V
w=1
exp(v
0T
w
v
w
I
)
(3)
where c is the context window size,
w
t
is the center word,
v
w
and v
0
w
are the “input” and
“output” vector representations of w.
More details can be found in [23].
Suppose website s
j
is constituted by K words.
As every word is represented as a d-
dimensional vector, the website can be represented as a dense matrix as V
i
= {v
1
, v
2
, . . . , v
K
} ∈
R
d×K
.
The simplest way to reduce the site matrix to a site vector is to take the sum or the
average of all the word vectors.
Some recent works fulfill the document (website
3
) represen-
tation task by employing neural
networks to model
non-linear interactions between words.
Our proposed method follows the simple summation fashion but exploits variants of tf-idf
weighting scheme to capture the distribution information of words amongst documents.
It
remains to be very simple and intuitive while our experiment results demonstrate that it
even outperforms the sophisticated neural network approaches.
3.2.2.
Tf-idf Weighting Scheme
Tf-idf,
short for “term frequency-inverse document frequency”,
is used to reflect the
contribution of a word to a document in a corpus.
It is widely used in information retrieval
3
A website is regarded as a document in this paper.
7
and text mining.
Typically,
tf-idf consists of two components,
namely the term frequency
(tf) and the inverse document frequency (idf).
The two terms are calculated as:
tf (t, d) = f
t,d
(4)
idf (t, D) = log(N/n
t
)
(5)
where the term f
t,d
is the raw frequency of term t in a document d, N is the total number
of documents in the corpus D,
and n
t
is the number of documents with term t in it.
And
then the weight of term t in the document d is just the product of the two components:
w
t,d
= tf (t, d) ∗ idf (t, D)
(6)
The weight increases proportionally to the term frequency in the given document and
is offset by the frequency that the term appears in the entire corpus.
This helps to filter
the common terms (e.g.,
the,
I,
you,
etc.)
which are actually non-relevant to the specific
document.
Many variations of the tf–idf weighting scheme have been proposed for various applica-
tions.
Some widely used variants of tf and idf weights [21]
are listed in Table 1 and Table
2.
The augmented term frequency is able to prevent a bias towards longer documents.
The
smoothed inverse document frequency is proposed in case that n
t
= 0.
A two-letter notation scheme is used to denote the variants of tf-idf weighting scheme
in the following sections according to the notations in Table 1 and Table 2.
The first letter
denotes the term frequency factor and the second the inverse document frequency.
The
classic tf-idf weighting scheme is denoted as dd.
The experiment results demonstrated that
the ad scheme results in the best information representing power.
The weight of term t in
a document d under the ad scheme is given as follows:
w
ad
t,d
=

0.5 + 0.5 ·
f
t,d
max{f
t
0
,d
: t
0
∈ d}
)

∗ log(
N
n
t
)
(7)
3.2.3.
Site Vector
Following the assumption that the website s
j
contains K words,
the website can be
represented by a dense matrix V
j
= {v
1
, v
2
, . . . , v
K
} ∈ R
d×K
by using pre-trained word
vectors.
The term d is the dimension of pre-trained word vectors.
By taking advantage of
the tf-idf
weighting scheme introduced in Section 3.2.2,
we can obtain the weights of
the
K terms in the website s
j
.
Let’s denote the weights of
the K terms as a vector W
s
j
=
{w
1,s
j
, w
2,s
j
, . . . , w
K,s
j
}.
Then we can get the site vector using the following equation:
s
j
= W
s
j
V
T
j
∈ R
d
(8)
Therefore, the website is represented by a low-dimensional vector.
3.3.
Aggregation
After representing each website as a vector,
an audience’s vector can be obtained by
aggregating the vectors of all the websites browsed by him or her.
This can be easily done
8
by taking a weighted average of all the website vectors by using the weight matrix R ∈ R
M×N
which gives the visit frequency of each user to all the websites S = {s
1
, s
2
, ..., s
N
} ∈ R
d×N
.
u
WA
i
= g(R
i·
)S
T
∈ R
d
(9)
where g(·) is a function to rescale values in a vector so that they sum up to be 1.
The weighted
average method faces a problem that some websites have much higher visit frequency thus
weights than others.
To address the problem,
we narrow down the weight differences by
mapping visit frequency to log space as follows:
u
LA
i
= g(log(R
i·
))S ∈ R
d
(10)
The implementation can effectively eliminate the influences of large weight difference.
For
example, suppose the visit frequency to sites {s
1
, s
2
} are {10, 1000}, the weight ratio would
be 1:100 using Equation 9 and 1:3 using Equation 10.
Another even simpler aggregation method is to take the simple average of the browsed
website vectors by ignoring the user wights on websites as follows:
u
SA
i
=
e
R
i·
S ∈ R
d
(11)
Where
e
R is derived from R with the following formula:
e
R
ij
=
(
1,
R
ij
> 0
0,
R
ij
= 0
(12)
3.4.
Classification
We use support vector machine (SVM) as our classifier.
However,
we have still
done
several experiments comparing the learning performance of SVM with several other baseline
machine learning algorithms,
i.e.,
logistic regression,
neural
network and random forest.
The results demonstrate that SVM achieves similar performance as neural
network and
outperforms logistic regression and random forest.
4.
Datasets
4.1.
Demographic Attributes
There are many important demographic attributes associated with a person,
e.g.
age,
gender, race, household income and so on.
In this paper, we focus on gender and age, because
they are easily and legally accessible.
However, the predictive model that we developed can
be easily applied to any other types of
audience classification as long as the attributes
are allowed to be used by law.
The gender attribute is categorized into male and female,
while the age attribute is broken down into four groups,
i.e.,
Teenage (<18 years),
Young
(18–34 years), Mid-age (35–49 years) and Elder (50+ years).
The breakdown method closely
corresponds to groups that are of interest to advertising agencies.
9
Table 3:
Data distribution over gender and age
of Demo20
Age
Gender
Total
Male
Female
Teenage
3.5%
3.1%
6.6%
Young
21.9% 16.7%
38.7%
Mid-age
16.7% 17.2%
33.9%
Elder
10.3% 10.5%
20.8%
Total
52.5% 47.5%
100%
Table 4:
Data distribution over gender and age
of Demo100
Age
Gender
Total
Male
Female
Teenage
1.6%
1.3%
2.9%
Young
18.3% 13.2%
31.4%
Mid-age
18.7% 18.2%
36.9%
Elder
16.4% 12.4%
28.8%
Total
54.9% 45.1%
100%
4.2.
Data Preparation
In our experiments, we use panelist data for training our learning model, and the trained
model
is applied on cookie browsing data for prediction.
The panel
data used contains
audiences’
browsing log data in May 2015.
Each record of
the data consists of
user id,
web-page clicked,
visit frequency of that web-page,
and gender/age information.
The data
contains 175640 distinct users and 2476338 unique websites.
The websites with very low
traffic (visit frequency of all users less than 100) are removed as the content of such websites
falls out of the scope of most users.
If a website is visited by a user by less than five times, it
is taken away from the list of visited websites of that user, since it is not reasonable to refer
a user’s attributes using websites that he or she randomly visits.
Then the websites that
cannot be crawled are filtered out.
We also filter the websites whose crawled content has less
than 10 words because such few content can hardly convey useful information.
The number
of
websites clicked by a user is an important variable for user’s demography prediction,
because it is not reasonable to predict a user’s demographic information based on very few
websites.
In this paper, we choose two threshold values for a minimum number of websites
visited,
namely 20 and 100 to investigate the impact.
Two datasets,
namely Demo20 and
Demo100,
are generated by filtering out the users who visit less than the corresponding
number of websites.
After all the processing, Demo20 has 70868 distinct users and 3499537
entries in sum,
while Demo100 has 4742 distinct users and a total
of 667019 entries.
The
gender and age distribution of the two datasets are shown in Table 3 and Table 4.
4.3.
Web Crawling
Web crawling of websites is necessary in our task,
in order to extract key information
from a web page.
As websites are now designed to appear in different formats and styles to
appeal to a diverse audience,
and different web platforms (eg.
mobile and computer),
it is
necessary for us to identify an internal protocol to extract web content, which is ubiquitous
to most websites,
in a generalized manner.
In our implementation,
the HTML tag-based
method is used to crawl the websites.
Specifically, the title tag <title>, headline tags <h1-h6>, paragraph tags <p>, link tags
<a>,
and image tags <img> are chosen and tested on the gender attribute of
a website
demography tendency dataset.
This dataset contains the top 2000 visited websites in the
USA,
with the visitors’
demographic tendency for each website.
To build our protocol,
we
10
h
p
a
i
hp ha hi
pa pi
ai
hpa hpi hai paihpai
v
tag combinations
0.110
0.112
0.114
0.116
0.118
0.120
0.122
0.124
0.126
rmse
Figure 3:
RMSE results by tag combinations on website demography tendency prediction.
The terms h,
p,
a,
i,
v are used to represent headlines,
paragraphs,
link words,
image descriptions,
and visible texts
respectively for brevity.
Tag combination xy means content associated with the corresponding tags x and y
are used.
For instance, hp stands for using both headlines and paragraph content.
tested a combination of
the tag information collected in our experiments.
Concretely,
we
want to understand if a certain (combination of) tag information will be more reliable and
useful to succinctly describe a website.
Intuitively, we believe that the <title> tag conveys
very important information about a website, therefore, it is always included in our protocol.
As these five HTML tags do not encompass the entire website’s content, we have also crawled
the visible text in each website.
Although the visible text contains more information for
each website,
it also includes more noise for our learning model.
This is demonstrated in
our experimental results.
The website demography tendency dataset uses continuous scores to indicate visitors’
demographic tendency (e.g.,
0.45 for male and 0.55 for female).
As such,
we employ a
regression model
named -support vector regression (-SVR) [31],
which is an extension of
SVM, to predict the tendency scores for a given website.
The prediction root mean square
error (RMSE) results of all the tag combinations are demonstrated in Figure 3.
We observe
that the combination hpai achieves the smallest RMSE, while the inclusion of all visible text
does not improve the performance due to the presence of additional
noise.
Therefore,
the
combination of hpai is employed to crawl website content in our model.
5.
Experiment Analysis
In this section, we evaluate the performance of the proposed model on the two audience
demography datasets, namely Demo20 and Demo100.
The impact of different tf-idf weight-
ing schemes on website representation is evaluated in Section 5.2.
The three aggregation
methods are compared in Section 5.3.
The prediction performance using different classifiers
are shown in Section 5.4.
Section 5.5 demonstrates the power of
new proposed website
representation method by comparing with existing baseline and state-of-art methods.
One
11
72.0
72.5
73.0
73.5
74.0
74.5
75.0
75.5
76.0
acc (%)
72.93
73.43
73.62
73.59
73.34
73.86
74.41
74.42
73.29
73.91
74.43
74.39
Demo20
du
bu
lu
au
dd
bd
ld
ad
ds
bs
ls
as
tf-idf weighting scheme
80
81
82
83
84
85
acc (%)
80.90
82.39
82.77
82.81
81.81
83.45
83.83
83.95
81.88
83.21
83.76
83.91
Demo100
Figure 4:
Accuracy comparison results on gender prediction using various tf-idf weighting scheme on Demo20
and Demo100 datasets.
The first letter in the scheme notation denotes the name of
the variant of
term
frequency weight.
The second letter accounts for the variant of inverse document frequency weight.
more experiment is done in Section 5.6 to demonstrate that our aggregation-classification
framework is better than regression-aggregation framework used in [12].
5.1.
Experiment Setup
The greatest advantage of the proposed model is that the audience feature representation
procedure has no parameters to tune.
We directly use the word embedding matrix word2vec
which has been pre-trained on Google news.
The dimension of
each word vector is 300.
Gensim toolkit [29]
is exploited to obtain the tf-idf weights.
The only parameters to tune
are the parameters in the SVM classifier.
The SVM classifier is implemented using the
LinearSVC package in scikit-learn toolkit [26].
Ridge norm penalty is used and the penalty
parameter C is determined using grid search and cross-validation.
The two datasets are split
into training and testing datasets by 3:2 ratio.
The linear SVC is trained on the training
dataset and all
the prediction results in the following sections are obtained on the testing
dataset.
5.2.
Performance of Different Tf-idf Weighting Schemes
Four variants of
tf
weight and three variants of
idf
weight are shown in Table 1 and
Table 2.
Therefore,
there are totally 12 different tf-idf weighting schemes.
In this section,
we compare the demography prediction results on both Demo20 and Demo100 using all the
12 weighting schemes.
As the comparison results on gender attribute and age attribute are
similar, only gender prediction results are shown for conciseness.
The log average aggregation
implementation is used.
Figure 4 demonstrates the accuracy comparison results on gender prediction using various
tf-idf weighting scheme on Demo20 and Demo100 datasets.
The results on both Demo20
12
60
65
70
75
80
85
90
SA
WA
LA
Demo20
68.13
73.87
74.42
40
45
50
55
60
SA
WA
LA
49.43
53.15
53.95
60
65
70
75
80
85
90
Gender
SA
WA
LA
Demo100
72.90
82.87
83.95
40
45
50
55
60
Age
SA
WA
LA
47.97
54.08
54.93
Figure 5:
Accuracy comparison results using various aggregation methods.
The orange (LA),turquoise
(WA), and navy (SA) bars denote log, weighted, and simple average methods respectively.
The upper two
sub-figures show results on Demo20 dataset while the bottom figures on Demo100.
The left two sub-figures
are results on gender attribute prediction while the right two on age.
and Demo100 show that binary tf weight scheme performs better than the default raw tf
weight,
e.g.,
bd (83.45%) vs.
dd (81.81%).
Furthermore,
other variants of
tf
weight also
outperform the raw tf
weight.
The logarithm and augmented weighting schemes perform
similarly,
both slightly better than binary features,
e.g.,
ld (83.83%) vs.
ad (83.95%).
We
can conclude that sub-linearly scaled tf weight improves representation capability compared
with raw term frequency.
When comparing the idf
weighting schemes,
we can find that the default idf
weight
helps improve representation power compared with unary idf weight,
e.g.,
ad (83.95%) vs.
au (82.81%).
The du weighting scheme actually takes the weighted average of
vectors of
all
words on the website,
without considering the word distribution information over the
corpus.
With the incorporation of idf weighting scheme,
the website representation vector
not only leverages word co-occurrence probabilities but also takes into consideration the
information about the general
distribution of
terms appearing in different documents.
In
[25], the authors claimed that the idf smoothing greatly improved the classification accuracy
on sentiment analysis task.
However,
our results show that smoothing idf weight does not
provide any advantage over the classic idf weight.
This is because all
the terms appear in
the corpus at least once (n
t
>= 1).
In our task, the ad weighting scheme achieves the best
prediction performance on both Demo20 and Demo100.
5.3.
Performance of Different Aggregation Methods
After obtaining the representation vectors for websites, an aggregation step is needed to
move from website vectors to audience vectors.
In Section 3.3,
three aggregation methods
are introduced.
They are compared in this section.
The best tf-idf
weighting scheme in
13
Table 5:
Gender prediction accuracy of SVM against other baseline classifiers (%)
Models
Demo20
Demo100
Logistic Regression
73.39
82.53
Random Forest
67.63
77.60
Neural Network
74.28
83.99
SVM
74.42
83.95
our task, namely ad, is used in all the experiments in this section.
The comparison results
on two datasets for both gender and age attributes are demonstrated in Figure 5.
We can
see that the prediction accuracy using weighted average method is much poorer than that
using simple average aggregation although the former taking into consideration the web
user’s visit frequency on each website.
This results from the fact that frequency of
few
websites may be tens of or hundreds of times larger than that of other sites.
Thus few most
frequently visited websites dominate the final audience representation and contributions of
other websites can almost be neglected.
The best aggregation method among the three is
the log average method.
It still
leverages on the visit frequency difference,
but rescale the
weights to a much narrower range so that all
the useful
websites’
information is included
in the final
audience representation.
According to the experimental
results in Section 5.2
and Section 5.3,
the ad tf-idf
weighting scheme and log average aggregation method are
employed to obtain the final audience vector representation.
5.4.
Performance of Different Classification Models
Several popular baseline classifiers are used as the classification model in our framework
and their prediction performance on gender attribute is shown in Table 5.
All the classifiers
have been implemented using scikit-learn toolkit [26].
For the neural
network,
one hidden
layer perceptron classifier has been used.
Random forest and logistic regression are imple-
mented using base classifiers in the toolkit.
Default settings are used for most parameters.
The most important hyper-parameters, like regularization strength of logistic regression, the
number of maximal features and estimators of the random forest, and the number of hidden
nodes of
the neural
network are determined using grid search and cross-validation.
The
results in Table 5 demonstrate that SVM achieves similar performance as neural
network
and outperforms logistic regression and random forest.
Random forest performs rather poor
in this task,
which should be expected from the noisy characteristic of web data.
The fact
that a linear SVM achieves rather satisfactory performance demonstrates that the feature
representations of audiences are effective and informative.
5.5.
Comparison of Website Representation Methods
One of the main contributions of the paper is our proposed website representation ap-
proach.
Our method is purely based on content and does not require any other audience-
specific input data.
To demonstrate the representation power of the proposed method.
We
compare it with some other website representation methods.
The comparison methods are
described as follows:
14
Table 6:
Comparison of the percentage (%) accuracy of different websites representation methods
Representation
methods
Demo20
Demo100
Gender
Age
Gender
Age
Category-based
71.71
53.65
79.34
53.03
LSI
71.93
53.49
79.81
53.55
RNN
68.35
49.65
75.49
50.71
TF-IDF
70.56
52.21
77.96
52.78
TF-IDF word2vec
74.42
53.95
83.95
54.93
• Category-based method:
represents websites using category-based features.
Web-
sites are spanned over 460 two level categories from Open Directory Project (ODP
4
).
ODP is the largest,
most comprehensive human-edited directory of
the Web.
Each
website is represented by a 460-dimensional vector with each value indicating the pos-
sibility of belonging to each category.
The possibility values are obtained by comparing
the web content with keyword library built for each category.
• Latent Semantic Analysis (LSA): is a widely used topic modeling method which
is able to find the relationships between terms and documents.
Gensim toolkit [29] is
used to implement LSA and the number of topics is set as 300.
• Recurrent Neural
Network (RNN):
is a powerful
deep learning model
which is
able to construct non-linear interactions between words.
It takes in word vectors in a
website one by one and outputs the vector representation of the website.
The model
is trained using demographic tendency scores of websites which are propagated from
attributes of audiences who have browsed the websites.
It is implemented using keras
5
.
• TF-IDF: uses tf-idf weights directly as feature vectors of websites without using word
vectors.
We also use gensim toolkit to obtain tf-idf
representations and the default
tf-idf is used.
The comparison results are demonstrated in Table 6.
The last row is the representation
method proposed in this paper.
The ad variant of
tf-idf
weighting scheme is used.
We
can see that the new representation method achieves a much better performance than other
approaches.
The performance of
category-based method relies on the completeness and
precision of
keyword library of
each category,
which is not easy to build.
This may be
the reason that category-based method only achieves average performance.
LSI has not
achieved very satisfactory performance either,
demonstrating that topic extraction from
messy website content is not an easy task.
Comparing TF-IDF and our word embedding
method, we can conclude that the incorporation of word vectors help significantly improve
the representation power.
TF-IDF representations are very sparse and with high dimension,
4
http://dmoz.org
5
http://keras.io
15
40
50
60
70
80
90
Demo20/gender
Demo20/age
Demo100/gender
Demo100/age
66.07
49.88
72.76
51.85
74.42
53.95
83.95
54.93
Regression-Aggregation
Aggregation-Classification
Figure 6:
Accuracy comparison results using aggregation-classification and regression-aggregation frame-
work.
The upper two are age and gender prediction on Demo100 dataset,
while the bottom two are
prediction results on Demo10.
resulting in difficulty in training a good classifier.
What is of interest is that the sophisticated
deep learning model RNN shows the worst performance.
One reason is that website content
usually contains a lot of
words.
RNN faces great challenges to memorize long sequences
because of
the problems of
‘vanishing gradient’
and ‘explosive gradient’.
Another reason
is that the content in websites is usually fragmented,
therefore,
this greatly limits the use
of deep learning within our framework.
Furthermore,
sophisticated RNN model
requires a
great amount of time to train while our model is computationally efficient.
The comparison
results demonstrate the effectiveness and power of the new website representation method.
5.6.
Study on Overall
Framework
In [12],
Hu et
al.
first trained a supervised regression model
to predict a web-page’s
gender and age tendency.
Then,
an audience’s gender and age are predicted based on
the age and gender tendency of the web-pages browsed by the audience within a Bayesian
framework.
The Bayesian framework assumed that the web-pages visited by the audiences
are independent.
Our model does audience attribute prediction after aggregating informa-
tion of
websites browsed by the user.
In this section,
we compare the two frameworks.
The framework in Hu’s paper is denoted as regression-aggregation, while our framework as
aggregation-classification.
The comparison results are depicted in Figure 6.
We can see that
the aggregation-classification framework achieves much higher accuracy than regression-
aggregation framework when predicting both gender and age attribute on Demo20 and
Demo100.
The results indicate the independent assumption is problematic.
The fact is that
websites visited by the same web user are usually similar or correlated.
The experimental
results demonstrate that our framework is more reasonable.
16
6.
Conclusion
Audience classification is of significant importance for targeted advertising.
In this paper,
a new method is proposed to estimate on-line audiences’
demographic attributes based on
their browsing histories,
as an example of
audience classifications.
We first retrieve the
content of the websites visited by audiences and employ an innovative website representation
method to represent the content as feature vectors.
Word embedding with variants of
tf-
idf weighting scheme turns out to be a simple but effective website representation method.
The method is unsupervised and requires no human annotations.
Various variants of tf-idf
weighting schemes have been tested and it is observed that sub-linearly scaled term frequency
weight together with the default inverse document frequency weight can significantly improve
the final classification accuracy.
After obtaining website vectors, a log average aggregation
method is shown to be effective in composing vectors of websites browsed by the audience
into a vector representing that audience.
The experimental results demonstrate that our new
audience feature representation method is more powerful
than existing baseline methods,
leading to a significant improvement in prediction accuracy.
Another contribution of
the
paper is examining various web crawling rules and identifying the best crawling pattern for
audience classification.
For future work, the proposed methodology will be tested on other
demographic attributes, like income, race, sexual orientation, and hobbies, etc.
References
[1]
Bachrach,
Y.,
Kosinski,
M.,
Graepel,
T.,
Kohli,
P.,
Stillwell,
D..
Personality and patterns of
facebook usage.
In:
Proceedings of
the 4th Annual
ACM Web Science Conference.
ACM;
2012.
p.
24–32.
[2]
Baglioni,
M.,
Ferrara,
U.,
Romei,
A.,
Ruggieri,
S.,
Turini,
F..
Preprocessing and mining web log
data for web personalization. In:
Congress of the Italian Association for Artificial Intelligence. Springer;
2003. p. 237–249.
[3]
Bengio,
Y.,
Ducharme,
R.,
Vincent,
P.,
Janvin,
C..
A neural
probabilistic language model.
The
Journal of Machine Learning Research 2003;3:1137–1155.
[4]
Collobert,
R.,
Weston,
J..
A unified architecture for natural
language processing:
Deep neural
networks with multitask learning.
In:
Proceedings of
the 25th international
conference on Machine
learning. ACM; 2008. p. 160–167.
[5]
Cortes,
C., Vapnik,
V..
Support-vector networks.
Machine learning 1995;20(3):273–297.
[6]
De Bock,
K., Van den Poel,
D.. Predicting website audience demographics forweb advertising targeting
using multi-website clickstream data.
Fundamenta Informaticae 2010;98(1):49–70.
[7]
Er,
M.J., Zhang,
Y., Wang,
N., Pratama,
M..
Attention pooling-based convolutional neural network
for sentence modelling.
Information Sciences 2016;373:388–403.
[8]
Funahashi,
K.i.,
Nakamura,
Y..
Approximation of dynamical
systems by continuous time recurrent
neural networks.
Neural networks 1993;6(6):801–806.
[9]
Goel,
S.,
Hofman,
J.M.,
Sirer,
M.I..
Who does what on the web:
A large-scale study of browsing
behavior.
In:
Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media.
2012. p. 130–137.
[10]
Golbeck,
J., Robles,
C., Turner,
K..
Predicting personality with social media.
In:
CHI’11 extended
abstracts on human factors in computing systems. ACM; 2011. p. 253–262.
[11]
Haenlein,
M.. Social interactions in customer churn decisions:
The impact of relationship directionality.
International Journal of Research in Marketing 2013;30(3):236–248.
17
[12]
Hu,
J.,
Zeng,
H.J.,
Li,
H.,
Niu,
C.,
Chen,
Z..
Demographic prediction based on user’s browsing
behavior.
In:
Proceedings of the 16th international
conference on World Wide Web.
ACM;
2007.
p.
151–160.
[13]
Huang,
E.H.,
Socher,
R.,
Manning,
C.D.,
Ng,
A.Y..
Improving word representations via global
context and multiple word prototypes.
In:
Proceedings of the 50th Annual Meeting of the Association
for Computational
Linguistics:
Long Papers-Volume 1.
Association for Computational
Linguistics;
2012. p. 873–882.
[14]
Jones,
R., Kumar,
R., Pang,
B., Tomkins,
A.. I know what you did last summer:
query logs and user
privacy. In:
Proceedings of the sixteenth ACM conference on Conference on information and knowledge
management. ACM; 2007. p. 909–914.
[15]
Kabbur,
S.,
Han,
E.H.,
Karypis,
G..
Content-based methods for predicting web-site demographic
attributes.
In:
Data Mining (ICDM),
2010 IEEE 10th International
Conference on.
IEEE;
2010.
p.
863–868.
[16]
Kalchbrenner,
N.,
Grefenstette,
E.,
Blunsom,
P..
A convolutional
neural
network for modelling
sentences. In:
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics
(ACL). 2014. p. 655–665.
[17]
Kannan,
P., et al.
Digital marketing:
A framework, review and research agenda.
International Journal
of Research in Marketing 2017;34(1):22–45.
[18]
Kim,
Y..
Convolutional
neural
networks for sentence classification.
In:
Proceedings of
the 2014
Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014. p. 1746–1751.
[19]
Le,
Q.V.,
Mikolov,
T..
Distributed representations of sentences and documents.
In:
Proceedings of
the 31th international conference on Machine learning (ICML). 2014. p. 1188–1196.
[20]
LeCun,
Y.,
Bottou,
L.,
Bengio,
Y.,
Haffner,
P..
Gradient-based learning applied to document
recognition.
Proceedings of the IEEE 1998;86(11):2278–2324.
[21]
Manning,
C.D.,
Raghavan,
P.,
Sch¨
utze,
H..
Scoring,
term weighting and the vector space model.
Introduction to information retrieval 2008;100:2–4.
[22]
Mikolov,
T., Chen,
K., Corrado,
G., Dean,
J..
Efficient estimation of word representations in vector
space.
In:
Proceedings of
Workshop at First International
Conference on Learning Representations
(ICLR). 2013. .
[23]
Mikolov,
T., Sutskever,
I., Chen,
K., Corrado,
G.S., Dean,
J..
Distributed representations of words
and phrases and their compositionality.
In:
Advances in Neural Information Processing Systems. 2013.
p. 3111–3119.
[24]
Murray,
D.,
Durrell,
K..
Inferring demographic attributes of
anonymous internet users.
In:
Web
Usage Analysis and User Profiling. Springer; 2000. p. 7–20.
[25]
Paltoglou,
G., Thelwall,
M..
A study of information retrieval weighting schemes for sentiment anal-
ysis.
In:
Proceedings of
the 48th Annual
Meeting of
the Association for Computational
Linguistics.
Association for Computational Linguistics; 2010. p. 1386–1395.
[26]
Pedregosa,
F.,
Varoquaux,
G.,
Gramfort,
A.,
Michel,
V.,
Thirion,
B.,
Grisel,
O.,
Blondel,
M.,
Prettenhofer,
P., Weiss,
R., Dubourg,
V., Vanderplas,
J., Passos,
A., Cournapeau,
D., Brucher,
M.,
Perrot,
M.,
Duchesnay,
E..
Scikit-learn:
Machine learning in Python.
Journal
of Machine Learning
Research 2011;12:2825–2830.
[27]
Pennington,
J.,
Socher,
R.,
Manning,
C.D..
Glove:
Global
vectors for word representation.
In:
Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP). 2014. p. 1532–1543.
[28]
Quercia,
D.,
Kosinski,
M.,
Stillwell,
D.,
Crowcroft,
J..
Our twitter profiles,
our selves:
Predicting
personality with twitter.
In:
Privacy,
Security,
Risk and Trust (PASSAT) and 2011 IEEE Third
Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference
on. IEEE; 2011. p. 180–185.
[29]
ˇ
Reh˚uˇrek,
R., Sojka,
P.. Software Framework for Topic Modelling with Large Corpora. In:
Proceedings
of the LREC 2010 Workshop on New Challenges for NLP Frameworks. Valletta, Malta:
ELRA; 2010.
p. 45–50.
http://is.muni.cz/publication/884893/en.
[30]
Schuster,
M.,
Paliwal,
K.K..
Bidirectional
recurrent neural
networks.
Signal
Processing,
IEEE
18
Transactions on 1997;45(11):2673–2681.
[31]
Smola,
A.J.,
Sch¨
olkopf,
B..
A tutorial
on support vector regression.
Statistics and computing
2004;14(3):199–222.
[32]
Socher,
R.,
Lin,
C.C.,
Manning,
C.,
Ng,
A.Y..
Parsing natural
scenes and natural
language with
recursive neural
networks.
In:
Proceedings of the 28th international
conference on machine learning
(ICML-11). 2011. p. 129–136.
[33]
Socher,
R., Perelygin,
A., Wu,
J.Y., Chuang,
J., Manning,
C.D., Ng,
A.Y., Potts,
C..
Recursive
deep models for semantic compositionality over a sentiment treebank. In:
Proceedings of the conference
on empirical methods in natural language processing (EMNLP). 2013. p. 1631–1642.
[34]
Turian,
J.,
Ratinov,
L.,
Bengio,
Y..
Word representations:
a simple and general
method for semi-
supervised learning.
In:
Proceedings of the 48th annual meeting of the association for computational
linguistics. Association for Computational Linguistics; 2010. p. 384–394.
[35]
Volkova,
S., Bachrach,
Y..
On predicting sociodemographic traits and emotions from communications
in social
networks and their implications to online self-disclosure.
Cyberpsychology,
Behavior,
and
Social Networking 2015;18(12):726–736.
[36]
Volkova,
S.,
Bachrach,
Y.,
Van Durme,
B..
Mining user interests to predict perceived psycho-
demographic traits on twitter. In:
2016 IEEE Second International Conference on Big Data Computing
Service and Applications (BigDataService). IEEE; 2016. p. 36–43.
[37]
Zhang,
Y.,
Er,
M.J.,
Pratama,
M..
Extractive document summarization based on convolutional
neural networks. In:
Industrial Electronics Society, IECON 2016-42nd Annual Conference of the IEEE.
IEEE; 2016. p. 918–922.
[38]
Zhang,
Y.,
Er,
M.J.,
Venkatesan,
R.,
Wang,
N.,
Pratama,
M..
Sentiment classification using
comprehensive attention recurrent models.
In:
Neural
Networks (IJCNN),
2016 International
Joint
Conference on. IEEE; 2016. p. 1562–1569.
[39]
Zhang,
Y.,
Er,
M.J.,
Zhao,
R..
Multi-document extractive summarization using window-based
sentence representation. In:
Computational Intelligence, 2015 IEEE Symposium Series on. IEEE; 2015.
p. 404–410.
[40]
Zhang,
Y., Er,
M.J., Zhao,
R., Pratama,
M..
Multiview convolutional neural networks for multidoc-
ument extractive summarization.
IEEE Transactions on Cybernetics 2016;PP(99):1–11.
[41]
Zou,
W.Y.,
Socher,
R.,
Cer,
D.M.,
Manning,
C.D..
Bilingual
word embeddings for phrase-based
machine translation.
In:
EMNLP. 2013. p. 1393–1398.
19
