10
Topic Modeling for Wikipedia Link Disambiguation
BRADLEY SKAGGS and LISE GETOOR
, University of Maryland, College Park
Many articles in the online encyclopedia Wikipedia have hyperlinks to ambiguous article titles; these am-
biguous links should be replaced with links to unambiguous articles, a process known as disambiguation.
We propose a novel statistical topic model based on link text, which we refer to as the Link Text Topic Model
(LTTM), that we use to suggest new link targets for ambiguous links. To evaluate our model, we describe
a method for extracting ground truth for this link disambiguation task from edits made to Wikipedia in a
specific time period. We use this ground truth to demonstrate the superiority of LTTM over other existing
link- and content-based approaches to disambiguating links in Wikipedia. Finally, we build a web service
that uses LTTM to make suggestions to human editors wanting to fix ambiguous links in Wikipedia.
Categories
and Subject
Descriptors:
I.2.7 [Artificial
Intelligence]:
Natural
Language Processing—
Language models; I.5.1 [Pattern Recognition]: Models—Statistical; H.5.4 [Information Interfaces and
Presentation (e.g., HCI)]: Hypertext/Hypermedia—Navigation, User issues
General Terms: Design, Algorithms, Performance
Additional Key Words and Phrases: Topic modeling, link disambiguation, Wikipedia
ACM Reference Format:
Bradley Skaggs and Lise Getoor. 2014. Topic modeling for Wikipedia link disambiguation. ACM Trans. Inf.
Syst. 32, 3, Article 10 (June 2014), 24 pages.
DOI: http://dx.doi.org/10.1145/2633044
1.
INTRODUCTION
The continued usefulness of
web-based linked resources such as Wikipedia (http://
wikipedia.org) is facilitated by the ability of editors to quickly and easily create links
to related content. However, the target of a link may be ambiguous; for example, if an
editor specifies a target of “organ,” it is unclear whether that means a musical organ
or an anatomical organ.
In an attempt to keep track of concepts with ambiguous names, Wikipedia has intro-
duced the notion of a disambiguation page, a special article that lists the correct titles
of articles that could be associated with the ambiguous title, known as disambiguation
candidates. When a link is made to a disambiguation page, it should be disambiguated,
or fixed to point at the correct meaning.
In this work, we describe a novel method for automatically disambiguating links in
Wikipedia.
Our approach learns a latent-variable model we call the Link Text Topic
Model (LTTM) from the millions of existing links in Wikipedia. LTTM combines infor-
mation from the text of the links and the contents of linking pages to predict the correct
link targets of ambiguous links. Our approach can then be used to replace the target
of a link to a disambiguation page with the correct, unambiguous target.
This material is based on work supported by the National Science Foundation under grant nos. 0746930 and
1218488.
Authors’
addresses:
B.
Skaggs,
(Current address)
bradley.skaggs@acm.org;
L.
Getoor,
(Current address)
Computer Science Department, University of California, Santa Cruz.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted
without fee provided that copies are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights for components of this work owned
by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
2014 Copyright held by the Owner/Author. Publication rights licensed to ACM. 1046-8188/2014/06-ART10
$15.00
DOI: http://dx.doi.org/10.1145/2633044
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:2
B. Skaggs and L. Getoor
We develop a novel
evaluation framework,
in which we test our approach based
on snapshots of
Wikipedia which capture links that have been disambiguated by
Wikipedia editors.
We also present and compare several baseline approaches which
use frequency,
text,
and link information for predicting the correct targets for links.
We show that the LTTM is extremely accurate (61% compared to a baseline of 30%),
and is able to correctly predict the target with high confidence. In addition, we describe
a webservice which supports the use of our disambiguation model in real-time to find
and fix ambiguous links.
2.
BACKGROUND
2.1.
Wikipedia
Wikipedia,
the online,
user-edited encyclopedia,
is the sixth most frequently visited
website on the Internet, seen by 12% of global Internet users daily [Alexa.com 2012].
In total, Wikipedia has more than eight billion words in more than 19 million articles
in more than 270 languages; the English language version of Wikipedia by itself has
over two billion words in over 4.1 million distinct articles [Wikipedia 2011c, 2012].
Wikipedia is the Internet’s largest wiki, a website where almost any visitor may edit
almost any article at almost any time.
The MediaWiki software running Wikipedia
makes every previous version of each article available at any time, while providing a
standard view that defaults to the latest version of an article.
Almost any reader of Wikipedia can become an editor and make a change to an
article. The extensive content of Wikipedia is the result of the collaboration of many
millions of editors,
some of whom contribute by writing complete articles,
others by
fixing typographical and grammatical errors, and still others by flagging nonneutral
statements, identifying other stylistic issues, and correcting and refining content. There
are 3.5 million edits per month made to English Wikipedia [Wikipedia 2012]. Although
some editors have made more than ten thousand edits each,
more than 99% of the
31 million editors to English Wikipedia have made fewer than one hundred edits; as of
2012, 32% of all edits were made by these least experienced editors.
1
The breadth of coverage in Wikipedia,
its diversity of contributors,
and its almost
complete record of
changes,
have made it more than just a simple reference ency-
clopedia;
Medelyan et al.
[2009] give a thorough overview of the many efforts made
to apply the data in Wikipedia towards applications in natural language processing,
information retrieval, information extraction, and ontology building.
There is one perennial weakness in Wikipedia:
the existence of hyperlinks to am-
biguous terms.
For example,
the “Organ” article is not a regular article.
Since both
anatomical structures and musical instruments are commonly referred to by that term,
there are separate articles for these two meanings (as well
as several
others).
The
“Organ” article itself is a disambiguation page, an article that contains a list of links
to possible meanings of the title. Thus, any link to the “Organ” article should probably
be corrected to link to one of these possible meanings.
One main advantage an online encyclopedia like Wikipedia has over paper encyclo-
pedias is the abundance of relevant in-text hyperlinks between articles; the Wikipedia
Manual of Style suggests creating a link for the first instance of any word or phrase
that a reader is likely to also want to read, since these links aid readers in the explo-
ration of related topics [Wikipedia 2011b]. Besides aiding readers, these links are also
the fodder for semantic extraction tools. MediaWiki wikitext, the markup language in
which Wikipedia articles are written, makes turning a word or phrase into a hyperlink
a trivial action; an editor simply adds a matched pair of double square brackets around
1
These statistics were derived from the database dumps the Wikimedia Foundation makes available at
http://dumps.wikimedia.org/.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:3
Fig. 1
.
The top of the “Apple” article at http://en.wikipedia.org/wiki/Apple links to the “Apple (disambigua-
tion)” page.
that word or phrase. For example, the following wikitext contains a link to the “Organ”
and “Human body” articles: “The kidney is an [[organ]] in the [[human body]].”
When an editor adds a link in an article, the link should have a target article that is
about the topic being referenced. However, this is complicated by polysemous words and
phrases. Since an article title is how an article is referenced in a URL, the MediaWiki
software that runs Wikipedia does not allow two or more articles to share an identical
title. For example, the article about anatomical organs and the article about musical
organs cannot both have the same title “Organ.” If multiple articles could justifiably
have the same title, there is a set of standard practices the Wikipedia community has
to resolve title ambiguity.
One option is for the article associated with the primary
or earliest meaning of the title to be given that title, and any other articles be given
different but still related titles. For example, the title “Apple” is assigned to the article
about the fruit, and the article about the computer company has been given the title
“Apple Inc.” Another option is to add a word or phrase in parentheses expressing
the specific sense of the title at the end of an ambiguous title. Following this model, the
article about anatomical organs is called “Organ (anatomy),” and the article about the
family of musical instruments is called “Organ (music)”.
In cases where there is a clear dominant or root sense for the ambiguous title in
question, a special italicized hyperlink is added at the top of the article to point either
to other senses of the term (if there are only a few), or to a disambiguation page. In
English Wikipedia, a disambiguation page usually has “(disambiguation)” in its title.
For example, in Figure 1, the primary “Apple” article about the fruit links to the “Apple
(disambiguation)” page, which links in turn to “Apple,” “Apple Inc,” and several other
articles related the word “apple”. In the editions of Wikipedia in other languages, an
equivalent term to “disambiguation” in the appropriate language is used.
In instances where there is no generally agreed upon dominant or root sense,
the
disambiguation page itself is usually given the ambiguous title.
Figure 2 shows the
“Organ” disambiguation page containing a short list of disambiguation candidates of
the word Organ: “Organ (anatomy),” “Organ (music),” and several other senses.
These disambiguation pages are useful for users who reach pages directly by typing
into Wikipedia’s search box a word or phrase that happens to be ambiguous,
or by
following a link from an external website. If the disambiguation page is assigned the
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:4
B. Skaggs and L. Getoor
Fig. 2
.
The “Organ” disambiguation page, with links to disambiguation candidates, is available at http://en.
wikipedia.org/wiki/Organ.
ambiguous title, the reader can view the disambiguation candidates and pick the link
to the article related to the meaning they intended. If, instead, one of the meanings is
assigned the ambiguous title, the reader clicks the link at the top of the article to go to
the disambiguation page and then clicks on the link to their intended meaning.
As useful as disambiguation pages are for aiding in searching,
articles should not
directly link to disambiguation pages in their text;
it is almost always the case that
an editor intended a link to a disambiguation page to be a link to one of the possible
meanings of the phrase rather than the disambiguation page itself [Wikipedia 2011a].
We assume that the primary way these undesired links to disambiguation pages are
created is by an editor turning a word or phrase into a link, either after the original
text was added to Wikipedia, or in the process of writing the text. For example, if an
article about a musical band contained the word “organ,” an editor might turn that
into “[[organ]],” expecting the “Organ” article to be about the musical instrument. If
the editor does not check to ensure that the linked article is not a disambiguation page
and that the contents match their intended meaning, this ambiguous link will persist
until corrected by another editor.
To make a link with the same text as the original
link, but that instead points to one of the unambiguous meanings, the editor should
have changed the link to be “[[Organ (music)|organ]]”; in wikitext, the pipe character
separates the link destination from the text of the link visible to readers.
Due to the enormous breadth of coverage of Wikipedia, the English version had more
than 196,000 disambiguation pages in April 2011. Each of these pages contains a notice
that it is a disambiguation page and, if it follows the Wikipedia Manual of Style prop-
erly, a list of disambiguation candidates. It is easy for an editor to accidentally create
a link to a disambiguation article rather than a more appropriate link to one of the
article’s disambiguation candidates; in September 2010 there were 442 disambiguation
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:5
Fig. 3
.
Distribution of number of inlinks per disambiguation page in English Wikipedia.
pages in the English version of Wikipedia that each had 100 or more incoming links.
Figure 3, shows the full distribution of the number of inlinks per disambiguation page.
Since there are so many undesired links, an automated or semiautomated disambigua-
tion system would be very helpful to editors who work on replacing these links to assist
readers in moving quickly between relevant articles.
Helping editors fix ambiguous links aids other projects that extract information from
Wikipedia. One example is Freebase, a web-based database derived from Wikipedia and
other sources [Bollacker et al.
2008].
It permits complex queries of semistructured
information extracted from Wikipedia articles,
corresponding to questions such as
“What is the most populous city with a female mayor?” and “What British bands have
an organ player?” In this last example, if the Wikipedia article for a British band with
an organ player incorrectly linked to the “Organ” disambiguation page rather than to
the “Organ (music)” page, that band would be incorrectly omitted from the results of
this last query.
Besides Freebase,
Wikipedia is a growing data set for other natural
language processing, artificial intelligence, and machine translation systems [Bunescu
and Pasca 2006],
and replacing ambiguous links should also improve the quality of
these applications.
2.2.
Word Sense and Link Disambiguation
Wikipedia link disambiguation is related to the generic problem of word sense dis-
ambiguation, the process of determining which of several potential meanings a word
has in a given context. These different meanings may be different parts of speech, so
natural language processing applications involving sentence parsing or part-of-speech
tagging need to address word sense disambiguation at some level.
In most
applications
involving word sense
disambiguation,
there
is
a strict
dichotomy between the topics uniquely identifying each document and the words being
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:6
B. Skaggs and L. Getoor
disambiguated. In link disambiguation, however, the set of topics and the set of link
targets are the same.
The general word sense disambiguation problem has been studied for many years,
and Agirre and Edmonds [2006] provide recent in-depth coverage of many aspects of
word sense disambiguation, from knowledge-based methods to unsupervised corpus-
based methods, as well as the importance of word sense disambiguation to such natural
language processing applications as machine translation. State-of-the-art word-sense-
disambiguation techniques typically use the parts of speech and identity of the sur-
rounding words to perform disambiguation.
Link disambiguation is similar to word sense disambiguation because picking the
destination of an unambiguous link relates to picking the underlying meaning of the
linked phrase.
However,
our techniques incorporate the richer structure of the link
graph, rather than relying on just plain text. Most Wikipedia articles have many links;
with 102 million links in total, there were more than 22 links per article on average in
English Wikipedia in September 2010. There were 1.03 million ambiguous links in all.
Mihalcea [2007] is the first to apply general word sense disambiguation to Wikipedia.
Her system uses Wikipedia as a sense-tagged corpus and uses articles listed in disam-
biguation pages as classes for a naive Bayes classifier; the features used for each word
are the part of speech and local context of links to each disambiguation candidate. A
manual map was created between WordNet senses and Wikipedia articles for 51 words
and used to evaluate the system against the SENSEVAL evaluations of word sense
disambiguation systems.
The system showed a large improvement over the baseline
system.
The
“Wikify!”
system of
Mihalcea and Csomai
[2007]
takes
this
word-sense-
disambiguation system and applies it to the task of adding Wikipedia article hyperlinks
to an existing text document, a process known as wikification. The system compares
several methods of candidate extraction and candidate ranking,
the most successful
ranking algorithm being the ratio of the number of articles in which a specific candidate
word or phrase from the document appears as a hyperlink compared to the number of
articles in which it appears, regardless of its status as a link or not. Once a phrase is
identified as a link, it is put through Mihalcea’s previous disambiguation system.
There are now several different wikification systems desribed in the literature (see
Hachey et al.
[2013]
for a review),
but all
have to deal
with a Disambiguation to
Wikipedia (D2W) step in which a candidate article must be picked for a given text
word or phrase [Ratinov et al.
2011].
Milne and Witten [2008] used an approach to
disambiguation in their wikification system they call Wikipedia Link Relatedness based
on the amount of overlap of the sets of inlinking articles each disambiguation candidate
has with inlinks to the other articles linked in the source text; we will describe and use
this scoring system in Section 4.3.
2.3.
Topic Modeling
Topic modeling is the process of describing documents in a text corpus in terms of a
small number of topics, which are probability distributions over words. It is motivated
by the problems associated with the extremely high dimensionality of the standard
document-vector bag-of-words model. With hundreds of thousands to millions of words
in a vocabulary, documents are treated as members of a huge vector space. Applying
standard document similarity techniques such as cosine similarity to raw document-
vectors can result in inadequate performance,
since this approach suffers both from
complete separation of related concepts as well as confusing polysemous words.
Before probabilistic topic models became popular, Latent Semantic Analysis (LSA)
(also known as Latent Semantic Indexing (LSI)) was the dominant method of perform-
ing useful dimensionality reduction [Deerwester et al. 1990]. In LSA, singular vector
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:7
Fig. 4
.
Plate diagram for Probabilistic Latent Semantic Analysis.
Fig. 5
.
Plate diagram for Latent Dirichlet Allocation.
decomposition (SVD) is performed on a term-document matrix X, yielding X = U V
T
,
where U and V are orthogonal matrices and  is a diagonal matrix.
The k largest
entries in  correspond to the square roots of the nonzero eigenvalues of X
∗
X, and the
corresponding row vectors in U and V are the best k-dimensional approximations of X
under the Frobenius norm.
The probabilistic model behind LSA is not immediately obvious. Probabilistic Latent
Semantic Analysis (PLSA)
was a first attempt at putting LSA into a probabilistic
framework [Hofmann 1999]. In PLSA, a document is treated as a mixture of underlying
topics. The topics are shared among all the documents, but in varying proportions. Each
document has its own mixture of topics. Figure 4 shows PLSA using plate notation.
One downside to PLSA is that
it
is prone to overfitting,
since the number of
parameters grows linearly with the number of documents. In addition, it is difficult to
evaluate the effectiveness of PLSA at the core task of document modeling, because it
is impossible to assign a probability to a held-out document.
Latent Dirichlet Allocation (LDA) is a popular extension to PLSA that solves these
shortcomings [Blei et al. 2003]. In LDA, the PLSA model is modified so that Dirichlet
priors are placed on the topic distributions as well as the per-document topic mixtures.
LDA is a true probabilistic generative model for describing how a corpus of documents
is created, and its effectiveness for document modeling can be evaluated by measuring
the perplexity of held out documents. Figure 5 shows LDA in plate notation.
There are several possible ways of inferring underlying topics in the LDA model.
The original
paper uses variational
expectation maximization;
other authors have
used collapsed variational inference for performing inference in a batch on a single
machine [Teh et al. 2006b], distributed among many machines, as well as in a streaming
environment [Sato et al. 2010]. We will briefly describe the implementation of a Gibbs
sampler for performing topic inference. See Heinrich [2009] for a full exposition of the
derivation.
In Gibbs sampling for LDA inference,
the topic assignments for each term in the
corpus are assigned at random to one of the K topics.
Then,
over several iterations,
the topic assignment
for each term is resampled based on the topic assignments
in the current document and the count of topic assignments for that word over the
entire corpus.
This is a simple algorithm to implement,
and recent computational
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:8
B. Skaggs and L. Getoor
improvements provide effective techniques to scale to thousands of topics or more with
minimal performance penalty [Yao et al. 2009].
Regardless of the choice of inference algorithm, it is not instantly clear how to choose
values for the hyperparameters α and β.
Recent work has shown that in many text
corpora, it is sufficient to pick a symmetric β, but that asymmetry in α does a reasonably
good job of collecting stop words into a small number of topics, as well as resulting in
better perplexity for held out documents [Wallach et al. 2009]. Wallach [2008] provides
a straightforward algorithm for optimizing α and β by maximum likelihood estimation
between rounds of Gibbs sampling.
Another frequent question brought up in performing inference with the LDA model
is that of choosing the best way to pick the number of topics K; practitioners frequently
pick K via cross-validation with held-out data.
The Hierarchical
Dirichlet Process
(HDP) is an alternative model that can be thought of as an extension to LDA with
an infinite number of topics, a finitely many of which are actually used in the corpus
[Teh et al. 2006a]. In HDP, the number of topics used is not specified in advance; at
each stage of the inference process, the topic assignment for any word will likely reuse
previously used topics, but there is a small probability that it will be assigned to an
unused topic.
LDA has been applied to modeling graphical data; specifically, Latent Dirichlet Al-
location for Graphs (LDA-G) uses the LDA generative model to describe how edges are
created between nodes in a graph [Henderson and Eliassi-Rad 2009]. In LDA-G, a node
is treated as a document, and the outlinks are treated as the words of the document. By
performing inference on the model, latent groups in the graph can be discovered. This
model has proved useful to applications such as identifying researchers in different
topic areas based on a co-authorship graph [Eliassi-Rad and Henderson 2010].
3.
LINK TEXT TOPIC MODEL
We shall describe a novel topic model based on LDA that provides a generative model
for the targets of links (both ambiguous and unambiguous) in an article as well the
text of those links; we call this model the Link Text Topic Model (LTTM). We will then
show how we can perform posterior inference with this model, and further show how
to perform link disambiguation on many links at the same time.
3.1.
Generative Model
Instead of using LDA for modeling the words in an article, we model the creation of
links between articles; this model will prove useful in our disambiguation task. As in
LDA,
we assume that each article has associated with it a mixture of shared topics
drawn from a common Dirichlet distribution. However,
instead of each topic being a
distribution over words as in LDA, each topic in LTTM is now a distribution over target
articles. Furthermore, we stipulate that the text of each inlink to a specific article is
drawn from a link-target-specific multinomial distribution over possible text; the text
of a link may be a single word or a multi-word phrase.
The generative story for LTTM is similar to LDA,
and thus we will
use similar
notation.
First,
a global number of topics K is picked,
as well as a total number of
articles N and set of possible link texts of size V .
2
Three nonnegative vectors are chosen
as parameters for Dirichlet distributions: α is K-dimensional, β is N-dimensional, and
γ is V -dimensional. The article distribution for each of the K topics is chosen from a
Dirichlet distribution parameterized by β. For each article r that is ever linked to, the
2
In the generative story we say that the set of articles and possible link text is picked in advance;
when
we actually perform inference using LTTM, these are simply the set of articles along with all observed link
texts.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:9
Fig.
6
.
Plate diagram for the Link Text Topic Model.
A prime next to a variable indicates it is used for
ambiguous links.
distribution of possible link texts π
r
is chosen from a Dirichlet distribution over link
texts parameterized by γ .
To generate the links for an article d, we will pick an associated topic mixture θ ; as in
LDA, the mixture is chosen from a Dirichlet distribution parameterized by α. We also
pick the number of unambiguous links U
d
as well as the number of ambiguous links
A
d
. Then, for each link, we pick the topic z
i
= k for the link from the topic mixture. We
then choose the target a
i
= r for the link from the corresponding topic φ
k
. Finally, we
pick the text t
i
= l for the link from that target’s link-text distribution π
r
. The identity
of a
i
and t
i
is readily available for each of the U
d
normal links, but we will assume that
the link topic z
i
, article specific topic mixture θ , and global topic distributions φ
k
are
latent and must be inferred. We will use z

j
, a

j
, and t

j
to refer to the jth of A
d
ambiguous
links in an article.
For all links,
the text of the link is visible.
For links to regular pages,
we assume
that the identity of the link is visible. However, we will assume that the true targets
of links that are to disambiguation pages are latent. Figure 6 has the plate notation
for this model, with the variables associated with the ambiguous links denoted with a
prime.
This model is similar to the LDA-ER model used for entity resolution [Bhattacharya
and Getoor 2006]. However, we assume that most link targets will be visible (rather
than always latent as in LDA-ER).
Because of this,
we do not use a noise model for
the link text,
and instead use a multinomial with a Dirichlet prior;
texts are either
considered to be identical
(ignoring case),
or they are considered to be completely
distinct, regardless of any matching substrings.
Another related model is the Relational Topic Model (RTM) [Chang and Blei 2009].
In RTM, the text of documents is generated as in LDA, with the chance of links forming
between two documents proportional to a function of the topic distributions of the two
documents. This model does not take into account the text of the links.
3.2.
Posterior Inference
Given this model,
it is possible to infer the posterior distributions over the latent
variables.
We use a collapsed Gibbs sampler to determine the values of z,
z

,
and a

.
The values for θ , φ, and π are never explicitly sampled, but instead are integrated out.
We will use the notation n
m,k,r,t
to represent the number of times in document m that
topic k is used for a link to target article r with link text t, regardless as to whether
or not the link is ambiguous. In addition, if any subscript is replaced with a “·”, then
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:10
B. Skaggs and L. Getoor
that subscript is being summed over. Finally, appending “; ¬i” means that the counts
should not include the variables associated with link i in the corpus.
The value for topic z
i
associated with the link a
i
pointing to target article r in position
i in document m is resampled proportional to:
p

z
i
= k|z
¬i
,

z

, 
a,

a


=
n
(m,k,·,·;¬i)
+ α
k

K
k

=1
n
(m,k

,·,·;¬i)
+ α
k

·
n
(·,k,r,·;¬i)
+ β
r

N
r

=1
n
(·,k,r

,·;¬i)
+ β
r

(1)
∝ (n
(m,k,·,·;¬i)
+ α
k
) ·
n
(·,k,r,·;¬i)
+ β
r

N
r

=1
n
(·,k,r

,·;¬i)
+ β
r

.
(2)
The sampling of a topic for an ambiguous link is identical to that of an unambiguous
link; the value for topic z

j
associated with ambiguous link a

j
pointing to the current
target article r in position j in document m is resampled proportional to:
p

z

j
= k|

z

¬ j
, z, 
a,

a


∝ (n
(m,k,·,·;¬ j)
+ α
k
) ·
n
(·,k,r,·;¬ j)
+ β
r

N
r

=1
n
(·,k,r

,·;¬ j)
+ β
r

.
(3)
Finally,
the target a

j
of ambiguous link j
given the current topic assignments is
resampled proportional to:
p

a

j
= l|

z

, z, 
a,

a

¬ j
,

t,

t


=
n
(·,k,l,·;¬ j)
+ β
l

N
l

=1
n
(·,k,l

,·;¬ j)
+ β
l

·
n
(·,·,l,t;¬ j)
+ γ
t

V
t

=1
n
(·,·,l,t

;¬ j)
+ γ
t

(4)
∝ (n
(·,k,l,·;¬l)
+ β
l
) ·
n
(·,·,l,t;¬ j)
+ γ
t

V
t

=1
n
(·,·,l,t

;¬ j)
+ γ
t

.
(5)
Under the model as described, it is possible that any article may be chosen for the
link target a

of an ambiguous link; however, most will have a very small probability
of being picked. Since it seems unreasonable for a disambiguation approach to suggest
a target article that has never been associated with a given link text, we will restrict
our sampler to only choose values for a

j
that have been used before in some a
i
where
t

j
= t
i
; we will call this set of candidate values A

j
.
We have described our Gibbs sampler for sampling the topics and the links sep-
arately;
instead we could use blocked Gibbs sampling for the (z

j
, a

j
)
variable pair
associated with each ambiguous link.
However,
by doing so,
we would have to sam-
ple from K · |A
j
| values, where A

j
is the number of possible values for a

j
. By instead
sampling each z

j
and a

j
separately, we only have to sample from K + |A

j
| values per
pair.
Finally, at the very end of this process, we do not actually care about the values for z
and z

; the only thing that matters for disambiguation is a

. Thus, we do not actually
make final predictions for the topic assignments; we integrate them out and produce
probability distributions over the possible article targets.
To make it practical to perform inference in a model with a large number of topics,
we use the SparseLDA technique described in Yao et al. [2009] to efficiently perform
Gibbs sampling on the link variables. Also, we stream through the links and store their
newly sampled latent variables on disk to not require the entire link structure to fit in
memory. We iterate the resampling process many times.
Since an asymmetric topic distribution prior is important for finding effective topic
distributions in LDA [Wallach et al.
2009],
we use an asymmetric α in our model.
In addition,
we implement the hyperparameter estimation technique based on the
digamma recurrence relation described in Wallach [2008] to quickly and accurately
sample values for the hyperparameters α, β, and γ .
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:11
A typical run is several hundred iterations of Gibbs sampling of the link variables,
with the hyperparameter updates occuring every ten iterations after a burn in period
of fifty iterations. To illustrate example values for the hyperparameters, we found that
after performing inference on a one-thousand-topic model, the components of α ranged
from 1.41×10
−5
to 9.88×10
−3
with a median of 1.77×10
−4
,
β was 5.52×10
−4
,
and γ
was 1.12×10
−7
.
3.3.
Link Disambiguation and Prioritization
Wikipedia link disambiguation with LTTM requires performing posterior inference as
just described on a dataset consisting of all the links and link texts found in Wikipedia.
After having calculated the posterior distribution over disambiguation candidates for
the target of each ambiguous link, we can make a candidate suggestion for each link by
simply ranking the disambiguation candidates by their posterior probability of being
the target of the ambiguous link under the model.
With LTTM, the score we assign to each disambiguation candidate is a probability,
and can be interpreted as a degree of belief that a given link would be assigned a
specific candidate. These probabilities can be directly compared across different links;
we can say that a specific candidate for a specific ambiguous link is more likely to be
correct than a different candidate for a different link.
This means that we can rank
all of the top candidates for every ambiguous link, and say that we are more confident
about the higher ranked link-candidate pairs than the lower ranked pairs.
This is
useful for suggesting high confidence links to a human editor to quickly confirm; some
other disambiguation techniques discussed in Section 4 are not obviously comparable
between different links.
4.
ALTERNATE DISAMBIGUATION TECHNIQUES
We compare LTTM to seven other algorithms for disambiguation:
two simple base-
lines that predict popular link targets, three text-similarity approaches, a graph-based
random-walk approach, and a link-based approach.
4.1.
Baseline
In a pattern classification problem with high class skew,
a useful baseline is always
picking the most frequent class, regardless of the feature values of a specific instance;
it can be quite effective and often is hard to beat. In word sense disambiguation (see
Section 2.2), this is known as the most-frequent sense baseline and is common in evalu-
ating word sense disambiguation techniques [Gale et al. 1992a]. We will use two forms
of this baseline; the first we call Most Frequent Candidate (MFC) chooses the disam-
biguation candidate with the most inlinks of any kind,
and the second we call Most
Frequent Candidate for Text (MFC-T) chooses the disambiguation candidate with the
most inlinks with the text of the ambiguous link in question.
With link disambiguation in Wikipedia,
we see just such a class skew.
Figure 7
demonstrates how one of the musical senses of the text “organ” accounts for most of
the links. Thus, we use a most-common link baseline by comparing the number of links
from other articles to the different disambiguation candidates for an ambiguous link.
We simply pick the disambiguation candidate with the highest number of inlinks with
that text. For example, since “Organ (music)” had more inlinks than any page linked
with “organ,” all links to “Organ” with the text “organ” would be replaced with links to
“Organ (music).”
However, it is important to note that the specific text used to link to a disambiguation
page can alter the meaning.
For example,
Figure 8 shows that most links with the
text “organs” link to the anatomical sense rather than the musical sense;
thus,
the
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:12
B. Skaggs and L. Getoor
Fig. 7
.
The distribution of inlinks with the text “organ” demonstrates the large class skew that can arise in
disambiguation; 40 articles with a single inlink are not shown.
Fig. 8
.
The distribution of inlinks with the text “organs” has the anatomical sense dominate rather than
the musical sense; 10 articles with a single inlink are not shown.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:13
text-specific most-frequent-class baseline would predict “Organ (anatomy)” for any
link with “organs” as the link text.
4.2.
Text-Based Approaches
The next set of approaches we consider involves the text of the articles, but not explicitly
the link structure.
Text Similarity. The first text-similarity technique we consider is Jaccard similarity
(Jacc) on the sets of words found in articles; looking only at the text of the articles in
question and not at the existence or frequency of any links, we pick the disambiguation
candidate that has the highest Jaccard similarity between the set of words present
in the candidate article page and the set of words in the source article. If W (d) is the
set of words in article d, and W (d

) is the set of words in article d

, then the Jaccard
similarity is defined as the cardinality of the intersection of the two word sets divided
by the cardinality of their union;
sim
Jacc
(d, d

)
=
|W(d)∩W(d

)|
|W(d)∪W(d

)|
.
The Jaccard similarity
ranges from zero, if the documents have no words in common, to one, if the documents
each contain the exact same set of words.
3
A second text-similarity technique we consider is tf–idf similarity. In tf–idf simi-
larity, we look at the the cosine similarity of articles under a tf–idf weighting scheme.
We let tf
t,d
be the term frequency of term t in article d,
the number of times term t
appears in article d.
We let df
t
be the document frequency of term t,
the number of
articles in which term t appears; we further let N be the total number of articles in
Wikipedia. We then map each article d to a vector ˆ
w
d
= w
t,d
indexed by term t, where
w
t,d
= (1+log tf
t
)·log
N
df
t
. To compare articles d and d

, we calculate the cosine similarity
between the two vectors ˆ
w
d
and ˆ
w
d

, sim
tf –idf
( ˆ
w
d
, ˆ
w
d

) =
ˆ
w
d
· ˆ
w
d

ˆ
w
d
ˆ
w
d

. With this weighting,
terms common to both articles that are also present in many articles contribute less
to the similarity than terms common to both that are present in few other articles.
There are other possible tf–idf weighting schemes, but this was the most effective of
the several we considered.
Latent Dirichlet Allocation. A third text-similarity technique we use is one based
on LDA similarity.
We assume that the text of Wikipedia articles is generated by a
100-topic LDA model.
We use the Gensim framework [
ˇ
Reh
˚
u
ˇ
rek and Sojka 2010] for
doing the model inference, since it allows for streaming inference without the need to
fit the entire corpus in memory.
With each Wikipedia article represented as a probability distribution over topics, we
next need some way to measure the similarity between topic distributions associated
with each article.
We use the popular Jensen-Shannon divergence to compare these
distributions; the disambiguation candidate we select for a given ambiguous link is the
one with a topic distribution that has the smallest Jensen–Shannon divergence with
the linking article.
4.3.
Link-Based Approaches
Due to the rich link structure of Wikipedia,
it is also reasonable to consider disam-
biguation techniques based solely on the links between articles.
Random Walk with Restart. The first link-based disambiguation technique we con-
sider is Random Walk with Restart (RWR),
also known as Personalized PageRank
3
For tokenization in this and other text-based approaches,
we use the StandardAnalyzer class from the
Apache Lucene software library.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:14
B. Skaggs and L. Getoor
[Berkhin 2006]. In this approach, we rank disambiguation candidates by their proba-
bility of being visited in a modified random walk on the Wikipedia link graph originat-
ing with the linking article, after first removing the original link to the disambiguation
page. Rather than always following random outlinks as in a normal random walk, a
random outlink is followed from a node with probability 1 − α, and with probability α,
the random walk returns to the originating node and restarts a random walk.
The
effect is similar to that of standard PageRank [Brin and Page 1998], but the rankings
are dependent on the originating node. We calculate approximate probabilities using
the Bookmark-Coloring Algorithm described by Berkhin [2006].
Link Relatedness. Another
link-based disambiguation approach we consider
is
Wikipedia Link Relatedness by Milne and Witten [2008], which is based on Normalized
Google Distance [Cilibrasi and Vitanyi 2007]. If A is the set of links into article a, and
B the set of links into b, and W the set of all Wikipedia articles, define:
relatedness(a, b) =
log(max(| A|, |B|)) − log(| A ∩ B|)
log(|W |) − log(min(| A|, |B|))
.
Relatedness would be zero if articles a and b have identical source articles linking in,
and it would be infinite if there is no overlap between the two sets. One problem with
the approach of Milne and Witten is that they use a weighted average of the relatedness
score between all the links in the source document and each disambiguation candidate;
if one of these scores is infinite, the average is thus infinite. As this average appears
motivated by the uncertainty of the links being used (since the application was for
wikification of completely unlinked text, rather than disambiguating existing links),
we choose to simply take the smallest relatedness score rather than an average.
To incorporate this link relatedness into a disambiguation algorithm,
we take the
article and determine all outlinks,
except to the ambiguous page in question.
Then,
we find the disambiguation candidate article with the minimum relatedness to any
noncandidate article linked from the source article, plus the source article itself.
5.
EVALUATION AND RESULTS
In order to evaluate our algorithms, we first describe the extraction of disambiguation
candidates. We then begin by describing the construction of our evaluation dataset. We
then move on to presenting results.
5.1.
Disambiguation Page Identification and Disambiguation Candidate Extraction
All the algorithms we consider require us to automatically identify ambiguous links
and make a suggestion of a disambiguation candidate. Therefore, we need to find all
the disambiguation pages and extract the disambiguation candidates from each page.
Since the MediaWiki software has no special internal representation of disambiguation
pages, we make use of the Wikipedia community standards to identify and extract the
information we need.
In Wikipedia,
the Manual
of Style covers many aspects of article creation,
rang-
ing from the proper use of dashes to general
layout guidelines for various kinds of
articles. Specifically, there is a section dedicated to the layout of disambiguation pages
[Wikipedia 2011b]. Importantly, the guide indicates the various templates that can be
placed on a page to identify it as a disambiguation page.
Thus,
we can identify dis-
ambiguation pages by going through all articles and finding those pages that contain
these templates.
The Wikipedia Manual of Style suggests that the various disambiguation candidates
should be placed in a specially formatted list, ideally with only links to disambiguation
candidates in the list; links to any other articles should be avoided, to make it easy for a
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:15
Fig. 9
.
The first page of the “Java (disambiguation)” article shows grouped disambiguation candidates. The
whole list is viewable at http://en.wikipedia.org/wiki/Java (disambiguation).
reader to know what to click on in each line. For many pages, this is a simple one-level
list, but for some topics, such as “Java (disambiguation)”, there is a complicated hierar-
chy (Figure 9). This hierarchy information potentially could be used in a hierarchical
classification system, but we currently flatten such a list to treat all disambiguation
candidates on equal footing.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:16
B. Skaggs and L. Getoor
The effectiveness of our algorithms rely on disambiguation pages following these
guidelines. We will ignore any links that do not appear in list form; making suggestions
to expand a disambiguation page would be an interesting problem in and of itself.
A simple review of 100 randomly chosen English disambiguation pages shows that
93 had their disambiguation candidates appear all
in list form;
7 had at least one
disambiguation candidate appear only elsewhere in the page, but not in a list. However,
27 of the 100 had more than one link per line, with the extra links often being very
general (for example, country names or years) that should not be treated as synonyms.
Because of this, we consider the heuristic of treating all the links in a single line of a
list as potential targets as too overly inclusive to be used effectively.
Instead of trying to derive possible candidates from disambiguation pages, we extract
candidates from the text of links, which is both convenient and effective; if a specific
link text has been used to link to a page before,
it is reasonable to consider it as a
potential link target in other contexts.
For each link to an ambiguous page,
we will
construct a list of all possible articles that are linked to with the same link text; we will
use this list as our disambiguation candidates. The distribution of number of candidate
sets of different sizes can be seen in Figure 10; the disambiguation page that has the the
largest such set of disambiguation candidates is “Here,” with 17,771 distinct targets.
5.2.
Evaluation
In order to use and evaluate the different disambiguation algorithms,
we need the
text content and outlinks for each article in Wikipedia.
Periodically,
the Wikimedia
Foundation makes available for download XML snapshots of the contents of the differ-
ent language editions of Wikipedia (http://download.wikipedia.org).
These snapshots
provide enough information to extract the data we need; they provide basic metadata
about each article such as title and last modification date, in addition to the wikitext
content of each article.
4
To determine ground truth,
we find all
links to disambiguation pages present at
one point in time in English Wikipedia that were later removed by human editors
and replaced with the same text but different targets.
We identify disambiguation
pages by finding those pages that included a disambiguation template, as discussed in
Section 5.1.
Our evaluation technique of mining previous edits to Wikipedia avoids having hu-
mans manually assess several hundred disambiguation predictions, as has been done
in previous work [Milne and Witten 2008].
Instead,
we leverage the work that has
already been done by the many Wikipedia editors who have undertaken the man-
ual disambiguation of links, and we can evaluate many thousands of disambiguation
predictions without any further human interaction.
To find our evaluation data,
we identify all
links to disambiguation pages in the
September 2010 snapshot of
English Wikipedia;
we then find links with identical
text but to a different target in the October 2010 snapshot.
This results in 36,009
links, of which we pick 1,000 at random for our test set. We are thus only evaluating
disambiguations that keep the same visible text;
if
the text is altered,
we are not
using it for evaluation. We do not take into account the location in the article or the
surrounding text of a link, so we consider a link to be unchanged if it is deleted and a
4
It is possible to import these snapshots into a private installation of
MediaWiki
to create a mirror of
Wikipedia. As part of this process, a list of links between pages is automatically generated. However, this
list of links makes no distinction between links directly included in the wikitext of an article, and those links
indirectly included via MediaWiki’s template expansion mechanism. Since the links included via a template
are duplicated for every page that includes that template,
and since these links are not visible to a user
when they are editing the wikitext for a page, we choose to ignore these links by extracting links directly
from the source wikitext.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:17
Fig. 10
.
Disambiguation-candidate-set size dis-
tribution for all disambiguation pages in English
Wikipedia in October 2010.
Fig. 11
.
Disambiguation-candidate-set sizes for
each fixed link in English Wikipedia between
September and October 2010.
Fig. 12
.
An example disambiguation in English Wikipedia made between September 2010 and October 2010
shows that a link in the “Bare Wires” article has been disambiguated by an editor from “Organ” to “Organ
(music)”.
new one is added elsewhere in an article to the same target. To measure the accuracy
of the various disambiguation techniques, we use them to make predictions for the new
targets of the links based on data in the September snapshot;
we consider a correct
prediction to be one that matches the new target of the link in the October snapshot.
Figure 11 shows the distribution of disambiguation-candidate-set sizes for the links
that were fixed; if multiple links to the same disambiguation page are fixed, that page’s
disambiguation-candidate-set size is counted multiple times. Figure 12 is an example
of an ambiguous link present in September that was fixed by October.
The motivating assumption for this evaluation technique is that blatant errors are
not likely to persist in Wikipedia. The ease-of-editing at the heart of Wikipedia does al-
low for malicious users to corrupt the content of articles, as well as permit well-meaning
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:18
B. Skaggs and L. Getoor
Fig. 13
.
Accuracy of eight different disambiguation algorithms on English Wikipedia,
at the top position
and in the top three positions.
users to mistakenly submit incorrect information. Priedhorsky et al. [2007] have clas-
sified the kinds of damage that takes place in Wikipedia and assessed how long the
damage persists. Using edit data and view logs, they estimate that 42% of all damaging
edits to English Wikipedia are fixed on the next page view, and roughly 70% are fixed
within ten page views. If an erroneous edit is made, it is likely to be corrected.
5.3.
Results
On our test set,
the most-frequent-candidate baseline achieved 30.1% accuracy,
and
the text-specific most-frequent-candidate baseline achieved 38.2% accuracy.
For text
similarity,
the Jaccard-similarity approach was 33.5% accurate,
the tf–idf approach
was 38.5% accurate, and the LDA approach was only 28.7% accurate. For link-based
approaches, Random Walk with Restart was 53.2% accurate when there was a restart
probability α of 0.3,
and Link Relatedness was 47.0% accurate.
We investigated the
effect of altering α in the random walk algorithm; adjusting this parameter over a wide
range of values had only a small decrease in accuracy.
Finally,
our novel LTTM ap-
proach with 1,000 topics was 61.9% accurate, the best of all approaches we considered.
Figure 13 compares these results,
which also demonstrate the improved accuracy of
each technique when we consider a correct answer to be one in the top three suggestions.
Since the scores produced by LTTM are probabilities, they are directly comparable
across predictions; it is possible to say a specific link is more likely to have one target
than another link is to have a different target based on their relative scores.
We
therefore considered ranking the chosen disambiguation candidates across all 1,000
links to see if any techniques were particularly good at the highest scores. As can be
seen in Figure 14,
LTTM was most effective at the highest scores.
We see a similar
pattern when we further applied LTTM to all possible disambiguated links, as shown
in Figure 15.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:19
Fig.
14
.
Cumulative accuracy by rank of eight
different disambiguation algorithms on English
Wikipedia test set.
Fig. 15
.
Cumulative accuracy by rank for LTTM
on all 36,009 disambiguated links.
Fig. 16
.
Counts of high-frequency links in four sample topics from a 1,000-topic LTTM model of Wikipedia
representing Maryland, Southeast Asia, The Simpsons television show, and Greek mythology.
We note that there appears to be a brief dip at accuracy for the highest probable
links under the LTTM model. We looked at the mistakes made in the highest probable
link suggestions to the left of the trough in Figure 14, and they all occured when a link
text has only been observed linking to two articles (the disambiguation page and one
regular article), but the ambiguous link used for evaluation was changed to point to a
completely new article. Under the LTTM, we naturally assign a very high probability
to the old regular article, as it is the only choice under our sampling strategy.
In addition to the disambiguation predictions, performing inference on LTTM also
produces each topic’s distribution over links. For example, Figure 16 shows the highly
probable links from four topics in the 1,000-topic LTTM model of Wikipedia.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:20
B. Skaggs and L. Getoor
6.
DISAMBIGUATION WEB SERVICE IMPLEMENTATION
Building on the experimental effectiveness of the LTTM results, we constructed a web
interface to aid Wikipedia editors in link disambiguation using the model.
We first
created a web server to host the various tools we need. We used the Sinatra microweb
framework for the Ruby programming language to make it easy to both serve static
JavaScript files, as well as respond to dynamic requests for disambiguation suggestions.
We used the JRuby implementation of the Ruby programming language and the Hadoop
Distributed File System for storage of our data. By building on Java-based technology,
the system is able to run unchanged on a variety of operating systems.
We started with the XML database dumps available from the Wikimedia Foundation.
We downloaded the latest XML file for English Wikipedia. We then processed the XML
to add page metadata and article text to our database. We also processed every article
to extract the links to other articles contained in the wikitext.
We then performed
inference on the links and their text using LTTM; we saved the topic distributions and
the link text distributions for performing inference to disambiguate links on demand.
“Navigation Popups” is a preexisting JavaScript add-on to Wikipedia that provides
several additions to the standard Wikipedia web interface.
First,
it provides a short
summary of the target page when a reader’s mouse pointer hovers over a link. Second,
it provides rudimentary disambiguation capabilities. By hovering over an ambiguous
link, the user may disambiguate it by clicking on one of the options in the displayed
set of disambiguation candidates. This list is automatically extracted from the disam-
biguation page,
and no recommendation is made.
Also,
there is no visual indication
that a link is ambiguous until the reader’s mouse pointer is hovering over it.
We extended Navigation Popups in two ways: first, we provide visual highlights to
indicate to the user the presence of ambiguous links; second, we calculate the LTTM
probabilites for the true destination of ambiguous links and return the highest-scoring
disambiguation candidates by these probabilities.
To use our extended popups, a user adds a few lines of JavaScript to their Wikipedia
JavaScript user page. From then on, whenever the user visits a Wikipedia page, their
browser loads and executes a script from our server,
making a list of
the links on
the page,
and sends it via an AJAX request to our server for analysis.
The server
queries the database to see if any are ambiguous, and the identity of any ambiguous
links is returned to the user’s browser. Then, the browser goes through the links and
highlights the ambiguous ones in yellow, as well as providing the user with a count of
ambiguous links at the top of the article.
Figure 17 illustrates how a user changes an ambiguous link. The highlights make it
easy for a user to see ambiguous links on the page. The user then points their mouse
at one such link, and another AJAX request is sent to our server with the list of other
links on the page. Our server then performs inference on just the article in question
using cached statistics from the other articles. This allows us to work with older copies
of the pages for building the initial model,
but then use the latest copy of the page
being edited in case links have been changed.
The distribution over disambiguation
candidates is calculated and sent back to the user’s browser,
where the most likely
candidates are displayed in the popup for the user to choose from; the links are listed
with the highest-probability links first. When a choice is clicked, the edit is immediately
processed and saved by Wikipedia.
The software for running our disambiguation server is available for download at
https://github.com/bskaggs/link
text topic model.
We provide instructions for ingest-
ing the latest XML database dumps, training LTTM on the data, as well as configuring
a user’s Wikipedia account to connect to the system.
In addition to our popup-based
interface, the service is easily extensible to support other modes of access.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:21
Fig. 17
.
The changed color and border of this link indicates that it is ambiguous, and needs to be corrected.
When the ambiguous link is hovered over with the mouse,
a set of disambiguation candidates appears,
ranked by probability under LTTM.
7.
DISCUSSION
The structure of
LTTM lends itself
to modification.
We see in the graphical
model
a structural component identical to LDA.
Thus,
it is straightforward to replace that
component with another topic model, such as the Hierarchical Dirichlet Process [Teh
et al. 2006a] which dynamically chooses an appropriate number of topics. In addition,
this nonparametric Bayesian model makes model selection easier.
It is worth going beyond just the text of the link being disambiguated, and consider
the surrounding text.
Using a combination of local and global contexts,
as has been
shown useful in information retrieval [Xu and Croft 1996],
may prove beneficial for
disambiguation. In addition, using a language model instead of a simple multinomial
distribution for a target article’s link-text distribution could handle never-before seen
texts such as misspellings.
There are several
improvements to consider.
One is to try new text
similarity
scores to see if
they provide better results.
Another approach would be to train a
per-disambiguation-page classification algorithm such as a support vector machine,
where the features are the words or links already existing on a page. For an individual
disambiguation page with disambiguation candidates that have many inlinks,
there
may be enough data to train a support vector machine to predict the disambiguation
candidate for a link from a given article. Furthermore, it would be possible to combine
the scores from all the algorithms we considered into one score using a ranking support
vector machine [Herbrich et al. 1999].
It would also be interesting to compare the effectiveness of
these approaches on
different language editions of Wikipedia. There may be different factors that affect how
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:22
B. Skaggs and L. Getoor
they perform: link density and article length are two examples. Also, Wikidata (http://
wikidata.org),
a new project of the Wikimedia Foundation to build a cross-language
knowledge base of structured data, will make it easy to incorporate the link structure of
one language edition of Wikipedia when making disambiguation decisions for another
language.
At the word level,
this kind of cross-language approach has been shown
to be effective in word sense disambiguation [Gale et al.
1992b].
Beyond additional
link information, we could extend our LTTM approach by modeling the creation of the
nonlinked words at the same time as the links.
Our models do not take into account any features of the edits themselves; it may be
that registered users do a better job of creating correct links, and perhaps experienced
editors even more so. If true, we could take advantage of this link quality by modifying
LTTM to incorporate the editor who added the link as a variable that affects either the
topic or a probability that an observed link is actually incorrect. Also, the amount of
time a link has lasted in an article is a good proxy for validity, as previous experiments
have shown with vandalism [Potthast et al. 2008].
Harnessing the edits of Wikipedia users in evaluating algorithms in natural lan-
guage processing may prove fruitful
in many areas.
First,
the effectiveness of
the
aggregate “wisdom of crowds” needs to be validated against standard metrics of in-
terannotator agreement. Previous work on Wikipedia disambiguation techniques has
used human judges to determine accuracy. One project used Amazon Mechanical Turk
for the evaluation [Milne and Witten 2008];
users from around the world were paid
to assess 449 links in 50 documents. To apply that evaluation for the techniques we
discuss here, a subset of the links disambiguated in our tests could be given to humans
to manually disambiguate, and then compare the results. Or, we could boldy commit
the suggestions to Wikipedia, and observe which of them are corrected.
8.
CONCLUSION
Semiautomated disambiguation is a promising approach to tackling the huge number
of ambiguous links in Wikipedia. By building a system that incorporates the text and
link structure of the rest of Wikipedia, we can be effective at improving this data source,
to ultimately improve any application that uses it.
We have shown that our proposed latent-variable model effectively makes use of both
text and link information and does a better job in terms of predictive accuracy than
several alternative models based on frequency, text, and link information. Our model
is scalable, capable of handling a dataset of several million highly-linked documents.
We developed a novel
evaluation dataset,
based on edits to Wikipedia,
and a web-
based tool for using our model to improve Wikipedia.
Both of these are available at
https://github.com/bskaggs/link
text topic model.
ACKNOWLEDGMENTS
The authors would like to thank the reviewers for their insightful comments. They would also like to thank
Prof. Philip Resnik of the University of Maryland, College Park, for his unfailing support to B. Skaggs during
his master’s thesis research on which this article is based.
REFERENCES
Eneko Agirre and Philip Edmonds (Eds.). 2006. Word Sense Disambiguation: Algorithms and Applications.
Text, Speech and Language Technology, Vol. 33, Springer.
Alexa.com. 2012. Wikipedia.org Site Info. http://www.alexa.com/siteinfo/wikipedia.org.
Pavel Berkhin. 2006. Bookmark-coloring algorithm for personalized pagerank computing. Internet Math. 3,
1, 41–62.
Indrajit Bhattacharya and Lise Getoor. 2006. A latent Dirichlet model for unsupervised entity resolution. In
Proceedings of the SIAM Conference on Data Mining (SDM). 47–58.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
Topic Modeling for Wikipedia Link Disambiguation
10:23
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. J. Machine Learn.
Res. 3, 993–1022.
Kurt Bollacker,
Colin Evans,
Praveen Paritosh,
Tim Sturge,
and Jamie Taylor.
2008.
Freebase:
A collab-
oratively created graph database for structuring human knowledge.
In Proceedings of the ACM SIG-
MOD International Conference on Management of Data (SIGMOD’08).
ACM,
New York,
1247–1250.
DOI:http://dx.doi.org/10.1145/1376616.1376746
Sergey Brin and Lawrence Page.
1998.
The anatomy of a large-scale hypertextual web search engine.
In
Proceedings of the 7th International Conference on World Wide Web. 107–117.
Razvan Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European Chapter of the Association for Computational
Linguistics (EACL’06). 9–16.
Jonathan Chang and David M. Blei. 2009. Relational topic models for document networks. J. Machine Learn.
Res. 81–88.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The Google similarity distance. IEEE Trans. Knowl. Data
Eng. 19, 3, 370–383.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. J. Amer. Soc. Inf. Sci. 41, 6, 391–407.
Tina Eliassi-Rad and Keith Henderson.
2010.
Literature search through mixed-membership community
discovery.
In Proceedings of the International Conference on Social Computing,
Behavioral Modeling,
and Prediction. 70–78.
William Gale,
Kenneth W.
Church,
and David Yarowsky.
1992a.
Estimating upper and lower bounds on
the performance of word-sense disambiguation programs. In Proceedings of the 30th Annual Meeting on
Association for Computational Linguistics (ACL’92). Association for Computational Linguistics, Strouds-
burg, PA, 249–256. DOI:http://dx.doi.org/10.3115/981967.981999
William Gale, Kenneth W. Church, and David Yarowsky. 1992b. Using bilingual materials to develop word
sense disambiguation methods.
In Proceedings of
the International
Conference on Theoretical
and
Methodological Issues in Machine Translation: Empiricist vs. Rationalist Methods in MT. 101–112.
Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating entity
linking with Wikipedia. Artificial Intell. 194, 130–150.
Gregor Heinrich. 2009. Parameter estimation for text analysis. Tech. Rep., Fraunhofer Institut f
¨
ur Graphis-
che Datenverarbeitung.
Keith Henderson and Tina Eliassi-Rad. 2009. Applying latent Dirichlet allocation to group discovery in large
graphs. In Proceedings of the ACM Symposium on Applied Computing. 1456–1461.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer. 1999. Support vector learning for ordinal regression.
In Proceedings of the International Conference on Artificial Neural Networks. 97–102.
Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Conference on Uncer-
tainty in Artificial Intelligence. 289–296.
Olena Medelyan, David Milne, Catherine Legg, and Ian H. Witten. 2009. Mining meaning from Wikipedia.
Int. J. Hum. Comput. Stud. 67, 9, 716–754.
Rada Mihalcea.
2007.
Using Wikipedia for automatic word sense disambiguation.
In Proceedings of
the
Conference of the North American Chapter of the Association for Computational Linguistics:
Human
Language Technologies. 196–203.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: Linking documents to encyclopedic knowledge. In Pro-
ceedings of the ACM Conference on Information and Knowledge Management. 233–242.
David Milne and Ian H.Witten. 2008. Learning to link with Wikipedia. In Proceeding of the ACM Conference
on Information and Knowledge Management. 509–518.
Martin Potthast, Benno Stein, and Robert Gerling. 2008. Automatic vandalism detection in Wikipedia. In
Proceedings of the European Conference on Information Retrieval. 663–668.
Reid Priedhorsky, Jilin Chen, Shyong (Tony) K. Lam, Katherine Panciera, Loren Terveen, and John Riedl.
2007. Creating, destroying, and restoring value in Wikipedia. In Proceedings of the International ACM
Conference on Supporting Group Work. 259–268.
Lev Ratinov,
Dan Roth,
Doug Downey,
and Mike Anderson.
2011.
Local and global algorithms for disam-
biguation to Wikipedia.
In Proceedings of
the Annual
Meeting of
the Association for Computational
Linguistics: Human Language Technologies. 1375–1384.
Radim
ˇ
Reh
˚
u
ˇ
rek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Pro-
ceedings of the LREC Workshop on New Challenges for NLP Frameworks. 45–50.
Issei Sato, Kenichi Kurihara, and Hiroshi Nakagawa. 2010. Deterministic single-pass algorithm for LDA.
In Advances in Neural Information Processing Systems, 2074–2082.
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
10:24
B. Skaggs and L. Getoor
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006a. Hierarchical Dirichlet pro-
cesses. J. Amer. Statist. Assoc. 101, 476, 1566–1581.
Yee Whye Teh, David Newman, and Max Welling. 2006b. A collapsed variational bayesian inference algorithm
for latent dirichlet allocation. In Advances in Neural Information Processing Systems, 1378–1385.
Hanna M. Wallach. 2008. Structured topic models for language. Ph.D. Dissertation, University of Cambridge.
Hanna M. Wallach, David Mimno, and Andrew McCallum. 2009.
Rethinking LDA: Why priors matter.
In
Advances in Neural Information Processing Systems, 1973–1981.
Wikipedia. 2011a. Wikipedia:Disambiguation. Wikipedia, The Free Encyclopedia. http://en.wikipedia.org/w/
index.php?title=Wikipedia:Disambiguation&oldid=462966392.
Wikipedia.
2011b.
Wikipedia:Manual of Style.
Disambiguation pages,
Wikipedia,
The Free Encyclopedia.
http://en.wikipedia.org/w/index.php?title=Wikipedia:Manual of Style (disambiguation pages)&oldid=
447636436.
Wikipedia. 2011c. Wikipedia:Size comparisons. Wikipedia, The Free Encyclopedia. http://en.wikipedia.org/w/
index.php?title=Wikipedia:Size comparisons&oldid=451578590.
Wikipedia. 2012. Wikipedia statistics English. http://stats.wikimedia.org/EN/TablesWikipediaEN.htm.
Jinxi Xu and Bruce W. Croft. 1996. Query expansion using local and global document analysis. In Proceedings
of the Annual International ACM Special Interest Group on Information Retrieval (SIGIR) Conference
on Research and Development in Information Retrieval. 4–11.
Limin Yao,
David Mimno,
and Andrew McCallum.
2009.
Efficient methods for topic model
inference on
streaming document collections. In Proceedings of the ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. 937–946.
Received April 2013; revised December 2013; accepted December 2013
ACM Transactions on Information Systems, Vol. 32, No. 3, Article 10, Publication date: June 2014.
