Limits to Surprise in Recommender Systems
Andre Paulino de Lima
Universidade de Sao Paulo
andre.p.lima@usp.br
Sarajane Marques Peres
Universidade de Sao Paulo
sarajane@usp.br
ABSTRACT
In this study,
we address the challenge of measuring the ability
of a recommender system to make surprising recommendations.
Although current evaluation methods make it possible to determine
if two algorithms can make recommendations with a significant
difference in their average surprise measure, it could be of interest
to our community to know how competent an algorithm is at em-
bedding surprise in its recommendations, without having to resort
to making a direct comparison with another algorithm. We argue
that a) surprise is a finite resource in a recommender system,
b)
there is a limit to how much surprise any algorithm can embed in
a recommendation,
and c) this limit can provide us with a scale
against which the performance of any algorithm can be measured.
By exploring these ideas, it is possible to define the concepts of max-
imum and minimum potential surprise and design a surprise metric
called “normalised surprise” that employs these limits to potential
surprise.
Two experiments were conducted to test the proposed
metric. The aim of the first was to validate the quality of the estim-
ates of minimum and maximum potential surprise produced by a
greedy algorithm. The purpose of the second experiment was to
analyse the behaviour of the proposed metric using the MovieLens
dataset.
The results confirmed the behaviour that was expected,
and showed that the proposed surprise metric is both effective and
consistent for differing choices of recommendation algorithms, data
representations and distance functions.
CCS CONCEPTS
• Information systems → Evaluation of retrieval results
; Re-
trieval effectiveness;
KEYWORDS
Recommender systems, beyond accuracy properties, serendipity,
surprise, unexpectedness, evaluation method, evaluation metrics,
off-line evaluation, one plus random.
1
INTRODUCTION
It has been well established that the effect of beyond-accuracy prop-
erties on user satisfaction is a critical success factor in deploying
a recommender system [15, 22]. Among these properties, surprise
has recently been the subject of several studies owing to its links
to serendipity [
1
,
18
,
31
] and the problem of over-specialisation in
content-based recommender systems [
8
], as well as its importance
in some application domains [25].
In the literature,
the notion of surprise generally reflects the
capacity to make recommendations that are dissimilar from the
items known to a given user:
the more a recommended item is
dissimilar,
the more it is surprising [
1
,
18
,
33
].
Current surprise
metrics and evaluation methods allow us to estimate the average
surprise in recommendations produced by an algorithm. Given a set
of algorithms and a fixed experimental setting, statistical tools can
be used to estimate if there is a significant difference in the average
degree of surprise between recommendations produced by any two
algorithms.
Although this kind of approach can be successfully
applied when selecting which algorithm might be more promising
for an application domain, it is unable to provide a common meas-
urement scale that remains consistent in different experimental
settings; nor does it reveal how much room there is for improve-
ment with respect to surprise.
In this study, we express the view that surprise can be regarded
as a system resource: at any given time,
a recommender system
has a limited “stock of surprise” that is available to each user. This
theoretical “stock of surprise” is referred to here as “maximum
potential surprise”. The recommendation algorithm, which is de-
signed to optimise a set of objectives, controls how much surprise is
embedded in each recommendation it produces. However, there is
a limit to how much of the available surprise any recommendation
algorithm can embed in a recommendation. By pursuing this line
of thought, we were able to devise a surprise metric, called “norm-
alised surprise”, which provides a measurement scale in which the
meaning remains consistent across different settings. For example,
it can provide the information that,
on average,
a recommender
system has embedded 20% of the available surprise in each recom-
mendation it produces. As a result, this means that there is still 80%
of the available surprise that can be appropriated by the system.
The remainder of this paper is structured as follows: Section 2
examines related work on surprise evaluation; in Section 3 a the-
oretical model is designed for potential surprise and the proposed
surprise metric; Section 4 describes the experiments conducted to
validate the proposed metric through both an ancillary synthetic
dataset and the popular MovieLens dataset, and the results are dis-
cussed in Section 5. Finally, we conclude by summarising the work
and making suggestions for possible future work in Section 6.
arXiv:1807.03905v1 [cs.IR] 10 Jul 2018
2
RELATED WORK
Before providing a review of relevant work related to this research,
in this section there is an examination of the properties of a re-
commender system that are related to surprise (Section 2.1). This
is followed by a discussion of several surprise-related metrics that
have been proposed in the literature (Section 2.2) and the one plus
random off-line evaluation method for surprise (Section 2.3).
2.1
The Surprise Property
The challenge of discovering new items that might be useful to a
user has been focus of a large number of works in literature on
recommender systems. In general, the approach involves finding
new items that bear some similarity to items which have been given
good ratings by some users. An even greater challenge is to find new
items that do not resemble items known to a user, yet would still
be useful to them. This would be a serendipitous recommendation.
Herlocker et al
.
[
14
] offers a definition of serendipity that is usu-
ally cited by work in this area: “A serendipitous recommendation
helps the user find a surprisingly interesting item he might not have
otherwise discovered.” In a sense, this definition supports a perspect-
ive whereby serendipity,
as a system property,
results from the
interaction of two other and more fundamental properties: surprise
and relevance. Being surprising and relevant (or useful) to a user
are the basic requirements of a serendipitous recommendation.
It has been recently pointed out by Kaminskas and Bridge [
19
]
that there is a conceptual overlap between the properties of novelty
or unexpectedness and the notion of surprise.
In this study,
we
subscribe to the categorisation suggested by these authors, in which
a) novelty is related to the notion of an item being popular, and
thus is not directly related to serendipity, b) unexpectedness usually
conveys the same notion as surprise, and, as mentioned earlier, c)
surprise can be regarded as a component of serendipity.
In view of this, our focus is on the metrics employed for estimat-
ing surprise, serendipity and unexpectedness. This review sets out
by pointing out that several authors have approached the problem
of measuring these properties by adopting strategies that, although
clearly distinct from each other, have some key features in common.
Each strategy is analysed on the basis of three factors:
Intrinsic vs extrinsic evaluation
1
: some studies have defined met-
rics that only use data that are within the system under evaluation
[
2
,
18
,
33
], while others have defined metrics that use data made
available by an external system (often referred to as PPM - Primitive
Prediction Model) in addition to the internal data [1, 11, 26]
Subjective vs objective view: some metrics assume that surprise is
subjective in nature, since it depends on the set of items known to
each user [
1
,
18
,
26
,
33
], while others view surprise as a property
of the item itself [2, 11] and, thus, is independent of the users.
Reductionist vs non-reductionist approach:
some authors have
employed a reductionist approach, in so far as they seek to isolate
the surprise and relevance components of serendipity and examine
them as separate metrics [
1
,
2
,
18
,
26
,
33
], while others have pro-
posed metrics that treat surprise and relevance in a more integrated
way [11].
1
Here, intrinsic vs extrinsic evaluation is an analogy to the same dichotomy employed
in clustering quality evaluation methods [12].
2.2
Surprise Metrics
In this section, there is a review of six surprise-related metrics in the
literature. Figure 1 provides a summary of the review, and illustrates
how they are positioned with regard to the factors described in
Section 2.1. As can be seen in the diagram, the metrics include the
year of publication, and the ellipses show trends or changes in the
factors. This review is not meant to be exhaustive but rather aims
to capture the approaches that have evolved over a period of time.
Figure 1: Evolution of surprise-related metrics over time.
A metric for unexpectedness
.
Murakami et al
.
[
26
] have pro-
posed a metric to evaluate serendipity that explores the idea that
a serendipitous recommendation must be “non-obvious”, whereas
recommendations made by a PPM are expected to be obvious. As
shown in Equation 1, the metric is calculated from a recommenda-
tion list
L
produced for the user
u
by the system under evaluation.
In this definition, the predicate
rscore
accounts for the predicted rel-
evance of an item
L
i
to the user
u
, while
isr el
accounts for surprise,
and reflects the degree to which an item
L
i
is similar to items rated
highly by the user (i.e. based on a subjective view). Since there are
separate metrics for surprise and relevance, it can be assumed that
a reductionist approach is being adopted. Note that the predicate
rscore
, in Equation 2, includes the relevance predicted by both the
system under evaluation (
Pr
) and the external system (
Pr
∗
), and
can thus be regarded as an extrinsic evaluation.
unexp(L, u) =
1
|L|
|L |
Õ
i=1
rscore(L
i
, u) × isrel (L
i
, u)
(1)
rscore(L
i
, u) = max(Pr (L
i
, u) − Pr
∗
(L
i
, u), 0).
(2)
A metric for serendipity
.
Ge et al
.
[
11
] devised a metric for
evaluating serendipity that follows the same line of thought pursued
by Murakami et al
.
[
26
], although the external system is employed
in a different way.
As shown in Equation 3,
srdp
is applied to a
recommendation list
L
δ
, and estimates the usefulness of each item
L
δ
i
, that accounts for relevance. In Equation 4,
L
δ
is defined as a
list that consists of the elements recommended to the user
u
by
the system under evaluation (
L
) and that do not appear in the list
drawn up for user
u
by an external system (
L
∗
). This means that
L
δ
comprises non-obvious, unexpected items, and hence only accounts
for surprise.
srdp(L
δ
, u) =
1
|L
δ
|
|L
δ
|
Õ
i=1
usefulness(L
δ
i
, u)
(3)
L
δ
= L\L
∗
(4)
2
In addition, as there is no specific metric for surprise in Equations
3 and 4,
it can be assumed that
srdp
adopts a non-reductionist
approach. Note that
srdp
operates in an objective way, since estim-
ating surprise (
L
δ
) does not involve evaluating the degree to which
new items are similar to items already known to the user.
A metric for general unexpectedness
.
Akiyama et al
.
[
2
] set
out a metric called “general unexpectedness” that explores a combin-
atorial intuition: an item that shows a rare combination of attributes
must be treated as unexpected. It assumes that each item has some
content combined with it, and that such a content can be described
by a set of attributes. This usually is the case with content-based
recommenders [
8
].
As shown in Equation 5,
the unexp metric is
estimated for
L
, the recommendation list produced to user
u
by the
system under evaluation, and this aggregates the uscore obtained
for each item
L
i
. The uscore, defined in Equation 6, is the reciprocal
of the average joint probability estimated for each pair of attrib-
utes of
L
i
. In this equation,
A(L
i
)
represents the set of attributes
that describe
L
i
,
N
a
denotes the number of items in the repository
that have attribute
a
and
N
a,b
is the number of items that have
both attributes
a
and
b
. Thus an objective view is adopted since
surprise can be seen as a property of the content of an item. Unlike
the metrics previously described, this metric does not employ an
external system (i.e. an intrinsic evaluation). In addition, it should
be noted that this metric only accounts for surprise,
and it thus
adopts a reductionist approach.
unexp(L) =
1
|L|
|L |
Õ
i=1
uscore(L
i
)
(5)
uscore(L
i
) =

1
|A(L
i
)|
Õ
a,b ∈A(L
i
)
N
a,b
N
a
+ N
b
− N
a,b

−1
(6)
A metric for unexpectedness
.
Adamopoulos and Tuzhilin [
1
]
propose a metric for unexpectedness that examines an intuition
about user expectation: an item is expected for user
u
if it is known
to them or bears some similarity to items known to them. As shown
in Equation 7,
the unexp metric is calculated from L,
the recom-
mendation list produced for user
u
by the system under evaluation,
and
L
s
, a list of obvious, expected items that is defined in Equation
8. In that equation,
L
∗
is a recommendation list produced for user
u
by an external system,
E
u
represents the set of items that have been
rated by user u, and the predicate neighbours represents the set of
items in the repository (
I
) that are similar to the items in
E
u
up to
some degree specified by threshold parameters in
θ
. This approach
adopts an external system (extrinsic evaluation), the metric only ac-
counts for surprise (through a reductionist approach), and adheres
to a subjective view, since it takes account of past experience of the
user.
unexp(L, L
s
) =
1
|L|
|L\L
s
|
(7)
L
s
(u) = L
∗
∪ E
u
∪ neiдhbours(I , E
u
, θ )
(8)
The unserendipity metric
.
Zhang et al
.
[
33
] explore the idea
that a serendipitous recommendation must be dissimilar to items
known to the user,
in a semantic sense.
It resembles the metric
proposed by Akiyama et al
.
[
2
], since it assumes that each item is
combined with some content,
but in this case content attributes
are represented as vectors in
R
m
instead of sets. As is shown in
Equation 9, the metric is computed from the recommendation list
drawn up for user
u
by the system under evaluation (
L
), and results
in a score that is the average cosine similarity obtained from the
items in
L
and the set of items known to the user (
E
u
).
a) This
approach does not employ an external system (intrinsic evaluation);
b) the metric only accounts for surprise (reductionist approach) and
c) it adheres to a subjective view of surprise. It should be noticed
that,
unlike the metrics shown earlier,
unsrdp
is scale-inverted,
since the lower the score, the more surprising the L is.
unsrdp(L, u) =
1
|L| |E
u
|
Õ
i ∈L
Õ
j ∈E
u
cossim(i, j)
(9)
A metric for surprise
.
In a similar way to Zhang et al
.
[
33
],
Kaminskas and Bridge [
18
] argue that a surprising recommenda-
tion must be dissimilar to items known to the user, but does not
require that this dissimilarity should be semantic in nature. They
also explore the interplay between the notions of distance and sim-
ilarity
2
. Equation 10 shows that the metric is calculated from the
recommendation list produced for user
u
by the system under eval-
uation (
L
), and produces the average surprise computed for each
item in
L
. The surprise of an item
i
in
L
is estimated as either a) the
minimum distance between
i
and each item known to the user (
E
u
),
as described in Equation 11, or b) the maximum degree of similarity
between the same items, as shown in Equation 12. The predicate
dist
is defined as the Jaccard distance between the set of attributes
recovered from contents linked to items
i
and
j
, while the predicate
sim
computes the normalised pointwise mutual information score
(NPMI) [
6
] for the same items. This approach does not employ an
external system (intrinsic evaluation); the metric only accounts for
surprise (reductionist approach) and supports a subjective view of
surprise, since it takes account of the past experience of the users.
surprise(L, u) =
1
|L|
Õ
i ∈L
S
i
(i, E
u
)
(10)
S
i
(i, E
u
) = min
j ∈E
u
dist (i, j )
(11)
S
i
(i, E
u
) = max
j ∈E
u
sim(i, j)
(12)
In summary, we argue that all metrics described involve (in an
abstract sense) a notion of distance in their surprise component,
when it is applied to a) known and unknown items (subjective view)
[
1
,
18
,
26
,
33
], b) expected and unknown items (extrinsic evaluation)
[1, 11, 26], or c) the content linked to different items [1, 2, 18, 33]:
•
Murakami et al
.
[
26
]: the predicate isrel measures how similar
an item is to the items known to a user.
•
Ge et al
.
[
11
]:
L
δ
is the difference between sets of items, thus
it is a distance based on a combinatorial intuition;
•
Akiyama et al
.
[
2
]: the predicate uscore is the reciprocal of a
measure of similarity given by joint-probability;
•
Adamopoulos and Tuzhilin [
1
]: the predicate
unexp
is the
difference between sets;
•
Zhang et al
.
[
33
]: the predicate
unsrdp
is defined as the re-
ciprocal of geometric similarity, and is thus a distance;
•
Kaminskas and Bridge [
18
]: the predicate
S
i
directly specifies
the distance and similarity functions.
2
Given a metric for distance, a similarity metric can be derived, and vice-versa [9].
3
Finally, if one accepts the idea that surprise is a form of distance,
and there is a tendency to separate surprise from relevance when
seeking serendipity,
it can be argued that it might be fruitful to
tackle the problem of evaluating surprise from an informational
perspective, by trying to answer the following questions:
•
Are there limits to how much surprise a recommender can
offer to a user?
•
Are there limits to how surprising a recommendation list
can be?
•
If these limits exist, is it possible to use them to create a scale
up in which the performance of a system can be measured?
•
If these limits exist, do decisions on how to represent data
or which distance function to employ influence them?
The current metrics for surprise do not address these questions,
and this study seeks to fill in the gaps.
2.3
Evaluation of Surprise
All the metrics described in Section 2.2 evaluate the surprise
3
of a
recommendation list that is produced by the system that is being
evaluated. An evaluation method is required to obtain an estimate
of how the system performs with regard to surprise. Most studies
follow a statistical procedure to compute this kind of estimate: a
sample of users is selected,
recommendation lists are produced,
surprise evaluations are made, and the average is calculated.
On the other hand, in response to a general consensus about the
limited ability of the accuracy metrics to evaluate the performance
of recommenders in top-N recommendation tasks, a new off-line
evaluation method called “one plus random” was designed to estim-
ate the recall of a recommender system [
5
,
7
]. This method follows
the intuition that, in a sufficiently large set
L
1
that consists of items
unknown to user
u
, most of these items are irrelevant to
u
. Suppos-
ing that an item that is highly rated by
u
, namely
i
∗
, is added to
L
1
,
if an algorithm attributes to
i
∗
a score such that
i
∗
ranks among
the top-N items
L
1
, then the algorithm has succeeded in the task.
In a recent study, Kaminskas and Bridge [
18
] adapted this method
to estimate the degree of surprise of a recommender system. The
intuition behind the one plus random method is retained and, in
addition to computing an estimate for recall, it also computes the
average surprise obtained from the recommendation lists produced
for a sample of users.
3
A THEORETICAL MODEL FOR SURPRISE
Before addressing the questions posed in Section 2.2, it should be
made clear what properties a metric for surprise should have.
Surprise must be subjective: Barto et al
.
[
4
] carried out a review of
the concepts of surprise and novelty in cognitive science, as well as
their quantitative models
4
. Reisenzein et al
.
[
30
] examined to which
extent the experimental evidence supports different quantitative
models of surprise. Both these studies portray these phenomena
as subjective in nature, and show that their quantitative models
usually involve some form of subjective probability,
which may
represent expectations or beliefs held by an individual. From a more
3
In this subsection, the term surprise will also encompass both serendipity and unex-
pectedness predicates described in section 2.2.
4
The notion of surprise in Kaminskas and Bridge [
19
] is closer to the notion of novelty
in cognitive science than that of surprise, as described in Barto et al. [4].
intuitive standpoint, and when applied to recommender systems,
this idea can be illustrated through the Scenario 1:
a)
Suppose items i and j are similar to each other;
b)
Suppose user
u
1
has not been exposed to items
i
and
j
, and
all the items known to u
1
are very dissimilar from i and j;
b)
Suppose user u
2
has been exposed to item j;
d)
If the system recommended item
i
for user
u
1
, it would be a
surprising recommendation;
e)
If it recommended the same item for user
u
2
, it would not
be as surprising, because user u
2
knows j, a similar item.
Surprise must be dynamic: assuming that surprise depends on be-
liefs or expectations, it seems reasonable to presume that it changes
over time, as the user is constantly being exposed to new experi-
ences. This idea can be illustrated through the Scenario 2:
a)
Suppose items i and j are similar to each other;
b)
Suppose user
u
1
has not been exposed to items
i
and
j
, and
all the items known to u
1
are very dissimilar from i and j;
c)
Suppose the system recommends item j to user u
1
;
d)
After some time has passed item i is recommended to u
1
;
e)
Unlike what happened in Scenario 1, recommending item
i
to u
1
is not as surprising, since u
1
knows j, a similar item.
Surprise is related to the notion of distance: all the metrics reviewed
in Section 2.2 involve the notion of distance; most of them reflect
the extent to which a new item resembles the items known to a
user. This is in accordance with the subjective and dynamic views
of surprise, since both involve assessing similarity between objects.
In adopting these three ideas as premises for this work, we sup-
port the definition of surprise made by Kaminskas and Bridge [
18
],
and described in Equation 11. This definition assumes that the sur-
prise of an item,
S
i
(i, E
u
)
, is inversely proportional to the degree to
which item
i
is similar to the items known to the user; it adopts a
subjective view, since it considers surprise to be a function of the
items known to the user. It also accounts for changes in the surprise
of an unobserved item, as the growth of the set of known items.
The remainder of this section has two objectives. First, to devise
a theoretical model that can be used to estimate the total amount of
surprise a system can offer to an arbitrary user (Sections 3.1 to 3.5).
Second, to employ the theoretical model to estimate the maximum
amount of surprise a system can embed in a recommendation list
of arbitrary length (Section 3.6). The theoretical model described
next has the following settings:
1)
Initial condition: each user has rated at least one item;
2)
Interaction: the system produces a recommendation list
L
to
u
that only contains one item, which is promptly consumed;
3)
The repository of the system has a finite number of items;
4)
The repository of the system remains stable (no new items
are introduced), and this after a finite number of interactions,
all the users will have been exposed to all of the items.
3.1
Surprise is a finite resource
At any given time, a recommender system has a finite number of
items in its repository. On the basis of this premise we argue that
surprise is a finite resource in this kind of system. Let
I
represent
the set of items in the repository of the system. Suppose user
u
has
been exposed to all but one item in the repository, namely item
i
. Let
4
N
u
= {i }
represent the set of items unknown to
u
, and
E
u
= I \ N
u
the set of items to which user
u
has been exposed. Thus, the total
amount of surprise the system can offer to u is given by S
i
(i, E
u
).
This scenario can be modified to allow for
|N
u
|
>
1: suppose that
user
u
has been exposed to all but two items, namely
i
and
j
. Then,
N
u
= {i, j }
and
E
u
= I \ N
u
. Suppose that the system recommends
items
i
and
j
in this order. Then the total amount of surprise the
system can offer to
u
is
S
i
(i, E
u
) + S
i
(j, E
u
∪ {i })
.
The last term
accounts for the fact that item i was known to user u when item j
was recommended. It should be noted that the order in which items
are recommended may produce a different amount of surprise.
3.2
The surprise of a sequence
Building on the previous scenario, suppose that user
u
has been ex-
posed to all but
m
items, namely
N
u
= {i
1
, i
2
, . . . , i
m
}
. Suppose that
the system recommends these
m
items in a specific order, represen-
ted by the sequence
seq = (i
1
, i
2
, . . . , i
m
)
. Then the surprise of such
a sequence of recommendations to the user
u
can be generalised
by the following predicate (surprise of a sequence):
S
s
(seq, E
u
) = S
i
(h, E
u
) + S
s
(t , E
u
∪ {h}),
(13)
where
h
represents the head of the sequence
seq
,
namely
seq
1
,
and
t
its remaining items, (
seq
2
, . . . , seq
m
). Since
S
s
is a recursive
predicate, let S
s
(seq, E
u
) = 0 when |seq|
= 0.
3.3
The potential surprise
As stated earlier, the amount of surprise a system can potentially
offer to a user
u
, is finite and depends on the sequence in which
items are ordered. Thus, the maximum potential surprise a system is
able to offer to the user
u
must correspond to the surprise obtained
by a specific ordering of the elements of N
u
:
S
pmax
(N
u
, E
u
) =
max
seq ∈permut (N
u
)
S
s
(seq, E
u
),
(14)
where
permut(N
u
)
is the set of permutations of items in
N
u
. The
maximum potential surprise,
S
pmax
,
claims that there are some
permutations of the items in
N
u
that maximise the surprise for the
user
u
. This amount of surprise can be interpreted as the “stock of
surprise” a system can offer to user
u
. Following the same principle,
the minimum potential surprise,
S
pmin
, is the permutation of items
that minimises the potential surprise for that user:
S
pmin
(N
u
, E
u
) =
min
seq ∈permut (N
u
)
S
s
(seq, E
u
).
(15)
3.4
The normalised surprise of a sequence
Once the maximum and the minimum amount of potential surprise
a system can offer to a user have been defined,
these limits can
be used to create a scale that allows the surprise of any sequence
comprising all items in N
u
to be measured:
S
sn
(seq, E
u
) =
S
s
(seq, E
u
)
− S
pmin
(seq, E
u
)
S
pmax
(seq, E
u
) − S
pmin
(seq, E
u
)
.
(16)
The normalised surprise of a sequence,
S
sn
, results in a score within
the interval
[
0
,
1
]
.
If
S
sn
(seq, E
u
) =
1,
then
seq
is a permutation
of items in
N
u
that maximises the surprise for the user
u
. On the
other hand, if
S
sn
(seq, E
u
) =
0, then it offers the minimum amount
of surprise to the user.
3.5
Computational costs and approximations
Real recommender systems have a huge number of items that are
unknown to any given user. Since calculating the
S
pmax
requires
evaluating surprise in
|N
u
|
! permutations, its exact computation is
not feasible. However, by applying the optimisation theory to com-
binatorial problems [
16
,
23
], an approximation to
S
pmax
(Equation
14) can be computed by means of a greedy estimation strategy:
ˆ
S
pmax
(N
u
, E
u
) = S
i
(i
∗
, E
u
) +
ˆ
S
pmax
(N
u
\{i
∗
}, E
u
∪ {i
∗
})
(17)
i
∗
= arg max
i ∈N
u
S
i
(i, E
u
).
(18)
In Equation 18,
i
∗
is the most surprising item in
N
u
with respect to
E
u
. The same technique can be used to obtain an approximation
for S
pmin
(Equation 15):
ˆ
S
pmin
(N
u
, E
u
) = S
i
(i
∗
, E
u
) +
ˆ
S
pmin
(N
u
\ {i
∗
}, E
u
∪ {i
∗
})
(19)
i
∗
= arg min
i ∈N
u
S
i
(i, E
u
).
(20)
In Equation 20,
i
∗
is the least surprising item in
N
u
with regard
to
E
u
. We can now define an approximation to
S
sn
(Equation 16)
using the approximations for the maximum and minimum potential
surprises (Equations 17 and 19, respectively):
ˆ
S
sn
(seq, E
u
) =
S
s
(seq, E
u
)
−
ˆ
S
pmin
(seq, E
u
)
ˆ
S
pmax
(seq, E
u
) −
ˆ
S
pmin
(seq, E
u
)
.
(21)
3.6
Surprise of a recommendation list
Up to this point, we have focused on estimating the total amount
of surprise a recommender system can offer to an arbitrary user.
The approach required finding a sequence consisting of all
the
items unknown to a user (
|seq |
= |N
u
|
) that maximise the potential
surprise for them. We now turn to the problem of estimating the
maximum amount of surprise the system can embed in a sequence
that does not contain all the items unknown to a user.
This se-
quence is referred to as a truncated sequence, and represented as
seq
′
. Now suppose that
seq
′
, consisting of
k < |N
u
|
items, obtains
the maximum amount of surprise for user
u
that can be embed-
ded in a sequence with
k
items.
Then it should be the case that
ˆ
S
sn
(seq
′
, E
u
) = 1. Solving for seq
′
using Equation 21, give us:
seq
′
∈
(
arg max
q ∈ ar r anдement s(N
u
, k )
S
s
(q, E
u
)
)
,
where
arranдements(N
u
, k )
is the set of
k
-arrangements of items in
N
u
. An approximation for
seq
′
can be obtained by a greedy strategy.
We recognise that the assumption that
seq
′
can represent a re-
commendation list
L
may be subject to criticism, since there are
important discrepancies between the settings assumed by the the-
oretical model and the real user interactions with the the system:
• A user may fail to notice a recommendation list L, so:
userSurprise(L, E
u
) = 0 <
ˆ
S
pmax
(L, E
u
);
• A user may not promptly consume all the items in L, so:
userSurprise(L, E
u
) <
ˆ
S
pmax
(L, E
u
);
•
A user may not consume an item in the order that it is ranked
in the list, so: user Surprise(L, E
u
) <
ˆ
S
pmax
(L, E
u
).
However, in our view, even in such cases, the theoretical model can
still be used to provide an upper bound estimation of the surprise
experienced by the user.
5
3.7
Adapted evaluation method
Since
ˆ
S
sn
estimates the normalised surprise of a recommendation
list, we need a method to assess the performance of a recommender
system with regard to this metric. We adopt the approach employed
by Kaminskas and Bridge [
18
] and adapt the one plus random
off-line evaluation method for this assessment, as described in Al-
gorithm 1. It has five parameters, namely a sample of users (
U
), the
set of items in the repository (
I
), user ratings (
Ratings
), the length
of a recommendation list (
topN
), and a meta model that, given the
set of items known to a user,
induces a model that computes a
score for an arbitrary, unknown item (
metamodel
). As will be de-
scribed in Section 4.2, this meta model plays a key role in evaluating
predictions supported by the theoretical model.
In line 3,
E
u
is assigned to the set of items known to the user
u
,
and in line 4,
N
u
is assigned to the set of items unknown to
them. In line 6,
L
1
is a list with 1,000 unknown items, and each of
these items is mapped to a tuple
(i, scor e )
and the resulting list is
assigned to
L
2
(line 7). The score is produced by the model induced
in line 5. In lines 8 and 9, tuples in
L
2
are sorted in descending order
and the items that rank in the first
topN
positions are assigned to
seq
′
. Finally, in line 10, the approximate normalised surprise of
seq
′
is computed and accumulated. The algorithm returns the average
amount of normalised surprise obtained from the user sample U.
Algorithm 1: Off-line evaluation method to estimate
ˆ
S
sn
inputs : U, I, Ratings, topN, metamodel
output : Estimated system surprise
1
acc ← 0;
2
foreach u ∈ U do
3
E
u
← {e.itemI D ∈ Ratings | e.user I D = u};
4
N
u
← I \ E
u
;
5
Θ
u
← metamodel (E
u
);
6
L
1
← random (N
u
, 1000);
7
L
2
← map (i
∈ L
1
,
(i, score ← Θ
u
(i )) );
8
L
3
← sort (L
2
, key = −score);
9
seq
′
← map ( (i, score) ∈ L
3
[1 : topN], i);
10
acc ← acc +
ˆ
S
sn
(seq
′
, E
u
);
11
end
12
return acc / |U |
4
EXPERIMENTS AND RESULTS
Two experiments were conducted: the first aims to assess the quality
of the greedy approximations for maximum and minimum potential
surprise;
the purpose of the second is to validate predictions of
the theoretical model through different choices of recommender
algorithm, data representation, and distance function.
4.1
Evaluating the approximation strategy
Method
: a synthetic dataset was employed to assess the differences
between the exact computations of maximum and minimum poten-
tial surprises and their greedy approximations.
Dataset
: the dataset contains a single user and eleven items,
la-
belled
{i
1
, . . . , i
11
}
.
The user was exposed to one item (
i
1
).
The
items are represented as vectors in R
2
, and arbitrarily distributed.
Procedure
: the degree of surprise was measured for all the per-
mutations of the set of unknown items (
N
u
= {i
2
, . . . , i
11
}
,
|N
u
|
!
=
10!), according to
S
s
(seq, E
u
)
in Equation 13.
The maximum and
minimum surprise measurements obtained were recorded.
The
greedy approximations for
S
pmax
and
S
pmin
, defined in Equations
17 and 19 respectively, were computed and results recorded.
Variations
:
since the surprise of a sequence,
S
s
(seq, E
u
)
,
uses a
distance function, the procedure was repeated using four different
functions: non-normalised Euclidean distance and cosine distance
(geometric intuition),
Jaccard distance (combinatorial intuition),
and Jensen-Shannon divergence (informational intuition). The Jac-
card distance and Jensen-Shannon divergence to vectors in
R
n
were
applied as described in [17].
Results
: Table 1 shows the values that were calculated using both
exact and approximate predicates.
Except for the case of
ˆ
S
pmax
where non-normalised Euclidean distance was used, no substan-
tial difference between the exact and approximate computations
was obtained.
That deviation occurred because
ˆ
S
pmax
underes-
timated
S
pmax
.
Since underestimating
S
pmax
or overestimating
S
pmin
could lead
ˆ
S
sn
to achieve a score outside the interval
[
0
,
1
]
,
in practice it seems reasonable to clip its value if it falls outside
this interval, and this solution was adopted in Experiment 2. Al-
though these results do not support the general claim that a greedy
algorithm will always achieve approximations as good as those ob-
tained, it suggests that the local approximation approach is feasible
in real settings.
Table 1: Greedy approximations for potential surprise.
Distance
S
pmax
ˆ
S
pmax
S
pmin
ˆ
S
pmin
Euclidean
37.684
36.948
23.935
23.935
cosine
1.367
1.367
0.277
0.277
Jaccard
3.784
3.784
2.552
2.552
Jensen-Shannon
2.089
2.089
0.494
0.494
4.2
Evaluating the theoretical model
The theoretical model supports the following predictions:
•
If a recommender system embeds the maximum amount of
surprise in each recommendation it produces, then its eval-
uation should achieve the maximum score in the potential
surprise scale (mean S
sn
= 1).
•
If a system embeds the minimum amount of surprise in
each recommendation, then its evaluation should obtain the
minimum value of the scale (mean S
sn
= 0).
•
Any recommender whose objective is neither to maximise
nor minimise surprise, will achieve an intermediate value
within the scale (0 ≤ mean S
sn
≤ 1).
These predictions should be confirmed regardless of the choices of
data representation and distance function.
6
Method
: a controlled environment was created to enable this ex-
periment to be carried out. This environment submits data from the
MovieLens-1M dataset to a process that produces a time series con-
sisting of measurements for surprise, according to
ˆ
S
sn
(Equation 21).
Dataset
:
the MovieLens-1M dataset was employed [
13
].
It con-
tains 3,883 items, 6,040 users and just over 1 million ratings. It was
enhanced by short movie descriptions collected from the online
MovieLens system in September 2017. Items whose short descrip-
tion was not available or was not written in English were rejected.
Process
:
the dataset is submitted to a process comprising three
stages: preprocessing, segmentation and measurement. In the pre-
processing stage,
a distance matrix is computed for each pair of
items in the dataset, by following the parameters specified in each
variation outlined in the next paragraph. During the segmentation
stage, the ratings are ordered by timestamp and aggregated into
timeframes that include 1,500 ratings each. Consecutive timeframes
that contain ratings from at least 30 common users are marked
as eligible for measurement if there is at least one 5-star rating
for each user.
These criteria enable us to: 1) control the number
of measurement samples; 2) control variation among the samples,
since the users that are randomly allocated to a timeframe
T
i
, will
be preferably allocated to
T
i+1
; 3) satisfy the conditions required by
the original one plus random method [
5
,
7
], by allowing us to obtain
recall evaluations for each sample, for future work. Finally, during
the measurement stage, Algorithm 1 is applied to each eligible in-
terval. An interval is a sequence of consecutive timeframes, starting
from the first timeframe and stretching to an eligible timeframe.
The measurements are sequenced and recorded as a time series.
Variations
: The process was repeated using different choices of re-
commender algorithm, data representation, and distance function.
Recommender algorithms: three algorithms were used: a) the tra-
ditional item-kNN (with
k =
50),
which scores items according
to the weighted average rating attributed to the
k
most similar
items known to a given user;
b) an algorithm that scores items
according to their degree of surprise (MSI - Most Surprising Item),
which promotes the generation of surprising recommendation lists;
and c) an algorithm that scores items according to its familiarity
(LSI - Least Surprising Items), which promotes non-surprising re-
commendation lists.
Since Algorithm 1 is used,
a set with 1,000
randomly-sampled unknown items is submitted to each algorithm
that, in its turn, attributes a score to each item. Then the top-ranking
items (topN = 10) are selected as a recommendation list.
Data representation: four models were used: Models C and P are
semantic models, Model U is a user-item, and Model N is NPMI.
Model C is a traditional vector space model of semantics (or
count-based VSM) [
3
,
32
]. It uses the short description linked to an
item to produce its respective semantic vector. Items for which the
description was too short
5
were rejected. Before computing tf-idf
scores [
21
],
the terms were stemmed by means of the Snowball
algorithm [20, 28].
5
Description is too short if it has less than 13 terms after stop words removal; default
NLTK stop words for English were employed [20].
Model P is a distributed vector space model of semantics (or
prediction-based VSM) [
3
]. It uses the short description linked to
each item to produce a semantic vector.
Semantic vectors were
extracted through an implementation of the Paragraph Vector [
24
,
29], which does not require stop words removal or stemming.
Model U is a user-item model [
27
]. Each item
i
is represented as
a vector
v
i
of a length equal to the number of users in the system
repository. Each component
v
i j
represents a) the rating the user
u
j
has attributed to item
i
, or b) zero if the item was not rated by
u
j
.
The items without any rating were rejected.
Model N is a NPMI score model [
18
]. The model consists of two
probability distributions,
P(i)
and
P(i, j)
: the first is estimated as
the proportion of users who have been exposed to each item
i
, and
the latter as the proportion of users who have been exposed to
both items
i and j
.
The distance between
i and j
is calculated by
means of the NPMI score [
6
] (which measures similarity), and then
inverted and rescaled so that the image [−1, 1] is mapped to [0, 1].
Distance functions: six distance functions were used for explor-
ing different intuitions: Euclidean and cosine distances (geometric),
Jaccard distance (combinatorial), Jensen-Shannon divergence and
NPMI (informational), and Aitchison distance (statistical) [
10
]. The
Jensen-Shannon divergence and Aitchison distance are not defined
when one vector has a zero component, so they require smoothed
vectors. When needed, smoothing was applied using Bayesian Mul-
tiplicative Treatment (BMT) with Perks prior [
10
]. In addition, since
there are premises behind each distance, some of them cannot be
applied to vectors from all the representation models. The Jaccard
distance, Jensen-Shannon divergence, and Aitchison distance re-
quire compositional data [
10
],
which means that they can only
be applied to vectors from Models C and U; and the NPMI score
requires an NPMI score model.
Results
: Table 2 shows the median, mean, and standard deviation
of values obtained from the time series that were produced by ex-
ecuting the process under different variations. The mean
ˆ
S
sn
under
kNN was within the predicted range,
but none of the variations
under MSI or LSI achieved their predicted results. The average val-
ues for
ˆ
S
sn
under MSI were consistently higher than those under
kNN and nearest to 1, whereas the values for
ˆ
S
sn
under LSI were
consistently lower than those under kNN and nearest to 0. There
are two reasons for this discrepancy. First, Algorithm 1 draws 1,000
items from the set of unknown items (
N
u
), while
ˆ
S
pmax
and
ˆ
S
pmin
selects
topN
items from
N
u
without sampling. In this situation, the
probability that a 1,000 sample will miss one of the
topN
items selec-
ted by
ˆ
S
pmax
(or
ˆ
S
pmin
) is over 66%. Since the mean
ˆ
S
sn
in Table 2
is calculated over a time series comprising 30 intervals (minimum),
each containing 30 users (minimum), it means that the probability
of all 900 draws will contain all the
topN
items is practically nil.
Second, Algorithm 1 does not apply a greedy search when selecting
items from the sample of 1,000 items, as
ˆ
S
pmax
and
ˆ
S
pmin
do. A
supplementary experiment was conducted, in which Algorithm 1
was altered so that it could use the
N
u
without sampling and with a
greedy search, and then be applied to the variations with the largest
discrepancies, namely Model N with NPMI score under MSI and
Model C with cosine distance under LSI. The results confirmed the
predicted results for both MSI and LSI.
7
Table 2: Median, mean and standard deviation of
ˆ
S
sn
over MSI, kNN, and LSI algorithms.
Variations
MSI
kNN
LSI
Model
Distance
Median
Mean
St.Dev.
Median
Mean
St.Dev.
Median
Mean
St.Dev.
C
Euclidean
0.912
0.910
0.031
0.448
0.443
0.087
0.023
0.024
0.010
C
cosine
0.985
0.980
0.019
0.742
0.740
0.077
0.211
0.219
0.093
C
Jaccard
0.969
0.964
0.025
0.704
0.697
0.084
0.172
0.193
0.102
C
Jensen-Shannon
0.984
0.975
0.031
0.626
0.615
0.088
0.081
0.097
0.074
C
Aitchison
0.979
0.978
0.015
0.512
0.510
0.088
0.037
0.040
0.018
P
Euclidean
0.985
0.984
0.011
0.615
0.605
0.082
0.096
0.099
0.042
P
cosine
0.971
0.951
0.061
0.571
0.566
0.083
0.093
0.096
0.050
U
Euclidean
0.921
0.918
0.032
0.835
0.813
0.102
0.004
0.007
0.018
U
cosine
0.983
0.970
0.036
0.618
0.633
0.179
0.037
0.042
0.027
U
Jaccard
0.999
0.939
0.097
0.585
0.609
0.203
0.053
0.059
0.038
U
Jensen-Shannon
0.953
0.948
0.029
0.603
0.602
0.162
0.081
0.085
0.036
U
Aitchison
0.946
0.943
0.025
0.758
0.745
0.098
0.008
0.011
0.015
N
NPMI
0.687
0.678
0.091
0.545
0.535
0.111
0.098
0.111
0.072
5
DISCUSSION
The aim of this study was to determine a) if there are limits to
how much surprise a recommender system can offer to its users, b)
how much surprise it can embed in a recommendation list, and c)
how these limits can be used to design a metric that reflects how
much room there is for improving surprise in recommendations.
While previous studies focused on designing metrics that explore
different intuitions about what makes a surprising recommendation
[
1
,
2
,
11
,
18
,
26
,
33
] or how to combine different metrics [
31
], we
explored a novel perspective: surprise as a finite resource in any
recommender system, whatever intuition about surprise is adopted.
As the results suggest, there are limits to surprise, and the pro-
posed metric obtained values consistent with these theoretical
bounds. They were also consistent for a number experimental set-
tings where the choices of data representation and distance function
varied. In fact, one contribution made by this work is that it provides
further evidence that such choices have a non-negligible effect. For
example, the coefficient of variation of
ˆ
S
sn
over Models C, P and U
under kNN, is 13.6% with cosine distance, and 29.9% with Euclidean
distance.
In addition,
the coefficient of variation under kNN for
Model U is 13.8%, and for Model C is 20.7%.
As briefly discussed in Section 3.6, we recognise that there are
important discrepancies between the theoretical model employed
in this study and a real-world setting. As argued, the model can still
be used to estimate an upper bound to the real surprise experienced
by a user. However, there is another limitation that still needs to
be addressed. The theoretical model assumes that the repository
remains stable. This requirement was necessary to allow for a fixed
upper bound of potential surprise. As a means of evaluating the
impact of undermining this premise, in Experiment 2, the dataset
is segmented in a chronological order so that each interval only
includes items with an estimated release date within that interval,
thus simulating an evolving repository. The behaviour of the met-
rics under this condition, as the results suggest, remained within
the limits predicted by the theoretical model.
It should also be noted that a surprise model, like any model, is a
simplification of the world. For example, the definition of surprise
adopted for this study (Equation 11) models the user experience as
a set of items. As a result, all the things known to a user are repres-
ented as points in
E
u
, and all they know is the subject of movies.
The definition also assumes that a user has no bias when recovering
from memory the one item that is most similar to that being recom-
mended, according to some notion of similarity. Both premises are
obviously unrealistic, but in our view, these oversimplified models
are necessary and still useful, especially in the absence of realistic
computational models of surprise that can feasibly be employed to
describe an arbitrary user interacting with a recommender system.
6
CONCLUSION
This study adopts a new approach to evaluate surprise in recom-
mender systems.
A theoretical
model
of surprise was designed
on the assumption that surprise is a limited resource in any re-
commender system. This model predicts the bounds to how much
surprise a recommender system can offer its users, and these bounds
were employed to design a surprise metric that can be used to de-
termine how competent a system is at embedding surprise in its
recommendations and how much room there is for improvement.
Further work should be carried out explore other datasets and
recommendation algorithms, as well as other the choices of local
optimisation algorithm. The decision to approximate the potential
surprise by using a greedy algorithm took account of the fact that
it is easy to manually check its results.
However,
this algorithm
lacks theoretical limits to its precision, as some alternatives have.
Finally, the theoretical model opens up a line of thought that it
may be fruitful to pursue. On the assumption that surprise arises
from a lack of information,
the concept of maximum potential
surprise can be framed as the total amount of information a system
can offer to a given user.
Since the order in which the items are
recommended to a user can lead to a lower amount of experienced
surprise, does this mean that information is lost? It might be the case
that this imbalance between maximum potential surprise and actual
user surprise, can establish a close relationship with relevance or
other properties of recommender systems. In a loose analogy with
mechanical systems, potential energy is never lost; rather, it can
only be transformed into something else.
8
REFERENCES
[1]
Panagiotis Adamopoulos and Alexander Tuzhilin. 2011.
On Unexpectedness in
Recommender Systems: Or How to Expect the Unexpected. In Proceedings of the
Workshop on Novelty and Diversity in Recommender Systems at the Fifth ACM
International Conference on Recommender Systems (DiveRS @ RecSys 2011). ACM,
New York, NY, USA, 11–18.
https://doi.org/10.1145/2043932.2044019
[2]
Takayuki Akiyama, Kiyohiro Obara, and Masaaki Tanizaki. 2010.
Proposal and
Evaluation of Serendipitous Recommendation Method Using General Unexpec-
tedness.
In Proceedings of the Workshop on the Practical Use of Recommender
Systems, Algorithms and Technologies at the Fouth ACM International Conference
on Recommender Systems (PRSAT @ RecSys 2010). ACM, New York, NY, USA, 3–10.
https://doi.org/10.1145/1864708.1864795
[3]
Marco Baroni,
Georgiana Dinu,
and Germán Kruszewski.
2014.
Don’t count,
predict! A systematic comparison of context-counting vs.
context-predicting
semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics, Stroudsburg, PA, USA, 238–247.
https://doi.org/10.3115/v1/P14-1023
[4]
Andrew Barto, Marco Mirolli, and Gianluca Baldassarre. 2013.
Novelty or Sur-
prise? Frontiers in Psychology 4 (2013), 907.
https://doi.org/10.3389/fpsyg.2013.
00907
[5]
Alejandro Bellogin, Pablo Castells, and Ivan Cantador. 2011.
Precision-oriented
Evaluation of Recommender Systems: An Algorithmic Comparison. In Proceedings
of the Fifth ACM Conference on Recommender Systems (RecSys ’11). ACM, New
York, NY, USA, 333–336.
https://doi.org/10.1145/2043932.2043996
[6]
Gerlof Bouma. 2009.
Normalized (Pointwise) Mutual Information in Collocation
Extraction. In Proceedings of the Conference of the German Society for Computa-
tional Linguistics and Language Technology (GSCL 2009). GSCL e.V., Manheim,
Germany,
31–40.
https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/
npmi-pfd.pdf
[7]
Paolo Cremonesi, Roberto Turrin, Eugenio Lentini, and Matteo Matteucci. 2008.
An Evaluation Methodology for Collaborative Recommender Systems. In Inter-
national Conference on Automated Solutions for Cross Media Content and Multi-
Channel Distribution (AXMEDIS 2008).
IEEE,
Washington,
DC,
USA,
224–231.
https://doi.org/10.1109/AXMEDIS.2008.13
[8]
Marco de Gemmis, Pasquale Lops, Cataldo Musto, Fedelucio Narducci, and Gio-
vanni
Semeraro.
2015.
Semantics-Aware Content-Based Recommender Sys-
tems.
In Recommender Systems Handbook (2nd.
ed.),
Francesco Ricci,
Lior
Rokach, and Bracha Shapira (Eds.). Springer, New York, NY, Chapter 26, 119–159.
https://doi.org/10.1007/978-1-4899-7637-6
[9]
Michel Marie Deza and Elena Deza. 2009.
Encyclopedia of Distances.
Springer,
Berlin, Heidelberg. 1–583 pages.
https://doi.org/10.1007/978-3-642-00234-2
[10]
Juan José Egozcue,
Carles Barceló-Vidal,
Josep Antoni Martín-Fernández,
Eu-
sebi Jarauta-Bragulat, José Luis Díaz-Barrero, and Glòria Mateu-Figueras. 2011.
Compositional Data Analysis.
Wiley-Blackwell, Hoboken, NJ, USA, Chapter 11,
139–157.
https://doi.org/10.1002/9781119976462.ch11
[11]
Mouzhi
Ge,
Carla Delgado-Battenfeld,
and Dietmar Jannach.
2010.
Beyond
Accuracy: Evaluating Recommender Systems by Coverage and Serendipity. In
Proceedings of the Fourth ACM Conference on Recommender Systems (RecSys ’10).
ACM, New York, NY, USA, 257–260.
https://doi.org/10.1145/1864708.1864761
[12]
Jiawei Han, Jian Pei, and Micheline Kamber. 2011.
Data Mining: Concepts and
Techniques (3rd. ed.).
Morgan Kaufmann, San Francisco, CA, USA.
[13]
F Maxwell Harper and Joseph A Konstan. 2015.
The Movielens Datasets: History
and Context.
ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4 (2015),
19.
https://doi.org/10.1145/2827872
[14]
Jon Herlocker, Joseph A Konstan, and John Riedl. 2002.
An Empirical Analysis
of Design Choices in Neighborhood-Based Collaborative Filtering Algorithms.
Information retrieval 5, 4 (2002), 287–310.
[15]
Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl.
2004.
Evaluating collaborative filtering recommender systems.
ACM Transactions
on Information Systems (TOIS) 22, 1 (2004), 5–53.
[16]
David S Johnson and Lyle A McGeoch. 2015.
The traveling salesman problem: A
case study in local optimization (preliminary version).
(2015).
http://www.csc.
kth.se/utbildning/kth/kurser/DD2440/avalg14/TSP-JohMcg97.pdf
[17]
Dan Jurafsky and James H Martin. 2018.
Speech and Language Processing (draft
manuscript of the 3rd ed.).
(2018).
https://web.stanford.edu/~jurafsky/slp3/
ed3book.pdf
[18]
Marius Kaminskas and Derek Bridge. 2014.
Measuring Surprise in Recommender
Systems. In Proceedings of the Workshop on Recommender Systems Evaluation:
Dimensions and Design,
at the 8th ACM Conference on Recommender Systems
(REDD @ RecSys ’14). ACM, New York, NY, USA, 393–394.
https://doi.org/10.
1145/2645710.2645780
[19]
Marius Kaminskas and Derek Bridge. 2016.
Diversity, Serendipity, Novelty, and
Coverage: A Survey and Empirical Analysis of Beyond-Accuracy Objectives in
Recommender Systems.
ACM Trans. Interact. Intell. Syst. 7, 1, Article 2 (Dec. 2016),
42 pages.
https://doi.org/10.1145/2926720
[20]
Edward Loper and Steven Bird.
2002.
NLTK:
The Natural Language Toolkit.
In Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Computational Linguistics - Volume
1 (ETMTNLP ’02). Association for Computational Linguistics, Stroudsburg, PA,
USA, 63–70.
https://doi.org/10.3115/1118108.1118117
[21]
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008.
In-
troduction to Information Retrieval.
Cambridge University Press, New York, NY,
USA.
[22]
Sean M. McNee, John Riedl, and Joseph A. Konstan. 2006.
Being Accurate is Not
Enough: How Accuracy Metrics Have Hurt Recommender Systems. In Extended
Abstracts on Human Factors in Computing Systems (CHI EA ’06). ACM, New York,
NY, USA, 1097–1101.
https://doi.org/10.1145/1125451.1125659
[23]
Malika Mehdi. 2011.
Parallel Hybrid Optimization Methods for Permutation Based
Problems.
Ph.D. Dissertation. Université des Sciences et Technologie de Lille -
Lille I, Lille, France.
[24]
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed Representations of Words and Phrases and their Compositionality.
In
Advances in Neural Information Processing Systems 26, C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.). Curran Associates, Inc.,
Red Hook, NY, USA, 3111–3119.
[25]
Fernando Mourão, Leonardo Rocha, Camila Araújo, Wagner Meira Jr, and Joseph
Konstan. 2017.
What surprises does your past have for you? Information Systems
71 (2017), 137–151.
https://doi.org/10.1016/j.is.2017.08.001
[26]
Tomoko Murakami,
Koichiro Mori,
and Ryohei
Orihara.
2008.
Metrics for
Evaluating the Serendipity of Recommendation Lists.
In New Frontiers in Ar-
tificial Intelligence, Ken Satoh, Akihiro Inokuchi, Katashi Nagao, and Takahiro
Kawamura (Eds.). Springer, Berlin, Heidelberg, 40–46.
https://doi.org/10.1007/
978-3-540-78197-4_5
[27]
Xia Ning,
Christian Desrosiers,
and George Karypis.
2015.
A comprehensive
survey of neighborhood-based recommendation methods.
In Recommender
Systems Handbook (2nd.
ed.),
Francesco Ricci,
Lior Rokach,
and Bracha Sha-
pira (Eds.). Springer, New York, NY, Chapter 2, 37–76.
https://doi.org/10.1007/
978-1-4899-7637-6
[28]
Martin F Porter. 2001.
Snowball: A language for stemming algorithms.
(2001).
http://snowball.tartarus.org/texts/introduction.html
[29]
Radim Řehůřek and Petr Sojka. 2010.
Software Framework for Topic Modelling
with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges
for NLP Frameworks. ELRA, Valletta, Malta, 45–50.
http://is.muni.cz/publication/
884893/en.
[30]
Rainer Reisenzein,
Gernot Horstmann,
and Achim Schützwohl.
2017.
The
Cognitive-Evolutionary Model of Surprise: A Review of the Evidence.
Topics in
Cognitive Science (Online Early View) (2017).
https://doi.org/10.1111/tops.12292
[31]
Thiago Silveira, Leonardo Rocha, Fernando Mourão, and Marcos Gonçalves. 2017.
A Framework for Unexpectedness Evaluation in Recommendation. In Proceedings
of the Symposium on Applied Computing (SAC ’17). ACM, New York, NY, USA,
1662–1667.
https://doi.org/10.1145/3019612.3019760
[32]
Peter D. Turney and Patrick Pantel. 2010.
From Frequency to Meaning: Vector
Space Models of Semantics.
Journal of Artificial Intelligence Research 37, 1 (jan
2010), 141–188.
http://dl.acm.org/citation.cfm?id=1861751.1861756
[33]
Yuan Cao Zhang, Diarmuid Ó Séaghdha, Daniele Quercia, and Tamas Jambor. 2012.
Auralist: Introducing Serendipity into Music Recommendation. In Proceedings of
the Fifth ACM International Conference on Web Search and Data Mining (WSDM
’12). ACM, New York, NY, USA, 13–22.
https://doi.org/10.1145/2124295.2124300
9
