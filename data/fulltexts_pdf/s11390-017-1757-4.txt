Liu ZR, Liu Y. Exploiting unlabeled data for neural grammatical
error detection.
JOURNAL OF COMPUTER SCIENCE
AND TECHNOLOGY 32(4):
758–767
July 2017.
DOI 10.1007/s11390-017-1757-4
Exploiting Unlabeled Data for Neural Grammatical Error Detection
Zhuo-Ran Liu
1
and Yang Liu
2,3,4,5,∗
, Member, CCF, ACM, IEEE
1
School
of Software, Beihang University, Beijing 100191, China
2
State Key Laboratory of Intelligent Technology and Systems, Tsinghua University, Beijing 100084, China
3
Tsinghua National
Laboratory for Information Science and Technology, Tsinghua University, Beijing 100084, China
4
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
5
Jiangsu Collaborative Innovation Center for Language Competence, Xuzhou 221009, China
E-mail:
liuzhuoran17@163.com; liuyang2011@tsinghua.edu.cn
Received December 20, 2016; revised May 19, 2017.
Abstract
Identifying and correcting grammatical errors in the text written by non-native writers have received increasing
attention in recent years.
Although a number of
annotated corpora have been established to facilitate data-driven gram-
matical
error detection and correction approaches,
they are still
limited in terms of quantity and coverage because human
annotation is labor-intensive,
time-consuming,
and expensive.
In this work,
we propose to utilize unlabeled data to train
neural network based grammatical error detection models.
The basic idea is to cast error detection as a binary classification
problem and derive positive and negative training examples from unlabeled data.
We introduce an attention-based neural
network to capture long-distance dependencies that influence the word being detected.
Experiments show that the proposed
approach significantly outperforms SVM and convolutional networks with fixed-size context window.
Keywords
unlabeled data, grammatical
error detection, neural network
1
Introduction
Automatic grammatical error detection and correc-
tion for natural languages have attracted increasing at-
tention,
for a large number of non-native speakers are
learning or using foreign languages.
Take English as an
example.
There are a large number of
English learn-
ers around the world who need instantaneous accurate
feedback to help improve their writings
[1]
.
In the do-
main of scientific paper writing in which English is the
main language,
authors also need effective grammar
checkers to help them in composing scientific articles
[2]
.
There have been several
Shared Tasks addressing
grammar errors in recent years.
HOO-2011
[3]
,
HOO-
2012
[4]
,
CoNLL-2013
[5]
and
CoNLL-2014
[6]
Shared
Task all
aim to correct grammar errors.
The AESW
Shared Task
[7]
aims to identify sentence-level grammar
errors.
These Shared Tasks help advance the research
of grammatical error detection and correction.
Despite these advances,
the scarcity of
annotated
data is still a major limitation on the research of gram-
matical
error
detection and correction.
Researchers
need mass annotated data to train a grammar checker,
but
unfortunately for
them,
there are only a small
amount
of
annotated corpora available in a limited
number of domains.
Most annotated corpora are in the
domain of learner English, e.g., NUCLE
[8]
and CLC
[9]
,
and others are from domains such as scientific papers,
e.g.,
AESW dataset
[2]
.
In order to train their systems
with enough data, researchers use multiple corpora in-
stead of one corpus
[10]
.
Data scarcity is partly due to difficulties in build-
Regular Paper
Special Issue on Deep Learning
This work is supported by the National
Natural Science Foundation of China under Grant Nos. 61522204 and 61331013 and the
National
High Technology Research and Development 863 Program of China under Grant No.
2015AA015407.
This research is also
supported by the National
Research Foundation of Singapore under its International
Research Centre@Singapore Funding Initiative
and administered by the IDM (Interactive Digital Media) Programme.
∗
Corresponding Author
©2017 Springer Science + Business Media, LLC & Science Press, China
Zhuo-Ran Liu et al.:
Exploiting Unlabeled Data for Neural Grammatical
Error Detection
759
ing an elaborately annotated corpus needed for train-
ing of
a grammatical
error correction system,
as de-
scribed by the team that built NUS Corpus of Learner
English
[8]
.
In order to obtain a reliable annotation,
the team set up a guideline for annotators so that cor-
rections are consistent.
To ensure that these annota-
tions are available,
several
annotators proposed their
correction independently, and annotations most agreed
upon were selected.
Such annotating process is labour-
intensive and time consuming,
and the quality of
the
corpus is subject to human judgment and other factors
such as budget.
For example,
the team was unable to
perform double annotation for the main corpus due to
budget constraints.
The team spent a long time (over
half a year) to annotate only 1 414 essays.
Given these difficulties in building annotated cor-
pus, we hope to utilize un-annotated error-free texts in
unsupervised training of a grammatical error correction
or grammatical error detection system.
Previously, ef-
forts have been made to explore how realistic grammati-
cal
errors could be counterfeited automatically from
error-free texts and therefore obtain a large amount of
annotated data
[11
-
14]
.
We therefore follow the idea of
building a corpus by generating artificial
errors,
since
there are large numbers of un-annotated texts available
and most of them are error-free.
We explore two ways
of artificial error generation, one of which is proved to
be effective in our experiment.
Training a system to correct
grammatical
errors
might be a more difficult task when there is no super-
vision,
since there are numerous error types and our
method to generate artificial
errors might not be so-
phisticated enough to cover all of them.
We thus focus
on grammatical error detection instead of correction.
It
is natural
to address this task as binary classification,
in which we make prediction as to whether a word is
grammatically correct.
2
Background
2.1
Problem Statement
The goal of word-level grammatical error detection
is to identify grammar errors at the word level.
For
example, given a sentence shown below, a grammatical
error detection system is expected to correctly identify
the erroneous word “birds” highlighted by an underline:
An ugly birds was observed by the man yesterday.
The
task of
word-level
grammatical
error
detec-
tion is formalized as such:
given a sequence of
token
X = (x
1
, x
2
, ..., x
n
) as input,
the error detector out-
puts its prediction Y = (y
1
, y
2
, ..., y
n
) where y
i
denotes
the correctness of x
i
in terms of grammaticality.
We address this problem as a binary classification
problem.
In order to predict y
t
given the current word
x
t
and the whole sentence X = (x
1
, x
2
, ..., x
n
),
we
need to find a function g(·) to calculate the conditional
probability of
each y
t
given x
t
and the whole input
sequence X:
p(y
t
|x
t
) = g(x
t
, X),
where
y
t
=

1,
if x
t
is correct,
0,
otherwise.
Our aim is to build a suitable classification model
for
g(·).
2.2
SVM Model for Error Detection
A natural
approach is to use support vector ma-
chine
(SVM)
to perform classification
[15
-
16]
.
SVM
is
trained given a training dataset
in the
form of
{(x
1
, y
1
), ..., (x
n
, y
n
)},
where
x
i
represents
a token
with a set of
selected linguistic features,
and y
i
de-
notes the grammatical correctness of the token.
It finds
a maximum-margin hyperplane that separates correct
words from incorrect ones.
The problem with this approach is that we need to
manually design features in x
i
.
Since human are unable
to tell
precisely which features are relevant,
human-
designed features are inadequate in some aspects while
being redundant in others.
As a result, these designed
features are unable to capture all
regularities,
which
might hurt the performance of our error detector.
2.3
Convolution Network with Fixed Window
Size
To circumvent the problem with feature engineering,
a natural
thought is to utilize the capability of neural
networks in automatic feature extraction
[17]
.
The sim-
plest way is to take into consideration a fixed-size win-
dow of words around the current word as its context by
applying temporal convolution over the fixed-size win-
dow.
In the example sentence given in Subsection 2.1,
when considering the grammatical
correctness of
the
word “was” given a context window of size 3, the con-
text window would be “birds was observed”.
The as-
sumption that underlies this method is that only neigh-
bouring words are grammatically related to the current
word.
