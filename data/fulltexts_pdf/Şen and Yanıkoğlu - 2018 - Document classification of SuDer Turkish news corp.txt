SuDer T
¨
urkc¸e Haber Derlemlerinin Dok
¨
uman
Sınıflandırması
Document Classification of SuDer Turkish News
Corpora
Mehmet Umut S¸ en
Sabancı
¨
Universitesi
umutsen@sabanciuniv.edu
Berrin Yanıko
˘
glu
Sabancı
¨
Universitesi
berrin@sabanciuniv.edu
¨
Ozetc¸e
—Kelime
Temsil
Vekt
¨
orleri,
Do
˘
gal
Dil
˙
Is¸leme
alanındaki
c¸es¸itli
problemlere bas¸arılı
bir s¸ekilde uygulanmak-
tadır;
ancak bu vekt
¨
orleri
e
˘
gitmek ic¸in b
¨
uy
¨
uk miktarda metin
verisi gereklidir.
˙
Ingilizce ic¸in metin derlemi pek c¸ok farklı konu
ve boyut ic¸in rahatlıkla bulunsa da, T
¨
urkc¸e ic¸in az sayıda derlem
bulunmaktadır.
Bu c¸alıs¸mada
iki
c¸evrimic¸i
haber
sitesinden
b
¨
uy
¨
uk miktarlı
metin derlemleri
toplanmıs¸
ve
etiket
olarak
internet
sayfalarında
bulunan
kategori
bilgisi
kullanılmıs¸tır.
Olus¸turulan derlemler c¸es¸itli
dok
¨
uman sınıflandırma modelleri
ile denenmis¸tir. Temsil vekt
¨
orleri kullanan modellerin, geleneksel
TF-TDF
¨
ozniteliklerini
kullanan y
¨
ontemlerden daha iyi
sonuc¸
verdi
˘
gi
g
¨
or
¨
ulm
¨
us¸t
¨
ur.
Aynı
anda hem kelime vekt
¨
orlerini
hem de
dok
¨
uman sınıflandırmasını
¨
o
˘
grenen bir yapay sinir a
˘
gı
en iyi
sonucu vermis¸tir.
Anahtar
Kelimeler—dok
¨
uman
sınıflandırma,
SuDer
haber
metinleri,
kelime temsil
vekt
¨
orleri,
yapay sinir a
˘
gları
Abstract—Word embeddings
are
successfully
employed in
various Natural
Language Processing tasks,
but
training them
requires large amount
of
text,
which is scarce for Turkish.
In
this work,
we collected large amounts of articles from two news
websites and tags within web pages are used as labels.
Obtained
corpora are tested with various document classification models.
Embedding based models performed better than models with the
traditional
TF-IDF features.
A neural
network that
simultane-
ously learns the word embeddings and document
classification
performed the best.
Keywords—document classification,
SuDer news corpora,
word
embeddings,
neural
networks
I.
G
˙
IR
˙
IS
¸
Metinlerin otomatik olarak kategorilerine ayrılması
olarak
tanımlanabilen
dok
¨
uman
sınıflandırma
probleminin;
konu
sınıflandırma,
t
¨
ur
sınıflandırma,
istenmeyen elektronik posta
filtreleme,
duygu analizi
gibi
uygulama alanları
vardır.
Bu
bildiride
konu
ve
t
¨
ur
sınıflandırma
uygulaması
¨
uzerinde
c¸alıs¸ılmıs¸tır.
Dok
¨
uman
sınıflandırma
ic¸in
geleneksel
y
¨
ontemler,
dok
¨
umanların
ic¸inde
gec¸en
kelimelerin
istatistiklerini
Mehmet Umut S¸ en is supported by a T
¨
UB
˙
ITAK Bideb-2211-A scholarship.
kullanarak
¨
oznitelik c¸ıkarmaya ve bu
¨
oznitelikleri
bir makine
¨
o
˘
grenimi
y
¨
ontemiyle modellemeye dayanır.
Terim Frekansı-
Ters
Dok
¨
uman
Frekansı
(TF-TDF)
y
¨
uksek
bas¸arı
oranı
ile
en
pop
¨
uler
¨
oznitelik
c¸ıkarımı
y
¨
ontemlerinden
biridir.
Literat
¨
urdeki
bu ve benzer
y
¨
ontemlerin ve makine
¨
o
˘
grenimi
modellerinin farklı
varyasyonlarının bir
derlemesi
Jindal
ve
arkadas¸larının makalesinde bulunabilir [1].
Kelime
Temsil
Vekt
¨
orleri,
kelimelerin
d
¨
us¸
¨
uk
boyutlu
sayısal
vekt
¨
orlerle temsilidir
ve son yıllarda pek c¸ok prob-
lemde kullanım alanı
bulmus¸lardır.
Bu vekt
¨
orler girdi
olarak
Yapay Sinir
A
˘
glarına (YSA)
verilip,
Geri
Yayılım algorit-
masıyla g
¨
uncellenerek e
˘
gitilirler.
Goldberg,
yapay sinir
a
˘
gı
modellerinin Do
˘
gal
Dil
˙
Is¸leme
(DD
˙
I)
problemleri
¨
uzerine
c¸
¨
oz
¨
umlerini incelemis¸tir [2].
Mikolov ve arkadas¸larının yaptı
˘
gı
bir
c¸alıs¸mada etiketsiz
ve b
¨
uy
¨
uk derlemlerde,
girdi
olarak metinden bir kelime alan
ve bu kelimenin yakınındaki kelimeleri kestirmeye c¸alıs¸an, tek
katmanlı bir modelden elde edilen vekt
¨
orlerin kelimelerle ilgili
anlam-bilimsel ve s
¨
oz-dizimsel bilgileri ic¸erdi
˘
gi g
¨
osterilmis¸tir
[3].
B
¨
uy
¨
uk etiketsiz derlemlerden bu s¸ekilde
¨
o
˘
grenilen kelime
vekt
¨
orleri,
k
¨
uc¸
¨
uk etiketli
derlemlerde
e
˘
gitilecek modellerin
ilklendirilmesi ic¸in kullanılabilmektedir.
T
¨
urkc¸e
metinlerin
sınıflandırılması
ic¸in
literat
¨
urde
c¸es¸itli
c¸alıs¸malar
mevcuttur.
Kılıc¸
ve
arkadas¸larının
c¸alıs¸masında
TF-TDF’nın
iki
yeni
varyasyonu
tanıtılmıs¸
ve T
¨
urkc¸e derlemde bas¸arımın arttı
˘
gı
g
¨
osterilmis¸tir
[4].
Ay
ve arkadas¸larının c¸alıs¸masında genetik algoritma kullanılmıs¸
ve
yeni
bir
nitelik
a
˘
gırlıklandırma
y
¨
ontemi
sunulmus¸tur
[5].
S¸ ahin’in
c¸alıs¸masında
g
¨
ozetimsiz
¨
o
˘
grenilen
kelime
temsil
vekt
¨
orlerinin ortalamaları
Destek Vekt
¨
or
Makinesine
(DVM) girdi olarak verilmis¸ ve TF-TDF’ndan daha iyi bas¸arı
sa
˘
gladı
˘
gı
g
¨
osterilmis¸tir
[6].
Bu bildiride bu son c¸alıs¸madaki
y
¨
ontem tekrarlanmıs¸,
ayrıca
kelime
vekt
¨
orlerinin g
¨
ozetimli
¨
o
˘
grenilmesinin bas¸arıyı daha da artırdı
˘
gı g
¨
osterilmis¸tir.
Dok
¨
uman sınıflandırma ic¸in T
¨
urkc¸e derlemler g
¨
un gec¸tikc¸e
artmaktadır.
S¸ ahin
ve
arkadas¸larının
c¸alıs¸masında
T
¨
urkc¸e
Vikipedi
sayfaları
otomatik kategorilenmis¸
ve
yaklas¸ık
10
milyon
kelimeli
bir
derlem olus¸turulmus¸tur
[7].
T
¨
ufekc¸i
ve
arkadas¸larının
c¸alıs¸masında
5
farklı
haber
portalından
toplanmıs¸
5 kategoriden olus¸an toplam
750
dok
¨
umanlık der-
lem olus¸turulmus¸ ve c¸es¸itli morfolojik
¨
onis¸leme y
¨
ontemlerinin
978-1-5386-1501-0/18/$31.00
c
2018 IEEE
sınıflandırmaya
etkisi
incelenmis¸tir
[8].
Kelime
haznesinde
sadece isim t
¨
ur
¨
undeki
kelimelerin kullanılmasıyla
¨
oznitelik
boyutlarının y
¨
uksek oranda d
¨
us¸
¨
ur
¨
uld
¨
u
˘
g
¨
u ve bas¸arımın azal-
madı
˘
gı
g
¨
osterilmis¸tir.
Kılınc¸
ve arkadas¸larının c¸alıs¸masında,
6 haber portalından toplanmıs¸
3, 600
dok
¨
umandan olus¸an bir
derlem paylas¸ılmıs¸tır [9].
Kelime temsillerinin
¨
o
˘
grenimi,
b
¨
uy
¨
uk metin derlemlerini
gerektirmektedir.
Bu nedenle,
bu c¸alıs¸mada
b
¨
uy
¨
uk
¨
olc¸ekli
iki
yeni
derlem toplanmıs¸tır.
Bu derlemler
¨
uzerinde TF-TDF,
Saklı
Dirichlet
Ataması
(SDA),
Kelime Temsil
Vekt
¨
orleri
ve
Yapay Sinir A
˘
gları kullanan dok
¨
uman sınıflandırma y
¨
ontemleri
uygulanmıs¸ ve de
˘
gerlendirilmis¸tir.
II.
D
ERLEMLER
Sabah
1
ve
Cumhuriyet
2
gazetelerinin c¸evrimic¸i
internet
sitelerinden metin ic¸erikli haber, k
¨
os¸e yazısı, resim galerisi ve
video paylas¸ımı
ic¸eren sayfalar
indirilmis¸
ve bu sayfalardan
metin,
bas¸lık,
tarih ve kategori bilgileri ayıklanmıs¸tır.
Sabah’ın sitesinden 2010-Ocak ile 2017-Temmuz arasında
yayınlanmıs¸
toplamda yaklas¸ık
426, 000
sayfa elde edilmis¸;
metin ve
bas¸lıktaki
toplam kelime
sayısı
10’dan az
olan
sayfalar elenmis¸ ve geriye
420, 513
sayfa kalmıs¸tır.
Toplamda
4
farklı
kategori
vardır
ve
bu
kategorilerle
ilgili
bilgiler
Tablo-I’de belirtilmis¸tir.Bu istatistikler bas¸lıklar kullanılmadan
c¸ıkarılmıs¸tır.
Deneylerde de bas¸lıklar kullanılmamaktadır.
Cumhuriyet’in
sitesinden
2017-Eyl
¨
ul
tarihine
kadar
yayınlanan,
toplamda yaklas¸ık
463, 000
sayfa elde edilmis¸tir.
Ancak 2014 senesinden
¨
onceki
sayfaların c¸o
˘
gunda kategori
bilgisi bulunmamaktadır; toplamda
273, 000
sayfanın kategori
bilgisi
mevcuttur.
Metindeki
kelime
sayısı
10’dan az
olan
ve
toplam sayfa
sayısı
az
olan 7 kategoriye
ait
sayfalar
elendikten
sonra
14
kategoriye
ait
268, 784
sayfa
elde
edilmis¸tir.
Kategoriler ile ilgili bilgiler Tablo-II’de verilmis¸tir.
TABLO I:
S
ABAH
D
ERLEM
˙
I
˙
I
STATISTIKLERI
Kategori
Dok
¨
uman Sayıları
Kelime Sayıları
Toplam
E
˘
gitim
Test
Toplam
Ortalama
g
¨
undem
143,842
117,019
26,823
35,749,880
248.54
yas¸am
123,086
108,202
14,884
22,878,732
180.86
ekonomi
85,485
75,512
9,973
22,261,600
247.38
yazarlar
68,100
60,683
7,417
16,335,364
239.87
Toplam
420,513
361,416
59,097
95,494,110
227.09
TABLO II:
C
UMHURIYET
D
ERLEM
˙
I
˙
I
STATISTIKLERI
Kategori
Dok
¨
uman Sayıları
Kelime Sayıları
Toplam
E
˘
gitim
Test
Toplam
Ortalama
t
¨
urkiye
84,741
56,140
28,524
22,829,220
269.39
yazarlar
33,835
29,694
4,141
16,663,717
492.49
video
33,409
23,686
9,723
2,007,691
60.09
spor
31,396
24,627
6,730
7,240,974
230.63
d
¨
unya
21,005
14,684
6,152
4,416,708
210.26
siyaset
15,969
11,274
4,686
6,409,811
401.39
foto
14,302
9,729
110
248,871
17.40
ekonomi
8,187
5,811
2,356
2,520,473
307.86
teknoloji
7,913
5,089
2,810
1,734,268
219.16
k
¨
ult
¨
ur-sanat
6,506
4,680
1,806
2,664,020
409.47
yas¸am
4,833
3,931
886
918,754
190.10
sa
˘
glık
2,573
2,047
514
863,208
335.48
e
˘
gitim
2,380
1,544
805
744,396
312.77
c¸evre
1,735
1,081
607
477,811
275.39
Toplam
268,784
194,017
69,850
69,739,922
259.46
1
www.sabah.com.tr
2
www.cumhuriyet.com.tr
Bu c¸alıs¸mamızda bu iki
derlemin de 1 Eyl
¨
ul
2016’dan
¨
onceki dok
¨
umanları e
˘
gitim k
¨
umesi,
sonrakiler ise test k
¨
umesi
olarak
kullanılmıs¸tır.
Kelime
haznesine,
kesme
is¸areti
ile
ayrılmıs¸ ekler dahil edilmis¸; tek harfli kelimeler ve sayılar dahil
edilmemis¸tir
3
.
III.
Y
¨
ONTEMLER
A.
TF-TDF ve Destek Vekt
¨
or Makineleri
Terim Frekansı
-
Ters
Dok
¨
uman
Frekansı
(TF-TDF)
¨
oznitelikleri,
her bir dok
¨
umanı
sabit
boyutta sayısal
vekt
¨
orler
s¸eklinde g
¨
osterebilen bir
y
¨
ontemdir.
Vekt
¨
orlerdeki
her
boyut
bir
terimin dok
¨
umanda gec¸me sıklı
˘
gına dayanır.
t
teriminin
d
dok
¨
umanındaki
g
¨
or
¨
ulme sayısına
c
dt
ve
d
dok
¨
umanındaki
toplam kelime sayısına
N
d
dersek,
Terim Frekansı
s¸u s¸ekilde
bulunur:
tf (d, t)
= c
dt
/N
d
.
C¸ ok fazla sayıda dok
¨
umanda
gec¸en,
dolayısıyla ba
˘
glamla ilgisi
olma ihtimali
d
¨
us¸
¨
uk ke-
limelerin
etkisini
azaltmak
amacıyla
da
bir
terimin
Ters
Dok
¨
uman Frekansı s¸u s¸ekilde tanımlanır:
tdf (t) = log

1 + D
1 + m
t

(1)
Burada,
D
toplam dok
¨
uman sayısı,
m
t
ise
t
teriminin
gec¸ti
˘
gi dok
¨
uman sayısıdır. TF-TDF
¨
oznitelikleri bu iki de
˘
gerin
c¸arpımıdır:
tf tdf (d, t) = tf (d, t) × tdf (t)
.
Bu
c¸alıs¸mada
terim olarak
sadece
tekli
kelimeler
kullanılmıs¸tır.
TF-TDF
¨
oznitelikleri
bulunurken kelime hazne
boyu
ic¸in
1, 000
ile
50, 000
arasında
de
˘
gis¸en
farklı
de
˘
gerler
denenmis¸tir.
Terim vekt
¨
orlerini
normalize
etmek
ic¸in
l
1
normalizasyonu kullanılmıs¸tır.
C¸ ıkarılan
¨
oznitelikler
do
˘
grusal Destek Vekt
¨
or Makinesi (DVM) ile sınıflandırılmıs¸tır.
Veri
sayısının
¨
oznitelik sayısından c¸ok oldu
˘
gu durumlarda
do
˘
grusal
DVM’nın
birincil
form
¨
ulasyonunun
optimizasy-
onunun do
˘
grusal olmayan c¸ekirdekli form
¨
ulasyonlara g
¨
ore c¸ok
daha hızlı oldu
˘
gu ve do
˘
gruluk oranlarında yakın sonuc¸ verdi
˘
gi
ic¸in [10] do
˘
grusal DVM kullanılmıs¸ ve birincil form
¨
ulasyonla
optimize edilmis¸tir.
C¸ ok sınıfla sınıflandırma ic¸in ”bire-hepsi”
y
¨
ontemi [11] kullanılmıs¸tır.
B.
Saklı Dirichlet Ataması
Saklı
Dirichlet
Ataması
(SDA) g
¨
ozetimsiz konu
¨
o
˘
grenimi
ic¸in sık kullanılan
¨
uretici
bir
olasılıksal
modeldir
[12].
Bu
y
¨
ontemde her
bir
dok
¨
uman bir
konuya atanmak yerine bir
konu da
˘
gılımına atanır
ve bu atama Dirichlet
Da
˘
gılımı
ile
temsil
edilir.
Dok
¨
uman ic¸indeki
her bir kelimenin ise tek bir
konudan geldi
˘
gi
varsayılır.
Konular
ise s
¨
ozc
¨
uk haznesindeki
kelimeler
¨
uzerine bir ihtimal
da
˘
gılımı
ile temsil
edilir.
Konu
sayısı modele girdi olarak verilir.
Bu
c¸alıs¸mada
SDA modelindeki
¨
onceden
sabitlenmesi
gereken
toplam konu
sayısı
(
K
)
ic¸in
farklı
de
˘
gerler
denenmis¸tir.
Veri
b
¨
uy
¨
uk
oldu
˘
gu
ic¸in
Varyasyonel
Bayes
y
¨
ontemiyle
c¸ıkarsama
yapan C¸ evrimic¸i
SDA [13]
y
¨
ontemi
kullanılmıs¸tır
4
.
Model
e
˘
gitildikten
sonra,
g
¨
ozetimli
sınıflandırmada kullanılmak
¨
uzere,
her
konu bir
kategoriye
atanmıs¸tır.
Bu atamayı
belirlemek ic¸in,
e
˘
gitim verisindeki
her
bir
dok
¨
uman
ic¸in
konu
da
˘
gılımları
bulunmus¸
(
γ
dk
:
3
Derlemler s¸u adresten indirilebilir: https://github.com/suverim/suder
4
SDA kodu: github.com/wellecks/online lda python
d
dok
¨
umanının
k
konusuna
ait
olma
ihtimali);
her
konu,
ihtimallerinin ortalaması en y
¨
uksek kategoriye atanmıs¸tır:
m
k
= arg max
c
1
|D
c
|
X
d:d∈D
c
γ
dk
(2)
Burada
D
c
,
c
sınıfına ait dok
¨
uman k
¨
umesi;
m
k
,
k
konusu-
nun hangi sınıfa ait oldu
˘
gudur.
C.
Kelime Temsilleri ve Destek Vekt
¨
or Makineleri
Kelime
haznesindeki
her
bir
kelimenin,
hazne
boyuna
kıyasla c¸ok daha k
¨
uc¸
¨
uk boyutlu,
rasyonel
vekt
¨
orlerle tem-
sil
edilmesine
kelime
temsili
denir.
B
¨
uy
¨
uk
veri
taban-
larında
g
¨
ozetimsiz
¨
o
˘
grenilen
vekt
¨
orlerin,
kelimelerle
il-
gili
anlam-bilimsel
ve s
¨
oz-dizimsel
bilgileri
yakalayabildi
˘
gi
g
¨
ozlemlenmis¸tir [3],
[14].
Bu c¸alıs¸mada kelime vekt
¨
orlerinin
g
¨
ozetimsiz
¨
o
˘
grenimi ic¸in Atla-Gram y
¨
ontemi kullanılmıs¸tır [3],
[15].
Bu y
¨
ontemde her kelimenin ”girdi” ve ”c¸ıktı” vekt
¨
orleri
bulunmaktadır.
Modele
girdi
olarak
bir
kelimenin
”girdi”
vekt
¨
or
¨
u verilir
ve yakındaki
kelimelerin ”c¸ıktı” vekt
¨
orlerinin
kestirimi,
c¸ıktı katmanındaki yumus¸ak-maksimum katmanı ile
hedeflenir.
E
˘
gitimden sonra ”girdi” vekt
¨
or
¨
u kelimenin tem-
sili
ic¸in kullanılır.
Standart
form
¨
ulasyon pratikte c¸alıs¸madı
˘
gı
ic¸in gelis¸tirilen yakınlas¸tırmalardan Eksi-
¨
Ornekleme y
¨
ontemi
kullanılmıs¸tır.
Bu
c¸alıs¸mada,
kelime
vekt
¨
orleri
bulunduktan
sonra
dok
¨
uman
¨
ozniteliklerini bulmak ic¸in dok
¨
umandaki kelime tem-
sillerinin ortalaması
alınmıs¸tır.
Daha sonra c¸ıkan
¨
oznitelikler
DVM’ne
girdi
olarak verilmis¸tir.
Bas¸ka
bir
c¸alıs¸mada,
bu
y
¨
ontemin T
¨
urkc¸e bir derlemde iyi c¸alıs¸tı
˘
gı g
¨
or
¨
ulm
¨
us¸t
¨
ur [6].
D.
Kelime Temsilleri ve Yapay Sinir A
˘
gları
Bu y
¨
ontemde dok
¨
umandaki kelime temsillerinin ortalaması
alındıktan sonra YSA ile konu sınıflandırması yapılmıs¸tır.
Bir
t
kelimesinin vekt
¨
or
¨
u w
t
∈ R
d
ve
f :
R
d
→ R
C
bir
YSA
olmak
¨
uzere (
C
sınıf sayısı),
verilen bir
S
d
= {t
1
, . . . , t
N
d
}
dok
¨
umanının sınıflandırması s¸u s¸ekilde yapılır:
y
c
(d) = f
1
|S
d
|
X
t∈S
d
w
t
!
(3)
Burada
y
c
(d)
,
d
dok
¨
umanının
c
sınıfına ait skorudur. Hedef
fonksiyonu olarak Ortalama Kareler Toplamı kullanılmıs¸tır:
Φ =
1
CD
D
X
d=1
C
X
c=1
(y
c
(d) − δ
dc
)
2
(4)
Burada
δ
dc
,
d
dok
¨
umanı
c
sınıfına aitse
1
, di
˘
ger durumlarda
0
’dır
ve
D
toplam dok
¨
uman sayısıdır.
Bu c¸alıs¸mada,
¨
onceki
benzer
T
¨
urkc¸e dok
¨
uman sınıflandırma y
¨
ontemlerinden (
¨
orn.
[6]) farklı olarak, kelime vekt
¨
orleri de geri yayılım algoritması
kullanılarak g
¨
uncellenmis¸tir.
B
¨
oylece daha
¨
once g
¨
ozetimsiz
¨
o
˘
grenilen
kelime
vekt
¨
orlerinin
etiket
bilgisi
kullanılarak
ayrıs¸tırıcı
e
˘
gitimi
sa
˘
glanmıs¸
ve
bunun do
˘
gruluk oranlarını
artırdı
˘
gı
g
¨
ozlenmis¸tir.
¨
Onceki
b
¨
ol
¨
umde bahsedilen g
¨
ozetimsiz
¨
o
˘
grenilen kelime
vekt
¨
orleri,
a
˘
gın kelime
vekt
¨
orlerinin ilk-
lendirilmesi ic¸in kullanılmıs¸tır.
IV.
D
ENEYLER
Metinler
modellere
verilmeden
¨
once
k
¨
uc¸
¨
uk
harflere
d
¨
on
¨
us¸t
¨
ur
¨
ulm
¨
us¸; daha sonra
¨
ozel isimlere eklenen ekleri yakala-
mak ic¸in aralarında kesme is¸areti bulunan kelimeler ayrılmıs¸ ve
bu ekler atılmıs¸tır.
Sonrasında tek harfli
kelimeler ve sayılar
atılmıs¸tır.
Ba
˘
glam dıs¸ı
kelimeler
de,
internetteki
c¸es¸itli
kay-
naklar kullanılarak atılmıs¸tır
5 6 7
. Toplamda 553 tane ba
˘
glam
dıs¸ı kelime elde edilmis¸tir.
Morfolojik is¸lem ic¸in Zemberek arac¸
kutusu [16]
kul-
lanılarak kelimelerin morfolojik analizi
yapılmıs¸
ve analizi
yapılabilen kelimelerin analiz sec¸eneklerinden en uzun k
¨
okl
¨
u
olanın k
¨
ok
¨
u kullanılmıs¸tır.
Bu y
¨
ontemin daha
¨
once iyi
sonuc¸
verdi
˘
gi literat
¨
urde g
¨
ozlemlenmis¸tir [8],
[14],
[17].
A.
Parametreler
TF-TDF vekt
¨
orleri,
1, 000
ile
50, 000
arasında
de
˘
gis¸en
kelime hazne boyu ic¸in c¸ıkarılmıs¸tır.
Gerc¸ekleme ic¸in Gensim
arac¸
kutusu kullanılmıs¸tır
[18].
Kelime haznesi
bulunurken
derlemde toplamda en sık gec¸en kelimeler kullanılmıs¸tır. DVM
gerc¸eklemesi
ic¸in scikit-learn arac¸ kutusu [19] ve
C
parame-
tresi ic¸in varsayılan de
˘
ger kullanılmıs¸tır. Farklı kelime haznesi
boylarına g
¨
ore sonuc¸lar Tablo-III’te verilmis¸tir.
TABLO III:
TF-TDF H
AZNE
B
OYUNUN
E
TKISI
(%)
Derlem/Hazne Boyu
1K
5K
10K
20K
50K
Sabah
84,29
86,22
86,41
86,52
86.50
Cumhuriyet
69,12
71,71
71,81
71,72
71,69
Sonuc¸lara g
¨
ore her iki
derlemde de kelime haznesi
boyu
olarak
10, 000
ile
20, 000
civarında iyi
sonuc¸lar elde edildi
˘
gi
ve bu boyu daha fazla arttırmanın do
˘
gruluk oranlarına bir
faydası
olmadı
˘
gı
g
¨
or
¨
ulmektedir.
Bu sonuc¸lara ba
˘
glı
olarak,
Saklı
Dirichlet
Ataması
(SDA)
deneylerinde her
iki
derlem
ic¸in de hazne boyu
10, 000
alınmıs¸tır.
C¸ evrimic¸i
Saklı
Dirichlet
Ataması
(SDA)
y
¨
ontemindeki,
ilk verilen dok
¨
umanların etkisini
azaltmak ic¸in olan
¨
o
˘
grenme
parametresi
(
τ
)
1024
,
d
¨
us¸
¨
us¸
fakt
¨
or
¨
u
parametresi
(
κ
)
0.7
alınmıs¸tır.
Toptan boyutu olarak
100
kullanılmıs¸
ve e
˘
gitim
verisinin
¨
uzerinden toplamda
3
devir yapılmıs¸tır.
Konu sayısı
ic¸in,
derlemlerdeki
sınıf
sayısı
ve
daha
y
¨
uksek
de
˘
gerler
denenmis¸tir.
Kelime vekt
¨
orlerinin g
¨
ozetimsiz
¨
o
˘
grenimi ic¸in Gensim arac¸
kutusu kullanılmıs¸tır [18]. Pencere boyutu
20
, Eksi-
¨
Ornekleme
parametresi
5
alınmıs¸tır.
E
˘
gitim derleminin
¨
uzerinden
20
kere gec¸ilmis¸tir.
Vekt
¨
or
boyutları
ic¸in
100
,
200
,
400
ve
600
denenmis¸tir.
Derlemde
10
’dan az gec¸en kelimeler elenmis¸ ve
geriye Cumhuriyet
derlemi
ic¸in
70, 118
,
Sabah derlemi
ic¸in
60, 718
kelime kalmıs¸tır.
YSA modelinde,
50
d
¨
u
˘
g
¨
uml
¨
u
ve
do
˘
grusal
olmayan
aktivasyon
fonksiyonu
ReLU olan
2
tane
saklı
katman
kullanılmıs¸tır.
C¸ ıktı
katmanının
aktivasyonu
ic¸in
S-bic¸im
fonksiyonu
kullanılmıs¸tır.
Optimizasyon
algoritması
olarak
RMSprop,
¨
o
˘
grenme
oranı
ic¸in
0.01
kullanılmıs¸tır.
E
˘
gitim
verisi
¨
uzerinde
toplamda
10
devir
yapılmıs¸tır.
Toptan
5
https://github.com/ahmetax/trstop/blob/master/dosyalar/turkce-stop-words
6
https://github.com/crodas/TextRank/blob/master/lib/TextRank/Stopword/turkish-
stopwords.txt
7
https://github.com/stopwords-iso/stopwords-tr/blob/master/stopwords-tr.txt
boyutu
100
alınmıs¸
ve bu toptanlar
yerine koyarak rastgele
¨
orneklemeyle
olus¸turulmus¸tur.
Bu model
Pytorch arac¸
ku-
tusuyla gerc¸eklenmis¸tir [20].
B.
Sonuc¸lar
Deneyi
yapılan
y
¨
ontemlerin
do
˘
gruluk
oranları
Tablo-
IV’te g
¨
osterilmis¸tir.
Etiket
bilgisi
kullanmayan Saklı
Dirich-
let
Ataması
en d
¨
us¸
¨
uk sonuc¸ları
vermis¸tir;
ancak SDA’nın
b
¨
uy
¨
uk miktarlarda
etiketsiz
verinin de
oldu
˘
gu durumlarda
daha iyi
sonuc¸lar
vermesi
beklenir.
Ayrıca Cumhuriyet
der-
leminde en iyi
sonuc¸
sınıf
sayısına es¸it
konu sayısı
ile elde
edilmesine ra
˘
gmen,
Sabah derleminde konu sayısını
artırmak
do
˘
gruluk oranını
artırmıs¸tır.
Bu sonucun muhtemel
sebebi
Sabah derleminde sadece 4 konu kategorisi olması, dolayısıyla
metinlerin konularına c¸ok ba
˘
glı olmamasıdır.
G
¨
ozetimli modellerde DVM ile birlikte kullanılan TF-TDF
¨
oznitelikleri
ile
KTV
¨
oznitelikleri
birbirine
yakın sonuc¸lar
vermis¸tir. Ancak kelime temsillerinin boyutlarını artırarak TF-
TDF ile alınan do
˘
gruluk oranlarından daha y
¨
uksek sonuc¸lar
elde edilebilmektedir;
oysa TF-TDF y
¨
onteminde
20, 000
ke-
limeden sonra
do
˘
gruluk oranlarının artmadı
˘
gı
g
¨
or
¨
ulm
¨
us¸t
¨
ur.
(Tablo-III). Bunun muhtemel sebebi olarak y
¨
uksek boyutlu TF-
TDF
¨
ozniteliklerinde DVM’nın etkili
¨
o
˘
grenememesi
oldu
˘
gu
d
¨
us¸
¨
un
¨
ulebilir.
KTV
ve
YSA
y
¨
ontemlerinde
Sabah
ve
Cumhuriyet
derlemleri
ic¸in
sırasıyla
yaklas¸ık
70, 000
ve
60, 000
kelimeden olus¸an kelime hazneleri kullanılmıs¸tır.
Kelime temsil vekt
¨
orlerini ve dok
¨
uman sınıflandırmayı aynı
anda
¨
o
˘
grenen Yapay Sinir A
˘
gı yaklas¸ımı b
¨
ut
¨
un vekt
¨
or boyutları
ic¸in KTV+DVM kombinasyonundan iyi
sonuc¸
vermis¸
ve en
iyi
sonuc¸lar
bu y
¨
ontemle alınmıs¸tır
(Sabah ve Cumhuriyet
derlemleri
ic¸in
sırasıyla
%88.28
ve
%74.31).
Bu
da
ke-
lime temsillerinin etiket
bilgisi
kullanılarak g
¨
uncellenmesinin
do
˘
gruluk oranlarını
artırdı
˘
gını
g
¨
ostermektedir.
Ayrıca vekt
¨
or
boyutu
k
¨
uc¸
¨
uld
¨
ukc¸e
bas¸arının
arttı
˘
gı
g
¨
or
¨
ulmektedir,
bu
da
dok
¨
uman sınıfı
ile ilgili
bilgilerin c¸ok d
¨
us¸
¨
uk boyutlu kelime
vekt
¨
orlerinde ihtiva edilebilece
˘
gini g
¨
ostermektedir.
TABLO IV:
D
O
˘
GRULUK
O
RANLARI
(%)
SDA ic¸in
K
de
˘
gerleri sırasıyla Sabah ve Cumhuriyet derlemleri ic¸indir.
Y
¨
ontem
Sabah
Cumhuriyet
SDA (
K = 4
/
K = 14
)
65.41
47.94
SDA (
K = 10
/
K = 20
)
67.60
43.31
SDA (
K = 20
/
K = 30
)
72.08
45.37
TF-TDF (
10K
K.
Haznesi) + DVM
86.41
71.81
KTV (
d = 100
) + DVM
85.47
70.34
KTV (
d = 200
) + DVM
86.16
71.55
KTV (
d = 400
) + DVM
86.72
72.24
KTV (
d = 600
) + DVM
86.89
72.50
KTV (
d = 100
) + YSA
88.28
74.31
KTV (
d = 200
) + YSA
87.93
73.64
KTV (
d = 400
) + YSA
87.94
72.29
KTV (
d = 600
) + YSA
87.53
72.97
V.
S
ONUC
¸
Bu c¸alıs¸mada, iki b
¨
uy
¨
uk ve yeni T
¨
urkc¸e metin derlemi konu
kategorileri
ile olus¸turulmus¸
ve paylas¸ıma ac¸ılmıs¸tır.
Kelime
vekt
¨
orlerinin ortalamasını alarak c¸alıs¸an bir yapay sinir a
˘
gının,
di
˘
ger y
¨
ontemlere g
¨
ore daha iyi sonuc¸ verdi
˘
gi g
¨
ozlemlenmis¸tir.
˙
Ileride,
etiketsiz veri
de kullanarak,
g
¨
ozetimsiz y
¨
ontemlerin
avantajlarından faydalanabilece
˘
gimiz yarı-g
¨
ozetimli y
¨
ontemler
¨
uzerinde c¸alıs¸ılacaktır.
K
AYNAKC
¸
A
[1]
R. Jindal, R. Malhotra, and A. Jain, “Techniques for text classification:
Literature review and current
trends,” webology,
vol.
12,
no.
2,
p.
1,
2015.
[2]
Y. Goldberg, “A primer on neural network models for natural language
processing.” J.
Artif.
Intell.
Res.(JAIR),
vol.
57,
pp.
345–420,
2016.
[3]
T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
word representations in vector space,” arXiv preprint arXiv:1301.3781,
2013.
[4]
E.
Kilic,
N.
Ates,
A.
Karakaya,
and D.
O.
Sahin,
“Two new feature
extraction methods for
text
classification:
Tesdf
and sadf,” in Signal
Processing and Communications Applications Conference (SIU),
2015
23th.
IEEE,
2015,
pp.
475–478.
[5]
S. Ay, Y. S. Do
˘
gan, S. Alver, and C¸ . Kaya, “A novel attribute weighting
method with genetic algorithm for document
classification,” in Signal
Processing and Communication Application Conference (SIU),
2016
24th.
IEEE,
2016,
pp.
1129–1132.
[6]
G.
S¸ ah
˙
In,
“Turkish document
classification based on word2vec and
svm classifier,” in Signal Processing and Communications Applications
Conference (SIU),
2017 25th.
IEEE,
2017,
pp.
1–4.
[7]
H.
B.
Sahin,
C.
Tirkaz,
E.
Yildiz,
M.
T.
Eren,
and O.
Sonmez,
“Automatically annotated turkish corpus for named entity recognition
and text
categorization using large-scale gazetteers,” arXiv preprint
arXiv:1702.02363,
2017.
[8]
P.
T
¨
ufekci,
E.
Uzun,
and B.
Sevinc¸,
“Text
classification of
web
based news articles by using turkish grammatical
features,” in Signal
Processing and Communications Applications Conference (SIU),
2012
20th.
IEEE,
2012,
pp.
1–4.
[9]
D.
Kılınc¸,
A.
¨
Ozc¸ift,
F.
Bozyigit,
P.
Yıldırım,
F.
Y
¨
ucalar,
and E.
Bo-
randag,
“Ttc-3600:
A new benchmark dataset
for turkish text
catego-
rization,” Journal of Information Science,
vol.
43,
no.
2,
pp.
174–185,
2017.
[10]
S.
S.
Keerthi
and D.
DeCoste,
“A modified finite newton method for
fast solution of large scale linear svms,” Journal of Machine Learning
Research,
vol.
6,
no.
Mar,
pp.
341–361,
2005.
[11]
R.
Rifkin and A.
Klautau,
“In defense of
one-vs-all
classification,”
Journal
of
machine learning research,
vol.
5,
no.
Jan,
pp.
101–141,
2004.
[12]
D.
M.
Blei,
A.
Y.
Ng,
and M.
I.
Jordan,
“Latent
dirichlet
allocation,”
Journal
of
machine Learning research,
vol.
3,
no.
Jan,
pp.
993–1022,
2003.
[13]
M.
Hoffman,
F.
R.
Bach,
and D.
M.
Blei,
“Online learning for latent
dirichlet
allocation,”
in advances
in neural
information processing
systems,
2010,
pp.
856–864.
[14]
M. U. Sen and H. Erdogan, “Learning word representations for turkish,”
in Signal
Processing and Communications
Applications
Conference
(SIU),
2014 22nd.
IEEE,
2014,
pp.
1742–1745.
[15]
T.
Mikolov,
I.
Sutskever,
K.
Chen,
G.
S.
Corrado,
and J.
Dean,
“Distributed representations of words and phrases and their composi-
tionality,” in Advances in neural information processing systems, 2013,
pp.
3111–3119.
[16]
A. A. Akın and M. D. Akın, “Zemberek, an open source nlp framework
for turkic languages,” Structure,
vol.
10,
pp.
1–5,
2007.
[17]
Z. Cataltepe, Y. Turan, and F. Kesgin, “Turkish document classification
using shorter roots,” in Signal Processing and Communications Appli-
cations,
2007.
SIU 2007.
IEEE 15th.
IEEE,
2007,
pp.
1–4.
[18]
R.
ˇ
Reh
˚
u
ˇ
rek and P.
Sojka,
“Software Framework for Topic Modelling
with Large Corpora,” in Proceedings of
the LREC 2010 Workshop on
New Challenges for NLP Frameworks.
Valletta,
Malta:
ELRA,
May
2010,
pp.
45–50,
http://is.muni.cz/publication/884893/en.
[19]
F.
Pedregosa,
G.
Varoquaux,
A.
Gramfort,
V.
Michel,
B.
Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-
plas,
A.
Passos,
D.
Cournapeau,
M.
Brucher,
M.
Perrot,
and E.
Duch-
esnay,
“Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research,
vol.
12,
pp.
2825–2830,
2011.
[20]
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A.
Desmaison,
L.
Antiga,
and A.
Lerer,
“Automatic differentiation in
pytorch,” 2017.
