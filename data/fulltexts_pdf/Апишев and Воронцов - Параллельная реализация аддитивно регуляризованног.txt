Московский государственный университет имени М. В. Ломоносова
Факультет Вычислительной Математики и Кибернетики
Магистерская программа «Логические и комбинаторные методы анализа данных»
Магистерская диссертация
Параллельная реализация аддитивно регуляризованного
тематического моделирования и её применение для поиска
этно-релевантного контента в социальных медиа
Работу выполнил:
Апишев Мурат Азаматович
Научный руководитель:
д.ф-м.н., доцент
Воронцов Константин Вячеславович
Москва, 2017
Содержание
1
Введение
4
2
Тематическое моделирование
6
2.1
PLSA .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
6
2.2
LDA .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
2.3
ARTM .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
8
3
Аддитивная регуляризация
9
3.1
Общий подход
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
3.2
Сглаживание и разреживание
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
10
3.3
Декорреляция тем .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
3.4
Модальность этнонимов .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
3.5
Модальности меток времени и геотегов .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
12
4
Библиотека BigARTM
13
4.1
Обзор
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
4.2
Реализации ЕМ-алгоритма
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
13
4.3
Онлайновый алгоритм DetAsync .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
18
5
Эксперименты
21
5.1
Оценивание реализации DetAsync .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
21
5.2
Эксперименты на коллекции LiveJournal
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
22
5.3
Эксперименты на коллекции IQBuzz
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
29
6
Результаты, выносимые на защиту
35
2
Аннотация
В современных исследованиях Интернета часто используются различные
методы анализа текстов для обучения без учителя с целью извлечения инфор-
мации, релевантной различным тематикам. Разработанный ранее подход адди-
тивной регуляризации тематических моделей (АРТМ) предоставляет возмож-
ность более гибкого вывода и контроля над темами, чем различные расширения
LDA. В данной работе подход АРТМ был применён в задаче извлечения этно-
социального контента из текстов русскоязычного медиапространства. В рамках
работы были представлены более совершенный онлайновый параллельный ЕМ-
алгоритм для обучения модели, новые регуляризаторы и проведено сравнение
моделей АРТМ и LDA.
С помощью экспертных оценок показано,
что подход
АРТМ лучше подходит для поиска релевантных и интерпретируемых тем.
3
1
Введение
Тематические модели стали одним из стандартных инструментов для извлечения
данных из больших текстовых коллекций.
По сути,
тематические модели произво-
дят разложение разреженной матрицы «слова-документы» в произведение матриц
«слова-темы» и «темы-документы».
Впервые эта идея появилась в модели вероят-
ностного латентного семантического анализа (PLSA) [9],
сейчас же основным ин-
струментом стала модель латентного размещения Дирихле (LDA), которая является
байесовской версией PLSA с априорными распределениями Дирихле для распреде-
ления слов в темах и тем в документах [3, 6].
В течение долгого времени LDA находится в центре внимания,
было опубли-
ковано множество работ,
предлагающий различные её модификации для решения
конкретных задач, однако такие модификации являются отдельными инструмента-
ми,
которые нельзя легко комбинировать друг с другом.
Разработка каждого по-
добного расширения — это большая работа для исследователя в области анализа
данных. Специалист из другой предметной области, например, социологии, не будет
заниматься разработкой новой модели LDA для каждой конкретной возникающей
задачи.
Более того,
даже небольшая модификация уже существующей реализации
модели может оказаться слишком сложной с технической точки зрения задачей.
В данной работе для решения задач из различных предметных предлагается ис-
пользовать подход аддитивной регуляризации тематических моделей (АРТМ) [25] и
реализующий его программный продукт с открытым кодом BigARTM [24].
АРТМ
обобщает базовую модель PLSA при помощи механизма регуляризации,
который
можно использовать для оказания прямого влияния на те или иные аспекты моде-
ли, которые важно учесть при её построении. По факту, модель LDA [23] является
частным случаем АРТМ с регуляризатором сглаживания.
Гибкость является огромным преимуществом АРТМ на практике. Обучив базо-
вую модель LDA или АРТМ без регуляризаторов, исследователь может понять, чего
ему не хватает, и сформулировать свои предпочтения в терминах регуляризаторов.
В большинстве случаев BigARTM позволяет исследователям комбинировать регуля-
ризаторы из встроенной библиотеки регуляризаторов для получения необходимого
качества модели по заданным метрикам.
Помимо гибкой настройки моделей библиотека BigARTM позволяет производить
их обучение быстрым онлайновым параллельным ЕМ-алгоритмом. Версия алгорит-
ма, реализованная на момент написания этой работы, уже превосходила существу-
ющие аналоги в скорости работы [24].
Тем не менее,
она имеет ряд недостатков:
4
недетерминированность, неочевидность подбора параметров обучения для достиже-
ния высокой производительности.
Эта работа состоит из двух связанных логических блоков.
Первый заключает-
ся в разработке и внедрении нового онлайнового параллельного ЕМ-алгоритма для
модели АРТМ в библиотеку BigARTM с целью его использования в дальнейшем
моделировании.
Во втором блоке показано применения АРТМ в решении пробле-
мы извлечения тем, связанных с этно-социальным дискурсом из большой текстовой
коллекции (постов блогов).
Входными данными,
помимо самой текстовой коллек-
ции, является словарь предметных терминов (этнонимов). Для получения хорошей
тематической модели множество всех тем делится на два подмножества: предметных
(или этнических) и фоновых. Разработан новый регуляризатор частичного обучения,
работающий со словарём этнонимов и информацией о разбиении тем, а также регу-
ляризатор учёта этнонимов в виде отдельной модальности.
Построена комбинация
регуляризаторов для получения более интерпретируемых, разреженных и разнооб-
разных тем. Для текстов, содержащих информацию дате и регионе публикации, по-
строены модели, учитывающие эти метаданные. АРТМ позволяет делать подобные
вещи довольно легко, без сложного вывода и разработки новых алгоритмов.
Для демонстрации результатов применения описанного подхода использовались
экспертные оценки качества тем.
Показано,
что слабо регуляризованный АРТМ и
LDA дают примерно одинаковые по качеству модели, в то время как модель АРТМ
с правильно подобранным набором регуляризаторов даёт более хороший резуль-
тат [13].
Работа имеет следующую структуру.
В разделе 2 представлены базовая модель
PLSA, её байесовская модификация LDA и общие понятия подхода АРТМ. Раздел 3
посвящён описанию регуляризаторов, использованных в этой работе и комментиро-
ванию эффектов их воздействия на итоговую модель. В разделе 4 рассказывается о
библиотеке тематического моделирования BigARTM,
реализованных в ней вариан-
тах ЕМ-алгоритмов и новом алгоритме DetAsync. Раздел 5 описывает проведённые
эксперименты: в первой его части показано тестирование нового алгоритма, вторая
же часть посвящена списку различных моделей, которые были обучены, а также ре-
зультатам оценивания качества полученных ими тем. В разделе 6 описаны основные
результаты, полученные при выполнении данной работы и выносимые на защиту.
5
2
Тематическое моделирование
Пусть 𝐷 обозначает конечное множество (коллекцию) документов (текстов) и
пусть 𝑊 — конечное множество (словарь) всех терминов,
из которых состоят эти
документы.
Под термином подразумевается либо слово,
либо целая фраза.
В со-
ответствии с гипотезой «мешка слов» каждый документ 𝑑 from 𝐷 представляется
в виде подмножества словаря 𝑊 ,
где каждому слову 𝑤 ставится в соответствие
число 𝑛
𝑑𝑤
раз,
которое он встретился в документе 𝑑.
Предположим,
что появле-
ния каждого термина в каждом документе связано с некоторой латентной темой
из конечного множества тем 𝑇 . Текстовая коллекция представляется в виде набора
троек (𝑑
𝑖
, 𝑤
𝑖
, 𝑡
𝑖
),
𝑖 = 1, . . . , 𝑛, выбранных независимо из дискретного распределения
𝑝(𝑑, 𝑤, 𝑡) над конечным вероятностным пространством 𝐷 × 𝑊 × 𝑇 .
Термины 𝑤
𝑖
и
документы 𝑑
𝑖
— это наблюдаемые переменные, а темы 𝑡
𝑖
— скрытые.
Вероятностная тематическая модель описывает вероятности 𝑝(𝑤 | 𝑑) появления
терминов в документах как смеси распределений слов в темах 𝜑
𝑤𝑡
= 𝑝(𝑤 | 𝑡) и тем в
документах 𝜃
𝑡𝑑
= 𝑝(𝑡 | 𝑑):
𝑝(𝑤 | 𝑑) =
∑︁
𝑡∈𝑇
𝑝(𝑤 | 𝑡) 𝑝(𝑡 | 𝑑) =
∑︁
𝑡∈𝑇
𝜑
𝑤𝑡
𝜃
𝑡𝑑
.
(1)
Эта смесь напрямую соответствует генеративному процессу, в процессе которого
модель порождает документы 𝑑:
для каждой позиции слова 𝑖 происходит генера-
ция индекса темы 𝑡
𝑖
из распределения 𝑝(𝑡 | 𝑑),
после чего сэмплируется слово 𝑤
𝑖
из
распределения 𝑝(𝑤 | 𝑡
𝑖
).
Параметры вероятностной тематической модели часто представляются в виде
матриц Φ =
(︀
𝜑
𝑤𝑡
)︀
𝑊×𝑇
и Θ =
(︀
𝜃
𝑡𝑑
)︀
𝑇×𝐷
с неотрицательными и нормированными столб-
цами 𝜑
𝑡
и 𝜃
𝑑
, представляющими собой мультиномиальные распределения слов в те-
мах и тем в документах.
2.1
PLSA
В вероятностном латентном семантическом анализе (PLSA) [9], тематическая
модель (1) обучается путём максимизации логарифма правдоподобия с линейными
6
ограничениями неотрицательности и нормировки:
𝐿(Φ, Θ) =
∑︁
𝑑∈𝐷
∑︁
𝑤∈𝑑
𝑛
𝑑𝑤
ln
∑︁
𝑡∈𝑇
𝜑
𝑤𝑡
𝜃
𝑡𝑑
→ max
Φ,Θ
∑︁
𝑤∈𝑊
𝜑
𝑤𝑡
= 1,
𝜑
𝑤𝑡
≥ 0,
∑︁
𝑡∈𝑇
𝜃
𝑡𝑑
= 1,
𝜃
𝑡𝑑
≥ 0.
где 𝑛
𝑑𝑤
, как было отмечено ранее, — абсолютная частота слова 𝑤 в документе 𝑑.
Решение этой оптимизационной задачи удовлетворяет условиям Каруша-Куна-
Такера со вспомогательными переменными 𝑝
𝑡𝑑𝑤
, 𝑛
𝑤𝑡
, 𝑛
𝑡𝑑
:
𝑝
𝑡𝑑𝑤
= norm
𝑡∈𝑇
(︀
𝜑
𝑤𝑡
𝜃
𝑡𝑑
)︀
(2)
𝜑
𝑤𝑡
= norm
𝑤∈𝑊
(𝑛
𝑤𝑡
),
𝑛
𝑤𝑡
=
∑︁
𝑑∈𝐷
𝑛
𝑑𝑤
𝑝
𝑡𝑑𝑤
,
(3)
𝜃
𝑡𝑑
= norm
𝑡∈𝑇
(𝑛
𝑡𝑑
),
𝑛
𝑡𝑑
=
∑︁
𝑤∈𝑑
𝑛
𝑑𝑤
𝑝
𝑡𝑑𝑤
,
(4)
где оператор «norm» преобразует вещественный вектор (𝑥
𝑡
)
𝑡∈𝑇
в вектор (˜
𝑥
𝑡
)
𝑡∈𝑇
, пред-
ставляющий собой дискретное распределение:
˜
𝑥
𝑡
= norm
𝑡∈𝑇
(𝑥
𝑡
) =
max{𝑥
𝑡
, 0}
∑︀
𝑠∈𝑇
max{𝑥
𝑠
, 0}
.
Метод простых итерация для решения этой системы уравнений эквивалентен
ЕМ-алгоритму и обычно на практике используется именно он.
E-шаг (2) может рассматриваться как применение формулы Байеса для получе-
ния вероятностей 𝑝
𝑡𝑑𝑤
= 𝑝(𝑡 | 𝑑, 𝑤) для каждого термина 𝑤 и документа 𝑑.
M-шаг
(3)-(4) интерпретируется как частотная оценка условных вероятностей 𝜑
𝑤𝑡
и 𝜃
𝑡𝑑
.
Итеративный процесс обычно начинается со случайных начальных приближений Φ
и Θ.
2.2
LDA
Модель латентного размещения Дирихле (LDA) [3, 6] вводит априорные распре-
деления Дирихле для векторов вероятностей слов в темах 𝜑
𝑡
∼ Dir(𝛽) и для векторов
вероятностей тем в документах 𝜃
𝑑
∼ Dir(𝛼) с векторами параметров 𝛽 = (𝛽
𝑤
)
𝑤∈𝑊
и
𝛼 = (𝛼
𝑡
)
𝑡∈𝑇
соответственно.
Вывод в LDA обычно производится либо с помощью вариационного приближе-
ния, либо с помощью сэмплирования Гиббса. Обычно используется свёрнутая схема
7
Гиббса,
где тема 𝑡
𝑖
для каждой позиции слова (𝑑
𝑖
, 𝑤
𝑖
) итеративно сэмплируется из
распределения 𝑝(𝑡 | 𝑑, 𝑤), такого же, как в PLSA, но со со сглаженными байесовскими
оценками условных переменных:
𝜑
𝑤𝑡
= norm
𝑤∈𝑊
(𝑛
𝑤𝑡
+ 𝛽
𝑤
),
𝜃
𝑡𝑑
= norm
𝑡∈𝑇
(𝑛
𝑡𝑑
+ 𝛼
𝑡
),
где 𝑛
𝑤𝑡
— число раз, которое термин 𝑤 был сгенерирован из темы 𝑡 и 𝑛
𝑡𝑑
это число
раз,
которое термины из документы 𝑑 были сгенерированы из темы 𝑡,
исключая
текущую тройку (𝑑
𝑖
, 𝑤
𝑖
, 𝑡
𝑖
).
В последние годы было опубликовано много работ с различными расширения-
ми LDA. Для описываемой здесь задачи извлечения пользовательской информации
по некоторой специфичной тематике (здесь — связанной с этничностями) наибо-
лее релевантными являются модель Topic-in-Set
knowledge и её расширение [2,
1],
где слова, связаны с «𝑧-метками» (𝑧-метка описывает тему, к которой должно быть
отнесено слово),
а также модель Interval
Semi-Supervised LDA (ISLDA) [4,
15],
где
выделенным темам присваиваются заданные слова, и сэмплирование распределений
проецируется на это множество.
2.3
ARTM
Тематическое моделирование может быть рассмотрено как специальный случай
матричного разложения, где задача состоит в том, чтобы найти низкоранговую ап-
проксимацию ΦΘ данной разреженной матрицы счётчиков терминов-документов.
Следует отметить, что произведение ΦΘ определено с точностью до невырожденно-
го линейного преобразования:
ΦΘ = (Φ𝑆)(𝑆
−1
Θ).
Таким образом,
задача является
некорректно поставленной и имеет бесконечное множество решений. Прошлые экс-
перименты на модельных [23]
и реальных [4]
показали,
что ни PLSA,
ни LDA не
удаётся достигнуть устойчивого решения.
Для увеличения стабильности обучения
следует добавить дополнительные оптимизационные ограничения, обычно называе-
мые регуляризаторами [21].
В аддитивной регуляризации тематических моделей (АРТМ) [25] модель обуча-
ется путём максимизации линейной комбинации логарифма правдоподобия 𝐿(Φ, Θ)
и 𝑟 регуляризаторов 𝑅
𝑖
(Φ, Θ),
𝑖 = 1, . . . , 𝑟 с коэффициентами регуляризации 𝜏
𝑖
:
𝑅(Φ, Θ) =
𝑟
∑︁
𝑖=1
𝜏
𝑖
𝑅
𝑖
(Φ, Θ),
𝐿(Φ, Θ) + 𝑅(Φ, Θ)
→ max
Φ,Θ
.
8
Условия Каруша-Куна-Такера для этой нелинейной оптимизационной задачи да-
ют (с учётом некоторых технических ограничений) необходимые условия локального
максимума как решения следующей системы уравнений [23]:
𝑝
𝑡𝑑𝑤
= norm
𝑡∈𝑇
(︀
𝜑
𝑤𝑡
𝜃
𝑡𝑑
)︀
;
(5)
𝜑
𝑤𝑡
= norm
𝑤∈𝑊
(︂
𝑛
𝑤𝑡
+ 𝜑
𝑤𝑡
𝜕𝑅
𝜕𝜑
𝑤𝑡
)︂
;
𝑛
𝑤𝑡
=
∑︁
𝑑∈𝐷
𝑛
𝑑𝑤
𝑝
𝑡𝑑𝑤
;
(6)
𝜃
𝑡𝑑
= norm
𝑡∈𝑇
(︂
𝑛
𝑡𝑑
+ 𝜃
𝑡𝑑
𝜕𝑅
𝜕𝜃
𝑡𝑑
)︂
;
𝑛
𝑡𝑑
=
∑︁
𝑤∈𝑑
𝑛
𝑑𝑤
𝑝
𝑡𝑑𝑤
.
(7)
Как и в случае PLSA, для решения этой системы может быть использован ЕМ-
алгоритм. Преимущество АРТМ заключается в том, что каждый аддитивный регу-
ляризатор превращается в простую модификацию М-шага.
Многие модели,
разра-
ботанные прежде в рамках байесовского подхода,
могут быть несложно интерпре-
тированы,
обучены и скомбинированы в рамках теории АРТМ [22,
23].
Например,
PLSA не использует никакой регуляризации, 𝑅 = 0, а LDA с априорными распреде-
лениями Дирихле 𝜑
𝑡
∼ Dir(𝛽) и 𝜃
𝑑
∼ Dir(𝛼) and оценками максимума апостериорных
вероятностей Φ, Θ соответствует модели со сглаживающим регуляризатором,
кото-
рый интепретируется как минимизатор KL-дивергенций между столбцами Φ, Θ и
заданными распределениями 𝛽, 𝛼 соответственно.
3
Аддитивная регуляризация
3.1
Общий подход
В этом разделе рассматривается задача разведочного поиска всех этнических в
большой коллекции постов блогов. Пусть дан набор этнонимов 𝑄 ⊂ 𝑊 , который мо-
жет быть слишком большим для традиционных поисковых систем. Для извлечения
этнических тем используется тематическая модель частичного обучения с заданной
априорной информацией. Похожая техника использовалась ранее в задаче кластери-
зации новостей [11], поиска тем, связанных со здоровьем, в социальных медиа [16] и
задаче поиска этно-релевантных тем в постах блогов [4, 15]. Во всех этих исследова-
ниях каждой теме задавался предопределённый набор ключевых слов, часто очень
маленький, т.е. категория новости или этничность. Это означает, что информация о
числе тем и их примерном содержимом известна заранее.
Модель interval semi-supervised LDA (ISLDA) позволяет присвоить каждой этнич-
9
ности больше, чем одну тему, однако, довольно сложно определить, сколько реально
тем соответствует каждой из этничностей. И если исследователь не задаст ключевые
слова для каждой из тем, обучение модели невозможно.
Например, в [4, 15], где цель исследования была схожа с описываемой в данной
работе, ISLDA использовался для поиска этнического контента, но, поскольку этно-
нимы были соотнесены с различными темами, появление мульти-этничных тем было
невозможно.
Предлагаемое решение описанной проблемы состоит в задании общей для всех
этнических тем лексической априорной информации 𝑄.
Модели предоставляется
самостоятельно определять распределение этничностей и их комбинаций по темам.
Используется аддитивная комбинация регуляризаторов сглаживания,
разрежи-
вания и декорреляции тем для того, чтобы сделать темы более интерпретируемыми,
разреженными и разнообразными [23].
АРТМ позволяет делать всё это легко,
без
разработки новых алгоритмов и сложного вывода.
Более того,
все эти регуляриза-
торы уже реализованы в библиотеке с открытым кодом BigARTM
1
.
Прежде всего необходимо произвести разбиение всех тем 𝑇 на два подмножества:
предметные темы 𝑆 и фоновые темы 𝑆. Регуляризаторы воздействуют на 𝐵 и 𝑆 по-
разному. Относительные размеры 𝑆 и 𝐵 могут варьироваться. Идея использования
фоновых тем состоит в сборе всех неинтересных слов, как это было продемонстри-
ровано в [5].
Отличие от описанной работы состоит том,
что используется не одна,
а много фоновых тем,
затем,
чтобы как можно лучше очистить предметные темы,
сделать их более этно-релевантными и повысить общее качество модели
3.2
Сглаживание и разреживание
Наиболее естественный способ внедрения априорной информации в модель состо-
ит в использовании регуляризаторов сглаживания и разреживания с равномерным
распределением 𝛽 следующего вида:
𝛽
𝑤
=
1
|𝑄|
[𝑤 ∈ 𝑄].
Основным регуляризатором является LDA-подобный регуляризатор сглажива-
ния,
поощряющий появление этнонимов 𝑤 ∈ 𝑄 в этнических темах 𝑆.
Также по-
лезен регуляризатор разреживания с противоположным знаком,
действующий на
1
http://bigartm.org
10
фоновых темах, т.е. предотвращающий появление этнонимов в них:
𝑅(Φ) = 𝜏
1
∑︁
𝑡∈𝑆
∑︁
𝑤∈𝑄
ln 𝜑
𝑤𝑡
− 𝜏
2
∑︁
𝑡∈𝐵
∑︁
𝑤∈𝑄
ln 𝜑
𝑤𝑡
.
В задаче разведочного поиска предполагается,
что доля релевантного содержи-
мого в коллекции незначительна.
В описываемой задаче ситуация именно такова,
весь этно-социальный дискурс содержится в не более чем одном проценте от об-
щего объёма коллекции.
Задача состоит в том,
чтобы хорошо описать тематиче-
скую структуру релевантного контента большим числом небольших,
но качествен-
ных тем 𝑆. В то же время, тематическая модель должна описывать гораздо больший
по объёму контент меньшим числом фоновых тем 𝐵.
Эти требования формализуются в терминах сглаживающего регуляризатора мат-
рицы Θ,
работающего только с фоновыми темами,
и регуляризатора,
равномерно
разреживающего этническими темы в этой же матрице:
𝑅(Θ) = 𝜏
3
∑︁
𝑑∈𝐷
∑︁
𝑡∈𝐵
ln 𝜃
𝑡𝑑
− 𝜏
4
∑︁
𝑑∈𝐷
∑︁
𝑡∈𝑆
ln 𝜃
𝑡𝑑
.
Идея этого регуляризатора Θ заключается в сглаживании фоновых тем для того,
чтобы они оттянули на себя как можно больше нерелевантных слов, и разреживании
этнических тем в надежде на то, что они станут более непохожими друг на друга.
3.3
Декорреляция тем
Повышение различности распределений слов в темах приводит к росту интер-
претируемости тем [20].
Для того, чтобы сделать темы настолько различными, насколько это возможно,
используется регуляризатор максимизации ковариаций между столбцами 𝜑
𝑡
для всех
этнических тем 𝑡:
𝑅(Φ) = −𝜏
5
∑︁
𝑡∈𝑆
∑︁
𝑠∈𝑆∖𝑡
∑︁
𝑤∈𝑊
𝜑
𝑤𝑡
𝜑
𝑤𝑠
+ 𝜏
6
∑︁
𝑡∈𝐵
∑︁
𝑤∈𝑊
ln 𝜑
𝑤𝑡
.
Декоррелятор также стимулирует разреженность и имеет тенденцию к группи-
ровке общих слов в отдельные темы [20].
Для того,
чтобы эти темы образовались
среди фоновых, в не предметных, применяется ещё один аддитивный регуляризатор,
равномерно сглаживающий все фоновые темы 𝐵 .
11
3.4
Модальность этнонимов
В качестве альтернативного метода регуляризации на основе априорной лексиче-
ской информации предлагается использование этнонимов в качестве отдельной мо-
дальности. В общем случае, модальность — это тип терминов в документе. Примера-
ми модальностей могут служить именованные сущности,
теги,
иностранные слова,
𝑛-граммы,
авторы,
категории,
метки времени,
ссылки и т.п.
Каждая модальность
имеет свой собственный словарь и свою матрицу Φ,
нормализуемую отдельно от
матриц Φ других модальностей. Мультимодальное расширение АРТМ было предло-
жено в [24] и уже реализовано в BigARTM.
Используются две модальности: обычные слова и этнонимы. Модальность этно-
нимов определяется словарём 𝑄 и матрицей
˜
Φ размера |𝑄| × |𝑇 |. В АРТМ логарифм
правдоподобия модальности рассматривается как регуляризатор:
𝑅(
˜
Φ, Θ) = 𝜏
7
∑︁
𝑑∈𝐷
∑︁
𝑤∈𝑄
𝑛
𝑑𝑤
ln
∑︁
𝑡∈𝑇
˜
𝜑
𝑤𝑡
𝜃
𝑡𝑑
,
где коэффициент регуляризации 𝜏
7
по сути является множителем счётчиков «слово-
документ» 𝑛
𝑑𝑤
второй модальности.
Для того, чтобы сделать этно-релевентные темы более различными с точки зре-
ния отражаемых в них подмножеств этнонимов,
задействован дополнительный ре-
гуляризатор декорреляции тем, действующий на модальности этнонимов:
𝑅(
˜
Φ) = −𝜏
8
∑︁
𝑡∈𝑆
∑︁
𝑠∈𝑆∖𝑡
∑︁
𝑤∈𝑄
˜
𝜑
𝑤𝑡
˜
𝜑
𝑤𝑠
.
Следует отметить,
что декорреляция предметных тем 𝑆 вводится отдельно для
слов матрицы Φ и модальности этнонимов с матрицей
˜
Φ.
3.5
Модальности меток времени и геотегов
Наличие информации о привязке текстов к меткам времени и геотегам может
быть также использовано при обучении моделей с помощью описанного выше меха-
низма мультимодальности. Это приносит двойную пользу: во-первых, дополнитель-
ная информация может быть использована алгоритмом для построения более каче-
ственной модели; во-вторых, в результате моделирования пользователь получает не
только информацию о составе тем, но и том, как они изменяются в пространстве и во
времени. Последнее свойство особенно ценно в рамках проводимого исследования.
12
Алгоритм 4.1. ProcessDocument(𝑑, Φ)
Входные данные: документ 𝑑 ∈ 𝐷, марица Φ = (𝜑
𝑤𝑡
);
Выходные данные: матрица (˜
𝑛
𝑤𝑡
), вектор (𝜃
𝑡𝑑
) для документа 𝑑;
1
инициализировать 𝜃
𝑡𝑑
:=
1
|𝑇 |
для всех 𝑡 ∈ 𝑇 ;
2
повторять
3
𝑝
𝑡𝑑𝑤
:= norm
𝑡∈𝑇
(︀
𝜑
𝑤𝑡
𝜃
𝑡𝑑
)︀
для всех 𝑤 ∈ 𝑑 и 𝑡 ∈ 𝑇 ;
4
𝜃
𝑡𝑑
:= norm
𝑡∈𝑇
(︀
∑︀
𝑤∈𝑑
𝑛
𝑑𝑤
𝑝
𝑡𝑑𝑤
+ 𝜃
𝑡𝑑
𝜕𝑅
𝜕𝜃
𝑡𝑑
)︀
для всех 𝑡 ∈ 𝑇 ;
5
до тех пор, пока 𝜃
𝑑
не сойдётся;
6
˜
𝑛
𝑤𝑡
:= 𝑛
𝑑𝑤
𝑝
𝑡𝑑𝑤
для всех 𝑤 ∈ 𝑑 и 𝑡 ∈ 𝑇 ;
4
Библиотека BigARTM
Данный раздел будет посвящён описанию библиотеки тематического моделиро-
вания больших текстовых коллекций BigARTM. Будет подробно описан существую-
щий вариант ЕМ-алгоритма, лежащего в её основе, а также предложен новый, более
совершенный алгоритм.
4.1
Обзор
BigARTM — это библиотека с открытым программным кодом для построения ре-
гуляризованных мультимодальных тематических моделей больших текстовых кол-
лекций.
Полностью поддерживая теорию АРТМ,
библиотека предоставляет пред-
определённый набор регуляризаторов,
а также метрик качества моделирования,
оставляя пользователю возможность добавлять собственные.
Написана на C++11,
имеет пользовательский API на Python. В BigARTM реализован параллельный он-
лайновый асинхронный ЕМ-алгоритм, обладающий высокой производительностью в
рамках одного вычислительного узле. Данные для библиотеки сохраняются на диске
или в памяти в виде специальных пакетов (батчей),
которые можно генерировать
с помощью встроенного парсера. Каждый батч содержит некоторое количество до-
кументов, и является атомарной порцией данных для обработки одним потоком.
4.2
Реализации ЕМ-алгоритма
Оффлайновый алгоритм
Базовым вариантом ЕМ-алгоритма для модели АРТМ
является оффлайновый алгоритм (4.2). Он основывается на функции ProcessDocument
(4.1), которая соответствует уравнениям 5, 7 решения задачи АРТМ. ProcessDocument
13
Алгоритм 4.2. Offline ARTM
Входные данные: коллекция 𝐷;
Выходные данные: матрица Φ = (𝜑
𝑤𝑡
);
1
инициализировать (𝜑
𝑤𝑡
);
2
создать батчи 𝐷 := 𝐷
1
⊔ 𝐷
2
⊔ · · · ⊔ 𝐷
𝐵
;
3
повторять
4
(𝑛
𝑤𝑡
) :=
∑︁
𝑏=1,...,𝐵
∑︁
𝑑∈𝐷
𝑏
ProcessDocument(𝑑, Φ);
5
(𝜑
𝑤𝑡
) := norm
𝑤∈𝑊
(𝑛
𝑤𝑡
+ 𝜑
𝑤𝑡
𝜕𝑅
𝜕𝜑
𝑤𝑡
);
6
до тех пор, пока (𝜑
𝑤𝑡
) не сойдётся;
требует на вход фиксированную матрицу Φ и вектор 𝑛
𝑑𝑤
частот слов для заданного
документа 𝑑 ∈ 𝐷. Выходными данными являются распределение на темах данного
документа 𝜃
𝑡𝑑
и матрица ˆ
𝑛
𝑤𝑡
размера |𝑑| × |𝑇 |, где |𝑑| обозначает число уникальных
слов в документе 𝑑.
ProcessDocument может также быть полезной как отдельная операция, позволя-
ющая получать векторы 𝜃
𝑡𝑑
для новых документов,
но в оффлайновом алгоритме
она используется в качестве базового блока обработки в ЕМ-алгоритме,
и предна-
значен для вычисления обновлений матрицы Φ.
Offline ARTM проходит по всей коллекции текстов, вызывая функцию ProcessDocument
для каждого документа 𝑑 ∈ 𝐷, а затем агрегирует результирующие матрицы ˆ
𝑛
𝑤𝑡
в
итоговую матрицу 𝑛
𝑤𝑡
размера |𝑊 | × |𝑇 |.
После каждого прохода по коллекции матрицы Φ обновляется в соответствии с
уравнением 6.
На шаге 2 производится разделение коллекции 𝐷 на батчи 𝐷
𝑏
2
.
В целях повы-
шения производительности внешний цикл по батчам 𝑏 = 1, . . . , 𝐵 распараллелива-
ется по нескольким потокам, и внутри каждого батча внутренний цикл по докумен-
там 𝑑 ∈ 𝐷
𝑏
выполняется в рамках одного потока.
Стоит обратить внимание на то, что значения 𝜃
𝑡𝑑
появляются лишь внутри функ-
ции ProcessDocument.
Это приводит к эффективному использованию памяти,
по-
скольку реализация никогда не хранит целую матрицу Θ. Вместо этого значения 𝜃
𝑡𝑑
пересчитываются с нуля на каждом проходе по коллекции.
На рис.
1 можно увидеть диаграмму Гантта для Offline ARTM.
В этой и по-
2
Этот шаг не является обязательным для самого оффлайнового алгоритма,
это часть работы
библиотеки
14
0s
4s
8s
12s
16s
20s
24s
28s
32s
36s
Main
Proc-1
Proc-2
Proc-3
Proc-4
Proc-5
Proc-6
Batch processing
Norm
Рис. 1: Диаграмма Гантта Offline ARTM (алгоритм 4.2)
следующей диаграммах показана одна итерация ЕМ-алгоритма на данных NYTimes
3
(|𝐷| = 300K, |𝑊 | = 102K) в модели с |𝑇 | = 16 темами. Прямоугольники ProcessBatch
соответствуют времени, потраченному на обработку одного батча. Финальный пря-
моугольник Norm, выполняющийся в главном потоке, соответствует времени, затра-
ченному на шаг 4 в алг.
4.2,
где счётчики 𝑛
𝑤𝑡
нормируются для создания новой
матрицы Φ.
Синхронный онлайновый алгоритм
Online ARTM (алгоритм 4.3) является обо-
щением онлайнового вариационного ЕМ-алгоритма, предложенного в [8] для модели
LDA. Онлайновый алгоритм улучшает сходимости оффлайнового за счёт пересчёта
матрицы Φ,
производимого не в конце обработки всей коллекции,
а в конце обра-
ботки некоторой порции батчей.
Для упрощения обозначений введём следующую
тривиальную функцию:
ProcessBatches({𝐷
𝑏
}, Φ) =
∑︁
𝐷
𝑏
∑︁
𝑑∈𝐷
𝑏
ProcessDocument(𝑑, Φ).
Она агрегирует результаты ProcessDocument для заданного множества батчей
при фиксированной матрице Φ.
В онлайновом алгоритме разбиение коллекции 𝐷 := 𝐷
1
⊔ 𝐷
2
⊔ · · · ⊔ 𝐷
𝐵
на бат-
чи играет гораздо более важную роль,
чем в алгоритме оффлайновом,
поскольку
различные разбиения будут приводить к различным результатам.
На шаге 6 новые значения 𝑛
𝑖+1
𝑤𝑡
вычисляются как выпуклая комбинация старых
3
https://archive.ics.uci.edu/ml/datasets/Bag+of+Words
15
Алгоритм 4.3. Online ARTM
Входные данные: коллекция 𝐷, гиперпараметры 𝜂, 𝜏
0
, 𝜅;
Выходные данные: matrix Φ = (𝜑
𝑤𝑡
);
1
создать батчи 𝐷 := 𝐷
1
⊔ 𝐷
2
⊔ · · · ⊔ 𝐷
𝐵
;
2
инициализировать (𝜑
0
𝑤𝑡
);
3
цикл 𝑖 = 1, . . . , ⌊𝐵/𝜂⌋ выполнять
4
(ˆ
𝑛
𝑖
𝑤𝑡
) := ProcessBatches({𝐷
𝜂(𝑖−1)+1
, . . . , 𝐷
𝜂𝑖
}, Φ
𝑖−1
);
5
𝜌
𝑖
:= (𝜏
0
+ 𝑖)
−𝜅
;
6
(𝑛
𝑖
𝑤𝑡
) := (1 − 𝜌
𝑖
) · (𝑛
𝑖−1
𝑤𝑡
) + 𝜌
𝑖
· (ˆ
𝑛
𝑖
𝑤𝑡
);
7
(𝜑
𝑖
𝑤𝑡
) := norm
𝑤∈𝑊
(𝑛
𝑖
𝑤𝑡
+ 𝜑
𝑖−1
𝑤𝑡
𝜕𝑅
𝜕𝜑
𝑤𝑡
);
0 s.
4 s.
8 s.
12 s.
16 s.
20 s.
24 s.
28 s.
32 s.
36 s.
Main
Proc-1
Proc-2
Proc-3
Proc-4
Proc-5
Proc-6
Odd batch
Even batch
Norm
Merge
Рис. 2: Диаграмма Гантта Online ARTM (алг. 4.3)
значений 𝑛
𝑖
𝑤𝑡
и значения ˆ
𝑛
𝑖
𝑤𝑡
, полученного по только что обработанным батчам. Ста-
рые счётчики 𝑛
𝑖
𝑤𝑡
умножаются на множитель (1−𝜌
𝑖
), зависящий от номера итерации.
Общепринятая стратегия состоит в использовании 𝜌
𝑖
= (𝜏
0
+ 𝑖)
−𝜅
б где стандартные
значения 𝜏
0
лежат в диапазоне от 64 до 1024, а 𝜅 — от 0.5 до 0.7.
Так же, как и в оффлайновом алгоритме, внешний цикл по батчам 𝐷
𝜂(𝑖−1)+1
, . . . , 𝐷
𝜂𝑖
выполняется параллельно в нескольких потоках. Проблема такого подхода в том, что
во время шагов 5-7 алг. Online ARTM рабочие потоки простаивают.
Потоки не могут начать обработку следующей порции батчей,
поскольку но-
вая версия матрицы Φ ещё не готова.
Результатом этого является неэффективное
использование процессорных ресурсов,
обычная диаграмма Гантта для алгоритма
Online ARTM показана на рис. 2.
Прямоугольники Even batch и Odd batch оба соответствуют шагу 4 и обозна-
чают версию матрицы Φ
𝑖
(чётное 𝑖 или нечётное 𝑖). Прямоугольник Merge соответ-
ствует времени,
затраченному на слияние 𝑛
𝑤𝑡
с ˆ
𝑛
𝑤𝑡
.
Norm,
как и выше,
обозначает
16
0s
4s
8s
12s
16s
20s
24s
28s
32s
36s
Merger
Proc-1
Proc-2
Proc-3
Proc-4
Proc-5
Proc-6
Odd batch
Even batch
Norm
Merge matrix
Merge increments
Рис. 3: Диаграмма Гантта Async ARTM — нормальная ситуация
0s
4s
8s
12s
16s
20s
24s
28s
32s
36s
Merger
Proc-1
Proc-2
Proc-3
Proc-4
Proc-5
Proc-6
Odd batch
Even batch
Norm
Merge matrix
Merge increments
Рис. 4: Диаграмма Гантта Async ARTM — проблемы производительности
время, затраченное на нормализацию счётчиков 𝑛
𝑤𝑡
для получения новой матрицы
Φ, которая будет использована во время следующей итерации.
Алгоритм Async
Алгоритм Async ARTM [24] был призван решить проблемы обыч-
ного онлайнового синхронного алгоритма, описанные выше. Идея заключается в ор-
ганизации асинхронной работы Offline ARTM и сохранении результирующих матриц
ˆ
𝑛
𝑤𝑡
в очередь.
Затем,
в тот момент,
когда количество матриц в очереди достигает
заданного 𝜂, алгоритм производит шаги 5-7 алгоритма Online ARTM (алг. 4.3).
Из соображений производительности слияние счётчиков ˆ
𝑛
𝑤𝑡
производится в фо-
новом режиме выделенным потоком слияния Merger.
Как было сказано выше, данный алгоритм показал высокую производительность
и масштабируемость в сравнении с аналогичными инструментами [24],
но у него
17
0s
4s
8s
12s
16s
20s
24s
28s
32s
36s
Main
Proc-1
Proc-2
Proc-3
Proc-4
Proc-5
Proc-6
Odd batch
Even batch
Norm
Merge
Рис. 5: Диаграмма Гантта для DetAsync ARTM (алг. 4.4)
имеется ряд недостатков.
Первая проблема состоит в том, что алгоритм не детерминирует порядка слияния
счётчиков ˆ
𝑛
𝑤𝑡
.
Этот порядок обычно отличается от порядка обработки батчей и
меняется от запуска к запуску. Это приводит к тому, что и результирующая матрица
Φ от запуска к запуску может быть различной.
Другая проблема, связанная с Async ARTM, состоит в том, что хранение счётчи-
ков ˆ
𝑛
𝑤𝑡
в очереди может существенно увеличить потребление памяти и привести к
тому, что поток Merger станет узким местом алгоритма с точки зрения производи-
тельности.
При правильном подборе параметров эффективность подобного алгоритма будет
высокой,
что можно видеть на диаграмме 3.
Однако несложно подобрать и такой
набор параметров (например,
слишком малый размер батча или маленькое число
внутренних итераций в ProcessDocument),
который приведёт к перегрузке потока
слияния.
В такой ситуации диаграмма Гантта примет вид,
показанный на рис.
4:
большинство потоков простаивают, поскольку в очереди нет места для новых счёт-
чиков 𝑛
𝑤𝑡
.
В следующем разделе данные проблемы будут решены с помощью предлагаемого
алгоритма DetAsync, который является полностью детерминированным и позволяет
производить обучение в онлайновом режиме с высокой производительностью без
необходимости тонкой настройки параметров пользователем.
4.3
Онлайновый алгоритм DetAsync
Описание
DetAsync ARTM [10]
(алг.
4.4) основан на двух новых функциях,
Await
и AsyncProcessBatches.
18
Алгоритм 4.4. DetAsync ARTM
Входные данные: коллекция 𝐷, параметры 𝜂, 𝜏
0
, 𝜅;
Выходные данные: matrix Φ = (𝜑
𝑤𝑡
);
1
создать батчи 𝐷 := 𝐷
1
⊔ 𝐷
2
⊔ · · · ⊔ 𝐷
𝐵
;
2
инициализировать (𝜑
0
𝑤𝑡
);
3
𝐹
1
:= AsyncProcessBatches({𝐷
1
, . . . , 𝐷
𝜂
}, Φ
0
);
4
цикл 𝑖 = 1, . . . , ⌊𝐵/𝜂⌋ выполнять
5
если 𝑖 ̸= ⌊𝐵/𝜂⌋ тогда
6
𝐹
𝑖+1
:= AsyncProcessBatches({𝐷
𝜂𝑖+1
, . . . , 𝐷
𝜂𝑖+𝜂
}, Φ
𝑖−1
);
7
(ˆ
𝑛
𝑖
𝑤𝑡
) := Await(𝐹
𝑖
);
8
𝜌
𝑖
:= (𝜏
0
+ 𝑖)
−𝜅
;
9
(𝑛
𝑖
𝑤𝑡
) := (1 − 𝜌
𝑖
) · (𝑛
𝑖−1
𝑤𝑡
) + 𝜌
𝑖
· (ˆ
𝑛
𝑖
𝑤𝑡
);
10
(𝜑
𝑖
𝑤𝑡
) := norm
𝑤∈𝑊
(𝑛
𝑖
𝑤𝑡
+ 𝜑
𝑖−1
𝑤𝑡
𝜕𝑅
𝜕𝜑
𝑤𝑡
);
Вторая во всём эквивалентна описанной ранее ProcessBatches, за исключением
того,
что она берёт задачу на асинхронную обработку и немедленно возвращает
управление в вызвавший поток.
Её результатом является future-объект (например,
std::future из стандарта C++11), который может быть затем передан в вызов Await
для получения вычисленного результата, в нашем случае счётчиков ˆ
𝑛
𝑤𝑡
.
Между вызовами AsyncProcessBatches and Await алгоритм может производить
различную вспомогательную работу,
пока рабочие потоки в фоновом режиме про-
изводят вычисление матрицы ˆ
𝑛
𝑤𝑡
.
Для вычисления ˆ
𝑛
𝑖+1
𝑤𝑡
DetAsync ARTM использует матрицу Φ
𝑖−1
с предыдущего
обновления.
Это добавляет некоторое запаздывание между моментом вычисления
очередной версии матрицы Φ и моментом её использования,
что даёт алгоритму в
результате дополнительную гибкость в распределении нагрузки на рабочие потоки.
Шаги 3 и 5 — это технический трюк, направленный на реализацию описанной идеи
с запаздыванием.
Добавление запаздывания может негативно сказаться на сходимости алгоритма
в сравнении с Online Async. Например, в AsyncProcessBatches начальная матрица
Φ
0
используется дважды,
в то время как последние две матрицы Φ
⌊𝐵/𝜂⌋−1
и Φ
⌊𝐵/𝜂⌋
вообще не будут использованы.
С другой стороны, асинхронный алгоритм позволет добиться более высокой сте-
пени загрузки ядер, что наглядно продемонстрировано на диаграмме 5.
В этом состоит некоторый компромисс между сходимостью и загрузкой CPU, и
он будет рассмотрен подробнее в разделе 5.1.
19
Processor threads:
ProcessBatch(
D
b
,

wt
)
D
b
ñ
wt
Merger thread:
Accumulate ñ
wt
Recalculate 

wt 
Queue
{D
b
}
Queue
{ñ
wt
}

wt
Sync()
D
b
Рис. 6: Схема компонентов BigARTM (Async)
MasterModel
Processor threads:
D
b
= LoadBatch(
b
)
ProcessBatch(
D
b
,

wt
)
Main thread:
Recalculate 

wt 
n
wt

wt
Transform({D
b
})
FitOffline({D
b
})
FitOnline({D
b
})
Рис. 7: Схема компонентов BigARTM (DetAsync)
Детали реализации
Существенной частью реализации является способ агрега-
ции матриц ˆ
𝑛
𝑤𝑡
со всех батчей при условии того, что они обрабатываются разными
потоками. Со сменой алгоритма в BigARTM этот способ изменился (рис. 6 и 7).
В старой архитектуре счётчики ˆ
𝑛
𝑤𝑡
сохранялись в очередь,
откуда агрегирова-
лись выделенным потоком Merger. В новой архитектуре этот поток ликвидирован, и
счётчики ˆ
𝑛
𝑤𝑡
пишутся напрямую в результирующую матрицу 𝑛
𝑤𝑡
асинхронно всеми
рабочими потоками.
Для синхронизации доступа на запись необходимо обеспечить
невозможность возникновения ситуации, в которой два потока одновременно произ-
водят запись в одну строку матрицы 𝑛
𝑤𝑡
. Это достигается с помощью спин-локов 𝑙
𝑤
,
по одному на каждое слово из словаря 𝑊 . В конце вызова ProcessDocument произ-
водится итерирование по всем 𝑤 ∈ 𝑑,
для каждого слова производится блокировка
соответствующего лока,
добавление ˆ
𝑛
𝑤𝑡
к 𝑛
𝑤𝑡
и разблокировка лока.
Этот подход
схож с тем,
что был предложен в [7],
где подобная система была организована в
распределённой среде.
В новой архитектуре также был ликвидирован выделенный поток загрузки дан-
ных DataLoader, который до этого загружал данные с диска в специализированную
очередь задач, откуда батчи уже выбирались для обработки рабочими потоками. Те-
перь процесс загрузки производится непосредственно самим потоком, что упростило
20
0
5
10
15
20
25
30
2,000
2,200
2,400
Time (min)
Perplexity
Offline
Online
Async
DetAsync
10
15
20
25
30
35
3,800
4,000
4,200
4,400
4,600
4,800
5,000
Time (min)
Perplexity
Offline
Online
Async
DetAsync
Рис. 8: График перплексии от времени работы для Pubmed (слева) и Wikipedia (справа), |𝑇 | = 100
topics
Таблица 1: Пиковое потребление памяти BigARTM, Гб
|𝑇 |
Offline
Online
DetAsync
Async (v0.6)
Pubmed
1000
5.17
4.68
8.18
13.4
Pubmed
100
1.86
1.62
2.17
3.71
Wiki
1000
1.74
2.44
3.93
7.9
Wiki
100
0.54
0.53
0.83
1.28
архитектуру без потери производительности.
5
Эксперименты
5.1
Оценивание реализации DetAsync
В данном разделе производится сравнение эффективности алгоритмов Offline
(алг. 4.2), Online (алг. 4.3), Async [24] и DetAsync (алг. 4.4).
Как уже было отмечено выше, алгоритм Async уже превосходит по скорости ана-
логи BigARTM [24]: в однопоточном режиме он почти в 10 раз быстрее Gensim [18]
и вдвое быстрее Vowpal Wabbit LDA (VW) [19]; в многопоточном режиме превосход-
ство ещё более выраженное.
В экспериментах этого раздела используются коллекция статей англоязычной
Википедии (Wikipedia) (|𝐷| = 3.7M статей, |𝑊 | = 100K слов в словаре) и коллекция
аннотаций Pubmed (|𝐷|
= 8.2M аннотаций,
|𝑊 |
= 141K слов в словаре).
Экспери-
менты производились на системе Intel
Xeon CPU E5-2650 v2 с 2 процессорами,
16
физических ядер в совокупности (32 с hyper-threading).
Рис.
8 показывает перплексию как функцию от времени,
потраченного описан-
ными выше алгоритмами на обучение.
21
Перплексия определяется как
P
(𝐷, 𝑝) = exp
(︂
−
1
𝑛
∑︁
𝑑∈𝐷
∑︁
𝑤∈𝑑
𝑛
𝑑𝑤
ln
∑︁
𝑡∈𝑇
𝜑
𝑤𝑡
𝜃
𝑡𝑑
)︂
,
(8)
где 𝑛 =
∑︀
𝑑
𝑛
𝑑
. Более низкое значение перплексии соответствует более хорошему ре-
зультату. Каждая точка на графике соответствует моменту завершения алгоритмом
очередного прохода по коллекции. Каждому алгоритму было выделено на работу 30
минут.
Таблица 1 демонстрирует пиковое потребление памяти каждый алгоритмом при
обучении моделей с |𝑇 |
= 1000 и |𝑇 |
= 100 темами на коллекциях Wikipedia и
Pubmed.
5.2
Эксперименты на коллекции LiveJournal
С социологической точки зрения, задача проекта заключается в поиске и монито-
ринге этно-релевантного дискурса в социальных сетях, в частности, в определении
степени популярности тем,
связанных с теми или иными этническими группами,
возможно,
в заданных регионах,
и выявлении зарождающихся негативных тенден-
ций,
могущих повлечь за собой возникновение конфликта на этнической почве.
В
данном разделе производится построение регуляризованных тематических моделей
коллекции постов самой популярной российской блог-платформы LiveJournal [13].
Данные и параметры Коллекция содержит примерно 1.58М лемматизирован-
ных постов, написанных топ-2000 блоггерами LiveJournal за годичный период с се-
редины 2013 до середины 2014. Полный словарь коллекции составил примерно 860К
слов, но после предобработки, во время которой были сохранены только слова, ко-
торые одновременно содержат только символы русского алфавита,
с не более,
чем
одним дефисом; имеют длину не менее 3 символов; встречаются во всей коллекции
как минимум 20 раз.
В процессе подбора числа тем были опробованы 100, 300 и 400 тем. В результате
работы экспертов по оцениванию качества моделирования было выявлено, что наи-
лучший результат достигается при |𝑇 | = 400, поэтому именно это число тем исполь-
зовалось во всех дальнейших экспериментах с этой коллекцией.
Это соответствует
более ранним экспериментам [4, 15].
Коллекция была разделена на батчи по 10000 документов в каждом. Все модели
АРТМ обучались с помощью онлайнового алгоритма с одним проходом по коллек-
22
ции и 25 проходами по каждому документу; обновления матрицы Φ производились
после каждого обработанного батча. Для регуляризатора частичного обучения был
подготовлен набор из нескольких сотен этнонимов — существительных, обозначаю-
щих различные этнические группы; 249 из этих слов встретились в коллекции.
Этнонимы выглядят лучшими кандидатами на роль средства улучшения каче-
ства извлечения тем,
связанных с этничностями и межэтничными отношениями.
Участниками таких отношений являются конкретные люди или группы людей. Нуж-
но отличать их от отношений международных,
в которых основную роль играют
государства,
их правительства или официальные представители,
а затрагиваемые
вопросы далеко не всегда касаются этничностей.
Межэтничные и международные
отношения тесно связаны и,
в некоторых ситуациях,
пересекаются,
однако,
инту-
итивно ясно,
что для мониторинга и предотвращения конфликтов на этнической
почве (связанных,
например,
с мигрантами) логичнее анализировать блогосферу,
чем новости официальных источников.
Предполагается, что в этнических темах будут превалировать этнонимы (турки),
в то время как прилагательные (турецкий) и названия стран (Турция) более связаны
с международными отношениями. В русском языке эти три категории, как правило,
представляют собой различные слова, что позволяет проще классифицировать темы
по рассматриваемым отношениям на международные и межэтничные.
Модели
В экспериментах с использованием BigARTM были обучены наборы те-
матических моделей.
Во всех моделях с гиперпараметрами коэффициенты регуля-
ризации подбирались вручную в ходе многократных запусков обучения. Во всех мо-
делях с регуляризацией темы были разделены на |𝑆| = 250 предметных и |𝐵| = 150
фоновых.
Далее приведён список различных моделей, которые были настроены и сравнены:
1.
plsa: базовая модель вероятностного латентного семантического анализа (PLSA)
без регуляризаторов;
2.
lda: базовая модель латентного размещения Дирихле (LDA), реализованная в
BigARTM как модель с регуляризаторами сглаживания Φ и Θ равномерными
распределениями 𝛼 и 𝛽 с гиперпараметрами 𝛼
0
= 𝛽
0
= 10
−4
;
3.
smooth:
модель АРТМ со сглаживанием и разреживанием по этнонимам,
с
коэффициентами регуляризации 𝜏
1
= 10
−5
and 𝜏
2
= 100;
кроме того,
в этом и
всех последующих экспериментах использовался описанный ранее регуляриза-
тор матрицы Θ с коэффициентами 𝜏
3
= 0.05 and 𝜏
4
= 1;
23
4.
decorrelated: модель АРТМ, обобщающая предыдущую путём добавления де-
корреляции с параметрами 𝜏
5
= 5 × 10
4
and 𝜏
6
= 10
−8
; коэффициент сглажива-
ния этнических тем 𝜏
1
= 10
−6
;
5.
restricted dictionary:
модель АРТМ,
обобщающая предыдущую путём до-
бавления декоррелируемой модальности этнонимов с коэффициентами 𝜏
7
=
100 and 𝜏
8
= 2 × 10
4
;
Другие коэффициенты приняли следующие значения:
𝜏
5
= 1.5 × 10
6
, 𝜏
6
= 10
−7
и 𝜏
1
= 1.1 × 10
−4
; в этой модели использовался словарь
из |𝑄| = 249 этнонимов;
6.
extended dictionary: модель АРТМ, идентичная предыдущей, в которой ис-
пользовался расширенный словарь: помимо этнонимов, в него были добавлены
прилагательные и названия стран для тех этничностей, для которых соответ-
ствующего этнонима в коллекции не нашлось;
7.
recursive: базовая модель PLSA, обученная на специальном подмножестве до-
кументов,
полученных из тем модели 5,
которые были сочтены этническими
экспертами: использовались все документы, которые в данных темах в матрице
Θ имели вероятность выше порога 10
−6
;
8.
keyword documents:
модель PLSA,
идентичная предыдущей,
но обученная
на подмножестве документов всей коллекции, содержащих хоть один этноним
из 𝑄.
Модели 7 и 8 были обучены для сравнения двух методов обогащения исходной
коллекции.
Модель 8 использовалась в качестве базовой при проверке предполо-
жения о том,
что циклическое использование тематических моделей может дать
лучший результат, чем извлечение текстов по ключевым словам.
Результаты В этом разделе обсуждаются количественные и качественные резуль-
таты обучения.
Сперва будет описана методология экспертного оценивания,
затем
будет проведено обсуждение полученных оценок.
Кроме того,
результаты оценива-
ния людьми будут сравнены со значениями tf-idf когерентности, предложенной ра-
нее в [15, 4]. Было показано, что такая метрика лучше коррелирует с человеческими
оценками, чем традиционная когерентность [14].
Результаты измерения средних когерентности и tf-idf
когерентности для каж-
дой модели показаны в таблице 2; представлены две версии метрик когерентности,
посчитанные на топ-10 и топ-20 словах в каждой теме.
24
0
50
100
150
200
250
300
350
400
600
500
400
300
200
100
1 (plsa)
2 (lda)
3 (smooth)
4 (decorrelated)
5 (restricted dict.)
6 (extended dict.)
7 (recursive)
8 (keyword)
0
50
100
150
200
250
300
350
400
2400
2200
2000
1800
1600
1400
1200
1000
800
600
1 (plsa)
2 (lda)
3 (smooth)
4 (decorrelated)
5 (restricted dict.)
6 (extended dict.)
7 (recursive)
8 (keyword)
(a)
(b)
0
50
100
150
200
250
300
350
400
600
500
400
300
200
100
0
100
1 (plsa)
2 (lda)
3 (smooth)
4 (decorrelated)
5 (restricted dict.)
6 (extended dict.)
7 (recursive)
8 (keyword)
0
50
100
150
200
250
300
350
400
2500
2000
1500
1000
500
0
1 (plsa)
2 (lda)
3 (smooth)
4 (decorrelated)
5 (restricted dict.)
6 (extended dict.)
7 (recursive)
8 (keyword)
(c)
(d)
Рис. 9: Сортированные метрики качества тем: (a) coh
10
; (b) tfidf
10
; (c) coh
20
; (d) tfidf
20
.
Модель
𝑇
coh
10
tfidf
10
coh
20
tfidf
20
1 (plsa)
400
-325.3
-212.0
-1447.0
-1011.6
2 (lda)
400
-344.2
-230.9
-1539.8
-1121.2
3 (smooth)
400
-367.1
-261.2
-1583.9
-1210.2
4 (decorr)
400
-378.9
-274.0
-1651.2
-1296.1
5 (restr. dict.) 400 -310.0 -196.4 -1341.9 -908.4
6 (ext. dict.)
400
-321.7
-209.6
-1409.1
-995.3
7 (recursive)
400
-326.5
-212.1
-1415.6
-982.5
8 (keyword)
400
-328.8
-214.4
-1463.6
-1014.5
Таблица 2:
Средние когерентность и tf-idf
коге-
рентность для всех обученных моделей.
Вопрос
Разница
1 (general understanding)
0.28
2 (event/phenomenon)
0.30
3 (ethnonyms)
0.07
4 (ethnic issues)
0.06
5 (international relations)
0.08
6 (other)
0.25
Таблица 3:
Согласованность кодиров-
щиков: общая доля различных ответов.
Распределения всех четырёх метрик также показаны в деталях на рис. 9, демон-
стрирующем отсортированные метрики (coh
10
,
tfidf
10
,
coh
20
,
и tfidf
20
) для каждой
из моделей,
и график,
идущий выше всех остальных,
соответствует лучшей моде-
ли.
Таблица 2 и рис.
9 показывают,
что хотя модели 5 (restricted dictionary) и 6
(extended dictionary) побеждают во всех четырёх случаях,
все остальные модели
имеют сопоставимые результаты,
кроме моделей 3 (smooth) и 4 (decorrelated).
Это
было подтверждено предварительными оценками людей, поэтому было принято ре-
шение исключить эти две модели из дальнейшего рассмотрения для более полезного
использования ограниченного количества человеческих ресурсов.
Для всех моделей экспертам было предложено интерпретировать каждую тему в
каждой модели по топ-20 словам этой темы. Для каждой темы два эксперта отвечали
на следующие вопросы,
связанные с качеством и степенью этничности темы;
на
25
# coh
10
tfidf
10
coh
20
tfidf
20
Частично интерпретируемые темы
1 (plsa)
139 -258.7
-145.3
-1145.9
-696.9
2 (lda)
192 -274.9
-163.3
-1224.1
-777.5
5 (restricted dict.)
237 -284.6
-163.0
-1247.9
-768.8
6 (extended dict.)
146 -258.6
-141.2
-1156.0
-686.1
7 (recursive)
239 -281.9
-166.3
-1235.7
-788.1
8 (keyword)
114 -256.3
-140.2
-1141.4
-682.8
Хорошо интерпретируемые темы
1 (plsa)
119 -318.0
-206.6
-1414.7
-982.5
2 (lda)
120 -389.5
-273.1
-1743.7
-1324.6
5 (restricted dict.)
87
-330.7
-227.0
-1410.7
-1028.2
6 (extended dict.)
103 -313.8
-199.9
-1372.6
-936.4
7 (recursive)
58
-349.2
-241.1
-1498.1
-1086.1
8 (keyword)
106 -310.0
-198.9
-1354.3
-914.8
Обе группы тем вместе
1 (plsa)
258 -286.0
-173.6
-1269.9
-828.7
2 (lda)
312 -319.0
-205.5
-1424.0
-988.0
5 (restricted dict.)
324 -297.0
-180.2
-1291.6
-838.5
6 (extended dict.)
249 -281.5
-165.5
-1245.6
-789.6
7 (recursive)
297 -295.1
-180.9
-1287.0
-846.3
8 (keyword)
220 -282.2
-168.5
-1244.0
-794.6
Таблица 4: Экспериментальные результаты: средние интерпретируемости и когерентности разных
групп тем.
каждый вопрос требовалось дать один из трёх ответов: «нет», «частично» и «да»:
1.
Понятно ли Вам, почему эти слова собрались вместе в данной теме?
2.
Если в вопросе 1 Вы дали ответ «частично» или «да»: понятно ли Вам, какое
явление или события может описываться в текстах, связанных с этой темой?
3.
Есть ли среди топ-слов этнонимы? Укажите количество.
4.
Если в вопросе 2 Вы дали ответ «частично» или «да»: связано ли это событие
с этничностями?
5.
Если в вопросе 2 Вы дали ответ «частично» или «да»: связано ли это событие
с международными отношениями?
6.
Если в вопросе 2 Вы дали ответ «частично» или «да»: связано ли это явление
или событие с другой темой, не имеющей отношения к этничностям?
Эксперты были проинструктированы по всем вопросам, включая различия меж-
ду межэтничными и межнациональными отношениями. Были собраны ответы семи
экспертов; таблица 3 суммирует значения общего согласия экспертов, демонстрируя
26
Темы
Релевантные темы
частично
хорошо
обе группы
# coh
10
tfidf
10
coh
20
tfidf
20
# coh
10
tfidf
10
coh
20
tfidf
20
# coh
10
tfidf
10
coh
20
tfidf
20
1 (plsa)
ethnic
5
-313.2
-190.2
-1399.2
-904.8
12 -334.0
-207.1
-1480.9
-996.3
17 -327.9
-202.1
-1456.9
-969.4
IR
20 -279.1
-150.7
-1227.0
-733.8
19 -315.3
-194.0
-1410.7
-946.8
39 -296.8
-171.8
-1316.5
-837.6
all relev.
20 -289.6
-163.0
-1271.2
-784.9
25 -315.9
-194.3
-1408.0
-938.7
45 -304.2
-180.4
-1347.2
-870.3
2 (lda)
ethnic
2
-239.7
-124.4
-1158.5
-646.0
13 -306.8
-190.0
-1369.1
-927.9
15 -297.9
-181.3
-1341.0
-890.3
IR
21 -285.1
-158.9
-1266.2
-763.1
29 -353.3
-225.7
-1580.6 -1097.5 50 -324.7
-197.7
-1448.6
-957.1
all relev.
18 -289.4
-162.3
-1287.3
-777.7
37 -336.3
-212.2
-1496.3 -1023.0 55 -320.9
-195.9
-1427.9
-942.7
5 (restricted dictionary)
ethnic
18 -288.7
-164.7
-1264.2
-798.5
30 -331.6
-222.3
-1419.0 -1015.8 48 -315.5
-200.7
-1360.9
-934.3
IR
33 -269.1
-142.5
-1190.8
-707.7
26 -323.1
-207.4
-1358.1
-917.3
59 -292.9
-171.1
-1264.5
-800.1
all relev.
36 -267.2
-142.0
-1177.6
-695.1
47 -322.7
-211.1
-1374.5
-958.4
83 -298.7
-181.1
-1289.1
-844.2
6 (extended dictionary)
ethnic
8
-288.4
-160.5
-1315.2
-805.1
22 -280.7
-150.0
-1226.8
-713.8
30 -282.8
-152.8
-1250.4
-738.2
IR
18 -250.0
-126.3
-1130.6
-641.1
29 -287.4
-156.3
-1240.9
-740.8
47 -273.1
-144.8
-1198.7
-702.6
all relev.
22 -261.2
-136.5
-1199.9
-707.7
37 -285.5
-158.3
-1234.6
-741.8
59 -276.4
-150.2
-1221.7
-729.1
7 (recursive)
ethnic
18 -308.2
-181.3
-1418.7
-952.6
22 -320.1
-201.8
-1431.0
-971.4
40 -314.7
-192.6
-1425.5
-962.9
IR
30 -283.3
-161.6
-1236.8
-780.4
30 -291.4
-171.4
-1292.9
-827.3
60 -287.4
-166.5
-1264.9
-803.9
all relev.
34 -285.4
-161.3
-1269.0
-810.6
47 -299.0
-180.1
-1331.3
-869.8
81 -293.3
-172.2
-1305.1
-844.9
8 (keyword)
ethnic
5
-289.7
-161.1
-1315.9
-805.0
37 -297.9
-175.6
-1318.9
-834.7
42 -297.0
-173.9
-1318.6
-831.1
IR
18 -264.7
-138.4
-1168.7
-670.7
32 -278.5
-164.3
-1240.7
-782.9
50 -273.5
-155.0
-1214.8
-742.5
all relev.
17 -279.5
-154.3
-1230.7
-741.3
52 -282.5
-165.5
-1260.1
-793.1
69 -281.8
-162.8
-1252.8
-780.4
Таблица 5: Релевантность и когерентность тем.
Вопрос 1
Вопрос 2
Вопрос 1
Вопрос 2
Релев. темы част.
хор.
все
част.
хор.
все
част.
хор.
все
част.
хор.
все
1 (plsa)
6 (extended dictionary)
ethnic
1.80
1.75
1.76
1.20
1.50
1.41
2.00
1.73
1.80
1.75
1.27
1.40
IR
1.90
1.68
1.79
1.75
1.26
1.51
1.94
1.72
1.81
1.72
1.17
1.38
all relev.
1.85
1.72
1.78
1.65
1.36
1.49
1.95
1.62
1.75
1.68
1.16
1.36
2 (lda)
7 (recursive)
ethnic
2.00
1.92
1.93
2.00
1.62
1.67
1.78
1.59
1.68
1.00
0.95
0.97
IR
2.00
1.69
1.82
1.86
1.21
1.48
1.87
1.87
1.87
1.43
1.20
1.32
all relev.
2.00
1.76
1.84
1.83
1.32
1.49
1.94
1.72
1.81
1.35
1.09
1.20
5 (restricted dictionary)
8 (keyword)
ethnic
2.00
1.40
1.62
1.89
1.27
1.50
2.00
1.76
1.79
1.20
0.89
0.93
IR
1.85
1.42
1.66
1.85
1.35
1.63
1.94
1.91
1.92
1.33
1.16
1.22
all relev.
1.89
1.45
1.64
1.86
1.32
1.55
1.94
1.83
1.86
1.41
1.08
1.16
Таблица 6: Результаты интерпретации тем, связанных с межэтничными и международными отно-
шениями.
доли различающихся ответов для каждого вопроса. В целом, эти результаты пока-
зывают хорошую согласованность между экспертами, по сравнению с более ранними
экспериментами по аналогичной проблеме [17]. В случае разногласия экспертов вме-
сто усреднения их ответов генерировались два набора метрик:
с максимальным и
с минимальными ответами соответственно. Таким образом определялись верхняя и
нижняя границы человеческого восприятия качества моделей.
Для каждой модели таблица 4 также демонстрирует среднее значение tf-idf ко-
герентности. Поскольку тренируемые модели пытались извлечь заданное число тем
высокого качества, отодвигая «мусорные» темы в фон, не имеет особого смысла про-
изводить сравнение моделей по всем темам. Важнее смотреть на когерентности тех
тем, которые были признаны качественными человеческими экспертами.
Таблица 4 суммирует наиболее важные для понимания результаты,
таких как
интерпретируемость (вопрос 2),
и их связь с tf-idf когерентностью.
В этой таблице
27
частично интерпретируемые — это те темы, которые получили «1» хотя бы у одного
эксперта; хорошо интерпретируемые темы — те, которым хоть один эксперт поставил
«2».
Лидерами являются модели 5 и 6 (restricted dictionary и extended dictionary,
соотвественно).
В таблице можно видеть,
что модель 6 превосходит все остальные
по общему качеству.
Модель 5 же даёт более высокие показатели когерентностей
в в группах интерпретируемых тем,
но надо учитывать,
что число найденных ею
интерпретируемых тем меньше. Это значит, что модель 5 находить меньше тем, но
эти темы более высокого качества.
Таблица 5 суммирует наиболее важные результаты,
определяющие степень ре-
левантности тем нашим целям. Под релевантностью в таблице подразумевается со-
ответствие темы межэтничным или международным отношениям.
Средняя интер-
претируемость рассчитывалась как среднее арифметическое оценок,
поставленных
соответствующим темам экспертами при ответе на вопрос 2. Здесь опять видны два
лидера — модели 5 и 6.
При этом первая превосходит вторую в терминах tf-idf ко-
герентности релевантных тем, а вторая превосходит первую в терминах числа тем,
которые эксперты сочли релевантными. Это верно и для межэтничных, и для меж-
дународных отношений,
а также для обоих уровней релевантности.
Это означает,
что расширения словаря приводит к появлению большего числа полезных тем мень-
шим качеством.
Таблица 6 показывает экспертные оценки интерпретируемости тем: она показы-
вает среднее значение оценок,
выставленных темам из каждого подмножества для
двух основных вопросов, т.е. верхний левый угол показывает, что в среднем эксперты
выставили оценку 1.80 в вопросе 1 (общая интерпретируемость) темам,
высоко ре-
левантным тематике этничности. Стоит отметить, что теперь модель 6 превосходит
модель 5 в терминах интерпретируемости:
по этой метрике,
в модели 6 не только
больше релевантных тем,
но они ещё и более интерпретируемые,
чем в модели 5.
Однако лишь часть из них связано со специфичными событиями (вопрос 2). Тем не
менее, с социологической точки зрения, модель 6 выглядит более предпочтительной
на этой стадии исследования.
В то же время, словарь модели 6 более широк: он подменяет отсутствующие эт-
нонимы соответствующими прилагательными и названиями стран.
Такой принцип
построения словаря позволяет выявить в модели этничности,
которые не были на-
званы напрямую,
и такой подход предотвращает переобучение в лучшей модели.
Поэтому в будущем планируется использовать именно такой словарь из соображе-
ний практичности и надёжности.
Интересные результаты показаны моделями 7 (recursive) и 8 (keyword texts). По
28
параметрами числа релевантных тем и когернтности модель 7 похожа на модель
5;
модель 8 же больше похожа на модель 6.
Это означает,
что ре-итерирование по-
строения тематической модели на подмножествах текстов не даёт преимущества,
или даже приводит к ухудшению качества. В то же время, однократное обучение по
подмножеству текстов, содержащих хотя бы один этноним из 𝑄 приводит к модели,
аналогичной по качеству (или немного худшей) лучшей модели 6.
Таким образом
можно сделать вывод, что обучение по фильтрованной коллекции может оказаться
полезным, особенно в случае обработки больших коллекций.
5.3
Эксперименты на коллекции IQBuzz
Описываемые в этом разделе эксперименты с коллекцией сообщений различных
социальных медиа IQBuzz направлены на дальнейшее совершенствование и упро-
щение тематического моделирования для решения поставленной задачи.
В рамках
этих экспериментов также рассматривается построение моделей, учитывающих ме-
таданные о метках времени и геотегах, привязанные к сообщениям.
Данные и параметры Коллекция была предоставлена в лемматизированном ви-
де и содержит почти 5.9М текстов. Источников текстов было около 6000, основные:
ВКонтакте — 4766761,
Twitter — 394060,
Google+ — 175213,
LiveInternet — 107211,
ursa-tm.ru — 58294, Эхо Москвы — 20729. Все прочие источники (более 6000, из каж-
дого меньше 20000 сообщений) были объединены в общий фоновый. Кроме того, все
такие сообщения получили общий геотег из-за сложности извлечения геотега из них,
либо из-за отсутствия такого геотега.
Из этих данных были извлечены этнонимы,
которые и составили словарь для последующего обучения, объём этого словаря со-
ставил 588 этнонимов (и постсоветстких, и международных). Исходный объём сло-
варя коллекции составил примерно 8.3М слов. Далее были произведены следующие
преобразования коллекции:
∙ все посты с некириллическими геотегами получили общий геотег;
∙ из меток времени была извлечена только дата в унифицированном формате;
∙ геотеги были сопоставлены с заранее подготовленным словарём для унифи-
кации (объединения в один геотег различных написаний одной и той метки
местоположения).
После была проведена фильтрация словаря коллекции, в ходе которой были уда-
лены слова
29
∙ встречающиеся в коллекции меньше 150 раз;
∙ встречающиеся в коллекции чаще 1 млн раз;
∙ с длиной меньше 4 символов;
∙ с длиной более 30 символов;
∙ содержащие что-либо, кроме кириллицы.
В итоге объём словаря составил около 75К слов, число меток времени — 715, чис-
ло геотегов — 98. Помимо этого словарь был дополнен примерно 10K биграммами,
содержащими этнонимы,
которые были получены путём выделения всех подобных
биграмм из коллекции с последующей частотной фильтрацией.
Средняя длина до-
кумента после фильтрации коллекции составила 262 слова.
При работе с данной коллекцией для простоты обучения было принято решения
использовать оффлайновый алгоритм.
Количество тем было выбрано в результате
асессорской работы: было выявлено, что начиная с 200 тем модели без регуляризации
перестают выявлять новые этнические тем, поэтому именно это количество тем было
зафиксировано во всех дальнейших экспериментах с коллекцией IQBuzz.
Для определения оптимального числа итераций по коллекции был произведён
следующий эксперимент. Аналогично всем предыдущим экспериментам (по подбору
числа тем),
строилась модель PLSA с 20 итерациями,
поскольку именно при та-
ком количестве проходов по коллекции сходилась метрика перплексии. Для провер-
ки гипотезы об избыточности такого количества проходов, были проанализированы
наиболее вероятные слова в темах,
получаемые после каждой итерации,
начиная
с 10 по 20.
Было выявлено,
что никаких существенных изменений в составе этих
слов не происходит уже начиная с 12 итерации, поэтому количество проходов было
зафиксировано равным 12. Для 200 тем это привело к тому, что итоговое время обу-
чения модели с распараллеливанием на 10 потоков составило примерно 6700 секунд.
Машина core i7, 6 ядер с hyper-threading.
Модели
По аналогии с экспериментами,
проводившимися ранее,
в тематической
модели коллекции IQBuzz был опробован ряд описанных инструментов АРТМ.
В
различных комбинациях и с разными коэффициентами опробованы регуляризато-
ры для частичного обучения по словарю этнонимов,
регуляризаторы сглаживания
и разреживания тем, их декорреляции, регуляризация с использованием модально-
стей этнонимов и биграмм с этнонимами. Оценивание производилось асессорами по
30
упрощённой схеме:
каждая тема оценивалась как этническая или неэтническая по
своим 20 наиболее вероятным словам. Выявлено, что наиболее значительный вклад
в усовершенствование модель данной коллекции вносят регуляризация с использо-
ванием этнонимов и биграмм.
Связано это во многом с особенностями коллекции:
она относительно велика, содержит большое количество длинных документов и нет
хорошо поддаётся более тонким методам регуляризации.
В результате перебора по
сетке значений оба выбранных регуляризатора получили коэффициенты 𝜏 равные
10. По результатам оценивания в наилучшей модели выявлено 87 тем, в той или иной
степени относящихся к этническим вопросам, либо касающихся внешней политики.
В PLSA таких тем получено 47, и качество их (с точки зрения интерпретируемости)
существенно ниже. В итоге эта модель с регуляризаторами модальностей и биграмм
была выбрана для дальнейших экспериментов с внедрением модальностей геотегов
и меток времени.
Коэффициенты регуляризации для данных модальностей были выбраны равны-
ми 1 по результатам экспериментов.
Причина в том,
что величины этих коэффи-
циентов оказывают сглаживающее или, наоборот, разреживающее влияние на соот-
ветствующие множества слов.
Так,
стремление коэффициентов к нулю приводит к
получению почти равномерных распределений,
что явно не соответствует постав-
ленной задаче. В то же время, значения, большие или равные 2, приводят к распре-
делениям, в которых вероятностная масса сосредоточена в 1-2 значениях, что также
является искажением. При значении 1 распределения как геотегов, так и меток вре-
мени имеют множество ненулевых вероятностей, но, в то же время, в большинстве
тем образуют несколько выраженных пиков,
позволяющих оценивать принадлеж-
ность подобных тем к различным регионам РФ в разрезе определённых временных
интервалов.
По этой же причине принято решение отказаться от дополнительных
регуляризаторов сглаживания/разреживания модальностей геотегов и меток време-
ни,
поскольку сами модальности оказались достаточно сильным регуляризатором,
справляющимся с этой задачей.
Результаты Поскольку для данной задачи отсутствуют подходящие автоматиче-
ские метрики оценивания, было проведено ручное кодирование, с целью поиска тем,
для которых найденные геотеги и метки времени могли быть интерпретированы и
соотнесены с событиями в реальности. Из 87 отобранных ранее тем были выделены
около 20, для которых подобные соответствия удалось установить без особых усилий.
В таблицах 7-9 наиболее вероятные слова, геотеги и метки времени для некоторых
из них (опущены темы-дубликаты).
31
Результаты
Комментарии:
Слова
чеченский,
чечня,
кадыров,
боевик,
терро-
рист,
убийство,
рамзан,
грозный,
спецназ,
наемник,
кавказ,
погибать,
операция,
тер-
акт, вооруженный, боевой, заложник, дуда-
ев, лидер, командир
Обсуждение
чечен-
ской войны в годов-
щину её начала.
Геотеги
Москва, Санкт-Петербург, Чечня.
Метки вре-
мени
Начале и конец декабря 2014.
Слова
украина,
олигарх,
украинский,
хунта,
ки-
евский, восточный, режим, юговосток, под-
держивать,
янукович,
переворот,
евромай-
дан, восток, революция, регион, одесса, под-
держка, независимость, правый, евросоюз
Обсуждение только
что
произошедше-
го
государственно-
го
переворота
на
Украине.
Геотеги
Москва, Общий геотег.
Метки вре-
мени
Март и апрель 2014.
Слова
независимость,
кричать,
вчера,
снова,
пре-
док,
русский,
кацап,
олигарх,
вспомнить,
завидовать,
разве,
ценность,
гордиться,
независимый,
украина,
вера,
политик,
громко, москаль, отказываться
Геотеги
Москва, Санкт-Петербург.
Метки вре-
мени
Март и август 2014.
Слова
армянин,
армянский,
армения,
азербай-
джан,
геноцид,
азербайджанский,
баку,
азербайджанец,
карабах,
ереван,
турция,
архив,
апрель,
кавказ,
жертва,
грузия,
па-
мять, убийство, русский, представитель
24 апреля было при-
знано
датой памя-
ти жертв геноцида
армян в Османской
Империи.
В
теме
смешались несколь-
ко
смежных
под-
тем.
Геотеги
Москва,
Краснодарский
край,
Санкт-
Петербург, Ростовская область.
Метки вре-
мени
Двадцатые числа апреля 2015.
Таблица 7: Результаты интерпретации модели с геотегами и метками времени.
32
Результаты
Комментарии:
Слова
переселенец,
северянин,
мигрант,
экстре-
мист,
иммигрант,
сотрудник,
документ,
нелегальный,
волонтер,
сообщать,
мятеж-
ник,
пункт,
данные,
миграционный,
поря-
док,
трудовой,
север,
приезжать,
преступ-
ление, флаг
Один из
всплесков
обсуждения
темы
прослеживается
в
июле
2014 в связи
с
прибытием
в
Мурманскую
об-
ласть мигрантов из
Украины.
Геотеги
Мурманская
область,
Москва,
Воронеж-
ская область, Санкт-Петербург
Метки вре-
мени
Тема размазана во времени, есть несколько
всплесков.
Слова
грузинский,
грузия,
русич,
буква,
шаман,
тува, тбилиси, корень, форма, либо, нибудь,
саакашвили,
носок,
праздничный,
южный,
гореть, огонь, тувинский, собирать, чулок
Обсуждение
войны
в
Южной
Осетии
в её
годовщину.
К
теме
примешалась
неясная
подтема,
связанная с Тывой.
Геотеги
Москва, Тыва.
Метки вре-
мени
Крупный всплеск 8 августа 2015.
Слова
украинский,
украина,
ополченец,
донецк,
славянск,
сбивать,
юговосток,
дебальцево,
самолет,
донецкий,
хунта,
вооружение,
но-
вороссия, боинг, боевой, тяжелый, луганск,
техника, точка, котел
Обсуждение
сбито-
го
над
территори-
ей Украины мала-
зийского «Боинга».
Геотеги
Москва, Общий геотег, Ростовская область.
Метки вре-
мени
17-18 июля 2014.
Слова
татарин,
русский,
крымский,
татарстан,
крымскотатарский, депортация, казань, му-
сульманин,
татарский,
проживать,
родной,
меджлис,
казанский,
родина,
мечеть,
эт-
нический,
тюркский,
коренной,
ислам,
му-
сульманский
Геотеги
Татарстан, Пермский край, Москва, Санкт-
Петербург.
Метки вре-
мени
Тема размазана во времени.
Таблица 8: Результаты интерпретации модели с геотегами и метками времени (продолжение).
33
Результаты
Комментарии:
Слова
еврей,
еврейский,
израиль,
холокост,
гит-
лер, израильский, фамилия, еврейка, жерт-
ва, убийство, комиссар, сионист, уничтоже-
ние,
газета,
раввин,
лагерь,
начальник,
ан-
тисемитизм, синагога, палестина
Обсуждение
ге-
ноцида
евреев
в
фашистской Герма-
нии в день памяти
жертв Холокоста.
Геотеги
Москва, Санкт-Петербург, Общий геотег.
Метки вре-
мени
27 января 2015.
Слова
китайский,
китаец,
сказка,
пекин,
восток,
дальний, восточный, золотой, азия, сибирь,
гонконг,
желтый,
продавать,
лиса,
аренда,
поднебесный, товар, остров, проект, балл
Обсуждение
сооб-
щений
о
сделках
Китая
и
РФ на
ПМЭФ
2015
на
крупные
суммы.
ПМЭФ проходил в
это время.
Геотеги
Москва,
Санкт-Петербург,
Пермский край,
Свердловская область.
Метки вре-
мени
Июнь и июль 2015.
Слова
аллах, мусульманин, сирия, сирийский, ис-
лам,
пророк,
иран,
исламский,
иранский,
саудовский, арабский, наносить, коран, ара-
вия,
кожа,
мухаммад,
мечеть,
посланник,
имам, мусульманский
Обсуждение сирий-
ского конфликта в
момент начала ак-
тивного вмешатель-
ства РФ в него.
Геотеги
Москва,
Дагестан,
Санкт-Петербург,
Чеч-
ня, Татарстан.
Метки вре-
мени
Октябрь 2015.
Таблица 9: Результаты интерпретации модели с геотегами и метками времени (продолжение).
Можно обратить внимание на то,
что почти все темы содержат в числе наи-
более вероятных геолокаций Москву и Санкт-Петербург,
что вполне закономерно,
поскольку суммарное число сообщений из этих городов составляет почти 40% от
общего числа всех текстов. Под общим геотегом понимается совокупность всех гео-
тегов, которые не были отнесены ни к одному из выделенных геотегов, так же к этой
геолокации были отнесены сообщения, у которых геотег изначально отсутствовал.
Таким образом, удалось без дополнительных усилий использовать информацию
о метках геолокации и времени в процессе обучения модели, а также на основании
полученных результатов выявить изменения полученных тематик в пространствах
этих меток.
34
6
Результаты, выносимые на защиту
На защиту в данной работе выносятся следующие результаты:
1.
Новый детерминированный онлайновый асинхронный ЕМ-алгоритм, позволя-
ющий производить эффективное обучение тематических моделей М-АРТМ.
2.
Использование методологии регуляризации для автоматического выявления
специфических тематик,
обсуждаемых в текстовых данных,
с возможностью
учёта дополнительной информации и оценивания на её основе.
Список литературы
[1]
Andrzejewski, D., Zhu, X.: Latent Dirichlet allocation with topic-in-set knowledge.
In:
Proc.
NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural
Language Processing. pp. 43–48. SemiSupLearn ’09, Association for Computational
Linguistics, Stroudsburg, PA, USA (2009)
[2]
Andrzejewski, D., Zhu, X., Craven, M.: Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In: Proc. 26th Annual International Conference
on Machine Learning. pp. 25–32. ICML ’09, ACM, New York, NY, USA (2009)
[3]
Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent Dirichlet allocation. Journal of Machine
Learning Research 3(4–5), 993–1022 (2003)
[4]
Bodrunova,
S.,
Koltsov,
S.,
Koltsova,
O.,
Nikolenko,
S.I.,
Shimorina,
A.:
Interval
semi-supervised lda: Classifying needles in a haystack. In: Proc. MICAI 2013, LNCS
vol. 8625, pp. 265–274. Springer (2013)
[5]
Chemudugunta, C., Smyth, P., Steyvers, M.: Modeling general and specific aspects
of documents with a probabilistic topic model. In: Advances in Neural Information
Processing Systems. vol. 19, pp. 241–248. MIT Press (2007)
[6]
Griffiths,
T.,
Steyvers,
M.:
Finding scientific topics.
Proceedings of
the National
Academy of Sciences 101 (Suppl. 1), 5228–5335 (2004)
[7]
A. Smola and S. Narayanamurthy: An architecture for parallel topic models. Proc.
VLDB Endow., 3(1-2):703–710, Sept. (2010)
[8]
M.
D.
Hoffman,
D.
M.
Blei,
and F.
R.
Bach.:
Online learning for latent dirichlet
allocation. In NIPS, pages 856–864. Curran Associates, Inc. (2010)
35
[9]
Hoffmann,
T.:
Unsupervised learning by probabilistic
latent
semantic
analysis.
Machine Learning 42(1), 177–196 (2001)
[10]
O.
Frei
and M.
Apishev:
Parallel
Non-blocking
Deterministic
Algorithm for
Online Topic Modeling.
Analysis
of
Images,
Social
Networks
and Texts.
AIST.
Communications in Computer and Information Science, vol 661. Springer, pp. 132-
144. (2016)
[11]
Jagarlamudi,
J.,
Daum´e,
III,
H.,
Udupa,
R.:
Incorporating lexical
priors into topic
models. In: Proc. EACL’12, pp. 204–213 (2012)
[12]
Koltcov,
S.,
Koltsova,
O.,
Nikolenko,
S.I.:
Latent dirichlet allocation:
Stability and
applications to studies of user-generated content. In: Proc. WebSci 2014, pp. 161–165
(2014)
[13]
Apishev M.,
Koltcov S.,
Koltsova O.,
Nikolenko S.,
Vorontsov K.:
Mining Ethnic
Content
Online with Additively Regularized Topic Models.
In:
Computacion y
Sistemas, Vol. 20, No. 3, pp. 387—403. (2016)
[14]
Mimno,
D.,
Wallach,
H.M.,
Talley,
E.,
Leenders,
M.,
McCallum,
A.:
Optimizing
semantic coherence in topic models. In: Proc. EMNLP’11, pp. 262–272 (2011)
[15]
Nikolenko,
S.I.,
Koltsova,
O.,
Koltsov,
S.:
Topic modelling for qualitative studies.
Journal of Information Science (2015)
[16]
Paul, M.J., Dredze, M.: Discovering health topics in social media using topic models.
PLoS ONE 9(8) (2014)
[17]
Sociopolitical
processes in the internet.
Laboratory for Internet Studies.
Internal
report,
National
Research
University
Higher
School
of
Economics,
reg.
no.
01201362573, Moscow (2013)
[18]
R.
Rehurek and P.
Sojka.:
Software
framework for
topic
modelling with large
corpora.
In Proceedings of
the LREC 2010 Workshop on New Challenges for NLP
Frameworks, pp. 45–50, Valletta, Malta (2010)
[19]
J.
Langford,
L.
Li,
and A.
Strehl.
Vowpal
wabbit open source project.
Technical
report, Yahoo! (2007)
[20]
Tan,
Y.,
Ou,
Z.:
Topic-weak-correlated
latent
dirichlet
allocation.
In:
7th
International Symposium Chinese Spoken Language Processing (ISCSLP). pp. 224–
228 (2010)
36
[21]
Tikhonov,
A.N.,
Arsenin,
V.Y.:
Solution of
ill-posed problems.
W.
H.
Winston,
Washington, DC (1977)
[22]
Vorontsov, K.V., Potapenko, A.A.: Tutorial on probabilistic topic modeling: Additive
regularization for stochastic matrix factorization. In: AIST’2014, Springer CCIS vol.
436, pp. 29–46 (2014)
[23]
Vorontsov, K.V., Potapenko, A.A.: Additive regularization of topic models. Machine
Learning,
Special
Issue
on Data
Analysis
and Intelligent
Optimization with
Applications 101(1), 303–323 (2015)
[24]
Vorontsov,
K.,
Frei,
O.,
Apishev,
M.,
Romov,
P.,
Suvorova,
M.,
Yanina,
A.:
Non-
bayesian additive regularization for multimodal topic modeling of large collections.
In: Proc. of TM ’15, pp. 29–37, ACM, New York, NY, USA (2015)
[25]
Vorontsov, K.: Additive regularization for topic models of text collections. Doklady
Mathematics 89(3), 301–304 (2014)
37
