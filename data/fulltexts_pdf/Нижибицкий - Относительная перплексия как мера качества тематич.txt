Тезисы конференции «Ломоносов — 2014»
Относительная перплексия как мера
качества тематических моделей
Нижибицкий Евгений Алексеевич
Студент
Факультет ВМК МГУ имени М. В. Ломоносова, Москва, Россия
E-mail: nizhibitsky@cs.msu.ru
Тематическое моделирование — это способ построения модели
коллекции текстовых документов, при котором каждая тема описы-
вается дискретным распределением на множестве терминов, а каж-
дый документ — дискретным распределением на множестве тем.
Пусть для каждого документа d из коллекции задано число n
dw
вхождений слова w в d.
Тематическая модель описывает вероят-
ность появления слов, опираясь на гипотезу условной независимости
p(w|t) = p(w|d, t) и формулу полной вероятности:
p(w|d) =
X
t∈T
p(t|d)p(w|t).
Для нахождения распределений p(t|d) и p(w|t) по исходным данным
(n
dw
) будем использовать модель LDA [2].
Существует несколько способов оценки качества построенной мо-
дели. Наиболее распространённым критерием является перплексия,
равная экспоненте от минус усреднённого логарифма правдоподо-
бия:
P = exp

−
1
n
X
d∈D
X
w∈d
n
dw
ln p(w|d)

,
где n — длина коллекции в словах. Перплексия зависит от мощности
словаря и распределения частот слов в коллекции p(w) = n
w
/n. По-
этому с её помощью невозможно оценивать качество удаления стоп-
слов и нетематических слов, сравнивать методы разреживания сло-
варя, а также униграммные и n-граммные модели.
Данная работа направлена на поиск критерия, также основанно-
го на
значении правдоподобия, но нечувствительного к изменению
состава словаря.
Предлагается относительная перплексия,
прини-
мающая значения из отрезка [0, 1] (чем меньше, тем лучше):
RP =
P − P
min
P
max
− P
min
,
где P
min
— минимальная перплексия униграммной модели докумен-
1
Текущая секция
тов (p(w|d)
= n
dw
/n
d
),
а P
max
— максимальная перплексия уни-
граммной модели коллекции (p(w|d) = n
w
/n, где n
w
— число вхожде-
ний слова w во всех документах коллекции, n
d
— длина документа d).
Относительная перплексия уменьшается с ростом числа тем |T |, до-
стигая 0 при T = min{W, D}, когда тематическая модель вырожда-
ется в униграммную модель документа,
и 1 при T = 1,
когда она
вырождается в униграммную модель коллекции. Таким образом, от-
носительная перплексия показывает положение модели относитель-
но наилучшего и наихудшего достижимых значений перплексии.
В работе исследуется зависимость относительной перплексии от
мощности словаря и числа тем. Для экспериментов использовалась
коллекция статей научной конференции NIPS за 1987–1999 гг. на ан-
глийском языке.
В каждом эксперименте при фиксированном чис-
ле тем из начального словаря коллекции отбрасывалась его случай-
но выбранная десятая часть, до полного исчерпания словаря. После
каждого отбрасывания производилось обучение модели с помощью
библиотеки gensim [3]. Полученные модели оценивались с помощью
перплексии и относительной перплексии.
На Рис. 1 каждой линии
соответствует один такой эксперимент,
начертания линий соответ-
ствуют различному числу тем.
Из правого графика можно сделать вывод,
что относительная
перплексия слабо зависит от мощности словаря,
лучше характери-
зует способность модели описывать коллекцию.
Её численное зна-
чение показывает,
насколько точность модели близка к предельно
достижимому минимуму перплексии.
Можно предполагать, что в коллекции существуют основные те-
мы,
существенно превышающие по мощности остальные.
Они вы-
являются даже после 7-кратного разреживания словаря.
В данном
эксперименте относительная перплексия практически не зависит от
разреженности словаря при |T | = 50. Поэтому можно предположить,
что данная коллекция содержит как раз около 50 основных тем.
При большем числе тем |T | относительная перплексия уменьша-
ется по мере разреживания словаря. Это объясняется тем, что темы
не одинаковы по мощности.
При случайном разреживании слова-
ря малые темы становятся статистически незначимыми и перестают
выявляться.
При меньшем числе тем |T | относительная перплексия увеличи-
вается по мере разреживания словаря. Предположительно, это свя-
зано с тем,
что тематическая модель вынужденно объединяет ос-
новные темы,
различия между объединёнными темами становятся
2
Тезисы конференции «Ломоносов — 2014»
незначимыми, темы сближаются и становятся более похожи на уни-
граммную модель коллекции.
Работа выполнена при поддержке гранта РФФИ 14-07-00965.
Иллюстрации
Рис. 1: Изменение функционалов при разреживании словаря.
Литература
1.
Воронцов К. В.,
Потапенко А. А.
Модификации EM-алгоритма
для вероятностного тематического моделирования // Машин-
ное обучение и анализ данных. 2013. T. 1, № 6. С. 657–686.
2.
David Blei,
Andrew Ng,
Michael
Jordan.
Latent Dirichlet alloca-
tion // Journal of Machine Learning Research, 2003, P. 993–1022.
3.
Radim
ˇ
Reh˚uˇrek, Petr Sojka, Software Framework for Topic Mod-
elling with Large Corpora // In Proceedings of
the LREC 2010
Workshop on New Challenges for NLP Frameworks,
2010,
P. 45–
50, http://radimrehurek.com/gensim/.
3
