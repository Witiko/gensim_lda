Asia-pacific Journal
of
Multimedia Services Convergent
with Art,
Humanities,
and Sociology
Vol.7,
No.2,
February (2017),
pp.
321-329
http://dx.doi.org/10.14257/AJMAHS.2017.02.53
ISSN:
2383-5281 AJMAHS
Copyright
2017 HSST
321
의미 정보가 강화된 워드 임베딩을 통한 감성 분석
의미 정보가 강화된 워드 임베딩을 통한 감성 분석
의미 정보가 강화된 워드 임베딩을 통한 감성 분석
의미 정보가 강화된 워드 임베딩을 통한 감성 분석
임미영 강신재
요 약
하나의 단어는 도메인에 따라서 표현하는 감성이 달라질 수 있다 예를 들어 과학 분야에서의 지
진 과 사회뉴스 분야에서의 지진 은 전자는 중성 후자는 부정적인 의미를 내포하고 있다 하지만
기존의 워드 임베딩에 관한 연구는 문장 내에서 단어의 통사적 정보와 같은 쓰임새 정보만 학습하고
있어서 쓰임새는 같으나 의미는 반대인 단어
과
와 같은 경우는 유사한 워드 임베딩 벡
터로 표현되는 한계점을 가지고 있다 이에 본 논문에서는 단어의 쓰임새와 도메인별 단어의 감성 정
보를 결합한 워드 임베딩 방법을 제안하고 이를 이용한 감성분류 시스템을 제안한다 제안하는 워드
임베딩 신경망은 두 부분으로 이루어져 있다 통사정보를 표현하기 위한
네트워크와 의미
정보를 표현하기 위한 감성분류 네트워크 부분이다 도메인별로 새로 학습해야 하는 의미 네트워크의
효율성을 높이기 위해서 도메인별 자질들을
로 축소시킨 다음 기존에 학습한
과 결
합하는 형식을 제안하여
만 사용한 방법에 비해
정도의 성능 제고를 가져왔다
핵심어 감성 분석 워드 임베딩 신경망 심층학습
Abstract
The meaning of
a word can be different
according to the different
domains.
For
example,
the sentiment
information of
“earthquake” in Science domain and the one in Social
News domain would be “neutral” and
“negative”
respectively.
However,
since
the
previous
word
embedding
methods
only
used
syntactical
information,
the
word “good”
and “bad”,
which have
same
usages
but
different
sentiment,
show similar
word embedding
vectors
respectively.
This
paper
proposes
a
word embedding
method
combines
syntax
information and sentiment
information by domain.
The network has two parts.
One is Word2vec network to
express
syntactic
information,
and the
other
is
semantic
network to represent
sentiment
information.
The
semantic network needs to be trained for
each domain.
To improve the semantic network’s efficiency,
we
Received (December
12,
2016),
Review Result
(December
26,
2016)
Accepted (January 2,
2017),
Published (February 28,
2017)
이 논문은 대구대학교 연구장학기금 지원으로 작성되었습니다
Copyright
2017 HSST
322
propose
to reduce
domain features
using basic
statistical
method
TF·IDF,
and combine
the
semantic
network with pre-trained Word2vec.
This resulted 2%p improvement
compare to the baseline system which
used Word2vec only.
Keywords :
Sentiment
Analysis,
Word Embedding,
Neural
Networks,
Deep Learning,
TF·IDF
서론
서론
서론
서론
최근 신경망 기반 모델들이 학습 방법의 개선으로 다층 학습이 가능하게 되어 이미지 인식 음
성 인식 등의 분야에서 월등한 성능을 보이고 있으며 자연어처리 분야에서도 이를 활용하려는 연
구가 활발해지고 있다 이 중 하나가 워드 임베딩
인데 이는 단어를 벡터로 표현
할 때 유사한 단어들은 유사한 값의 분포를 가지는 벡터들로 표현되도록 하여 어휘 의미를 표현
하는 기술이다 이는 기존의 어휘 기반의 표현방식에 비해 단어의 의미를 어느 정도 표현할 수 있
다는 장점이 있다 예를 들면 단어
와 중동호흡기 증후군 은 워드 임베딩 벡터로 표현을
했을 때 비슷한 벡터 값을 가지며
이상의 유사도를 보이는 것으로 나타났다
한편 감성분석 분야는 오래된 분야지만 여전히 많은 연구가 진행되고 있는 분야이다 감성 분석
은 주로 소셜 네트워크 서비스 블로그 쇼핑몰 등에서 발생하는 사용자의 의견을 서술한 텍스트들
을 대상으로 특정 사건이나 해당 상품에 대한 여론 추세 사용자 의견 등을 파악하는데 많이 사용
되고 있다 하지만 단어 혹은 단어 연쇄의 감성 정보는 도메인에 따라서 서로 다른 극성을 나타낼
수도 있다
이에 본 논문에서는 통사적 측면과 의미적 측면을 모두 고려한 워드 임베딩을 구
성하고 도메인별 감성분석에 적용하여 기존 통사적 측면만 고려한 워드 임베딩 방식보다 높은 성
능을 얻었다
기존연구
기존연구
기존연구
기존연구
감성 분류
감성 분류
감성 분류
감성 분류
감성 분류는 자연어처리의 한 분야로 특정 문서에 대하여 긍정 부정 등의 분류를 하는 연구 분
야이다 감성 분류를 위한 방법으로는 감성 사전을 이용한 연구
와
등 통계적 수치들을
사용한 연구들
이 있었고 또 최근에는 대량의 데이터와 신경망을 이용한 연구들
도 많
이 진행되고 있다
감성 사전을 이용한 연구는 단어의 극성 정보를 포함하는 감성 사전을 먼저 구축한 후 문서에
출현하는 단어를 검색하여 이를 기반으로 분류하는 방법 이 있고
과
에서는 범용 감성 사
전이 아닌 도메인에 특화된 감성 사전을 사용하여 보다 좋은 성능을 얻기도 하였다
이에 본 논문에서는 현재 많은 자연어처리 분야에 응용되고 있는 워드 임베딩 벡터를 사용하여
도메인 특화된 워드 임베딩 벡터를 제안한다
Asia-pacific Journal
of
Multimedia Services Convergent
with Art,
Humanities,
and Sociology
Vol.7,
No.2,
February (2017)
ISSN:
2383-5281 AJMAHS
Copyright
2017 HSST
323
워드 임베딩
워드 임베딩
워드 임베딩
워드 임베딩
워드 임베딩에 관한 연구는
년대 후반부터 활발히 진행되었다
와
는 신
경망을 이용하여 단어 벡터를 구축하였고 이 벡터들로 자연어처리에서의 여러 가지 작업들을 수
행하여 시맨틱 레이블링 등의 테스크에서 좋은 성능을 보였다
그림
와
의
모델
는 기본적인
을 제안하였고
는
이에 기초하여 은닉층을 제거한
을 제안하였다 이 모델은 기존 방법에 비해 시간을
배 이상 단축시키고 기존 방법보다 높은 성능을 보였다
그림
의
모델
Copyright
2017 HSST
324
은
에서 제안한
이 의미보다는 통사정보 즉 단어나 구의 쓰임새 정보를 활
용하고 있음을 밝혀냈다 이를 개선하기 위하여 감성분석에 사용하기 위한 의미 학습 네트워크를
제안하였으며
과
자질들을 사용하고
과 결합하여
값
정도의 좋은 성능을 보였다
본 논문에서는
에 기초한
도
모델과 마찬가지로 의미정보보다는 통사정
보에 치중하여 있음을 발견하였다 예를 들면 영어단어
과
는 서로 상반되는 뜻이지만
의 유사도 계산에서 높은 유사도를 보인다 이는
의 입력과 출력과 관계가 있
다 단어 연쇄 
















라고 할 때 입력은











의 연접된
이고 출력은




의
이기 때문에 해당 단어의 주변 단어가 비슷하면 해당
단어의 벡터 값도 비슷해진다 즉 해당 단어의 쓰임새를 학습한다고 볼 수 있다
에서는 모든 도메인에서 사용가능한 일반적인 단어벡터를 제안하려 하였다 하지만 현실에서
는 같은 단어라도 다른 도메인에서는 다른 감성을 나타낼 수 있다 예를 들면 지진 이라는 단어는
뉴스 도메인에서는 매우 부정적인 단어이지만 지구과학 도메인에서는 중성일 가능성이 높다 따라
서 본 논문에서는 도메인별 감성 분류 네트워크를 제안한다
의 장점인 적은 계산량을 유지하기 위해 기존 통계적 언어처리에서 기본적인 자질 선
정 기법인
기법을 사용하여 보다 빠르게 학습할 수 있는 워드 임베딩 벡터를 만든다 본
논문은 기존
네트워크에 특정 도메인 데이터로 학습시킨 감성분류 네트워크를 결합하는
방식을 제안한다 즉 통사 네트워크는 도메인 간에 공유하여 사용하고 감성 네트워크는 도메인별
로 구축하여 결합함으로써 감성 분류의 성능 향상을 기하고자 하였다
연구 방법
연구 방법
연구 방법
연구 방법
실험환경
실험환경
실험환경
실험환경
을 사용하여
과
를 구현하였고
의
를 사용하여
감성분류
네트워크를
구현하였다
감성분류
평가는
파이썬용
기계학습
라이브러리인
의
을 사용하였다 표 은 실험 데이터에 대한 설명이다
표
데이터
통사 정보 학습용 데이터
세종데이터 원시말뭉치
문장
단어
네이버 영화평
개
문장
단어
의미 정보 감성 분류 학습용 데이터
네이버 영화평
개
감성 분류 평가용 데이터
학습
네이버 영화평
개
테스트
네이버 영화평
개
Asia-pacific Journal
of
Multimedia Services Convergent
with Art,
Humanities,
and Sociology
Vol.7,
No.2,
February (2017)
ISSN:
2383-5281 AJMAHS
Copyright
2017 HSST
325
제안 모델
제안 모델
제안 모델
제안 모델
전체적인 시스템의 구조는 그림 과 같다 시스템은 세 부분으로 나뉘어져 있는데 그림에서 왼
쪽 부분은 기존
워드 임베딩을 구축하는 과정이다 오른쪽 부분은 제안하는 도메인 의
미정보 강화를 위한 감성분류 네트워크이다 마지막으로 중간 부분은 두 네트워크에서 생성된 워
드 임베딩 벡터들을 연결하여
분류기에 입력으로 넣어 최종적인 감성 분류를
하는 과정이다
그림
감성분류 시스템 프로세스
단어별 의미 벡터를 생성하기 위한 감성 분류 네트워크의 구조는 그림 와 같다 기존 연구들
과 마찬가지로 간단하게
두 계층을 쌓아서 구축하였다 먼저 형태소 분석된 자질들
을
를 이용한 간단한 자질선정 단계를 거친 후
를 구성하여 단어들을 표현한다
문장












 



는 단어 라고 할 때 슬라이딩 윈도우 방식으로 문장을 잘라서
입력으로 주게 되고 긍정 부정 점수가 출력으로 나오는 네트워크이다 긍정은
부정은
로 표현된다 이때
를 통과하기 바로 직전
의 출력이 그림 에서 예시된 중간 단어


를 나타내는 의미 벡터가 된다
Copyright
2017 HSST
326
그림
도메인별 감성 분류 의미 네트워크 구조
그림 은 각각 학습된 단어 벡터들을 연결하여 감성 분류를 위한 문장 벡터를 구축하는 과정을
보여 준다 단어별 벡터의 동일한 자질 차원 의 값 가운데 최대값을 선택하였고
벡터의 구성 방
법은
구조를 사용하였다
그림
문장 벡터 구축 방법
실험 결과
실험 결과
실험 결과
실험 결과
표 와 표 은 각각
과 감성 의미 정보만을 워드 임베딩 벡터로 사용하여
분류기로 감성분류 테스트를 하였을 때의 결과이다 제안하는 감성 네트워크를
랭킹 자질 상위 자질로 차원으로 학습시켰을 때의 정확도이다 이를
로 표기하였다 도메
인 의미정보 워드 임베딩이
보다 조금 떨어지는 성능을 보여주고 있다
Asia-pacific Journal
of
Multimedia Services Convergent
with Art,
Humanities,
and Sociology
Vol.7,
No.2,
February (2017)
ISSN:
2383-5281 AJMAHS
Copyright
2017 HSST
327
표
워드 임베딩만 이용한 실험 결과 정확도
100d
200d
300d
Word2vec
0.772
0.792
0.801
표
의미 네트워크 워드 임베딩만 이용한 실험 결과 정확도
0.748
0.749
0.748
0.784
0.784
0.787
0.793
0.792
0.795
표 는 두 가지 워드 임베딩 벡터들을 연결하여 구성한 새로운 워드 임베딩 벡터로 수행한 실
험 결과이다 표 에서 세로줄은
출력 차원이고 가로줄은 표 과 마찬가지로
로
표기하였다 감성 벡터는
차원의 자질을 선택하였을 때 좋은 성능을 보여 주었고 주로 이를
대상으로
차원으로 임베딩하고
은
차원으로 하였을 때 가장 좋은 성능을 보였다
표
실험 결과 정확도
표 와 표 의 비교를 통해 감성 의미 분류 네트워크와
네트워크를 같이 사용했을
때 기존
네트워크보다
정도 나은 성능을 얻을 수 있었다 이는 기존
워
드 임베딩 벡터에서 표현하지 못했던 정보를 감성 의미 워드 임베딩 벡터가 보완하여 좋은 성능을
얻었음을 의미한다
결론
결론
결론
결론
본 논문에서는
방식으로 학습한
네트웍과 마찬가지로
으로 학
습한 네트워크인
도 감성분류를 위한 의미 표현에는 적합하지 않음을 발견하고 이를 감
성 분류를 위한 의미 네트워크와 결합시켰다 실험을 통해 제안하는 네트워크는
만 사용
한 시스템과 의미 네트워크만 사용한 시스템보다
정도 제고된 성능을 보여 주었다 통사 네
Copyright
2017 HSST
328
트워크로 표현된
네트워크는 여러 도메인에서 공유하여 사용하여도 되지만 감성 분류
의미 네트워크는 도메인별로 구축하여 사용하는 방식이 유리함을 확인하였다
References
[1]
S.
Kim and S.
Lee.
Automatic
Extraction of
Alternative
Word Candidates
Using the
Word2Vec
Model.
Proceedings of
KIISE Winter
Conference.
(2015),
pp.769-771
[2]
J.
Song and S.
Lee.
Automatic Construction of
Positive/Negative Feature Predicate Dictionary for
Polarity
Classification of
Product
Reviews.
Journal
of
KIISE:
Software
and Applications.
(2013),
Vol.
38,
No.
3,
pp.260-265.
[3]
S.
Lee,
J.
Cui
and J.
Kim.
Sentiment
Analysis
on Movie
Review Through Building Modified Sentiment
Dictionary
by
Movie
Genre.
Journal
of
Intelligent
and
Information
Systems.
(2016),
Vol.
22,
No.
2,
pp.97-113.
[4]
P.
D.
Turney
and
M.
L.
Littleman.
Unsupervised
Learning
of
Semantic
Orientation
from a
Hundred-Billion-Word Corpus.
National
Research Council,
Institute
for
Information Technology,
Technical
Report.
(2002),
ERB-1094.
[5]
W.
Jin,
H.
H.
Ho and R.
K.
Srihari.
Opinion Miner:A Novel
MachineLearning Sustem for
Web Opinion
Mining
and
Extraction.
KDD Proceedings
of
the
15th
ACM SIGKDD International
Conference
on
Knowledge Discovery and Data Mining.
(2009),
pp.
1195-1204.
[6]
C.
N.
dos
Santos
and M.
Gatti.
Deep Convolutional
Neural
Networks
for
Sentiment
Analysis
of
Short
Texts.
COLING.
(2014),
pp.
69-78.
[7]
I.
Huh,
D.
Kim and J.
Lee.
Neural
Network Based Emotion Classification b Creating Emotion-Intensity
Vector.
Proceedings of
Korean Institute of
Intelligent
Systems Conference.
(2015),
Vol.
25,
No.
2,
pp.9-10.
[8]
S.
Kim and N.
Kim.
A Study on the Effect
of
Using Sentiment
Lexicon in Opinion Classification.
Journal
of
Intelligent
Information System.
(2014),
Vol.
20,
pp.133-148.
[9]
R.
Collobert,
J.
Weston,
L.
Bottou,
M.
Karlen,
K.
Kavukcuoglu and P.
Kuksa.
Natural
Language Processing
(Almost)
from Scratch.
Journal
of
Machine Learning Research.
(2011),
Vol.
12,
pp.2493-2537.
[10]
Y.
Bengio.
Deep Learning of
Representations:
Looking Forward.
arXiv preprint
arX-iv:1305.0445.
(2013).
[11]
T.
Milokov,
I.
Sutskever,
K.
Chen,
G.
Corrado and J.
Dean.
Distributed Representations
of
Words
and
Phrases and Their
Compositionality.
The Conference on Neural
Information Processing Systems.
(2013).
[12]
D.
Tang,
F.
Wei,
N.
Yang,
M.
Zhou,
T.
Liu and B.
Qin.
Learning Sentiment-Specific Word Embedding
for
Twitter
Sentiment
Classification.
Association
for
Computational
Linguistics.
(2014),
Vol.
1,
pp.
1555-1565.
[13]
R.
Rehurek and P.
Sojka.
Software Framework for
Topic Modelling With Large Corpora.
Proceedings
of
the LREC 2010 Workshop on New Challenges for
NLP Frameworks.
(2010).
[14]
M.
Abadi,
A.
Agarwal,
P.
Barham,
E.
Brevdo,
Z.
Chen,
C.
Citro,
G.
S.
Corrado,
A.
Davis,
J.
Dean,
M.
DEvin,
S.
Ghemawat,
I.
Goodfellow,
A.
Harp,
G.
Irving,
M.
Isard,
R.
Jozefowicz,
Y.
Jia,
L.
Kaiser,
M.
Kudlur,
J.
Levenberg,
D.
Mane,
M.
Schuster,
R.
Monga,
S.
Moore,
D.
Murray,
C.
Olah,
J.
Shlens,
B.
Asia-pacific Journal
of
Multimedia Services Convergent
with Art,
Humanities,
and Sociology
Vol.7,
No.2,
February (2017)
ISSN:
2383-5281 AJMAHS
Copyright
2017 HSST
329
Steiner,
I.
Sutskever,
K.
Talwar,
P.
Tucher,
V.
Vanhoucke,
V.
Vasudevan,
F.
Viegas,
O.
Vinyals,
P.
Warden,
M.
Wattenberg,
M.
Wicke,
Y.
Yu and X.
Zheng.
Tensorflow:
Large-Scale Machine Learning on
Heterogenous Systems.
https://tensorflow.org.
(2015).
[15]
E.
Jones,
T.
Oliphant
and P.
Peterson.
Open Source
Scientific
Tools
for
Python.
https://www.scipy.org.
(2001).
[16]
R.
Collobert
and J.
Weston.
Fast
Semantic Extraction Using A Novel
Neural
Network Architecture.
Annual
Meeting-association for
Computational
Linguistics.
(2007),
Vol.
45,
No.
1,
pp.560-567.
[17]
R.
Collobert
and
J.
Weston.
A Unified
Architecture
for
Natural
Language
Processing:
Deep
Neural
Networks With Multitask Learning.
International
Conference on Machine Learning.
(2008).
