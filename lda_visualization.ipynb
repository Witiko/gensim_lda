{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA visualization of gensim citations\n",
    "\n",
    "### A visualization of citations of \"Software Framework for Topic Modelling with Large Corpora\" represented as LDA probability distribution towards the selected number of topics.\n",
    "\n",
    "### Note the configurable parameters to play with:\n",
    "\n",
    "#### Preprocessing:\n",
    "\n",
    "* Text filter: Whether to filter out some part of texts, or not\n",
    "* Filter parameters (for filtering of HTML content) based on: \n",
    "    * minimal length of a valid text sentence, \n",
    "    * minimal length of a valid text line\n",
    "* Text preprocess method - currently gensim's preprocess_text() - works the best only for English\n",
    "\n",
    "#### LDA:\n",
    "\n",
    "* Number of topics\n",
    "* Number of passes over the input corpus\n",
    "* Many others, so far left on defaults: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "#### Tf-Idf:\n",
    "\n",
    "* Corpus representation: 1. BoW counts of words, or 2. Tf-Idf weights\n",
    "* Representative term set filtering: to consider only a given percentile of top-important terms according to tf-idf weights for each doc\n",
    "\n",
    "#### Visualization:\n",
    "\n",
    "* Docs representation: currently over a distance matrix\n",
    "    * Distance method for distance matrix: correlation, cosine, euclidean, ... choose from https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "* Dimensionality reduction: currently MDS from distance matrix\n",
    "\n",
    "#### Topic representation in visualization:\n",
    "\n",
    "* Number of most important words for topic\n",
    "* For each word from a set of each topic, maximum number of occurrences of this word in other clusters representations\n",
    "    * Can clarify the meaning of the topic in contrast to other inferred topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load:\n",
    "\n",
    "1. using filter for short sentences (to get rid of HTML tags content) + gensim preprocess_text()\n",
    "2. plaintext + gensim preprocess_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content_dir = \"data/fulltexts_html\"\n",
    "pdf_content_dir = \"data/fulltexts_pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences_from_text(text_lines, min_line_len=20, min_sen_len=15):\n",
    "    fulltext = \" \".join([line for line in list(text_lines) if len([l for l in line.split(\" \") \n",
    "                                                                   if len(l) > 0]) > min_line_len])\n",
    "    sens = filter(lambda sen: len(sen) >= min_sen_len, fulltext.split(\".\"))\n",
    "    return \". \".join(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_texts_from_dir(texts_dir, filter_sen=False):\n",
    "    txt_files = os.listdir(texts_dir)\n",
    "    txt_files = [os.path.join(texts_dir, txt) for txt in txt_files]\n",
    "    texts = dict()\n",
    "    for txt_f in list(filter(lambda path: path.endswith(\".txt\"), txt_files)):\n",
    "        try:\n",
    "            if filter_sen:\n",
    "                # custom filtering based on sentences length:\n",
    "                text = filter_sentences_from_text(open(txt_f, \"r\").readlines())\n",
    "            else:\n",
    "                # no filtering:\n",
    "                text = open(txt_f, \"r\").read()\n",
    "            text = open(txt_f, \"r\").read()\n",
    "            texts[os.path.basename(txt_f)] = preprocess_string(text)\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Utf-8 decode error on %s\" % txt_f)\n",
    "            continue\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_no_preproc(texts_dir):\n",
    "    txt_files = os.listdir(texts_dir)\n",
    "    txt_files = [os.path.join(texts_dir, txt) for txt in txt_files]\n",
    "    for txt_f in list(filter(lambda path: path.endswith(\".txt\"), txt_files)):\n",
    "        yield open(txt_f, \"r\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: using filter for short sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffWarning: The NCBI web site requires JavaScript to function. more... \\n',\n",
       " '    • NCBI\\n',\n",
       " '    • Skip to main content\\n',\n",
       " '    • Skip to navigation\\n',\n",
       " '    • Resources\\n',\n",
       " '    • How To\\n',\n",
       " '    • About NCBI Accesskeys\\n',\n",
       " '\\n',\n",
       " 'PMC\\n',\n",
       " 'US National Library of Medicine \\n',\n",
       " 'National Institutes of Health \\n',\n",
       " 'Search database\\n',\n",
       " 'Search term\\n',\n",
       " '\\n',\n",
       " 'Search\\n',\n",
       " '    • Advanced \\n',\n",
       " '    • Journal list \\n',\n",
       " '    • Help \\n',\n",
       " '    • Journal List\\n',\n",
       " '    • HHS Author Manuscripts\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#: comparison of a text before and after filtering:\n",
    "texts = read_texts_no_preproc(html_content_dir)\n",
    "text = list(texts)[42]\n",
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*Address correspondence to: Harriet de Wit, Department of Psychiatry and Behavioral Neuroscience, MC 3077, University of Chicago, 5841 S. , Chicago, IL, 60637 USA, ude. ogacihcu@wedh',\n",
       " ' ±3,4-methylenedioxymethamphetamine (MDMA) is widely believed to increase sociability.  The drug alters speech production and fluency, and may influence speech content.  Here, we investigated the effect of MDMA on speech content, which may reveal how this drug affects social interactions. ',\n",
       " ' 35 healthy volunteers with prior MDMA experience completed this two-session, within-subjects, double-blind study during which they received 1. 5 mg/kg oral MDMA and placebo.  Participants completed a 5-min standardized talking task during which they discussed a close personal relationship (e. , a friend or family member) with a research assistant.  The conversations were analyzed for selected content categories (e. , words pertaining to affect, social interaction, and cognition), using both a standard dictionary method (Pennebaker’s Linguistic Inquiry and Word Count: LIWC) and a machine learning method using random forest classifiers. ',\n",
       " ' Both analytic methods revealed that MDMA altered speech content relative to placebo.  Using LIWC scores, the drug increased use of social and sexual words, consistent with reports that MDMA increases willingness to disclose.  Using the machine learning algorithm, we found that MDMA increased use of social words and words relating to both positive and negative emotions. ',\n",
       " ' These findings are consistent with reports that MDMA acutely alters speech content, specifically increasing emotional and social content during a brief semistructured dyadic interaction.  Studying effects of psychoactive drugs on speech content may offer new insights into drug effects on mental states, and on emotional and psychosocial interaction. ',\n",
       " ' The drug ±3,4-methylenedioxymethamphetamine (MDMA, “ecstasy”, “molly”) is known among drug users for its positive social-emotional effects, such as increased feelings of empathy, interpersonal closeness, and sociability (Bravo 2001; Kelly et al.  2006; Rodgers et al.  2006; Sumnall et al.  In addition, before it was classified in the US as a controlled substance, MDMA was used as an adjunct to psychotherapy by therapists because it appeared to decrease defensiveness and enhance feelings of emotional closeness (Greer and Tolbert 1986; Wolfson 1986).  More recently, clinical trials have suggested that the drug may be an effective therapeutic adjunct in patients with post-traumatic stress disorder (Mithoefer et al.  2013; Oehen et al.  Thus, both anecdotal and experimental data indicate that MDMA produces positive acute social-emotional effects that may underlie the drug’s putative therapeutic potential.  Analysis of speech content may shed light on the processes by which this drug produces its apparently unique prosocial effects. ',\n",
       " ' Several controlled laboratory studies support the idea that MDMA produces prosocial effects.  Single doses of MDMA increase feelings of friendliness and euphoria, and feeling close to others (Bedi et al.  2010; Bedi et al.  2009; Harris et al.  2002; Hysek and Liechti 2012; Kirkpatrick et al.  2012; Kirkpatrick et al.  2014; Tancer and Johanson 2003).  On measures of cognitive-emotional function, it increases recognition of positive emotions such as friendliness in others (Hysek et al.  2012) and decreases recognition of negative expressions such as fear (Bedi et al.  2010; Hysek et al.  2012), suggesting that it increases social behavior in part by enhancing sensitivity to positive emotions and reducing sensitivity to negative emotions in others.  MDMA also reduces the negative affect produced by simulated social exclusion (Frye et al.  Most of the research to date has utilized standardized, computerized tasks that are typically administered to individual participants, tested in nonsocial contexts.  To understand the effects of a drug on social processes, it may be more appropriate to use procedures involving interpersonal interactions. ',\n",
       " ' Speech is a key element of human social interaction.  Several drugs, including MDMA, alter speech production and fluency.  Amphetamine and alcohol increase speech production (Higgins and Stitzer 1988; Wardle et al.  2012), whereas MDMA (100 mg) disrupted speech fluency (Marrone et al.  Drugs can also alter the content of speech.  2014) recently reported that MDMA increased the social content of speech using a machine learning analysis: the drug increased the use of words that were semantically close to “friend”, “support”, “rapport”, and “empathy”.  The current study used similar approaches to further investigate the effects of MDMA on speech content, using a separate sample of participants and two different methods of speech content analysis. ',\n",
       " ' Healthy experienced drug users received single doses of MDMA (1. 5 mg/kg oral) and placebo.  They performed a standardized 5-min dyadic speaking task with a research assistant in which they spoke about their relationship with another person.  From the transcriptions we examined speech production and content.  We hypothesized that MDMA would increase 1) the amount the talking (i. , total number of words) and 2) proportion of emotional and social words used and that 3) machine learning methods would be able to distinguish MDMA from placebo based on word usage. ',\n",
       " ' Healthy men and women (N=35; 12 female, 23 male) with light-to-moderate past “ecstasy” experience (i. , 4–40 times in their lifetime) were recruited via newspaper, community bulletin board, and online advertisements.  Potential participants completed an initial telephone and an in-person psychiatric evaluation and medical examination, including an electrocardiogram and physical examination.  Inclusion criteria were: age 18 – 35, at least high school education, fluency in English, and BMI 18 – 30.  All participants were Caucasian because this was part of a larger genetic study.  Exclusion criteria were: smoking more than 10 cigarettes per day, night shift work, any significant medical or psychiatric condition (e. , cardiovascular, neurological, or major psychiatric illness including all Axis I disorders). ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same one using filtering:\n",
    "text_f = filter_sentences_from_text(text)\n",
    "text_f.split(\"\\n\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['978-3-319-69835-9_29.txt',\n",
       " 'IzpisGradiva.txt',\n",
       " '1709.txt',\n",
       " 'S0957417418302938.txt',\n",
       " '1504.txt',\n",
       " '7840632.txt',\n",
       " '8417270.txt',\n",
       " '69963.txt',\n",
       " '978-3-319-91947-8_4.txt',\n",
       " '8025903.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmls_preproc = get_texts_from_dir(html_content_dir, filter_sen=True)\n",
    "list(htmls_preproc.keys())[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: without text filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-s2.0-S0363811116300212-main.txt',\n",
       " '1-s2.0-S002002551830094X-main.txt',\n",
       " 'Bhuiyan and Al Hasan - 2016 - Waiting to be sold Prediction of time-dependent h.txt',\n",
       " 'Do not blame it on the algorithm an empirical assessment of multiple recommender systems and their impact on content diversity.txt',\n",
       " 'Jebbara and Cimiano - 2017 - Aspect-Based Relational Sentiment Analysis Using a.txt',\n",
       " 'ÁLVARO - 2016 - Analysis of the Formality of Text and its Impact o.txt',\n",
       " 'Khandpur et al. - 2017 - Crowdsourcing cybersecurity Cyber attack detectio.txt',\n",
       " 'Chardin et al. - 2013 - Query rewriting for rule mining in databases.txt',\n",
       " \"Rahman and Finin - 2017 - Deep Understanding of a Document's Structure.txt\",\n",
       " 'Yang and Hsu - 2016 - Hdpauthor A new hybrid author-topic model using l.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs_preproc = get_texts_from_dir(pdf_content_dir, filter_sen=False)\n",
    "list(pdfs_preproc.keys())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with htmls\n",
    "merged_texts_preproc = {**htmls_preproc, **pdfs_preproc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [t for t in merged_texts_preproc.values() if len(t) > 0]\n",
    "texts_links = merged_texts_preproc.keys()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf integration\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "1. use idf weights instead of BoW frequencies\n",
    "2. filter out given percentile of least important words from Docs' representation\n",
    "3. combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. use idf weights for top-given percentile of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_for_doc(doc_id):\n",
    "    return tfidf[corpus[doc_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_idf_for_doc(doc_id, percentile):\n",
    "    doc_terms_ordered = sorted(terms_for_doc(doc_id), key=lambda term: term[1],  reverse=True)\n",
    "    return [term[0] for term in doc_terms_ordered[:int(len(doc_terms_ordered)*percentile)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_terms_in_tuples(doc_corpus):\n",
    "    return [(dictionary.get(tup[0]), tup[1]) for tup in doc_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_idf_for_doc(doc_id, percentile):\n",
    "    doc_terms_ordered = sorted(terms_for_doc(doc_id), key=lambda term: term[1], reverse=True)\n",
    "    return [(doc_terms_ordered[i][0], doc_terms_ordered[i][1]) \n",
    "            for i in range(int(len(doc_terms_ordered)*percentile))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_tfidf_for_doc(doc_idx):\n",
    "    return [(tfidf_term_tuple[0], tfidf_term_tuple[1]) \n",
    "            for tfidf_term_tuple in tfidf_corpus[doc_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus1 = [top_terms_idf_for_doc(doc_i, 0.18) for doc_i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(96, 0.5256214468617546),\n",
       " (61, 0.47458063764105685),\n",
       " (62, 0.4447236782404861),\n",
       " (40, 0.42353982842035914),\n",
       " (101, 0.11551086229646161),\n",
       " (133, 0.09556753579304629),\n",
       " (80, 0.09465985755388896),\n",
       " (68, 0.08628738866201034),\n",
       " (39, 0.08038232292672068),\n",
       " (83, 0.06928599370798487)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus1[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. use term frequencies for top-given percentile of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_freqs_for_doc(doc_id, percentile):\n",
    "    doc_terms_idf = dict(terms_for_doc(doc_id))\n",
    "    doc_terms_count = corpus[doc_id]\n",
    "    doc_terms_count_ordered = sorted(doc_terms_count, \n",
    "                                     key=lambda term_count: doc_terms_idf[term_count[0]], reverse=True)\n",
    "    return [(doc_terms_count_ordered[i][0], doc_terms_count_ordered[i][1])\n",
    "            for i in range(int(len(doc_terms_count_ordered)*percentile))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(96, 11),\n",
       " (61, 11),\n",
       " (62, 11),\n",
       " (40, 11),\n",
       " (101, 3),\n",
       " (133, 2),\n",
       " (80, 7),\n",
       " (68, 2),\n",
       " (39, 3),\n",
       " (83, 7)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus2 = [top_terms_freqs_for_doc(doc_i, 0.1) for doc_i in range(len(texts))]\n",
    "tfidf_corpus2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA computation\n",
    "\n",
    "Train LDA on a given type of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_from_texts(given_corpus, num_topics, passes):\n",
    "    lda = LdaModel(given_corpus, num_topics=num_topics, alpha='auto', eval_every=5, passes=passes)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distro_for_text(text):\n",
    "    return lda.get_document_topics(dictionary.doc2bow(text), minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_for_topic(topic_id, top_terms=10):\n",
    "    topic_top_terms = lda.get_topic_terms(topic_id, topn=top_terms)\n",
    "    return [dictionary.get(term[0]) for term in topic_top_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_num_topics = 30\n",
    "num_passes = 10\n",
    "\n",
    "# TODO: choose between corpus, tfidf_corpus1, tfidf_corpus2:\n",
    "lda = lda_from_texts(tfidf_corpus2, num_topics=lda_num_topics, passes=num_passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Documents representation\n",
    "\n",
    "Each doc is represented as a probability distribution towards LDA topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distros = np.array([topic_distro_for_text(text) for text in texts[:-1]])\n",
    "topic_distros = topic_distros[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 30)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distros.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization projections: approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get a projection of docs so that the docs are close to their major topic\n",
    "\n",
    "Doc distance to a topic must be proportional to it's probability of belonging to it\n",
    "\n",
    "Does not consider relative distance of the documents, which seems to me as having no interpretation value - if it does, it is again equal to the second approach\n",
    "\n",
    "... thus is not implemented\n",
    "\n",
    "Yet, other projection methods surely deserve a consideration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get a projection of topic according to their relative similarity\n",
    "\n",
    "Relative similarity is a correlation of documents' belonging to it\n",
    "\n",
    "Topics centers are documents with one-hot distribution of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_topic_docs_distros = np.identity(lda_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distros = np.append(topic_distros, base_topic_docs_distros) \\\n",
    "                  .reshape((len(topic_distros)+lda_num_topics, lda_num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1288, 30)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.887894  , 0.2849529 , ..., 1.08339084, 0.92996449,\n",
       "        1.08346365],\n",
       "       [0.887894  , 0.        , 0.76575193, ..., 1.0739843 , 0.54060026,\n",
       "        1.02831219],\n",
       "       [0.2849529 , 0.76575193, 0.        , ..., 1.07090122, 0.87243759,\n",
       "        1.0711996 ],\n",
       "       ...,\n",
       "       [1.08339084, 1.0739843 , 1.07090122, ..., 0.        , 1.03448276,\n",
       "        1.03448276],\n",
       "       [0.92996449, 0.54060026, 0.87243759, ..., 1.03448276, 0.        ,\n",
       "        1.03448276],\n",
       "       [1.08346365, 1.02831219, 1.0711996 , ..., 1.03448276, 1.03448276,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance matrix by selected metric\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "dists = squareform(pdist(topic_distros, metric=\"correlation\"))\n",
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1288, 1288)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDS(dissimilarity='precomputed', eps=0.001, max_iter=300, metric=True,\n",
       "  n_components=2, n_init=4, n_jobs=1, random_state=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projection to 2D using MDS\n",
    "\n",
    "from sklearn import manifold\n",
    "\n",
    "adist = dists\n",
    "\n",
    "amax = np.amax(adist)\n",
    "adist /= amax\n",
    "\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\")\n",
    "results = mds.fit(adist)\n",
    "\n",
    "coords = results.embedding_\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib visualization:\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "# plt.subplots_adjust(bottom = 0.1)\n",
    "# plt.scatter(\n",
    "#     coords[:, 0], coords[:, 1], marker = 'o'\n",
    "#     )\n",
    "# for label, x, y in zip([\"\"]*len(coords), coords[:, 0], coords[:, 1]):\n",
    "#     plt.annotate(\n",
    "#         label,\n",
    "#         xy = (x, y), xytext = (-20, 20),\n",
    "#         textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "#         bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
    "#         arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "    \n",
    "# # figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "# plt.figsize = (100, 100)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selective labeling of topics by only the unique words among topics\n",
    "\n",
    "word_occurrence_bound = int(lda.num_topics / 10)\n",
    "top_terms_per_topic = 20\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "all_topics_w = [terms_for_topic(i, top_terms=top_terms_per_topic) for i in range(lda_num_topics)]\n",
    "all_words = reduce(lambda x, y: set(x) | set(y), all_topics_w)\n",
    "intersect_words = list(filter(lambda w: sum([w in t_words for t_words in all_topics_w]) > word_occurrence_bound, \n",
    "                              all_words))\n",
    "unique_topics_w = [[w for w in t_words if w not in intersect_words] for t_words in all_topics_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='stmichal', api_key='OXox9Rf8jzEHqUsNPqwn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~stmichal/103.embed\" height=\"1000px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "docs_len = len(texts)\n",
    "\n",
    "# Documents trace\n",
    "trace_docs = go.Scatter(\n",
    "    x = coords[:docs_len, 0],\n",
    "    y = coords[:docs_len, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(color = 'rgba(0, 0, 255, .5)', size = 5),\n",
    "    text = list(texts_links)\n",
    ")\n",
    "# Bases (topics documents) trace\n",
    "trace_bases = go.Scatter(\n",
    "    x = coords[docs_len:, 0],\n",
    "    y = coords[docs_len:, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(color = 'rgba(255, 0, 122, .2)', size = 60),\n",
    "    text = [\"T %s: %s\" % (i, unique_topics_w[i]) for i in range(lda_num_topics)]\n",
    ")\n",
    "\n",
    "data = [trace_docs, trace_bases]\n",
    "\n",
    "# label = 'MDS over LDA %s topics. tfidf for top 0.5 terms as frequencies - TODO: check' % lda.num_topics\n",
    "\n",
    "label = 'TODO: fill appropriately: MDS over LDA X topics. tfidf:counts for top X terms as frequencies.'\n",
    "\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "layout = dict(title=label,  \n",
    "              font=dict(size=12),\n",
    "              showlegend=True,\n",
    "              width=1000,\n",
    "              height=1000,\n",
    "              margin=dict(l=40, r=40, b=85, t=100),\n",
    "              hovermode='closest',\n",
    "              plot_bgcolor='rgb(256,256,256)'          \n",
    "              )\n",
    "py.iplot(dict(data=data, layout=layout), filename=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
